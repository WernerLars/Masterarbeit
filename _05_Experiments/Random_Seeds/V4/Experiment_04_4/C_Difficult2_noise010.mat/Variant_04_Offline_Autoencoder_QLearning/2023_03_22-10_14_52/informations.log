Experiment_path: Random_Seeds//V4/Experiment_04_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise010.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Random_Seeds//V4/Experiment_04_4/C_Difficult2_noise010.mat/Variant_04_Offline_Autoencoder_QLearning/2023_03_22-10_14_52
Punishment_Coefficient: 0.37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000154018670B8>
Sampling rate: 24000.0
Raw: [ 0.15602285  0.13816666  0.12280393 ... -0.08081559 -0.08529616
 -0.09321123]
Times: [    182     667     748 ... 1438018 1438700 1439563]
Cluster: [1 3 3 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3462
First aligned Spike Frame: [ 0.0569593   0.06304523  0.0540705   0.04226901  0.04435466  0.07367561
  0.11842591  0.15581396  0.18051202  0.20464622  0.25110595  0.34905547
  0.52973433  0.78604807  1.00019855  1.02993402  0.87276972  0.64136808
  0.42542707  0.24213728  0.08732396 -0.0251061  -0.08440505 -0.1076534
 -0.12386236 -0.14599821 -0.16968468 -0.19109174 -0.20831529 -0.21879359
 -0.21563414 -0.19606358 -0.16928275 -0.14859233 -0.13954347 -0.13618571
 -0.12902379 -0.12127763 -0.12365015 -0.13615822 -0.14611472 -0.13936073
 -0.11885552 -0.10582878 -0.11163038 -0.12511067 -0.12700369]
Cluster 0, Occurrences: 1187
Cluster 1, Occurrences: 1136
Cluster 2, Occurrences: 1139
Train Index: 3116
x_train: 3116
y_train: 3116
x_test: 346
y_test: 346
<torch.utils.data.dataloader.DataLoader object at 0x0000015402298550>
<torch.utils.data.dataloader.DataLoader object at 0x0000015401508DD8>
Epoch 1
-------------------------------
loss: 0.144623  [    0/ 3116]
loss: 0.090627  [  100/ 3116]
loss: 0.065603  [  200/ 3116]
loss: 0.065008  [  300/ 3116]
loss: 0.040659  [  400/ 3116]
loss: 0.013992  [  500/ 3116]
loss: 0.014973  [  600/ 3116]
loss: 0.021820  [  700/ 3116]
loss: 0.010911  [  800/ 3116]
loss: 0.014401  [  900/ 3116]
loss: 0.013490  [ 1000/ 3116]
loss: 0.059751  [ 1100/ 3116]
loss: 0.012762  [ 1200/ 3116]
loss: 0.005722  [ 1300/ 3116]
loss: 0.008159  [ 1400/ 3116]
loss: 0.007484  [ 1500/ 3116]
loss: 0.007897  [ 1600/ 3116]
loss: 0.005471  [ 1700/ 3116]
loss: 0.015021  [ 1800/ 3116]
loss: 0.007098  [ 1900/ 3116]
loss: 0.010867  [ 2000/ 3116]
loss: 0.057555  [ 2100/ 3116]
loss: 0.002427  [ 2200/ 3116]
loss: 0.007299  [ 2300/ 3116]
loss: 0.012226  [ 2400/ 3116]
loss: 0.005052  [ 2500/ 3116]
loss: 0.013859  [ 2600/ 3116]
loss: 0.007043  [ 2700/ 3116]
loss: 0.004407  [ 2800/ 3116]
loss: 0.014121  [ 2900/ 3116]
loss: 0.115434  [ 3000/ 3116]
loss: 0.007179  [ 3100/ 3116]
Epoch 2
-------------------------------
loss: 0.002015  [    0/ 3116]
loss: 0.012624  [  100/ 3116]
loss: 0.011233  [  200/ 3116]
loss: 0.008465  [  300/ 3116]
loss: 0.008205  [  400/ 3116]
loss: 0.006109  [  500/ 3116]
loss: 0.002161  [  600/ 3116]
loss: 0.016826  [  700/ 3116]
loss: 0.008891  [  800/ 3116]
loss: 0.017563  [  900/ 3116]
loss: 0.009858  [ 1000/ 3116]
loss: 0.038551  [ 1100/ 3116]
loss: 0.014748  [ 1200/ 3116]
loss: 0.003459  [ 1300/ 3116]
loss: 0.004798  [ 1400/ 3116]
loss: 0.007384  [ 1500/ 3116]
loss: 0.008358  [ 1600/ 3116]
loss: 0.004868  [ 1700/ 3116]
loss: 0.018563  [ 1800/ 3116]
loss: 0.005348  [ 1900/ 3116]
loss: 0.006511  [ 2000/ 3116]
loss: 0.051662  [ 2100/ 3116]
loss: 0.002000  [ 2200/ 3116]
loss: 0.007903  [ 2300/ 3116]
loss: 0.012457  [ 2400/ 3116]
loss: 0.004506  [ 2500/ 3116]
loss: 0.013286  [ 2600/ 3116]
loss: 0.006901  [ 2700/ 3116]
loss: 0.003746  [ 2800/ 3116]
loss: 0.012803  [ 2900/ 3116]
loss: 0.116705  [ 3000/ 3116]
loss: 0.007610  [ 3100/ 3116]
Epoch 3
-------------------------------
loss: 0.002030  [    0/ 3116]
loss: 0.012583  [  100/ 3116]
loss: 0.010050  [  200/ 3116]
loss: 0.007719  [  300/ 3116]
loss: 0.008776  [  400/ 3116]
loss: 0.004282  [  500/ 3116]
loss: 0.001105  [  600/ 3116]
loss: 0.018094  [  700/ 3116]
loss: 0.008224  [  800/ 3116]
loss: 0.016611  [  900/ 3116]
loss: 0.009373  [ 1000/ 3116]
loss: 0.034825  [ 1100/ 3116]
loss: 0.015163  [ 1200/ 3116]
loss: 0.003516  [ 1300/ 3116]
loss: 0.004157  [ 1400/ 3116]
loss: 0.007350  [ 1500/ 3116]
loss: 0.008014  [ 1600/ 3116]
loss: 0.004721  [ 1700/ 3116]
loss: 0.017561  [ 1800/ 3116]
loss: 0.004949  [ 1900/ 3116]
loss: 0.005540  [ 2000/ 3116]
loss: 0.051457  [ 2100/ 3116]
loss: 0.002027  [ 2200/ 3116]
loss: 0.008211  [ 2300/ 3116]
loss: 0.012043  [ 2400/ 3116]
loss: 0.004141  [ 2500/ 3116]
loss: 0.013391  [ 2600/ 3116]
loss: 0.006796  [ 2700/ 3116]
loss: 0.003412  [ 2800/ 3116]
loss: 0.012521  [ 2900/ 3116]
loss: 0.116663  [ 3000/ 3116]
loss: 0.007764  [ 3100/ 3116]
Epoch 4
-------------------------------
loss: 0.002035  [    0/ 3116]
loss: 0.012619  [  100/ 3116]
loss: 0.009667  [  200/ 3116]
loss: 0.007950  [  300/ 3116]
loss: 0.008869  [  400/ 3116]
loss: 0.003410  [  500/ 3116]
loss: 0.000884  [  600/ 3116]
loss: 0.018363  [  700/ 3116]
loss: 0.007583  [  800/ 3116]
loss: 0.015996  [  900/ 3116]
loss: 0.009232  [ 1000/ 3116]
loss: 0.033705  [ 1100/ 3116]
loss: 0.015014  [ 1200/ 3116]
loss: 0.003478  [ 1300/ 3116]
loss: 0.003895  [ 1400/ 3116]
loss: 0.007485  [ 1500/ 3116]
loss: 0.007835  [ 1600/ 3116]
loss: 0.004734  [ 1700/ 3116]
loss: 0.016854  [ 1800/ 3116]
loss: 0.004806  [ 1900/ 3116]
loss: 0.005106  [ 2000/ 3116]
loss: 0.051957  [ 2100/ 3116]
loss: 0.002007  [ 2200/ 3116]
loss: 0.008235  [ 2300/ 3116]
loss: 0.011972  [ 2400/ 3116]
loss: 0.003888  [ 2500/ 3116]
loss: 0.013276  [ 2600/ 3116]
loss: 0.006687  [ 2700/ 3116]
loss: 0.003107  [ 2800/ 3116]
loss: 0.012325  [ 2900/ 3116]
loss: 0.116462  [ 3000/ 3116]
loss: 0.007890  [ 3100/ 3116]
Epoch 5
-------------------------------
loss: 0.002040  [    0/ 3116]
loss: 0.012681  [  100/ 3116]
loss: 0.009280  [  200/ 3116]
loss: 0.008463  [  300/ 3116]
loss: 0.008848  [  400/ 3116]
loss: 0.002957  [  500/ 3116]
loss: 0.000797  [  600/ 3116]
loss: 0.018372  [  700/ 3116]
loss: 0.007168  [  800/ 3116]
loss: 0.015515  [  900/ 3116]
loss: 0.009072  [ 1000/ 3116]
loss: 0.033102  [ 1100/ 3116]
loss: 0.014376  [ 1200/ 3116]
loss: 0.003428  [ 1300/ 3116]
loss: 0.003736  [ 1400/ 3116]
loss: 0.007653  [ 1500/ 3116]
loss: 0.007665  [ 1600/ 3116]
loss: 0.004758  [ 1700/ 3116]
loss: 0.016530  [ 1800/ 3116]
loss: 0.004894  [ 1900/ 3116]
loss: 0.004864  [ 2000/ 3116]
loss: 0.052361  [ 2100/ 3116]
loss: 0.001988  [ 2200/ 3116]
loss: 0.008188  [ 2300/ 3116]
loss: 0.011886  [ 2400/ 3116]
loss: 0.003780  [ 2500/ 3116]
loss: 0.013095  [ 2600/ 3116]
loss: 0.006585  [ 2700/ 3116]
loss: 0.002843  [ 2800/ 3116]
loss: 0.012113  [ 2900/ 3116]
loss: 0.116260  [ 3000/ 3116]
loss: 0.007921  [ 3100/ 3116]
Epoch 6
-------------------------------
loss: 0.002038  [    0/ 3116]
loss: 0.012731  [  100/ 3116]
loss: 0.008957  [  200/ 3116]
loss: 0.008966  [  300/ 3116]
loss: 0.008936  [  400/ 3116]
loss: 0.002686  [  500/ 3116]
loss: 0.000774  [  600/ 3116]
loss: 0.018241  [  700/ 3116]
loss: 0.006782  [  800/ 3116]
loss: 0.014980  [  900/ 3116]
loss: 0.008992  [ 1000/ 3116]
loss: 0.032734  [ 1100/ 3116]
loss: 0.016568  [ 1200/ 3116]
loss: 0.003383  [ 1300/ 3116]
loss: 0.003673  [ 1400/ 3116]
loss: 0.007818  [ 1500/ 3116]
loss: 0.007486  [ 1600/ 3116]
loss: 0.004818  [ 1700/ 3116]
loss: 0.016281  [ 1800/ 3116]
loss: 0.004784  [ 1900/ 3116]
loss: 0.004895  [ 2000/ 3116]
loss: 0.052880  [ 2100/ 3116]
loss: 0.002020  [ 2200/ 3116]
loss: 0.008146  [ 2300/ 3116]
loss: 0.011978  [ 2400/ 3116]
loss: 0.003651  [ 2500/ 3116]
loss: 0.012755  [ 2600/ 3116]
loss: 0.006544  [ 2700/ 3116]
loss: 0.002640  [ 2800/ 3116]
loss: 0.011941  [ 2900/ 3116]
loss: 0.116196  [ 3000/ 3116]
loss: 0.007891  [ 3100/ 3116]
Epoch 7
-------------------------------
loss: 0.002037  [    0/ 3116]
loss: 0.012740  [  100/ 3116]
loss: 0.008581  [  200/ 3116]
loss: 0.009442  [  300/ 3116]
loss: 0.008727  [  400/ 3116]
loss: 0.002622  [  500/ 3116]
loss: 0.000778  [  600/ 3116]
loss: 0.018110  [  700/ 3116]
loss: 0.006372  [  800/ 3116]
loss: 0.014568  [  900/ 3116]
loss: 0.008887  [ 1000/ 3116]
loss: 0.033137  [ 1100/ 3116]
loss: 0.013637  [ 1200/ 3116]
loss: 0.003279  [ 1300/ 3116]
loss: 0.003709  [ 1400/ 3116]
loss: 0.008118  [ 1500/ 3116]
loss: 0.007310  [ 1600/ 3116]
loss: 0.004840  [ 1700/ 3116]
loss: 0.016048  [ 1800/ 3116]
loss: 0.005112  [ 1900/ 3116]
loss: 0.004581  [ 2000/ 3116]
loss: 0.053229  [ 2100/ 3116]
loss: 0.001999  [ 2200/ 3116]
loss: 0.008045  [ 2300/ 3116]
loss: 0.011845  [ 2400/ 3116]
loss: 0.003724  [ 2500/ 3116]
loss: 0.012440  [ 2600/ 3116]
loss: 0.006435  [ 2700/ 3116]
loss: 0.002488  [ 2800/ 3116]
loss: 0.011358  [ 2900/ 3116]
loss: 0.116022  [ 3000/ 3116]
loss: 0.007976  [ 3100/ 3116]
Epoch 8
-------------------------------
loss: 0.002042  [    0/ 3116]
loss: 0.012757  [  100/ 3116]
loss: 0.008262  [  200/ 3116]
loss: 0.009520  [  300/ 3116]
loss: 0.008706  [  400/ 3116]
loss: 0.002654  [  500/ 3116]
loss: 0.000789  [  600/ 3116]
loss: 0.018046  [  700/ 3116]
loss: 0.006015  [  800/ 3116]
loss: 0.014325  [  900/ 3116]
loss: 0.008872  [ 1000/ 3116]
loss: 0.033158  [ 1100/ 3116]
loss: 0.013015  [ 1200/ 3116]
loss: 0.003273  [ 1300/ 3116]
loss: 0.003529  [ 1400/ 3116]
loss: 0.008354  [ 1500/ 3116]
loss: 0.007120  [ 1600/ 3116]
loss: 0.004891  [ 1700/ 3116]
loss: 0.015578  [ 1800/ 3116]
loss: 0.005149  [ 1900/ 3116]
loss: 0.004496  [ 2000/ 3116]
loss: 0.053686  [ 2100/ 3116]
loss: 0.002044  [ 2200/ 3116]
loss: 0.007889  [ 2300/ 3116]
loss: 0.011729  [ 2400/ 3116]
loss: 0.003739  [ 2500/ 3116]
loss: 0.011997  [ 2600/ 3116]
loss: 0.006442  [ 2700/ 3116]
loss: 0.002386  [ 2800/ 3116]
loss: 0.011047  [ 2900/ 3116]
loss: 0.115883  [ 3000/ 3116]
loss: 0.007892  [ 3100/ 3116]
Number of Clusters: 3
Q_Learning:     1/  346]
Q_Learning:     2/  346]
Q_Learning:     3/  346]
Q_Learning:     4/  346]
Q_Learning:     5/  346]
Q_Learning:     6/  346]
Q_Learning:     7/  346]
Q_Learning:     8/  346]
Q_Learning:     9/  346]
Q_Learning:    10/  346]
Q_Learning:    11/  346]
Q_Learning:    12/  346]
Q_Learning:    13/  346]
Q_Learning:    14/  346]
Q_Learning:    15/  346]
Q_Learning:    16/  346]
Q_Learning:    17/  346]
Q_Learning:    18/  346]
Q_Learning:    19/  346]
Q_Learning:    20/  346]
Q_Learning:    21/  346]
Q_Learning:    22/  346]
Q_Learning:    23/  346]
Q_Learning:    24/  346]
Q_Learning:    25/  346]
Q_Learning:    26/  346]
Q_Learning:    27/  346]
Q_Learning:    28/  346]
Q_Learning:    29/  346]
Q_Learning:    30/  346]
Q_Learning:    31/  346]
Q_Learning:    32/  346]
Q_Learning:    33/  346]
Q_Learning:    34/  346]
Q_Learning:    35/  346]
Q_Learning:    36/  346]
Q_Learning:    37/  346]
Q_Learning:    38/  346]
Q_Learning:    39/  346]
Q_Learning:    40/  346]
Q_Learning:    41/  346]
Q_Learning:    42/  346]
Q_Learning:    43/  346]
Q_Learning:    44/  346]
Q_Learning:    45/  346]
Q_Learning:    46/  346]
Q_Learning:    47/  346]
Q_Learning:    48/  346]
Q_Learning:    49/  346]
Q_Learning:    50/  346]
Q_Learning:    51/  346]
Q_Learning:    52/  346]
Q_Learning:    53/  346]
Q_Learning:    54/  346]
Q_Learning:    55/  346]
Q_Learning:    56/  346]
Q_Learning:    57/  346]
Q_Learning:    58/  346]
Q_Learning:    59/  346]
Q_Learning:    60/  346]
Q_Learning:    61/  346]
Q_Learning:    62/  346]
Q_Learning:    63/  346]
Q_Learning:    64/  346]
Q_Learning:    65/  346]
Q_Learning:    66/  346]
Q_Learning:    67/  346]
Q_Learning:    68/  346]
Q_Learning:    69/  346]
Q_Learning:    70/  346]
Q_Learning:    71/  346]
Q_Learning:    72/  346]
Q_Learning:    73/  346]
Q_Learning:    74/  346]
Q_Learning:    75/  346]
Q_Learning:    76/  346]
Q_Learning:    77/  346]
Q_Learning:    78/  346]
Q_Learning:    79/  346]
Q_Learning:    80/  346]
Q_Learning:    81/  346]
Q_Learning:    82/  346]
Q_Learning:    83/  346]
Q_Learning:    84/  346]
Q_Learning:    85/  346]
Q_Learning:    86/  346]
Q_Learning:    87/  346]
Q_Learning:    88/  346]
Q_Learning:    89/  346]
Q_Learning:    90/  346]
Q_Learning:    91/  346]
Q_Learning:    92/  346]
Q_Learning:    93/  346]
Q_Learning:    94/  346]
Q_Learning:    95/  346]
Q_Learning:    96/  346]
Q_Learning:    97/  346]
Q_Learning:    98/  346]
Q_Learning:    99/  346]
Q_Learning:   100/  346]
Q_Learning:   101/  346]
Q_Learning:   102/  346]
Q_Learning:   103/  346]
Q_Learning:   104/  346]
Q_Learning:   105/  346]
Q_Learning:   106/  346]
Q_Learning:   107/  346]
Q_Learning:   108/  346]
Q_Learning:   109/  346]
Q_Learning:   110/  346]
Q_Learning:   111/  346]
Q_Learning:   112/  346]
Q_Learning:   113/  346]
Q_Learning:   114/  346]
Q_Learning:   115/  346]
Q_Learning:   116/  346]
Q_Learning:   117/  346]
Q_Learning:   118/  346]
Q_Learning:   119/  346]
Q_Learning:   120/  346]
Q_Learning:   121/  346]
Q_Learning:   122/  346]
Q_Learning:   123/  346]
Q_Learning:   124/  346]
Q_Learning:   125/  346]
Q_Learning:   126/  346]
Q_Learning:   127/  346]
Q_Learning:   128/  346]
Q_Learning:   129/  346]
Q_Learning:   130/  346]
Q_Learning:   131/  346]
Q_Learning:   132/  346]
Q_Learning:   133/  346]
Q_Learning:   134/  346]
Q_Learning:   135/  346]
Q_Learning:   136/  346]
Q_Learning:   137/  346]
Q_Learning:   138/  346]
Q_Learning:   139/  346]
Q_Learning:   140/  346]
Q_Learning:   141/  346]
Q_Learning:   142/  346]
Q_Learning:   143/  346]
Q_Learning:   144/  346]
Q_Learning:   145/  346]
Q_Learning:   146/  346]
Q_Learning:   147/  346]
Q_Learning:   148/  346]
Q_Learning:   149/  346]
Q_Learning:   150/  346]
Q_Learning:   151/  346]
Q_Learning:   152/  346]
Q_Learning:   153/  346]
Q_Learning:   154/  346]
Q_Learning:   155/  346]
Q_Learning:   156/  346]
Q_Learning:   157/  346]
Q_Learning:   158/  346]
Q_Learning:   159/  346]
Q_Learning:   160/  346]
Q_Learning:   161/  346]
Q_Learning:   162/  346]
Q_Learning:   163/  346]
Q_Learning:   164/  346]
Q_Learning:   165/  346]
Q_Learning:   166/  346]
Q_Learning:   167/  346]
Q_Learning:   168/  346]
Q_Learning:   169/  346]
Q_Learning:   170/  346]
Q_Learning:   171/  346]
Q_Learning:   172/  346]
Q_Learning:   173/  346]
Q_Learning:   174/  346]
Q_Learning:   175/  346]
Q_Learning:   176/  346]
Q_Learning:   177/  346]
Q_Learning:   178/  346]
Q_Learning:   179/  346]
Q_Learning:   180/  346]
Q_Learning:   181/  346]
Q_Learning:   182/  346]
Q_Learning:   183/  346]
Q_Learning:   184/  346]
Q_Learning:   185/  346]
Q_Learning:   186/  346]
Q_Learning:   187/  346]
Q_Learning:   188/  346]
Q_Learning:   189/  346]
Q_Learning:   190/  346]
Q_Learning:   191/  346]
Q_Learning:   192/  346]
Q_Learning:   193/  346]
Q_Learning:   194/  346]
Q_Learning:   195/  346]
Q_Learning:   196/  346]
Q_Learning:   197/  346]
Q_Learning:   198/  346]
Q_Learning:   199/  346]
Q_Learning:   200/  346]
Q_Learning:   201/  346]
Q_Learning:   202/  346]
Q_Learning:   203/  346]
Q_Learning:   204/  346]
Q_Learning:   205/  346]
Q_Learning:   206/  346]
Q_Learning:   207/  346]
Q_Learning:   208/  346]
Q_Learning:   209/  346]
Q_Learning:   210/  346]
Q_Learning:   211/  346]
Q_Learning:   212/  346]
Q_Learning:   213/  346]
Q_Learning:   214/  346]
Q_Learning:   215/  346]
Q_Learning:   216/  346]
Q_Learning:   217/  346]
Q_Learning:   218/  346]
Q_Learning:   219/  346]
Q_Learning:   220/  346]
Q_Learning:   221/  346]
Q_Learning:   222/  346]
Q_Learning:   223/  346]
Q_Learning:   224/  346]
Q_Learning:   225/  346]
Q_Learning:   226/  346]
Q_Learning:   227/  346]
Q_Learning:   228/  346]
Q_Learning:   229/  346]
Q_Learning:   230/  346]
Q_Learning:   231/  346]
Q_Learning:   232/  346]
Q_Learning:   233/  346]
Q_Learning:   234/  346]
Q_Learning:   235/  346]
Q_Learning:   236/  346]
Q_Learning:   237/  346]
Q_Learning:   238/  346]
Q_Learning:   239/  346]
Q_Learning:   240/  346]
Q_Learning:   241/  346]
Q_Learning:   242/  346]
Q_Learning:   243/  346]
Q_Learning:   244/  346]
Q_Learning:   245/  346]
Q_Learning:   246/  346]
Q_Learning:   247/  346]
Q_Learning:   248/  346]
Q_Learning:   249/  346]
Q_Learning:   250/  346]
Q_Learning:   251/  346]
Q_Learning:   252/  346]
Q_Learning:   253/  346]
Q_Learning:   254/  346]
Q_Learning:   255/  346]
Q_Learning:   256/  346]
Q_Learning:   257/  346]
Q_Learning:   258/  346]
Q_Learning:   259/  346]
Q_Learning:   260/  346]
Q_Learning:   261/  346]
Q_Learning:   262/  346]
Q_Learning:   263/  346]
Q_Learning:   264/  346]
Q_Learning:   265/  346]
Q_Learning:   266/  346]
Q_Learning:   267/  346]
Q_Learning:   268/  346]
Q_Learning:   269/  346]
Q_Learning:   270/  346]
Q_Learning:   271/  346]
Q_Learning:   272/  346]
Q_Learning:   273/  346]
Q_Learning:   274/  346]
Q_Learning:   275/  346]
Q_Learning:   276/  346]
Q_Learning:   277/  346]
Q_Learning:   278/  346]
Q_Learning:   279/  346]
Q_Learning:   280/  346]
Q_Learning:   281/  346]
Q_Learning:   282/  346]
Q_Learning:   283/  346]
Q_Learning:   284/  346]
Q_Learning:   285/  346]
Q_Learning:   286/  346]
Q_Learning:   287/  346]
Q_Learning:   288/  346]
Q_Learning:   289/  346]
Q_Learning:   290/  346]
Q_Learning:   291/  346]
Q_Learning:   292/  346]
Q_Learning:   293/  346]
Q_Learning:   294/  346]
Q_Learning:   295/  346]
Q_Learning:   296/  346]
Q_Learning:   297/  346]
Q_Learning:   298/  346]
Q_Learning:   299/  346]
Q_Learning:   300/  346]
Q_Learning:   301/  346]
Q_Learning:   302/  346]
Q_Learning:   303/  346]
Q_Learning:   304/  346]
Q_Learning:   305/  346]
Q_Learning:   306/  346]
Q_Learning:   307/  346]
Q_Learning:   308/  346]
Q_Learning:   309/  346]
Q_Learning:   310/  346]
Q_Learning:   311/  346]
Q_Learning:   312/  346]
Q_Learning:   313/  346]
Q_Learning:   314/  346]
Q_Learning:   315/  346]
Q_Learning:   316/  346]
Q_Learning:   317/  346]
Q_Learning:   318/  346]
Q_Learning:   319/  346]
Q_Learning:   320/  346]
Q_Learning:   321/  346]
Q_Learning:   322/  346]
Q_Learning:   323/  346]
Q_Learning:   324/  346]
Q_Learning:   325/  346]
Q_Learning:   326/  346]
Q_Learning:   327/  346]
Q_Learning:   328/  346]
Q_Learning:   329/  346]
Q_Learning:   330/  346]
Q_Learning:   331/  346]
Q_Learning:   332/  346]
Q_Learning:   333/  346]
Q_Learning:   334/  346]
Q_Learning:   335/  346]
Q_Learning:   336/  346]
Q_Learning:   337/  346]
Q_Learning:   338/  346]
Q_Learning:   339/  346]
Q_Learning:   340/  346]
Q_Learning:   341/  346]
Q_Learning:   342/  346]
Q_Learning:   343/  346]
Q_Learning:   344/  346]
Q_Learning:   345/  346]
Q_Learning:   346/  346]
Number of Samples after Autoencoder testing: 346
First Spike after testing: [-0.9187273   0.20061836]
[2, 0, 1, 1, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 2, 0, 0, 2, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 1, 2, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 2, 1, 2, 0, 2, 0, 1, 1, 0, 0, 2, 2, 0, 1, 0, 1, 2, 0, 2, 1, 1, 0, 2, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 2, 1, 2, 2, 1, 1, 0, 0, 2, 2, 0, 2, 1, 2, 2, 0, 1, 1, 1, 2, 0, 2, 2, 2, 0, 2, 2, 1, 1, 0, 2, 0, 2, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 1, 0, 0, 0, 2, 1, 2, 2, 1, 0, 1, 0, 0, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 2, 2, 1, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 0, 1, 2, 1, 1, 2, 2, 1, 0, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 2, 2, 1, 0, 0, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 2]
[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 0, 1, 3, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 1, 3, 0, 0, 2, 1, 0, 2, 3, 2, 3, 0, 0, 0, 1, 2, 0, 0, 3, 3, 0, 1, 0, 1, 3, 0, 3, 1, 1, 0, 0, 0, 3, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 1, 3, 1, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 1, 1, 1, 1, 0, 3, 3, 0, 3, 1, 3, 3, 1, 1, 0, 0, 3, 3, 0, 3, 1, 3, 3, 0, 1, 1, 1, 3, 0, 3, 0, 3, 0, 3, 3, 1, 2, 0, 3, 0, 3, 3, 0, 4, 0, 3, 0, 0, 3, 0, 0, 3, 0, 3, 3, 3, 1, 0, 0, 0, 3, 1, 3, 3, 1, 0, 4, 0, 0, 1, 3, 1, 3, 3, 1, 0, 4, 1, 1, 3, 0, 1, 1, 2, 0, 3, 3, 4, 1, 3, 0, 1, 0, 3, 4, 3, 0, 3, 0, 2, 0, 4, 1, 3, 3, 4, 0, 4, 2, 3, 2, 3, 3, 3, 3, 3, 3, 0, 4, 0, 0, 3, 3, 4, 0, 0, 1, 3, 0, 3, 0, 3, 3, 4, 3, 1, 1, 1, 0, 0, 3, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 3, 4, 0, 3, 0, 3, 4, 0, 0, 0, 0, 0, 4, 0, 3, 2, 1, 0, 4, 0, 1, 3, 1, 4, 0, 0, 0, 2, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 4, 2, 0, 4, 3]
Centroids: [[-1.024698, -0.2681363], [0.2580921, 2.2664764], [-1.0133895, 0.20899978]]
Centroids: [[-1.0227643, -0.19550513], [0.29661632, 2.3093925], [-0.111972585, 1.8119628], [-1.0133884, 0.2272079], [0.49591595, 2.5783706]]
Contingency Matrix: 
[[125   0   0   1   0]
 [  0  81  20   0  18]
 [ 27   0   0  74   0]]
[[-1, -1, -1, -1, -1], [-1, 81, 20, 0, 18], [-1, 0, 0, 74, 0]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, 0, 74, 0]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {0: 0, 1: 1, 2: 3}
New Contingency Matrix: 
[[125   0   1   0   0]
 [  0  81   0  20  18]
 [ 27   0  74   0   0]]
New Clustered Label Sequence: [0, 1, 3, 2, 4]
Diagonal_Elements: [125, 81, 74], Sum: 280
All_Elements: [125, 0, 1, 0, 0, 0, 81, 0, 20, 18, 27, 0, 74, 0, 0], Sum: 346
Accuracy: 0.8092485549132948
Done!
