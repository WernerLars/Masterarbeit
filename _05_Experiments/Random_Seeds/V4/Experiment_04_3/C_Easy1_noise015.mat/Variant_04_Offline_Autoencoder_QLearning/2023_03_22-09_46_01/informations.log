Experiment_path: Random_Seeds//V4/Experiment_04_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Random_Seeds//V4/Experiment_04_3/C_Easy1_noise015.mat/Variant_04_Offline_Autoencoder_QLearning/2023_03_22-09_46_01
Punishment_Coefficient: 0.92
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E38520FAC8>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
Train Index: 3129
x_train: 3129
y_train: 3129
x_test: 348
y_test: 348
<torch.utils.data.dataloader.DataLoader object at 0x000001E385D1A438>
<torch.utils.data.dataloader.DataLoader object at 0x000001E385D1A080>
Epoch 1
-------------------------------
loss: 0.182637  [    0/ 3129]
loss: 0.306414  [  100/ 3129]
loss: 0.124004  [  200/ 3129]
loss: 0.060313  [  300/ 3129]
loss: 0.022586  [  400/ 3129]
loss: 0.021013  [  500/ 3129]
loss: 0.059786  [  600/ 3129]
loss: 0.044798  [  700/ 3129]
loss: 0.031172  [  800/ 3129]
loss: 0.040888  [  900/ 3129]
loss: 0.025639  [ 1000/ 3129]
loss: 0.008105  [ 1100/ 3129]
loss: 0.025508  [ 1200/ 3129]
loss: 0.022597  [ 1300/ 3129]
loss: 0.016302  [ 1400/ 3129]
loss: 0.027179  [ 1500/ 3129]
loss: 0.012268  [ 1600/ 3129]
loss: 0.011911  [ 1700/ 3129]
loss: 0.008900  [ 1800/ 3129]
loss: 0.016709  [ 1900/ 3129]
loss: 0.024572  [ 2000/ 3129]
loss: 0.024421  [ 2100/ 3129]
loss: 0.021993  [ 2200/ 3129]
loss: 0.011626  [ 2300/ 3129]
loss: 0.011388  [ 2400/ 3129]
loss: 0.096004  [ 2500/ 3129]
loss: 0.084554  [ 2600/ 3129]
loss: 0.009466  [ 2700/ 3129]
loss: 0.081173  [ 2800/ 3129]
loss: 0.020654  [ 2900/ 3129]
loss: 0.018926  [ 3000/ 3129]
loss: 0.012456  [ 3100/ 3129]
Epoch 2
-------------------------------
loss: 0.018013  [    0/ 3129]
loss: 0.031040  [  100/ 3129]
loss: 0.033582  [  200/ 3129]
loss: 0.031593  [  300/ 3129]
loss: 0.012293  [  400/ 3129]
loss: 0.008919  [  500/ 3129]
loss: 0.016607  [  600/ 3129]
loss: 0.024088  [  700/ 3129]
loss: 0.016807  [  800/ 3129]
loss: 0.025943  [  900/ 3129]
loss: 0.021848  [ 1000/ 3129]
loss: 0.007286  [ 1100/ 3129]
loss: 0.020980  [ 1200/ 3129]
loss: 0.019307  [ 1300/ 3129]
loss: 0.016065  [ 1400/ 3129]
loss: 0.025643  [ 1500/ 3129]
loss: 0.010316  [ 1600/ 3129]
loss: 0.012664  [ 1700/ 3129]
loss: 0.006387  [ 1800/ 3129]
loss: 0.016359  [ 1900/ 3129]
loss: 0.024267  [ 2000/ 3129]
loss: 0.025964  [ 2100/ 3129]
loss: 0.020760  [ 2200/ 3129]
loss: 0.011111  [ 2300/ 3129]
loss: 0.009528  [ 2400/ 3129]
loss: 0.085319  [ 2500/ 3129]
loss: 0.083667  [ 2600/ 3129]
loss: 0.008141  [ 2700/ 3129]
loss: 0.113234  [ 2800/ 3129]
loss: 0.019765  [ 2900/ 3129]
loss: 0.018500  [ 3000/ 3129]
loss: 0.012139  [ 3100/ 3129]
Epoch 3
-------------------------------
loss: 0.016052  [    0/ 3129]
loss: 0.029365  [  100/ 3129]
loss: 0.027153  [  200/ 3129]
loss: 0.030282  [  300/ 3129]
loss: 0.018441  [  400/ 3129]
loss: 0.009848  [  500/ 3129]
loss: 0.018723  [  600/ 3129]
loss: 0.024350  [  700/ 3129]
loss: 0.013687  [  800/ 3129]
loss: 0.024847  [  900/ 3129]
loss: 0.022267  [ 1000/ 3129]
loss: 0.007122  [ 1100/ 3129]
loss: 0.020414  [ 1200/ 3129]
loss: 0.020069  [ 1300/ 3129]
loss: 0.016623  [ 1400/ 3129]
loss: 0.024375  [ 1500/ 3129]
loss: 0.012420  [ 1600/ 3129]
loss: 0.013674  [ 1700/ 3129]
loss: 0.007337  [ 1800/ 3129]
loss: 0.013098  [ 1900/ 3129]
loss: 0.023106  [ 2000/ 3129]
loss: 0.026513  [ 2100/ 3129]
loss: 0.020895  [ 2200/ 3129]
loss: 0.011442  [ 2300/ 3129]
loss: 0.006833  [ 2400/ 3129]
loss: 0.076269  [ 2500/ 3129]
loss: 0.082741  [ 2600/ 3129]
loss: 0.007512  [ 2700/ 3129]
loss: 0.145674  [ 2800/ 3129]
loss: 0.018916  [ 2900/ 3129]
loss: 0.017356  [ 3000/ 3129]
loss: 0.011195  [ 3100/ 3129]
Epoch 4
-------------------------------
loss: 0.013962  [    0/ 3129]
loss: 0.022306  [  100/ 3129]
loss: 0.024801  [  200/ 3129]
loss: 0.030883  [  300/ 3129]
loss: 0.018693  [  400/ 3129]
loss: 0.009304  [  500/ 3129]
loss: 0.017996  [  600/ 3129]
loss: 0.026390  [  700/ 3129]
loss: 0.011573  [  800/ 3129]
loss: 0.020717  [  900/ 3129]
loss: 0.022218  [ 1000/ 3129]
loss: 0.005094  [ 1100/ 3129]
loss: 0.020433  [ 1200/ 3129]
loss: 0.016586  [ 1300/ 3129]
loss: 0.016779  [ 1400/ 3129]
loss: 0.021333  [ 1500/ 3129]
loss: 0.012528  [ 1600/ 3129]
loss: 0.013708  [ 1700/ 3129]
loss: 0.007814  [ 1800/ 3129]
loss: 0.010553  [ 1900/ 3129]
loss: 0.023627  [ 2000/ 3129]
loss: 0.024805  [ 2100/ 3129]
loss: 0.021103  [ 2200/ 3129]
loss: 0.012687  [ 2300/ 3129]
loss: 0.006648  [ 2400/ 3129]
loss: 0.081301  [ 2500/ 3129]
loss: 0.080932  [ 2600/ 3129]
loss: 0.005421  [ 2700/ 3129]
loss: 0.146930  [ 2800/ 3129]
loss: 0.020304  [ 2900/ 3129]
loss: 0.015949  [ 3000/ 3129]
loss: 0.009865  [ 3100/ 3129]
Epoch 5
-------------------------------
loss: 0.016072  [    0/ 3129]
loss: 0.016849  [  100/ 3129]
loss: 0.027877  [  200/ 3129]
loss: 0.029661  [  300/ 3129]
loss: 0.014657  [  400/ 3129]
loss: 0.007852  [  500/ 3129]
loss: 0.017591  [  600/ 3129]
loss: 0.027552  [  700/ 3129]
loss: 0.010357  [  800/ 3129]
loss: 0.019031  [  900/ 3129]
loss: 0.023027  [ 1000/ 3129]
loss: 0.003916  [ 1100/ 3129]
loss: 0.021969  [ 1200/ 3129]
loss: 0.013876  [ 1300/ 3129]
loss: 0.016335  [ 1400/ 3129]
loss: 0.021282  [ 1500/ 3129]
loss: 0.011429  [ 1600/ 3129]
loss: 0.013390  [ 1700/ 3129]
loss: 0.008813  [ 1800/ 3129]
loss: 0.009505  [ 1900/ 3129]
loss: 0.023468  [ 2000/ 3129]
loss: 0.023918  [ 2100/ 3129]
loss: 0.020164  [ 2200/ 3129]
loss: 0.013326  [ 2300/ 3129]
loss: 0.006381  [ 2400/ 3129]
loss: 0.081162  [ 2500/ 3129]
loss: 0.083337  [ 2600/ 3129]
loss: 0.004750  [ 2700/ 3129]
loss: 0.149759  [ 2800/ 3129]
loss: 0.020844  [ 2900/ 3129]
loss: 0.014873  [ 3000/ 3129]
loss: 0.009314  [ 3100/ 3129]
Epoch 6
-------------------------------
loss: 0.018554  [    0/ 3129]
loss: 0.016506  [  100/ 3129]
loss: 0.029021  [  200/ 3129]
loss: 0.029191  [  300/ 3129]
loss: 0.014364  [  400/ 3129]
loss: 0.007313  [  500/ 3129]
loss: 0.017308  [  600/ 3129]
loss: 0.027630  [  700/ 3129]
loss: 0.009921  [  800/ 3129]
loss: 0.018151  [  900/ 3129]
loss: 0.024080  [ 1000/ 3129]
loss: 0.004178  [ 1100/ 3129]
loss: 0.022523  [ 1200/ 3129]
loss: 0.013056  [ 1300/ 3129]
loss: 0.015485  [ 1400/ 3129]
loss: 0.020947  [ 1500/ 3129]
loss: 0.010880  [ 1600/ 3129]
loss: 0.012619  [ 1700/ 3129]
loss: 0.009759  [ 1800/ 3129]
loss: 0.009428  [ 1900/ 3129]
loss: 0.023419  [ 2000/ 3129]
loss: 0.023182  [ 2100/ 3129]
loss: 0.022563  [ 2200/ 3129]
loss: 0.013324  [ 2300/ 3129]
loss: 0.006651  [ 2400/ 3129]
loss: 0.081897  [ 2500/ 3129]
loss: 0.082814  [ 2600/ 3129]
loss: 0.003823  [ 2700/ 3129]
loss: 0.140855  [ 2800/ 3129]
loss: 0.020923  [ 2900/ 3129]
loss: 0.014475  [ 3000/ 3129]
loss: 0.008908  [ 3100/ 3129]
Epoch 7
-------------------------------
loss: 0.020905  [    0/ 3129]
loss: 0.016780  [  100/ 3129]
loss: 0.031385  [  200/ 3129]
loss: 0.028880  [  300/ 3129]
loss: 0.013156  [  400/ 3129]
loss: 0.006956  [  500/ 3129]
loss: 0.017728  [  600/ 3129]
loss: 0.027536  [  700/ 3129]
loss: 0.009889  [  800/ 3129]
loss: 0.017212  [  900/ 3129]
loss: 0.025162  [ 1000/ 3129]
loss: 0.004613  [ 1100/ 3129]
loss: 0.023372  [ 1200/ 3129]
loss: 0.012082  [ 1300/ 3129]
loss: 0.014731  [ 1400/ 3129]
loss: 0.021088  [ 1500/ 3129]
loss: 0.010951  [ 1600/ 3129]
loss: 0.011771  [ 1700/ 3129]
loss: 0.011029  [ 1800/ 3129]
loss: 0.009573  [ 1900/ 3129]
loss: 0.023124  [ 2000/ 3129]
loss: 0.021619  [ 2100/ 3129]
loss: 0.022081  [ 2200/ 3129]
loss: 0.013398  [ 2300/ 3129]
loss: 0.006999  [ 2400/ 3129]
loss: 0.082531  [ 2500/ 3129]
loss: 0.082497  [ 2600/ 3129]
loss: 0.003333  [ 2700/ 3129]
loss: 0.138417  [ 2800/ 3129]
loss: 0.020946  [ 2900/ 3129]
loss: 0.013899  [ 3000/ 3129]
loss: 0.009066  [ 3100/ 3129]
Epoch 8
-------------------------------
loss: 0.022581  [    0/ 3129]
loss: 0.016594  [  100/ 3129]
loss: 0.031323  [  200/ 3129]
loss: 0.028412  [  300/ 3129]
loss: 0.012672  [  400/ 3129]
loss: 0.006933  [  500/ 3129]
loss: 0.017004  [  600/ 3129]
loss: 0.026998  [  700/ 3129]
loss: 0.010628  [  800/ 3129]
loss: 0.017088  [  900/ 3129]
loss: 0.025493  [ 1000/ 3129]
loss: 0.005022  [ 1100/ 3129]
loss: 0.023180  [ 1200/ 3129]
loss: 0.011589  [ 1300/ 3129]
loss: 0.014384  [ 1400/ 3129]
loss: 0.020840  [ 1500/ 3129]
loss: 0.010419  [ 1600/ 3129]
loss: 0.011401  [ 1700/ 3129]
loss: 0.011739  [ 1800/ 3129]
loss: 0.009694  [ 1900/ 3129]
loss: 0.022806  [ 2000/ 3129]
loss: 0.019554  [ 2100/ 3129]
loss: 0.022964  [ 2200/ 3129]
loss: 0.013255  [ 2300/ 3129]
loss: 0.007367  [ 2400/ 3129]
loss: 0.081892  [ 2500/ 3129]
loss: 0.082949  [ 2600/ 3129]
loss: 0.003405  [ 2700/ 3129]
loss: 0.137861  [ 2800/ 3129]
loss: 0.021339  [ 2900/ 3129]
loss: 0.013252  [ 3000/ 3129]
loss: 0.009971  [ 3100/ 3129]
Number of Clusters: 3
Q_Learning:     1/  348]
Q_Learning:     2/  348]
Q_Learning:     3/  348]
Q_Learning:     4/  348]
Q_Learning:     5/  348]
Q_Learning:     6/  348]
Q_Learning:     7/  348]
Q_Learning:     8/  348]
Q_Learning:     9/  348]
Q_Learning:    10/  348]
Q_Learning:    11/  348]
Q_Learning:    12/  348]
Q_Learning:    13/  348]
Q_Learning:    14/  348]
Q_Learning:    15/  348]
Q_Learning:    16/  348]
Q_Learning:    17/  348]
Q_Learning:    18/  348]
Q_Learning:    19/  348]
Q_Learning:    20/  348]
Q_Learning:    21/  348]
Q_Learning:    22/  348]
Q_Learning:    23/  348]
Q_Learning:    24/  348]
Q_Learning:    25/  348]
Q_Learning:    26/  348]
Q_Learning:    27/  348]
Q_Learning:    28/  348]
Q_Learning:    29/  348]
Q_Learning:    30/  348]
Q_Learning:    31/  348]
Q_Learning:    32/  348]
Q_Learning:    33/  348]
Q_Learning:    34/  348]
Q_Learning:    35/  348]
Q_Learning:    36/  348]
Q_Learning:    37/  348]
Q_Learning:    38/  348]
Q_Learning:    39/  348]
Q_Learning:    40/  348]
Q_Learning:    41/  348]
Q_Learning:    42/  348]
Q_Learning:    43/  348]
Q_Learning:    44/  348]
Q_Learning:    45/  348]
Q_Learning:    46/  348]
Q_Learning:    47/  348]
Q_Learning:    48/  348]
Q_Learning:    49/  348]
Q_Learning:    50/  348]
Q_Learning:    51/  348]
Q_Learning:    52/  348]
Q_Learning:    53/  348]
Q_Learning:    54/  348]
Q_Learning:    55/  348]
Q_Learning:    56/  348]
Q_Learning:    57/  348]
Q_Learning:    58/  348]
Q_Learning:    59/  348]
Q_Learning:    60/  348]
Q_Learning:    61/  348]
Q_Learning:    62/  348]
Q_Learning:    63/  348]
Q_Learning:    64/  348]
Q_Learning:    65/  348]
Q_Learning:    66/  348]
Q_Learning:    67/  348]
Q_Learning:    68/  348]
Q_Learning:    69/  348]
Q_Learning:    70/  348]
Q_Learning:    71/  348]
Q_Learning:    72/  348]
Q_Learning:    73/  348]
Q_Learning:    74/  348]
Q_Learning:    75/  348]
Q_Learning:    76/  348]
Q_Learning:    77/  348]
Q_Learning:    78/  348]
Q_Learning:    79/  348]
Q_Learning:    80/  348]
Q_Learning:    81/  348]
Q_Learning:    82/  348]
Q_Learning:    83/  348]
Q_Learning:    84/  348]
Q_Learning:    85/  348]
Q_Learning:    86/  348]
Q_Learning:    87/  348]
Q_Learning:    88/  348]
Q_Learning:    89/  348]
Q_Learning:    90/  348]
Q_Learning:    91/  348]
Q_Learning:    92/  348]
Q_Learning:    93/  348]
Q_Learning:    94/  348]
Q_Learning:    95/  348]
Q_Learning:    96/  348]
Q_Learning:    97/  348]
Q_Learning:    98/  348]
Q_Learning:    99/  348]
Q_Learning:   100/  348]
Q_Learning:   101/  348]
Q_Learning:   102/  348]
Q_Learning:   103/  348]
Q_Learning:   104/  348]
Q_Learning:   105/  348]
Q_Learning:   106/  348]
Q_Learning:   107/  348]
Q_Learning:   108/  348]
Q_Learning:   109/  348]
Q_Learning:   110/  348]
Q_Learning:   111/  348]
Q_Learning:   112/  348]
Q_Learning:   113/  348]
Q_Learning:   114/  348]
Q_Learning:   115/  348]
Q_Learning:   116/  348]
Q_Learning:   117/  348]
Q_Learning:   118/  348]
Q_Learning:   119/  348]
Q_Learning:   120/  348]
Q_Learning:   121/  348]
Q_Learning:   122/  348]
Q_Learning:   123/  348]
Q_Learning:   124/  348]
Q_Learning:   125/  348]
Q_Learning:   126/  348]
Q_Learning:   127/  348]
Q_Learning:   128/  348]
Q_Learning:   129/  348]
Q_Learning:   130/  348]
Q_Learning:   131/  348]
Q_Learning:   132/  348]
Q_Learning:   133/  348]
Q_Learning:   134/  348]
Q_Learning:   135/  348]
Q_Learning:   136/  348]
Q_Learning:   137/  348]
Q_Learning:   138/  348]
Q_Learning:   139/  348]
Q_Learning:   140/  348]
Q_Learning:   141/  348]
Q_Learning:   142/  348]
Q_Learning:   143/  348]
Q_Learning:   144/  348]
Q_Learning:   145/  348]
Q_Learning:   146/  348]
Q_Learning:   147/  348]
Q_Learning:   148/  348]
Q_Learning:   149/  348]
Q_Learning:   150/  348]
Q_Learning:   151/  348]
Q_Learning:   152/  348]
Q_Learning:   153/  348]
Q_Learning:   154/  348]
Q_Learning:   155/  348]
Q_Learning:   156/  348]
Q_Learning:   157/  348]
Q_Learning:   158/  348]
Q_Learning:   159/  348]
Q_Learning:   160/  348]
Q_Learning:   161/  348]
Q_Learning:   162/  348]
Q_Learning:   163/  348]
Q_Learning:   164/  348]
Q_Learning:   165/  348]
Q_Learning:   166/  348]
Q_Learning:   167/  348]
Q_Learning:   168/  348]
Q_Learning:   169/  348]
Q_Learning:   170/  348]
Q_Learning:   171/  348]
Q_Learning:   172/  348]
Q_Learning:   173/  348]
Q_Learning:   174/  348]
Q_Learning:   175/  348]
Q_Learning:   176/  348]
Q_Learning:   177/  348]
Q_Learning:   178/  348]
Q_Learning:   179/  348]
Q_Learning:   180/  348]
Q_Learning:   181/  348]
Q_Learning:   182/  348]
Q_Learning:   183/  348]
Q_Learning:   184/  348]
Q_Learning:   185/  348]
Q_Learning:   186/  348]
Q_Learning:   187/  348]
Q_Learning:   188/  348]
Q_Learning:   189/  348]
Q_Learning:   190/  348]
Q_Learning:   191/  348]
Q_Learning:   192/  348]
Q_Learning:   193/  348]
Q_Learning:   194/  348]
Q_Learning:   195/  348]
Q_Learning:   196/  348]
Q_Learning:   197/  348]
Q_Learning:   198/  348]
Q_Learning:   199/  348]
Q_Learning:   200/  348]
Q_Learning:   201/  348]
Q_Learning:   202/  348]
Q_Learning:   203/  348]
Q_Learning:   204/  348]
Q_Learning:   205/  348]
Q_Learning:   206/  348]
Q_Learning:   207/  348]
Q_Learning:   208/  348]
Q_Learning:   209/  348]
Q_Learning:   210/  348]
Q_Learning:   211/  348]
Q_Learning:   212/  348]
Q_Learning:   213/  348]
Q_Learning:   214/  348]
Q_Learning:   215/  348]
Q_Learning:   216/  348]
Q_Learning:   217/  348]
Q_Learning:   218/  348]
Q_Learning:   219/  348]
Q_Learning:   220/  348]
Q_Learning:   221/  348]
Q_Learning:   222/  348]
Q_Learning:   223/  348]
Q_Learning:   224/  348]
Q_Learning:   225/  348]
Q_Learning:   226/  348]
Q_Learning:   227/  348]
Q_Learning:   228/  348]
Q_Learning:   229/  348]
Q_Learning:   230/  348]
Q_Learning:   231/  348]
Q_Learning:   232/  348]
Q_Learning:   233/  348]
Q_Learning:   234/  348]
Q_Learning:   235/  348]
Q_Learning:   236/  348]
Q_Learning:   237/  348]
Q_Learning:   238/  348]
Q_Learning:   239/  348]
Q_Learning:   240/  348]
Q_Learning:   241/  348]
Q_Learning:   242/  348]
Q_Learning:   243/  348]
Q_Learning:   244/  348]
Q_Learning:   245/  348]
Q_Learning:   246/  348]
Q_Learning:   247/  348]
Q_Learning:   248/  348]
Q_Learning:   249/  348]
Q_Learning:   250/  348]
Q_Learning:   251/  348]
Q_Learning:   252/  348]
Q_Learning:   253/  348]
Q_Learning:   254/  348]
Q_Learning:   255/  348]
Q_Learning:   256/  348]
Q_Learning:   257/  348]
Q_Learning:   258/  348]
Q_Learning:   259/  348]
Q_Learning:   260/  348]
Q_Learning:   261/  348]
Q_Learning:   262/  348]
Q_Learning:   263/  348]
Q_Learning:   264/  348]
Q_Learning:   265/  348]
Q_Learning:   266/  348]
Q_Learning:   267/  348]
Q_Learning:   268/  348]
Q_Learning:   269/  348]
Q_Learning:   270/  348]
Q_Learning:   271/  348]
Q_Learning:   272/  348]
Q_Learning:   273/  348]
Q_Learning:   274/  348]
Q_Learning:   275/  348]
Q_Learning:   276/  348]
Q_Learning:   277/  348]
Q_Learning:   278/  348]
Q_Learning:   279/  348]
Q_Learning:   280/  348]
Q_Learning:   281/  348]
Q_Learning:   282/  348]
Q_Learning:   283/  348]
Q_Learning:   284/  348]
Q_Learning:   285/  348]
Q_Learning:   286/  348]
Q_Learning:   287/  348]
Q_Learning:   288/  348]
Q_Learning:   289/  348]
Q_Learning:   290/  348]
Q_Learning:   291/  348]
Q_Learning:   292/  348]
Q_Learning:   293/  348]
Q_Learning:   294/  348]
Q_Learning:   295/  348]
Q_Learning:   296/  348]
Q_Learning:   297/  348]
Q_Learning:   298/  348]
Q_Learning:   299/  348]
Q_Learning:   300/  348]
Q_Learning:   301/  348]
Q_Learning:   302/  348]
Q_Learning:   303/  348]
Q_Learning:   304/  348]
Q_Learning:   305/  348]
Q_Learning:   306/  348]
Q_Learning:   307/  348]
Q_Learning:   308/  348]
Q_Learning:   309/  348]
Q_Learning:   310/  348]
Q_Learning:   311/  348]
Q_Learning:   312/  348]
Q_Learning:   313/  348]
Q_Learning:   314/  348]
Q_Learning:   315/  348]
Q_Learning:   316/  348]
Q_Learning:   317/  348]
Q_Learning:   318/  348]
Q_Learning:   319/  348]
Q_Learning:   320/  348]
Q_Learning:   321/  348]
Q_Learning:   322/  348]
Q_Learning:   323/  348]
Q_Learning:   324/  348]
Q_Learning:   325/  348]
Q_Learning:   326/  348]
Q_Learning:   327/  348]
Q_Learning:   328/  348]
Q_Learning:   329/  348]
Q_Learning:   330/  348]
Q_Learning:   331/  348]
Q_Learning:   332/  348]
Q_Learning:   333/  348]
Q_Learning:   334/  348]
Q_Learning:   335/  348]
Q_Learning:   336/  348]
Q_Learning:   337/  348]
Q_Learning:   338/  348]
Q_Learning:   339/  348]
Q_Learning:   340/  348]
Q_Learning:   341/  348]
Q_Learning:   342/  348]
Q_Learning:   343/  348]
Q_Learning:   344/  348]
Q_Learning:   345/  348]
Q_Learning:   346/  348]
Q_Learning:   347/  348]
Q_Learning:   348/  348]
Number of Samples after Autoencoder testing: 348
First Spike after testing: [-0.9457233 -1.1784085]
[0, 0, 1, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 0, 1, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 2, 0, 2, 1, 0, 0, 2, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 0, 2, 2, 0, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 2, 2, 0, 2, 0, 0, 2, 0, 2, 1, 0, 0, 1, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 2, 0, 1, 1, 0, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 1, 2, 0, 2, 0, 1, 2, 1, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 2, 0, 2, 2, 1, 2, 2, 1, 0, 1, 2, 2, 1, 0, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 2, 2, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 0, 2, 1, 2, 1, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 0, 0, 1, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 2, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 0, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1]
[0, 0, 1, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 0, 1, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 2, 0, 2, 1, 0, 0, 2, 3, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 0, 2, 2, 0, 2, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 4, 0, 0, 0, 1, 2, 2, 0, 2, 0, 0, 2, 0, 4, 4, 0, 0, 1, 2, 1, 0, 0, 2, 4, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 2, 0, 1, 1, 0, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 0, 0, 0, 1, 0, 2, 2, 3, 2, 2, 1, 1, 2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 1, 2, 0, 2, 0, 1, 2, 1, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 0, 4, 4, 2, 2, 1, 0, 5, 2, 0, 1, 1, 1, 5, 2, 0, 2, 2, 1, 2, 2, 1, 0, 1, 2, 2, 1, 0, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 2, 2, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 0, 2, 1, 2, 4, 0, 0, 0, 1, 2, 1, 2, 1, 1, 0, 0, 0, 1, 0, 2, 4, 4, 1, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1, 2, 4, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 2, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 0, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1]
Centroids: [[-0.9024902, -1.3091806], [1.2606437, 2.5793924], [1.2785766, -1.2257334]]
Centroids: [[-0.8688603, -1.2856457], [1.2289816, 2.6691823], [1.265199, -1.2692935], [-2.641015, -2.5031724], [1.2644521, 0.33820435], [2.8827143, 3.212581]]
Contingency Matrix: 
[[101   0   0   2   1   0]
 [  0 120   0   0   6   2]
 [  0   0 113   0   3   0]]
[[101, -1, 0, 2, 1, 0], [-1, -1, -1, -1, -1, -1], [0, -1, 113, 0, 3, 0]]
[[101, -1, -1, 2, 1, 0], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1]]
Match_Labels: {1: 1, 2: 2, 0: 0}
New Contingency Matrix: 
[[101   0   0   2   1   0]
 [  0 120   0   0   6   2]
 [  0   0 113   0   3   0]]
New Clustered Label Sequence: [0, 1, 2, 3, 4, 5]
Diagonal_Elements: [101, 120, 113], Sum: 334
All_Elements: [101, 0, 0, 2, 1, 0, 0, 120, 0, 0, 6, 2, 0, 0, 113, 0, 3, 0], Sum: 348
Accuracy: 0.9597701149425287
Done!
