Experiment_path: Random_Seeds//V4/Experiment_04_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise005.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise005.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Random_Seeds//V4/Experiment_04_5/C_Easy1_noise005.mat/Variant_04_Offline_Autoencoder_QLearning/2023_03_22-09_39_23
Punishment_Coefficient: 0.9
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001CB4665BCF8>
Sampling rate: 24000.0
Raw: [-0.05265172 -0.03124187 -0.00282162 ...  0.01798155  0.01678863
  0.0119459 ]
Times: [    283     469    1484 ... 1438285 1438773 1439067]
Cluster: [2 1 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3514
First aligned Spike Frame: [-2.11647214e-02 -2.00144278e-02 -2.48166304e-02 -2.70972753e-02
 -1.11241704e-02  1.86904987e-02  3.99716833e-02  4.40400999e-02
  4.38833221e-02  5.06364129e-02  6.02243042e-02  3.59622148e-02
 -9.64451652e-02 -3.71359573e-01 -6.92987060e-01 -8.74449953e-01
 -7.13363902e-01 -1.84182190e-01  4.08997970e-01  7.26119515e-01
  7.19977210e-01  5.61000789e-01  4.04007238e-01  2.96025242e-01
  2.22861462e-01  1.69209408e-01  1.33269005e-01  1.11481721e-01
  9.67043158e-02  8.35988040e-02  6.87571423e-02  5.74871826e-02
  5.26722178e-02  4.53956038e-02  3.31356602e-02  2.21250606e-02
  1.35048482e-02 -4.41592673e-04 -2.31921908e-02 -4.69576347e-02
 -6.03503288e-02 -6.27551095e-02 -6.19812766e-02 -6.37499251e-02
 -6.42747873e-02 -5.93586264e-02 -5.06150772e-02]
Cluster 0, Occurrences: 1165
Cluster 1, Occurrences: 1157
Cluster 2, Occurrences: 1192
Train Index: 3163
x_train: 3163
y_train: 3163
x_test: 351
y_test: 351
<torch.utils.data.dataloader.DataLoader object at 0x000001CB4739A898>
<torch.utils.data.dataloader.DataLoader object at 0x000001CB4739A9B0>
Epoch 1
-------------------------------
loss: 0.120976  [    0/ 3163]
loss: 0.132662  [  100/ 3163]
loss: 0.053323  [  200/ 3163]
loss: 0.051721  [  300/ 3163]
loss: 0.019000  [  400/ 3163]
loss: 0.012780  [  500/ 3163]
loss: 0.008355  [  600/ 3163]
loss: 0.003677  [  700/ 3163]
loss: 0.001829  [  800/ 3163]
loss: 0.004351  [  900/ 3163]
loss: 0.006270  [ 1000/ 3163]
loss: 0.094783  [ 1100/ 3163]
loss: 0.002559  [ 1200/ 3163]
loss: 0.002224  [ 1300/ 3163]
loss: 0.091955  [ 1400/ 3163]
loss: 0.000683  [ 1500/ 3163]
loss: 0.006334  [ 1600/ 3163]
loss: 0.005587  [ 1700/ 3163]
loss: 0.226213  [ 1800/ 3163]
loss: 0.008053  [ 1900/ 3163]
loss: 0.002273  [ 2000/ 3163]
loss: 0.005373  [ 2100/ 3163]
loss: 0.000594  [ 2200/ 3163]
loss: 0.001278  [ 2300/ 3163]
loss: 0.002114  [ 2400/ 3163]
loss: 0.007054  [ 2500/ 3163]
loss: 0.003574  [ 2600/ 3163]
loss: 0.002807  [ 2700/ 3163]
loss: 0.007448  [ 2800/ 3163]
loss: 0.002789  [ 2900/ 3163]
loss: 0.006319  [ 3000/ 3163]
loss: 0.001926  [ 3100/ 3163]
Epoch 2
-------------------------------
loss: 0.004184  [    0/ 3163]
loss: 0.002148  [  100/ 3163]
loss: 0.006852  [  200/ 3163]
loss: 0.003901  [  300/ 3163]
loss: 0.004715  [  400/ 3163]
loss: 0.004362  [  500/ 3163]
loss: 0.007418  [  600/ 3163]
loss: 0.002599  [  700/ 3163]
loss: 0.001547  [  800/ 3163]
loss: 0.004060  [  900/ 3163]
loss: 0.006994  [ 1000/ 3163]
loss: 0.093008  [ 1100/ 3163]
loss: 0.002876  [ 1200/ 3163]
loss: 0.002009  [ 1300/ 3163]
loss: 0.070118  [ 1400/ 3163]
loss: 0.001123  [ 1500/ 3163]
loss: 0.005816  [ 1600/ 3163]
loss: 0.006397  [ 1700/ 3163]
loss: 0.207968  [ 1800/ 3163]
loss: 0.005636  [ 1900/ 3163]
loss: 0.001827  [ 2000/ 3163]
loss: 0.005503  [ 2100/ 3163]
loss: 0.000665  [ 2200/ 3163]
loss: 0.001319  [ 2300/ 3163]
loss: 0.001940  [ 2400/ 3163]
loss: 0.006646  [ 2500/ 3163]
loss: 0.003747  [ 2600/ 3163]
loss: 0.002524  [ 2700/ 3163]
loss: 0.007431  [ 2800/ 3163]
loss: 0.001966  [ 2900/ 3163]
loss: 0.003825  [ 3000/ 3163]
loss: 0.001637  [ 3100/ 3163]
Epoch 3
-------------------------------
loss: 0.002772  [    0/ 3163]
loss: 0.001571  [  100/ 3163]
loss: 0.006838  [  200/ 3163]
loss: 0.003742  [  300/ 3163]
loss: 0.004184  [  400/ 3163]
loss: 0.004109  [  500/ 3163]
loss: 0.006757  [  600/ 3163]
loss: 0.002394  [  700/ 3163]
loss: 0.001471  [  800/ 3163]
loss: 0.004386  [  900/ 3163]
loss: 0.006755  [ 1000/ 3163]
loss: 0.093067  [ 1100/ 3163]
loss: 0.002617  [ 1200/ 3163]
loss: 0.002071  [ 1300/ 3163]
loss: 0.069169  [ 1400/ 3163]
loss: 0.000949  [ 1500/ 3163]
loss: 0.005517  [ 1600/ 3163]
loss: 0.005924  [ 1700/ 3163]
loss: 0.189707  [ 1800/ 3163]
loss: 0.004585  [ 1900/ 3163]
loss: 0.001587  [ 2000/ 3163]
loss: 0.005625  [ 2100/ 3163]
loss: 0.000660  [ 2200/ 3163]
loss: 0.001311  [ 2300/ 3163]
loss: 0.001804  [ 2400/ 3163]
loss: 0.006724  [ 2500/ 3163]
loss: 0.003771  [ 2600/ 3163]
loss: 0.002586  [ 2700/ 3163]
loss: 0.007353  [ 2800/ 3163]
loss: 0.001575  [ 2900/ 3163]
loss: 0.003474  [ 3000/ 3163]
loss: 0.001670  [ 3100/ 3163]
Epoch 4
-------------------------------
loss: 0.002668  [    0/ 3163]
loss: 0.001164  [  100/ 3163]
loss: 0.007155  [  200/ 3163]
loss: 0.003636  [  300/ 3163]
loss: 0.004351  [  400/ 3163]
loss: 0.004025  [  500/ 3163]
loss: 0.006565  [  600/ 3163]
loss: 0.002267  [  700/ 3163]
loss: 0.001374  [  800/ 3163]
loss: 0.004274  [  900/ 3163]
loss: 0.006599  [ 1000/ 3163]
loss: 0.092527  [ 1100/ 3163]
loss: 0.002199  [ 1200/ 3163]
loss: 0.002075  [ 1300/ 3163]
loss: 0.069001  [ 1400/ 3163]
loss: 0.000965  [ 1500/ 3163]
loss: 0.004915  [ 1600/ 3163]
loss: 0.005680  [ 1700/ 3163]
loss: 0.175023  [ 1800/ 3163]
loss: 0.004208  [ 1900/ 3163]
loss: 0.001408  [ 2000/ 3163]
loss: 0.005423  [ 2100/ 3163]
loss: 0.000664  [ 2200/ 3163]
loss: 0.001329  [ 2300/ 3163]
loss: 0.001721  [ 2400/ 3163]
loss: 0.006726  [ 2500/ 3163]
loss: 0.003786  [ 2600/ 3163]
loss: 0.002857  [ 2700/ 3163]
loss: 0.007466  [ 2800/ 3163]
loss: 0.001442  [ 2900/ 3163]
loss: 0.003241  [ 3000/ 3163]
loss: 0.001613  [ 3100/ 3163]
Epoch 5
-------------------------------
loss: 0.002733  [    0/ 3163]
loss: 0.000891  [  100/ 3163]
loss: 0.007261  [  200/ 3163]
loss: 0.003550  [  300/ 3163]
loss: 0.004443  [  400/ 3163]
loss: 0.004125  [  500/ 3163]
loss: 0.006473  [  600/ 3163]
loss: 0.002136  [  700/ 3163]
loss: 0.001270  [  800/ 3163]
loss: 0.004229  [  900/ 3163]
loss: 0.006363  [ 1000/ 3163]
loss: 0.092226  [ 1100/ 3163]
loss: 0.001871  [ 1200/ 3163]
loss: 0.002112  [ 1300/ 3163]
loss: 0.069068  [ 1400/ 3163]
loss: 0.001010  [ 1500/ 3163]
loss: 0.004622  [ 1600/ 3163]
loss: 0.006927  [ 1700/ 3163]
loss: 0.163463  [ 1800/ 3163]
loss: 0.003946  [ 1900/ 3163]
loss: 0.001308  [ 2000/ 3163]
loss: 0.005244  [ 2100/ 3163]
loss: 0.000646  [ 2200/ 3163]
loss: 0.001378  [ 2300/ 3163]
loss: 0.001634  [ 2400/ 3163]
loss: 0.006957  [ 2500/ 3163]
loss: 0.003847  [ 2600/ 3163]
loss: 0.003100  [ 2700/ 3163]
loss: 0.007460  [ 2800/ 3163]
loss: 0.001404  [ 2900/ 3163]
loss: 0.003059  [ 3000/ 3163]
loss: 0.001655  [ 3100/ 3163]
Epoch 6
-------------------------------
loss: 0.002952  [    0/ 3163]
loss: 0.000745  [  100/ 3163]
loss: 0.007174  [  200/ 3163]
loss: 0.003435  [  300/ 3163]
loss: 0.004530  [  400/ 3163]
loss: 0.004150  [  500/ 3163]
loss: 0.006421  [  600/ 3163]
loss: 0.002039  [  700/ 3163]
loss: 0.001194  [  800/ 3163]
loss: 0.004216  [  900/ 3163]
loss: 0.006229  [ 1000/ 3163]
loss: 0.091859  [ 1100/ 3163]
loss: 0.001708  [ 1200/ 3163]
loss: 0.002102  [ 1300/ 3163]
loss: 0.069833  [ 1400/ 3163]
loss: 0.000916  [ 1500/ 3163]
loss: 0.004171  [ 1600/ 3163]
loss: 0.007210  [ 1700/ 3163]
loss: 0.154735  [ 1800/ 3163]
loss: 0.003823  [ 1900/ 3163]
loss: 0.001289  [ 2000/ 3163]
loss: 0.005228  [ 2100/ 3163]
loss: 0.000639  [ 2200/ 3163]
loss: 0.001429  [ 2300/ 3163]
loss: 0.001644  [ 2400/ 3163]
loss: 0.007006  [ 2500/ 3163]
loss: 0.003914  [ 2600/ 3163]
loss: 0.003316  [ 2700/ 3163]
loss: 0.007428  [ 2800/ 3163]
loss: 0.001573  [ 2900/ 3163]
loss: 0.003119  [ 3000/ 3163]
loss: 0.001436  [ 3100/ 3163]
Epoch 7
-------------------------------
loss: 0.003502  [    0/ 3163]
loss: 0.000694  [  100/ 3163]
loss: 0.007454  [  200/ 3163]
loss: 0.003217  [  300/ 3163]
loss: 0.004543  [  400/ 3163]
loss: 0.003877  [  500/ 3163]
loss: 0.006269  [  600/ 3163]
loss: 0.002334  [  700/ 3163]
loss: 0.001113  [  800/ 3163]
loss: 0.004264  [  900/ 3163]
loss: 0.006018  [ 1000/ 3163]
loss: 0.091396  [ 1100/ 3163]
loss: 0.001553  [ 1200/ 3163]
loss: 0.002143  [ 1300/ 3163]
loss: 0.069958  [ 1400/ 3163]
loss: 0.000918  [ 1500/ 3163]
loss: 0.003842  [ 1600/ 3163]
loss: 0.007026  [ 1700/ 3163]
loss: 0.148961  [ 1800/ 3163]
loss: 0.003685  [ 1900/ 3163]
loss: 0.001291  [ 2000/ 3163]
loss: 0.005159  [ 2100/ 3163]
loss: 0.000629  [ 2200/ 3163]
loss: 0.001461  [ 2300/ 3163]
loss: 0.001711  [ 2400/ 3163]
loss: 0.007104  [ 2500/ 3163]
loss: 0.003950  [ 2600/ 3163]
loss: 0.003622  [ 2700/ 3163]
loss: 0.007353  [ 2800/ 3163]
loss: 0.001521  [ 2900/ 3163]
loss: 0.002705  [ 3000/ 3163]
loss: 0.001394  [ 3100/ 3163]
Epoch 8
-------------------------------
loss: 0.003825  [    0/ 3163]
loss: 0.000625  [  100/ 3163]
loss: 0.007558  [  200/ 3163]
loss: 0.003085  [  300/ 3163]
loss: 0.004509  [  400/ 3163]
loss: 0.003838  [  500/ 3163]
loss: 0.006196  [  600/ 3163]
loss: 0.002485  [  700/ 3163]
loss: 0.001101  [  800/ 3163]
loss: 0.004350  [  900/ 3163]
loss: 0.005916  [ 1000/ 3163]
loss: 0.091114  [ 1100/ 3163]
loss: 0.001359  [ 1200/ 3163]
loss: 0.002683  [ 1300/ 3163]
loss: 0.070200  [ 1400/ 3163]
loss: 0.000904  [ 1500/ 3163]
loss: 0.003813  [ 1600/ 3163]
loss: 0.006887  [ 1700/ 3163]
loss: 0.146270  [ 1800/ 3163]
loss: 0.003673  [ 1900/ 3163]
loss: 0.001291  [ 2000/ 3163]
loss: 0.004983  [ 2100/ 3163]
loss: 0.000624  [ 2200/ 3163]
loss: 0.001514  [ 2300/ 3163]
loss: 0.001923  [ 2400/ 3163]
loss: 0.006909  [ 2500/ 3163]
loss: 0.004021  [ 2600/ 3163]
loss: 0.003472  [ 2700/ 3163]
loss: 0.007277  [ 2800/ 3163]
loss: 0.001469  [ 2900/ 3163]
loss: 0.002481  [ 3000/ 3163]
loss: 0.001304  [ 3100/ 3163]
Number of Clusters: 3
Q_Learning:     1/  351]
Q_Learning:     2/  351]
Q_Learning:     3/  351]
Q_Learning:     4/  351]
Q_Learning:     5/  351]
Q_Learning:     6/  351]
Q_Learning:     7/  351]
Q_Learning:     8/  351]
Q_Learning:     9/  351]
Q_Learning:    10/  351]
Q_Learning:    11/  351]
Q_Learning:    12/  351]
Q_Learning:    13/  351]
Q_Learning:    14/  351]
Q_Learning:    15/  351]
Q_Learning:    16/  351]
Q_Learning:    17/  351]
Q_Learning:    18/  351]
Q_Learning:    19/  351]
Q_Learning:    20/  351]
Q_Learning:    21/  351]
Q_Learning:    22/  351]
Q_Learning:    23/  351]
Q_Learning:    24/  351]
Q_Learning:    25/  351]
Q_Learning:    26/  351]
Q_Learning:    27/  351]
Q_Learning:    28/  351]
Q_Learning:    29/  351]
Q_Learning:    30/  351]
Q_Learning:    31/  351]
Q_Learning:    32/  351]
Q_Learning:    33/  351]
Q_Learning:    34/  351]
Q_Learning:    35/  351]
Q_Learning:    36/  351]
Q_Learning:    37/  351]
Q_Learning:    38/  351]
Q_Learning:    39/  351]
Q_Learning:    40/  351]
Q_Learning:    41/  351]
Q_Learning:    42/  351]
Q_Learning:    43/  351]
Q_Learning:    44/  351]
Q_Learning:    45/  351]
Q_Learning:    46/  351]
Q_Learning:    47/  351]
Q_Learning:    48/  351]
Q_Learning:    49/  351]
Q_Learning:    50/  351]
Q_Learning:    51/  351]
Q_Learning:    52/  351]
Q_Learning:    53/  351]
Q_Learning:    54/  351]
Q_Learning:    55/  351]
Q_Learning:    56/  351]
Q_Learning:    57/  351]
Q_Learning:    58/  351]
Q_Learning:    59/  351]
Q_Learning:    60/  351]
Q_Learning:    61/  351]
Q_Learning:    62/  351]
Q_Learning:    63/  351]
Q_Learning:    64/  351]
Q_Learning:    65/  351]
Q_Learning:    66/  351]
Q_Learning:    67/  351]
Q_Learning:    68/  351]
Q_Learning:    69/  351]
Q_Learning:    70/  351]
Q_Learning:    71/  351]
Q_Learning:    72/  351]
Q_Learning:    73/  351]
Q_Learning:    74/  351]
Q_Learning:    75/  351]
Q_Learning:    76/  351]
Q_Learning:    77/  351]
Q_Learning:    78/  351]
Q_Learning:    79/  351]
Q_Learning:    80/  351]
Q_Learning:    81/  351]
Q_Learning:    82/  351]
Q_Learning:    83/  351]
Q_Learning:    84/  351]
Q_Learning:    85/  351]
Q_Learning:    86/  351]
Q_Learning:    87/  351]
Q_Learning:    88/  351]
Q_Learning:    89/  351]
Q_Learning:    90/  351]
Q_Learning:    91/  351]
Q_Learning:    92/  351]
Q_Learning:    93/  351]
Q_Learning:    94/  351]
Q_Learning:    95/  351]
Q_Learning:    96/  351]
Q_Learning:    97/  351]
Q_Learning:    98/  351]
Q_Learning:    99/  351]
Q_Learning:   100/  351]
Q_Learning:   101/  351]
Q_Learning:   102/  351]
Q_Learning:   103/  351]
Q_Learning:   104/  351]
Q_Learning:   105/  351]
Q_Learning:   106/  351]
Q_Learning:   107/  351]
Q_Learning:   108/  351]
Q_Learning:   109/  351]
Q_Learning:   110/  351]
Q_Learning:   111/  351]
Q_Learning:   112/  351]
Q_Learning:   113/  351]
Q_Learning:   114/  351]
Q_Learning:   115/  351]
Q_Learning:   116/  351]
Q_Learning:   117/  351]
Q_Learning:   118/  351]
Q_Learning:   119/  351]
Q_Learning:   120/  351]
Q_Learning:   121/  351]
Q_Learning:   122/  351]
Q_Learning:   123/  351]
Q_Learning:   124/  351]
Q_Learning:   125/  351]
Q_Learning:   126/  351]
Q_Learning:   127/  351]
Q_Learning:   128/  351]
Q_Learning:   129/  351]
Q_Learning:   130/  351]
Q_Learning:   131/  351]
Q_Learning:   132/  351]
Q_Learning:   133/  351]
Q_Learning:   134/  351]
Q_Learning:   135/  351]
Q_Learning:   136/  351]
Q_Learning:   137/  351]
Q_Learning:   138/  351]
Q_Learning:   139/  351]
Q_Learning:   140/  351]
Q_Learning:   141/  351]
Q_Learning:   142/  351]
Q_Learning:   143/  351]
Q_Learning:   144/  351]
Q_Learning:   145/  351]
Q_Learning:   146/  351]
Q_Learning:   147/  351]
Q_Learning:   148/  351]
Q_Learning:   149/  351]
Q_Learning:   150/  351]
Q_Learning:   151/  351]
Q_Learning:   152/  351]
Q_Learning:   153/  351]
Q_Learning:   154/  351]
Q_Learning:   155/  351]
Q_Learning:   156/  351]
Q_Learning:   157/  351]
Q_Learning:   158/  351]
Q_Learning:   159/  351]
Q_Learning:   160/  351]
Q_Learning:   161/  351]
Q_Learning:   162/  351]
Q_Learning:   163/  351]
Q_Learning:   164/  351]
Q_Learning:   165/  351]
Q_Learning:   166/  351]
Q_Learning:   167/  351]
Q_Learning:   168/  351]
Q_Learning:   169/  351]
Q_Learning:   170/  351]
Q_Learning:   171/  351]
Q_Learning:   172/  351]
Q_Learning:   173/  351]
Q_Learning:   174/  351]
Q_Learning:   175/  351]
Q_Learning:   176/  351]
Q_Learning:   177/  351]
Q_Learning:   178/  351]
Q_Learning:   179/  351]
Q_Learning:   180/  351]
Q_Learning:   181/  351]
Q_Learning:   182/  351]
Q_Learning:   183/  351]
Q_Learning:   184/  351]
Q_Learning:   185/  351]
Q_Learning:   186/  351]
Q_Learning:   187/  351]
Q_Learning:   188/  351]
Q_Learning:   189/  351]
Q_Learning:   190/  351]
Q_Learning:   191/  351]
Q_Learning:   192/  351]
Q_Learning:   193/  351]
Q_Learning:   194/  351]
Q_Learning:   195/  351]
Q_Learning:   196/  351]
Q_Learning:   197/  351]
Q_Learning:   198/  351]
Q_Learning:   199/  351]
Q_Learning:   200/  351]
Q_Learning:   201/  351]
Q_Learning:   202/  351]
Q_Learning:   203/  351]
Q_Learning:   204/  351]
Q_Learning:   205/  351]
Q_Learning:   206/  351]
Q_Learning:   207/  351]
Q_Learning:   208/  351]
Q_Learning:   209/  351]
Q_Learning:   210/  351]
Q_Learning:   211/  351]
Q_Learning:   212/  351]
Q_Learning:   213/  351]
Q_Learning:   214/  351]
Q_Learning:   215/  351]
Q_Learning:   216/  351]
Q_Learning:   217/  351]
Q_Learning:   218/  351]
Q_Learning:   219/  351]
Q_Learning:   220/  351]
Q_Learning:   221/  351]
Q_Learning:   222/  351]
Q_Learning:   223/  351]
Q_Learning:   224/  351]
Q_Learning:   225/  351]
Q_Learning:   226/  351]
Q_Learning:   227/  351]
Q_Learning:   228/  351]
Q_Learning:   229/  351]
Q_Learning:   230/  351]
Q_Learning:   231/  351]
Q_Learning:   232/  351]
Q_Learning:   233/  351]
Q_Learning:   234/  351]
Q_Learning:   235/  351]
Q_Learning:   236/  351]
Q_Learning:   237/  351]
Q_Learning:   238/  351]
Q_Learning:   239/  351]
Q_Learning:   240/  351]
Q_Learning:   241/  351]
Q_Learning:   242/  351]
Q_Learning:   243/  351]
Q_Learning:   244/  351]
Q_Learning:   245/  351]
Q_Learning:   246/  351]
Q_Learning:   247/  351]
Q_Learning:   248/  351]
Q_Learning:   249/  351]
Q_Learning:   250/  351]
Q_Learning:   251/  351]
Q_Learning:   252/  351]
Q_Learning:   253/  351]
Q_Learning:   254/  351]
Q_Learning:   255/  351]
Q_Learning:   256/  351]
Q_Learning:   257/  351]
Q_Learning:   258/  351]
Q_Learning:   259/  351]
Q_Learning:   260/  351]
Q_Learning:   261/  351]
Q_Learning:   262/  351]
Q_Learning:   263/  351]
Q_Learning:   264/  351]
Q_Learning:   265/  351]
Q_Learning:   266/  351]
Q_Learning:   267/  351]
Q_Learning:   268/  351]
Q_Learning:   269/  351]
Q_Learning:   270/  351]
Q_Learning:   271/  351]
Q_Learning:   272/  351]
Q_Learning:   273/  351]
Q_Learning:   274/  351]
Q_Learning:   275/  351]
Q_Learning:   276/  351]
Q_Learning:   277/  351]
Q_Learning:   278/  351]
Q_Learning:   279/  351]
Q_Learning:   280/  351]
Q_Learning:   281/  351]
Q_Learning:   282/  351]
Q_Learning:   283/  351]
Q_Learning:   284/  351]
Q_Learning:   285/  351]
Q_Learning:   286/  351]
Q_Learning:   287/  351]
Q_Learning:   288/  351]
Q_Learning:   289/  351]
Q_Learning:   290/  351]
Q_Learning:   291/  351]
Q_Learning:   292/  351]
Q_Learning:   293/  351]
Q_Learning:   294/  351]
Q_Learning:   295/  351]
Q_Learning:   296/  351]
Q_Learning:   297/  351]
Q_Learning:   298/  351]
Q_Learning:   299/  351]
Q_Learning:   300/  351]
Q_Learning:   301/  351]
Q_Learning:   302/  351]
Q_Learning:   303/  351]
Q_Learning:   304/  351]
Q_Learning:   305/  351]
Q_Learning:   306/  351]
Q_Learning:   307/  351]
Q_Learning:   308/  351]
Q_Learning:   309/  351]
Q_Learning:   310/  351]
Q_Learning:   311/  351]
Q_Learning:   312/  351]
Q_Learning:   313/  351]
Q_Learning:   314/  351]
Q_Learning:   315/  351]
Q_Learning:   316/  351]
Q_Learning:   317/  351]
Q_Learning:   318/  351]
Q_Learning:   319/  351]
Q_Learning:   320/  351]
Q_Learning:   321/  351]
Q_Learning:   322/  351]
Q_Learning:   323/  351]
Q_Learning:   324/  351]
Q_Learning:   325/  351]
Q_Learning:   326/  351]
Q_Learning:   327/  351]
Q_Learning:   328/  351]
Q_Learning:   329/  351]
Q_Learning:   330/  351]
Q_Learning:   331/  351]
Q_Learning:   332/  351]
Q_Learning:   333/  351]
Q_Learning:   334/  351]
Q_Learning:   335/  351]
Q_Learning:   336/  351]
Q_Learning:   337/  351]
Q_Learning:   338/  351]
Q_Learning:   339/  351]
Q_Learning:   340/  351]
Q_Learning:   341/  351]
Q_Learning:   342/  351]
Q_Learning:   343/  351]
Q_Learning:   344/  351]
Q_Learning:   345/  351]
Q_Learning:   346/  351]
Q_Learning:   347/  351]
Q_Learning:   348/  351]
Q_Learning:   349/  351]
Q_Learning:   350/  351]
Q_Learning:   351/  351]
Number of Samples after Autoencoder testing: 351
First Spike after testing: [-0.86740875 -1.6470071 ]
[2, 1, 0, 1, 1, 0, 2, 0, 2, 2, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 1, 0, 1, 2, 0, 1, 0, 1, 0, 0, 2, 2, 0, 1, 2, 2, 1, 0, 2, 1, 2, 2, 2, 0, 1, 1, 1, 1, 0, 2, 2, 0, 1, 2, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 2, 2, 2, 0, 2, 1, 0, 2, 0, 2, 1, 2, 2, 0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 2, 1, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 1, 2, 2, 2, 0, 1, 2, 1, 2, 1, 0, 1, 0, 0, 2, 0, 2, 2, 0, 1, 1, 0, 1, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 1, 0, 2, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1, 0, 2, 0, 1, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0, 0, 1, 2, 2, 2, 0, 2, 0, 2, 0, 2, 1, 0, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 2, 2, 1, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 0, 0, 2, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 1, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 1, 1, 2, 2, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 1, 2, 0, 0, 1, 0, 2, 0, 1, 0, 1]
[0, 1, 2, 1, 1, 2, 0, 2, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 2, 1, 0, 2, 1, 2, 1, 2, 2, 0, 0, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 1, 0, 2, 1, 2, 2, 1, 1, 1, 2, 0, 0, 0, 3, 0, 1, 2, 0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 2, 2, 1, 2, 2, 0, 1, 2, 0, 2, 0, 1, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 1, 0, 1, 2, 1, 2, 2, 0, 2, 2, 0, 2, 1, 1, 2, 1, 2, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 2, 3, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 1, 0, 2, 1, 1, 1, 2, 2, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 0, 2, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 2, 0, 1, 0, 2, 2, 0, 1, 2, 1, 2, 0, 2, 1, 2, 2, 2, 0, 2, 0, 2, 1, 0, 0, 2, 1, 1, 2, 2, 2, 0, 4, 0, 1, 2, 2, 2, 1, 0, 2, 0, 4, 2, 1, 1, 0, 0, 1, 1, 2, 1, 2, 2, 1, 0, 2, 2, 2, 2, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 0, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 2, 2, 1, 2, 0, 2, 1, 2, 1]
Centroids: [[-0.9887121, 0.8912765], [1.5223979, -0.10223387], [-0.7995157, -1.6621507]]
Centroids: [[-0.7965897, -1.6616175], [1.5328312, -0.11902786], [-0.98031014, 0.86802745], [0.30456692, 1.355947], [1.4027042, 1.961583]]
Contingency Matrix: 
[[  1   0 114   1   0]
 [  1 112   0   1   2]
 [118   0   1   0   0]]
[[-1, 0, 114, 1, 0], [-1, 112, 0, 1, 2], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, 112, -1, 1, 2], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {2: 0, 0: 2, 1: 1}
New Contingency Matrix: 
[[114   0   1   1   0]
 [  0 112   1   1   2]
 [  1   0 118   0   0]]
New Clustered Label Sequence: [2, 1, 0, 3, 4]
Diagonal_Elements: [114, 112, 118], Sum: 344
All_Elements: [114, 0, 1, 1, 0, 0, 112, 1, 1, 2, 1, 0, 118, 0, 0], Sum: 351
Accuracy: 0.98005698005698
Done!
