Experiment_path: Random_Seeds//V4/Experiment_04_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise005.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise005.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Random_Seeds//V4/Experiment_04_6/C_Difficult2_noise005.mat/Variant_04_Offline_Autoencoder_QLearning/2023_03_22-10_32_26
Punishment_Coefficient: 0.37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000233B56D5588>
Sampling rate: 24000.0
Raw: [ 0.02085333  0.02043967  0.02052644 ... -0.02218732 -0.02150573
 -0.01811243]
Times: [   1583    1934    2430 ... 1439313 1439656 1439854]
Cluster: [3 3 3 ... 2 2 1]
Number of different clusters:  3
Number of Spikes: 3364
First aligned Spike Frame: [-0.05170878 -0.0548761  -0.06029554 -0.06053219 -0.04807119 -0.02780025
 -0.01550543 -0.01702494 -0.02945104 -0.04493807 -0.07056858 -0.07003585
  0.07629654  0.43081562  0.80470191  0.96319627  0.89198123  0.73643948
  0.58987232  0.46714337  0.36345495  0.2828462   0.22743292  0.182731
  0.13931053  0.09524506  0.05136602  0.01367166 -0.01393093 -0.03985679
 -0.07387102 -0.11218435 -0.1444455  -0.16672578 -0.17809238 -0.18020802
 -0.17953732 -0.18246903 -0.18617363 -0.18205375 -0.17299738 -0.16958427
 -0.17248955 -0.17516876 -0.1727246  -0.16696514 -0.15993314]
Cluster 0, Occurrences: 1120
Cluster 1, Occurrences: 1109
Cluster 2, Occurrences: 1135
Train Index: 3028
x_train: 3028
y_train: 3028
x_test: 336
y_test: 336
<torch.utils.data.dataloader.DataLoader object at 0x00000234044E4BE0>
<torch.utils.data.dataloader.DataLoader object at 0x0000023402B98780>
Epoch 1
-------------------------------
loss: 0.117723  [    0/ 3028]
loss: 0.101655  [  100/ 3028]
loss: 0.076631  [  200/ 3028]
loss: 0.079634  [  300/ 3028]
loss: 0.004473  [  400/ 3028]
loss: 0.048630  [  500/ 3028]
loss: 0.032263  [  600/ 3028]
loss: 0.031681  [  700/ 3028]
loss: 0.018395  [  800/ 3028]
loss: 0.017982  [  900/ 3028]
loss: 0.008335  [ 1000/ 3028]
loss: 0.004486  [ 1100/ 3028]
loss: 0.010749  [ 1200/ 3028]
loss: 0.005949  [ 1300/ 3028]
loss: 0.089169  [ 1400/ 3028]
loss: 0.004950  [ 1500/ 3028]
loss: 0.006343  [ 1600/ 3028]
loss: 0.005958  [ 1700/ 3028]
loss: 0.002913  [ 1800/ 3028]
loss: 0.004950  [ 1900/ 3028]
loss: 0.004628  [ 2000/ 3028]
loss: 0.006073  [ 2100/ 3028]
loss: 0.005256  [ 2200/ 3028]
loss: 0.009184  [ 2300/ 3028]
loss: 0.001061  [ 2400/ 3028]
loss: 0.002616  [ 2500/ 3028]
loss: 0.001993  [ 2600/ 3028]
loss: 0.003049  [ 2700/ 3028]
loss: 0.003801  [ 2800/ 3028]
loss: 0.002721  [ 2900/ 3028]
loss: 0.003837  [ 3000/ 3028]
Epoch 2
-------------------------------
loss: 0.001366  [    0/ 3028]
loss: 0.002624  [  100/ 3028]
loss: 0.004738  [  200/ 3028]
loss: 0.000946  [  300/ 3028]
loss: 0.000804  [  400/ 3028]
loss: 0.037519  [  500/ 3028]
loss: 0.003968  [  600/ 3028]
loss: 0.006239  [  700/ 3028]
loss: 0.002919  [  800/ 3028]
loss: 0.005237  [  900/ 3028]
loss: 0.003279  [ 1000/ 3028]
loss: 0.003509  [ 1100/ 3028]
loss: 0.002610  [ 1200/ 3028]
loss: 0.005033  [ 1300/ 3028]
loss: 0.086890  [ 1400/ 3028]
loss: 0.003431  [ 1500/ 3028]
loss: 0.005948  [ 1600/ 3028]
loss: 0.002961  [ 1700/ 3028]
loss: 0.002006  [ 1800/ 3028]
loss: 0.003811  [ 1900/ 3028]
loss: 0.003174  [ 2000/ 3028]
loss: 0.004418  [ 2100/ 3028]
loss: 0.003591  [ 2200/ 3028]
loss: 0.008272  [ 2300/ 3028]
loss: 0.000781  [ 2400/ 3028]
loss: 0.002616  [ 2500/ 3028]
loss: 0.002569  [ 2600/ 3028]
loss: 0.002863  [ 2700/ 3028]
loss: 0.003664  [ 2800/ 3028]
loss: 0.001949  [ 2900/ 3028]
loss: 0.003934  [ 3000/ 3028]
Epoch 3
-------------------------------
loss: 0.001081  [    0/ 3028]
loss: 0.002640  [  100/ 3028]
loss: 0.004811  [  200/ 3028]
loss: 0.000873  [  300/ 3028]
loss: 0.000752  [  400/ 3028]
loss: 0.036011  [  500/ 3028]
loss: 0.003397  [  600/ 3028]
loss: 0.005003  [  700/ 3028]
loss: 0.002890  [  800/ 3028]
loss: 0.003908  [  900/ 3028]
loss: 0.002606  [ 1000/ 3028]
loss: 0.003503  [ 1100/ 3028]
loss: 0.002496  [ 1200/ 3028]
loss: 0.004818  [ 1300/ 3028]
loss: 0.085198  [ 1400/ 3028]
loss: 0.003332  [ 1500/ 3028]
loss: 0.006077  [ 1600/ 3028]
loss: 0.002500  [ 1700/ 3028]
loss: 0.001708  [ 1800/ 3028]
loss: 0.003686  [ 1900/ 3028]
loss: 0.002627  [ 2000/ 3028]
loss: 0.004223  [ 2100/ 3028]
loss: 0.003632  [ 2200/ 3028]
loss: 0.007683  [ 2300/ 3028]
loss: 0.000823  [ 2400/ 3028]
loss: 0.002599  [ 2500/ 3028]
loss: 0.002346  [ 2600/ 3028]
loss: 0.002814  [ 2700/ 3028]
loss: 0.003664  [ 2800/ 3028]
loss: 0.001807  [ 2900/ 3028]
loss: 0.003914  [ 3000/ 3028]
Epoch 4
-------------------------------
loss: 0.001089  [    0/ 3028]
loss: 0.002649  [  100/ 3028]
loss: 0.004806  [  200/ 3028]
loss: 0.000819  [  300/ 3028]
loss: 0.000706  [  400/ 3028]
loss: 0.033964  [  500/ 3028]
loss: 0.003150  [  600/ 3028]
loss: 0.004480  [  700/ 3028]
loss: 0.002871  [  800/ 3028]
loss: 0.003478  [  900/ 3028]
loss: 0.002418  [ 1000/ 3028]
loss: 0.003511  [ 1100/ 3028]
loss: 0.002465  [ 1200/ 3028]
loss: 0.004692  [ 1300/ 3028]
loss: 0.084368  [ 1400/ 3028]
loss: 0.003309  [ 1500/ 3028]
loss: 0.006153  [ 1600/ 3028]
loss: 0.002221  [ 1700/ 3028]
loss: 0.001615  [ 1800/ 3028]
loss: 0.003740  [ 1900/ 3028]
loss: 0.002501  [ 2000/ 3028]
loss: 0.004081  [ 2100/ 3028]
loss: 0.003705  [ 2200/ 3028]
loss: 0.007340  [ 2300/ 3028]
loss: 0.000814  [ 2400/ 3028]
loss: 0.002582  [ 2500/ 3028]
loss: 0.002265  [ 2600/ 3028]
loss: 0.002893  [ 2700/ 3028]
loss: 0.003712  [ 2800/ 3028]
loss: 0.001768  [ 2900/ 3028]
loss: 0.003955  [ 3000/ 3028]
Epoch 5
-------------------------------
loss: 0.001151  [    0/ 3028]
loss: 0.002635  [  100/ 3028]
loss: 0.004749  [  200/ 3028]
loss: 0.000817  [  300/ 3028]
loss: 0.000706  [  400/ 3028]
loss: 0.032690  [  500/ 3028]
loss: 0.003027  [  600/ 3028]
loss: 0.004428  [  700/ 3028]
loss: 0.002954  [  800/ 3028]
loss: 0.003282  [  900/ 3028]
loss: 0.002395  [ 1000/ 3028]
loss: 0.003500  [ 1100/ 3028]
loss: 0.002486  [ 1200/ 3028]
loss: 0.004593  [ 1300/ 3028]
loss: 0.083980  [ 1400/ 3028]
loss: 0.003372  [ 1500/ 3028]
loss: 0.006231  [ 1600/ 3028]
loss: 0.002162  [ 1700/ 3028]
loss: 0.001557  [ 1800/ 3028]
loss: 0.003849  [ 1900/ 3028]
loss: 0.002445  [ 2000/ 3028]
loss: 0.004167  [ 2100/ 3028]
loss: 0.003718  [ 2200/ 3028]
loss: 0.007192  [ 2300/ 3028]
loss: 0.000999  [ 2400/ 3028]
loss: 0.002747  [ 2500/ 3028]
loss: 0.002004  [ 2600/ 3028]
loss: 0.002889  [ 2700/ 3028]
loss: 0.003625  [ 2800/ 3028]
loss: 0.001773  [ 2900/ 3028]
loss: 0.003961  [ 3000/ 3028]
Epoch 6
-------------------------------
loss: 0.001129  [    0/ 3028]
loss: 0.002640  [  100/ 3028]
loss: 0.004824  [  200/ 3028]
loss: 0.000759  [  300/ 3028]
loss: 0.000713  [  400/ 3028]
loss: 0.031650  [  500/ 3028]
loss: 0.003007  [  600/ 3028]
loss: 0.004398  [  700/ 3028]
loss: 0.002709  [  800/ 3028]
loss: 0.003308  [  900/ 3028]
loss: 0.002365  [ 1000/ 3028]
loss: 0.003532  [ 1100/ 3028]
loss: 0.002407  [ 1200/ 3028]
loss: 0.004526  [ 1300/ 3028]
loss: 0.083640  [ 1400/ 3028]
loss: 0.003223  [ 1500/ 3028]
loss: 0.006215  [ 1600/ 3028]
loss: 0.002330  [ 1700/ 3028]
loss: 0.001537  [ 1800/ 3028]
loss: 0.003763  [ 1900/ 3028]
loss: 0.002470  [ 2000/ 3028]
loss: 0.004014  [ 2100/ 3028]
loss: 0.003678  [ 2200/ 3028]
loss: 0.007162  [ 2300/ 3028]
loss: 0.000875  [ 2400/ 3028]
loss: 0.002612  [ 2500/ 3028]
loss: 0.001903  [ 2600/ 3028]
loss: 0.002851  [ 2700/ 3028]
loss: 0.003633  [ 2800/ 3028]
loss: 0.001699  [ 2900/ 3028]
loss: 0.003978  [ 3000/ 3028]
Epoch 7
-------------------------------
loss: 0.001203  [    0/ 3028]
loss: 0.002628  [  100/ 3028]
loss: 0.004733  [  200/ 3028]
loss: 0.000814  [  300/ 3028]
loss: 0.000733  [  400/ 3028]
loss: 0.031281  [  500/ 3028]
loss: 0.002939  [  600/ 3028]
loss: 0.004443  [  700/ 3028]
loss: 0.002772  [  800/ 3028]
loss: 0.003177  [  900/ 3028]
loss: 0.002472  [ 1000/ 3028]
loss: 0.003521  [ 1100/ 3028]
loss: 0.002416  [ 1200/ 3028]
loss: 0.004454  [ 1300/ 3028]
loss: 0.083748  [ 1400/ 3028]
loss: 0.003330  [ 1500/ 3028]
loss: 0.006266  [ 1600/ 3028]
loss: 0.002222  [ 1700/ 3028]
loss: 0.001481  [ 1800/ 3028]
loss: 0.003884  [ 1900/ 3028]
loss: 0.002435  [ 2000/ 3028]
loss: 0.003995  [ 2100/ 3028]
loss: 0.003762  [ 2200/ 3028]
loss: 0.007089  [ 2300/ 3028]
loss: 0.000843  [ 2400/ 3028]
loss: 0.002649  [ 2500/ 3028]
loss: 0.002070  [ 2600/ 3028]
loss: 0.002739  [ 2700/ 3028]
loss: 0.003577  [ 2800/ 3028]
loss: 0.001732  [ 2900/ 3028]
loss: 0.004003  [ 3000/ 3028]
Epoch 8
-------------------------------
loss: 0.001193  [    0/ 3028]
loss: 0.002646  [  100/ 3028]
loss: 0.004626  [  200/ 3028]
loss: 0.000797  [  300/ 3028]
loss: 0.000720  [  400/ 3028]
loss: 0.030829  [  500/ 3028]
loss: 0.002877  [  600/ 3028]
loss: 0.004102  [  700/ 3028]
loss: 0.003059  [  800/ 3028]
loss: 0.003330  [  900/ 3028]
loss: 0.002430  [ 1000/ 3028]
loss: 0.003497  [ 1100/ 3028]
loss: 0.002445  [ 1200/ 3028]
loss: 0.004419  [ 1300/ 3028]
loss: 0.083663  [ 1400/ 3028]
loss: 0.003404  [ 1500/ 3028]
loss: 0.006179  [ 1600/ 3028]
loss: 0.002285  [ 1700/ 3028]
loss: 0.001452  [ 1800/ 3028]
loss: 0.003831  [ 1900/ 3028]
loss: 0.002445  [ 2000/ 3028]
loss: 0.004136  [ 2100/ 3028]
loss: 0.003740  [ 2200/ 3028]
loss: 0.007087  [ 2300/ 3028]
loss: 0.000866  [ 2400/ 3028]
loss: 0.002605  [ 2500/ 3028]
loss: 0.002178  [ 2600/ 3028]
loss: 0.002633  [ 2700/ 3028]
loss: 0.003577  [ 2800/ 3028]
loss: 0.001705  [ 2900/ 3028]
loss: 0.003968  [ 3000/ 3028]
Number of Clusters: 3
Q_Learning:     1/  336]
Q_Learning:     2/  336]
Q_Learning:     3/  336]
Q_Learning:     4/  336]
Q_Learning:     5/  336]
Q_Learning:     6/  336]
Q_Learning:     7/  336]
Q_Learning:     8/  336]
Q_Learning:     9/  336]
Q_Learning:    10/  336]
Q_Learning:    11/  336]
Q_Learning:    12/  336]
Q_Learning:    13/  336]
Q_Learning:    14/  336]
Q_Learning:    15/  336]
Q_Learning:    16/  336]
Q_Learning:    17/  336]
Q_Learning:    18/  336]
Q_Learning:    19/  336]
Q_Learning:    20/  336]
Q_Learning:    21/  336]
Q_Learning:    22/  336]
Q_Learning:    23/  336]
Q_Learning:    24/  336]
Q_Learning:    25/  336]
Q_Learning:    26/  336]
Q_Learning:    27/  336]
Q_Learning:    28/  336]
Q_Learning:    29/  336]
Q_Learning:    30/  336]
Q_Learning:    31/  336]
Q_Learning:    32/  336]
Q_Learning:    33/  336]
Q_Learning:    34/  336]
Q_Learning:    35/  336]
Q_Learning:    36/  336]
Q_Learning:    37/  336]
Q_Learning:    38/  336]
Q_Learning:    39/  336]
Q_Learning:    40/  336]
Q_Learning:    41/  336]
Q_Learning:    42/  336]
Q_Learning:    43/  336]
Q_Learning:    44/  336]
Q_Learning:    45/  336]
Q_Learning:    46/  336]
Q_Learning:    47/  336]
Q_Learning:    48/  336]
Q_Learning:    49/  336]
Q_Learning:    50/  336]
Q_Learning:    51/  336]
Q_Learning:    52/  336]
Q_Learning:    53/  336]
Q_Learning:    54/  336]
Q_Learning:    55/  336]
Q_Learning:    56/  336]
Q_Learning:    57/  336]
Q_Learning:    58/  336]
Q_Learning:    59/  336]
Q_Learning:    60/  336]
Q_Learning:    61/  336]
Q_Learning:    62/  336]
Q_Learning:    63/  336]
Q_Learning:    64/  336]
Q_Learning:    65/  336]
Q_Learning:    66/  336]
Q_Learning:    67/  336]
Q_Learning:    68/  336]
Q_Learning:    69/  336]
Q_Learning:    70/  336]
Q_Learning:    71/  336]
Q_Learning:    72/  336]
Q_Learning:    73/  336]
Q_Learning:    74/  336]
Q_Learning:    75/  336]
Q_Learning:    76/  336]
Q_Learning:    77/  336]
Q_Learning:    78/  336]
Q_Learning:    79/  336]
Q_Learning:    80/  336]
Q_Learning:    81/  336]
Q_Learning:    82/  336]
Q_Learning:    83/  336]
Q_Learning:    84/  336]
Q_Learning:    85/  336]
Q_Learning:    86/  336]
Q_Learning:    87/  336]
Q_Learning:    88/  336]
Q_Learning:    89/  336]
Q_Learning:    90/  336]
Q_Learning:    91/  336]
Q_Learning:    92/  336]
Q_Learning:    93/  336]
Q_Learning:    94/  336]
Q_Learning:    95/  336]
Q_Learning:    96/  336]
Q_Learning:    97/  336]
Q_Learning:    98/  336]
Q_Learning:    99/  336]
Q_Learning:   100/  336]
Q_Learning:   101/  336]
Q_Learning:   102/  336]
Q_Learning:   103/  336]
Q_Learning:   104/  336]
Q_Learning:   105/  336]
Q_Learning:   106/  336]
Q_Learning:   107/  336]
Q_Learning:   108/  336]
Q_Learning:   109/  336]
Q_Learning:   110/  336]
Q_Learning:   111/  336]
Q_Learning:   112/  336]
Q_Learning:   113/  336]
Q_Learning:   114/  336]
Q_Learning:   115/  336]
Q_Learning:   116/  336]
Q_Learning:   117/  336]
Q_Learning:   118/  336]
Q_Learning:   119/  336]
Q_Learning:   120/  336]
Q_Learning:   121/  336]
Q_Learning:   122/  336]
Q_Learning:   123/  336]
Q_Learning:   124/  336]
Q_Learning:   125/  336]
Q_Learning:   126/  336]
Q_Learning:   127/  336]
Q_Learning:   128/  336]
Q_Learning:   129/  336]
Q_Learning:   130/  336]
Q_Learning:   131/  336]
Q_Learning:   132/  336]
Q_Learning:   133/  336]
Q_Learning:   134/  336]
Q_Learning:   135/  336]
Q_Learning:   136/  336]
Q_Learning:   137/  336]
Q_Learning:   138/  336]
Q_Learning:   139/  336]
Q_Learning:   140/  336]
Q_Learning:   141/  336]
Q_Learning:   142/  336]
Q_Learning:   143/  336]
Q_Learning:   144/  336]
Q_Learning:   145/  336]
Q_Learning:   146/  336]
Q_Learning:   147/  336]
Q_Learning:   148/  336]
Q_Learning:   149/  336]
Q_Learning:   150/  336]
Q_Learning:   151/  336]
Q_Learning:   152/  336]
Q_Learning:   153/  336]
Q_Learning:   154/  336]
Q_Learning:   155/  336]
Q_Learning:   156/  336]
Q_Learning:   157/  336]
Q_Learning:   158/  336]
Q_Learning:   159/  336]
Q_Learning:   160/  336]
Q_Learning:   161/  336]
Q_Learning:   162/  336]
Q_Learning:   163/  336]
Q_Learning:   164/  336]
Q_Learning:   165/  336]
Q_Learning:   166/  336]
Q_Learning:   167/  336]
Q_Learning:   168/  336]
Q_Learning:   169/  336]
Q_Learning:   170/  336]
Q_Learning:   171/  336]
Q_Learning:   172/  336]
Q_Learning:   173/  336]
Q_Learning:   174/  336]
Q_Learning:   175/  336]
Q_Learning:   176/  336]
Q_Learning:   177/  336]
Q_Learning:   178/  336]
Q_Learning:   179/  336]
Q_Learning:   180/  336]
Q_Learning:   181/  336]
Q_Learning:   182/  336]
Q_Learning:   183/  336]
Q_Learning:   184/  336]
Q_Learning:   185/  336]
Q_Learning:   186/  336]
Q_Learning:   187/  336]
Q_Learning:   188/  336]
Q_Learning:   189/  336]
Q_Learning:   190/  336]
Q_Learning:   191/  336]
Q_Learning:   192/  336]
Q_Learning:   193/  336]
Q_Learning:   194/  336]
Q_Learning:   195/  336]
Q_Learning:   196/  336]
Q_Learning:   197/  336]
Q_Learning:   198/  336]
Q_Learning:   199/  336]
Q_Learning:   200/  336]
Q_Learning:   201/  336]
Q_Learning:   202/  336]
Q_Learning:   203/  336]
Q_Learning:   204/  336]
Q_Learning:   205/  336]
Q_Learning:   206/  336]
Q_Learning:   207/  336]
Q_Learning:   208/  336]
Q_Learning:   209/  336]
Q_Learning:   210/  336]
Q_Learning:   211/  336]
Q_Learning:   212/  336]
Q_Learning:   213/  336]
Q_Learning:   214/  336]
Q_Learning:   215/  336]
Q_Learning:   216/  336]
Q_Learning:   217/  336]
Q_Learning:   218/  336]
Q_Learning:   219/  336]
Q_Learning:   220/  336]
Q_Learning:   221/  336]
Q_Learning:   222/  336]
Q_Learning:   223/  336]
Q_Learning:   224/  336]
Q_Learning:   225/  336]
Q_Learning:   226/  336]
Q_Learning:   227/  336]
Q_Learning:   228/  336]
Q_Learning:   229/  336]
Q_Learning:   230/  336]
Q_Learning:   231/  336]
Q_Learning:   232/  336]
Q_Learning:   233/  336]
Q_Learning:   234/  336]
Q_Learning:   235/  336]
Q_Learning:   236/  336]
Q_Learning:   237/  336]
Q_Learning:   238/  336]
Q_Learning:   239/  336]
Q_Learning:   240/  336]
Q_Learning:   241/  336]
Q_Learning:   242/  336]
Q_Learning:   243/  336]
Q_Learning:   244/  336]
Q_Learning:   245/  336]
Q_Learning:   246/  336]
Q_Learning:   247/  336]
Q_Learning:   248/  336]
Q_Learning:   249/  336]
Q_Learning:   250/  336]
Q_Learning:   251/  336]
Q_Learning:   252/  336]
Q_Learning:   253/  336]
Q_Learning:   254/  336]
Q_Learning:   255/  336]
Q_Learning:   256/  336]
Q_Learning:   257/  336]
Q_Learning:   258/  336]
Q_Learning:   259/  336]
Q_Learning:   260/  336]
Q_Learning:   261/  336]
Q_Learning:   262/  336]
Q_Learning:   263/  336]
Q_Learning:   264/  336]
Q_Learning:   265/  336]
Q_Learning:   266/  336]
Q_Learning:   267/  336]
Q_Learning:   268/  336]
Q_Learning:   269/  336]
Q_Learning:   270/  336]
Q_Learning:   271/  336]
Q_Learning:   272/  336]
Q_Learning:   273/  336]
Q_Learning:   274/  336]
Q_Learning:   275/  336]
Q_Learning:   276/  336]
Q_Learning:   277/  336]
Q_Learning:   278/  336]
Q_Learning:   279/  336]
Q_Learning:   280/  336]
Q_Learning:   281/  336]
Q_Learning:   282/  336]
Q_Learning:   283/  336]
Q_Learning:   284/  336]
Q_Learning:   285/  336]
Q_Learning:   286/  336]
Q_Learning:   287/  336]
Q_Learning:   288/  336]
Q_Learning:   289/  336]
Q_Learning:   290/  336]
Q_Learning:   291/  336]
Q_Learning:   292/  336]
Q_Learning:   293/  336]
Q_Learning:   294/  336]
Q_Learning:   295/  336]
Q_Learning:   296/  336]
Q_Learning:   297/  336]
Q_Learning:   298/  336]
Q_Learning:   299/  336]
Q_Learning:   300/  336]
Q_Learning:   301/  336]
Q_Learning:   302/  336]
Q_Learning:   303/  336]
Q_Learning:   304/  336]
Q_Learning:   305/  336]
Q_Learning:   306/  336]
Q_Learning:   307/  336]
Q_Learning:   308/  336]
Q_Learning:   309/  336]
Q_Learning:   310/  336]
Q_Learning:   311/  336]
Q_Learning:   312/  336]
Q_Learning:   313/  336]
Q_Learning:   314/  336]
Q_Learning:   315/  336]
Q_Learning:   316/  336]
Q_Learning:   317/  336]
Q_Learning:   318/  336]
Q_Learning:   319/  336]
Q_Learning:   320/  336]
Q_Learning:   321/  336]
Q_Learning:   322/  336]
Q_Learning:   323/  336]
Q_Learning:   324/  336]
Q_Learning:   325/  336]
Q_Learning:   326/  336]
Q_Learning:   327/  336]
Q_Learning:   328/  336]
Q_Learning:   329/  336]
Q_Learning:   330/  336]
Q_Learning:   331/  336]
Q_Learning:   332/  336]
Q_Learning:   333/  336]
Q_Learning:   334/  336]
Q_Learning:   335/  336]
Q_Learning:   336/  336]
Number of Samples after Autoencoder testing: 336
First Spike after testing: [ 1.9331014 -2.5725253]
[0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 0, 2, 0, 1, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1, 0, 2, 1, 1, 1, 1, 2, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 1, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 0, 2, 2, 1, 2, 2, 0, 0, 1, 2, 2, 2, 1, 0, 0, 0, 2, 1, 2, 2, 0, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 2, 0, 2, 0, 0, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 0, 1, 0, 2, 2, 0, 2, 1, 2, 1, 2, 0, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 2, 0, 2, 1, 2, 1, 2, 0, 2, 0, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 1, 0, 2, 2, 1, 0, 1, 2, 0, 2, 2, 2, 2, 2, 1, 0, 1, 1, 0, 0, 2, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, 1, 2, 0, 1, 2, 0, 2, 0, 1, 2, 0, 0, 1, 0, 2, 1, 0, 2, 2, 1, 2, 0, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 0, 0, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 1, 2, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1, 1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 0]
[0, 1, 2, 3, 4, 3, 5, 3, 6, 7, 4, 7, 4, 7, 4, 7, 4, 3, 6, 0, 8, 4, 3, 4, 4, 6, 9, 4, 5, 4, 3, 7, 3, 10, 2, 7, 10, 6, 3, 6, 7, 7, 4, 9, 7, 4, 9, 2, 5, 6, 4, 3, 3, 8, 6, 3, 6, 4, 3, 4, 6, 11, 12, 3, 13, 9, 10, 9, 6, 13, 6, 4, 3, 3, 4, 4, 6, 4, 5, 14, 15, 9, 7, 7, 10, 16, 7, 2, 4, 10, 7, 7, 7, 3, 2, 4, 2, 13, 3, 5, 7, 9, 10, 2, 3, 3, 3, 6, 3, 7, 7, 9, 7, 4, 9, 2, 6, 7, 3, 5, 17, 9, 5, 2, 9, 2, 7, 5, 3, 13, 4, 2, 4, 7, 2, 7, 4, 6, 9, 5, 7, 4, 5, 3, 5, 6, 7, 2, 6, 1, 8, 7, 17, 4, 6, 7, 13, 3, 5, 6, 7, 5, 3, 2, 2, 3, 5, 5, 3, 2, 9, 6, 3, 4, 10, 3, 18, 9, 3, 5, 13, 4, 4, 4, 7, 3, 2, 4, 9, 6, 4, 5, 9, 7, 3, 7, 6, 5, 9, 7, 4, 10, 10, 13, 5, 5, 7, 6, 9, 7, 18, 12, 5, 6, 7, 2, 6, 16, 7, 5, 10, 9, 3, 13, 4, 7, 5, 5, 5, 13, 6, 4, 3, 19, 20, 4, 5, 2, 20, 7, 6, 6, 6, 2, 4, 2, 21, 9, 10, 22, 2, 2, 7, 6, 3, 7, 23, 11, 6, 5, 5, 24, 13, 7, 25, 11, 3, 10, 7, 2, 3, 5, 2, 8, 4, 6, 12, 2, 0, 1, 4, 7, 6, 4, 12, 25, 1, 5, 4, 4, 6, 4, 3, 5, 4, 6, 7, 6, 6, 4, 4, 2, 5, 13, 5, 18, 0, 7, 9, 5, 7, 4, 4, 18, 8, 2, 3, 7, 4, 4, 10, 7, 4, 3, 10, 6, 4, 7, 3, 6, 4, 22, 3, 6, 3, 4]
Centroids: [[3.3995337, -4.821105], [-0.6570796, -5.078174], [2.240255, -6.020852]]
Centroids: [[2.1534078, -2.380517], [-1.4338543, -3.2194898], [3.5470142, -5.447249], [-0.7220595, -5.026283], [3.4620333, -4.904755], [2.1038256, -5.7996006], [-0.4203805, -5.672947], [2.33596, -6.418142], [1.4500606, -4.185379], [3.246793, -4.2015257], [-0.9608999, -4.3884883], [2.8839588, -3.0627594], [1.7758623, -4.9084816], [2.514107, -7.0600734], [0.3927741, -10.41222], [5.391131, -11.96906], [3.95813, -6.640937], [1.906792, -4.891365], [-1.3651187, -2.2669554], [-1.2058783, -6.7402515], [2.4751625, -7.222201], [-0.6256973, -2.7476366], [1.5679197, -3.2126992], [0.86349726, -1.9800032], [2.3460867, -8.109846], [1.1508508, -2.6893451]]
Contingency Matrix: 
[[ 4  0 28  0 55  0  0  0  0 19  0  3  0  0  0  0  1  0  0  0  1  0  1  0
   1  0]
 [ 0  4  0 44  0  0 40  0  0  0 14  0  0  1  1  0  0  0  4  1  1  1  1  0
   0  0]
 [ 0  0  0  0  0 34  0 49  5  2  0  0  4 10  0  1  1  2  0  0  0  0  0  1
   0  2]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [0, 4, 0, 44, -1, 0, 40, 0, 0, 0, 14, 0, 0, 1, 1, 0, 0, 0, 4, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, -1, 34, 0, 49, 5, 2, 0, 0, 4, 10, 0, 1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 2]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [0, 4, 0, 44, -1, 0, 40, -1, 0, 0, 14, 0, 0, 1, 1, 0, 0, 0, 4, 1, 1, 1, 1, 0, 0, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {0: 4, 2: 7, 1: 3}
New Contingency Matrix: 
[[55  0  0  4  0 28  0  0  0 19  0  3  0  0  0  0  1  0  0  0  1  0  1  0
   1  0]
 [ 0 44  0  0  4  0  0 40  0  0 14  0  0  1  1  0  0  0  4  1  1  1  1  0
   0  0]
 [ 0  0 49  0  0  0 34  0  5  2  0  0  4 10  0  1  1  2  0  0  0  0  0  1
   0  2]]
New Clustered Label Sequence: [4, 3, 7, 0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
Diagonal_Elements: [55, 44, 49], Sum: 148
All_Elements: [55, 0, 0, 4, 0, 28, 0, 0, 0, 19, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 44, 0, 0, 4, 0, 0, 40, 0, 0, 14, 0, 0, 1, 1, 0, 0, 0, 4, 1, 1, 1, 1, 0, 0, 0, 0, 0, 49, 0, 0, 0, 34, 0, 5, 2, 0, 0, 4, 10, 0, 1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 2], Sum: 336
Accuracy: 0.44047619047619047
Done!
