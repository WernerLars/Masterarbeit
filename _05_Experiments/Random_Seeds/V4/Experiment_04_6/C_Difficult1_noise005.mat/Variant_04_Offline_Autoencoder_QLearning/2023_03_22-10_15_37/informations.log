Experiment_path: Random_Seeds//V4/Experiment_04_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise005.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise005.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Random_Seeds//V4/Experiment_04_6/C_Difficult1_noise005.mat/Variant_04_Offline_Autoencoder_QLearning/2023_03_22-10_15_37
Punishment_Coefficient: 0.32
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000233B9BD1630>
Sampling rate: 24000.0
Raw: [-0.02396372 -0.02524464 -0.02236968 ... -0.00445509 -0.00436778
 -0.00470578]
Times: [    634     868    2584 ... 1437994 1438740 1439460]
Cluster: [3 2 3 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3383
First aligned Spike Frame: [ 0.00503762 -0.00373478 -0.02417005 -0.05492281 -0.07823403 -0.07649548
 -0.06285267 -0.06865366 -0.09676273 -0.11004904 -0.09516198 -0.02689536
  0.18218225  0.56508663  0.95357316  1.00263054  0.57634096 -0.04324787
 -0.47305592 -0.6155027  -0.61852552 -0.60964372 -0.60484482 -0.57289026
 -0.52334621 -0.49235523 -0.47468281 -0.4416077  -0.40763637 -0.38725194
 -0.36627613 -0.33462257 -0.30781191 -0.30310449 -0.30176569 -0.28764362
 -0.27487686 -0.27588822 -0.27512317 -0.25186462 -0.21649826 -0.18877803
 -0.16831802 -0.15216626 -0.15550926 -0.17919117 -0.19056035]
Cluster 0, Occurrences: 1115
Cluster 1, Occurrences: 1113
Cluster 2, Occurrences: 1155
Train Index: 3045
x_train: 3045
y_train: 3045
x_test: 338
y_test: 338
<torch.utils.data.dataloader.DataLoader object at 0x00000233C17DAE10>
<torch.utils.data.dataloader.DataLoader object at 0x00000233B892C940>
Epoch 1
-------------------------------
loss: 0.182723  [    0/ 3045]
loss: 0.075493  [  100/ 3045]
loss: 0.045127  [  200/ 3045]
loss: 0.035977  [  300/ 3045]
loss: 0.011107  [  400/ 3045]
loss: 0.012525  [  500/ 3045]
loss: 0.012736  [  600/ 3045]
loss: 0.018485  [  700/ 3045]
loss: 0.010530  [  800/ 3045]
loss: 0.012727  [  900/ 3045]
loss: 0.112264  [ 1000/ 3045]
loss: 0.057439  [ 1100/ 3045]
loss: 0.033715  [ 1200/ 3045]
loss: 0.014814  [ 1300/ 3045]
loss: 0.007954  [ 1400/ 3045]
loss: 0.015161  [ 1500/ 3045]
loss: 0.010095  [ 1600/ 3045]
loss: 0.002920  [ 1700/ 3045]
loss: 0.013802  [ 1800/ 3045]
loss: 0.015315  [ 1900/ 3045]
loss: 0.009888  [ 2000/ 3045]
loss: 0.004424  [ 2100/ 3045]
loss: 0.013020  [ 2200/ 3045]
loss: 0.014140  [ 2300/ 3045]
loss: 0.004143  [ 2400/ 3045]
loss: 0.029664  [ 2500/ 3045]
loss: 0.009830  [ 2600/ 3045]
loss: 0.002622  [ 2700/ 3045]
loss: 0.005776  [ 2800/ 3045]
loss: 0.005844  [ 2900/ 3045]
loss: 0.005002  [ 3000/ 3045]
Epoch 2
-------------------------------
loss: 0.007548  [    0/ 3045]
loss: 0.010431  [  100/ 3045]
loss: 0.009067  [  200/ 3045]
loss: 0.005737  [  300/ 3045]
loss: 0.002253  [  400/ 3045]
loss: 0.008254  [  500/ 3045]
loss: 0.006449  [  600/ 3045]
loss: 0.014407  [  700/ 3045]
loss: 0.008005  [  800/ 3045]
loss: 0.002913  [  900/ 3045]
loss: 0.094342  [ 1000/ 3045]
loss: 0.058863  [ 1100/ 3045]
loss: 0.026119  [ 1200/ 3045]
loss: 0.012713  [ 1300/ 3045]
loss: 0.004560  [ 1400/ 3045]
loss: 0.013464  [ 1500/ 3045]
loss: 0.001834  [ 1600/ 3045]
loss: 0.002079  [ 1700/ 3045]
loss: 0.010071  [ 1800/ 3045]
loss: 0.013885  [ 1900/ 3045]
loss: 0.004223  [ 2000/ 3045]
loss: 0.003787  [ 2100/ 3045]
loss: 0.007474  [ 2200/ 3045]
loss: 0.004445  [ 2300/ 3045]
loss: 0.003729  [ 2400/ 3045]
loss: 0.025765  [ 2500/ 3045]
loss: 0.001049  [ 2600/ 3045]
loss: 0.002773  [ 2700/ 3045]
loss: 0.006231  [ 2800/ 3045]
loss: 0.005949  [ 2900/ 3045]
loss: 0.004467  [ 3000/ 3045]
Epoch 3
-------------------------------
loss: 0.004563  [    0/ 3045]
loss: 0.005985  [  100/ 3045]
loss: 0.004311  [  200/ 3045]
loss: 0.003375  [  300/ 3045]
loss: 0.002372  [  400/ 3045]
loss: 0.004004  [  500/ 3045]
loss: 0.001074  [  600/ 3045]
loss: 0.002178  [  700/ 3045]
loss: 0.007753  [  800/ 3045]
loss: 0.002140  [  900/ 3045]
loss: 0.093361  [ 1000/ 3045]
loss: 0.057648  [ 1100/ 3045]
loss: 0.025400  [ 1200/ 3045]
loss: 0.009256  [ 1300/ 3045]
loss: 0.004017  [ 1400/ 3045]
loss: 0.014459  [ 1500/ 3045]
loss: 0.002560  [ 1600/ 3045]
loss: 0.001613  [ 1700/ 3045]
loss: 0.011683  [ 1800/ 3045]
loss: 0.014106  [ 1900/ 3045]
loss: 0.003069  [ 2000/ 3045]
loss: 0.003377  [ 2100/ 3045]
loss: 0.007971  [ 2200/ 3045]
loss: 0.001720  [ 2300/ 3045]
loss: 0.003741  [ 2400/ 3045]
loss: 0.024917  [ 2500/ 3045]
loss: 0.001043  [ 2600/ 3045]
loss: 0.002852  [ 2700/ 3045]
loss: 0.006211  [ 2800/ 3045]
loss: 0.006109  [ 2900/ 3045]
loss: 0.003771  [ 3000/ 3045]
Epoch 4
-------------------------------
loss: 0.003930  [    0/ 3045]
loss: 0.005907  [  100/ 3045]
loss: 0.005139  [  200/ 3045]
loss: 0.002708  [  300/ 3045]
loss: 0.002501  [  400/ 3045]
loss: 0.004263  [  500/ 3045]
loss: 0.001051  [  600/ 3045]
loss: 0.001786  [  700/ 3045]
loss: 0.007455  [  800/ 3045]
loss: 0.001542  [  900/ 3045]
loss: 0.091983  [ 1000/ 3045]
loss: 0.057787  [ 1100/ 3045]
loss: 0.024644  [ 1200/ 3045]
loss: 0.009356  [ 1300/ 3045]
loss: 0.004110  [ 1400/ 3045]
loss: 0.014838  [ 1500/ 3045]
loss: 0.002562  [ 1600/ 3045]
loss: 0.001592  [ 1700/ 3045]
loss: 0.011470  [ 1800/ 3045]
loss: 0.014029  [ 1900/ 3045]
loss: 0.003045  [ 2000/ 3045]
loss: 0.003386  [ 2100/ 3045]
loss: 0.008261  [ 2200/ 3045]
loss: 0.001721  [ 2300/ 3045]
loss: 0.003767  [ 2400/ 3045]
loss: 0.023648  [ 2500/ 3045]
loss: 0.000993  [ 2600/ 3045]
loss: 0.002874  [ 2700/ 3045]
loss: 0.006283  [ 2800/ 3045]
loss: 0.006230  [ 2900/ 3045]
loss: 0.003172  [ 3000/ 3045]
Epoch 5
-------------------------------
loss: 0.003834  [    0/ 3045]
loss: 0.005975  [  100/ 3045]
loss: 0.005672  [  200/ 3045]
loss: 0.002511  [  300/ 3045]
loss: 0.002550  [  400/ 3045]
loss: 0.004320  [  500/ 3045]
loss: 0.001069  [  600/ 3045]
loss: 0.001736  [  700/ 3045]
loss: 0.007341  [  800/ 3045]
loss: 0.001417  [  900/ 3045]
loss: 0.091468  [ 1000/ 3045]
loss: 0.057836  [ 1100/ 3045]
loss: 0.024472  [ 1200/ 3045]
loss: 0.009377  [ 1300/ 3045]
loss: 0.004119  [ 1400/ 3045]
loss: 0.014919  [ 1500/ 3045]
loss: 0.002556  [ 1600/ 3045]
loss: 0.001581  [ 1700/ 3045]
loss: 0.011369  [ 1800/ 3045]
loss: 0.013997  [ 1900/ 3045]
loss: 0.003049  [ 2000/ 3045]
loss: 0.003405  [ 2100/ 3045]
loss: 0.008337  [ 2200/ 3045]
loss: 0.001734  [ 2300/ 3045]
loss: 0.003770  [ 2400/ 3045]
loss: 0.023277  [ 2500/ 3045]
loss: 0.000975  [ 2600/ 3045]
loss: 0.002850  [ 2700/ 3045]
loss: 0.006314  [ 2800/ 3045]
loss: 0.006292  [ 2900/ 3045]
loss: 0.002974  [ 3000/ 3045]
Epoch 6
-------------------------------
loss: 0.003818  [    0/ 3045]
loss: 0.005956  [  100/ 3045]
loss: 0.005389  [  200/ 3045]
loss: 0.002438  [  300/ 3045]
loss: 0.002578  [  400/ 3045]
loss: 0.004322  [  500/ 3045]
loss: 0.001074  [  600/ 3045]
loss: 0.001731  [  700/ 3045]
loss: 0.007296  [  800/ 3045]
loss: 0.001359  [  900/ 3045]
loss: 0.091307  [ 1000/ 3045]
loss: 0.057842  [ 1100/ 3045]
loss: 0.024422  [ 1200/ 3045]
loss: 0.009370  [ 1300/ 3045]
loss: 0.004121  [ 1400/ 3045]
loss: 0.014941  [ 1500/ 3045]
loss: 0.002553  [ 1600/ 3045]
loss: 0.001574  [ 1700/ 3045]
loss: 0.011307  [ 1800/ 3045]
loss: 0.013993  [ 1900/ 3045]
loss: 0.003044  [ 2000/ 3045]
loss: 0.003416  [ 2100/ 3045]
loss: 0.008373  [ 2200/ 3045]
loss: 0.001743  [ 2300/ 3045]
loss: 0.003926  [ 2400/ 3045]
loss: 0.023767  [ 2500/ 3045]
loss: 0.001048  [ 2600/ 3045]
loss: 0.002817  [ 2700/ 3045]
loss: 0.006189  [ 2800/ 3045]
loss: 0.006302  [ 2900/ 3045]
loss: 0.002957  [ 3000/ 3045]
Epoch 7
-------------------------------
loss: 0.003792  [    0/ 3045]
loss: 0.005940  [  100/ 3045]
loss: 0.005400  [  200/ 3045]
loss: 0.002427  [  300/ 3045]
loss: 0.002607  [  400/ 3045]
loss: 0.004323  [  500/ 3045]
loss: 0.001044  [  600/ 3045]
loss: 0.001752  [  700/ 3045]
loss: 0.007313  [  800/ 3045]
loss: 0.001376  [  900/ 3045]
loss: 0.091455  [ 1000/ 3045]
loss: 0.057812  [ 1100/ 3045]
loss: 0.024175  [ 1200/ 3045]
loss: 0.009351  [ 1300/ 3045]
loss: 0.004168  [ 1400/ 3045]
loss: 0.014952  [ 1500/ 3045]
loss: 0.002554  [ 1600/ 3045]
loss: 0.001573  [ 1700/ 3045]
loss: 0.011333  [ 1800/ 3045]
loss: 0.013929  [ 1900/ 3045]
loss: 0.003043  [ 2000/ 3045]
loss: 0.003406  [ 2100/ 3045]
loss: 0.008418  [ 2200/ 3045]
loss: 0.001731  [ 2300/ 3045]
loss: 0.003727  [ 2400/ 3045]
loss: 0.022218  [ 2500/ 3045]
loss: 0.000965  [ 2600/ 3045]
loss: 0.002815  [ 2700/ 3045]
loss: 0.006225  [ 2800/ 3045]
loss: 0.006295  [ 2900/ 3045]
loss: 0.002928  [ 3000/ 3045]
Epoch 8
-------------------------------
loss: 0.003795  [    0/ 3045]
loss: 0.005930  [  100/ 3045]
loss: 0.005405  [  200/ 3045]
loss: 0.002410  [  300/ 3045]
loss: 0.002590  [  400/ 3045]
loss: 0.004320  [  500/ 3045]
loss: 0.001061  [  600/ 3045]
loss: 0.001750  [  700/ 3045]
loss: 0.007296  [  800/ 3045]
loss: 0.001368  [  900/ 3045]
loss: 0.091459  [ 1000/ 3045]
loss: 0.057804  [ 1100/ 3045]
loss: 0.024329  [ 1200/ 3045]
loss: 0.009348  [ 1300/ 3045]
loss: 0.004144  [ 1400/ 3045]
loss: 0.014969  [ 1500/ 3045]
loss: 0.002548  [ 1600/ 3045]
loss: 0.001574  [ 1700/ 3045]
loss: 0.011300  [ 1800/ 3045]
loss: 0.013982  [ 1900/ 3045]
loss: 0.003016  [ 2000/ 3045]
loss: 0.003422  [ 2100/ 3045]
loss: 0.008367  [ 2200/ 3045]
loss: 0.001745  [ 2300/ 3045]
loss: 0.003794  [ 2400/ 3045]
loss: 0.022342  [ 2500/ 3045]
loss: 0.000977  [ 2600/ 3045]
loss: 0.002799  [ 2700/ 3045]
loss: 0.006203  [ 2800/ 3045]
loss: 0.006287  [ 2900/ 3045]
loss: 0.002901  [ 3000/ 3045]
Number of Clusters: 3
Q_Learning:     1/  338]
Q_Learning:     2/  338]
Q_Learning:     3/  338]
Q_Learning:     4/  338]
Q_Learning:     5/  338]
Q_Learning:     6/  338]
Q_Learning:     7/  338]
Q_Learning:     8/  338]
Q_Learning:     9/  338]
Q_Learning:    10/  338]
Q_Learning:    11/  338]
Q_Learning:    12/  338]
Q_Learning:    13/  338]
Q_Learning:    14/  338]
Q_Learning:    15/  338]
Q_Learning:    16/  338]
Q_Learning:    17/  338]
Q_Learning:    18/  338]
Q_Learning:    19/  338]
Q_Learning:    20/  338]
Q_Learning:    21/  338]
Q_Learning:    22/  338]
Q_Learning:    23/  338]
Q_Learning:    24/  338]
Q_Learning:    25/  338]
Q_Learning:    26/  338]
Q_Learning:    27/  338]
Q_Learning:    28/  338]
Q_Learning:    29/  338]
Q_Learning:    30/  338]
Q_Learning:    31/  338]
Q_Learning:    32/  338]
Q_Learning:    33/  338]
Q_Learning:    34/  338]
Q_Learning:    35/  338]
Q_Learning:    36/  338]
Q_Learning:    37/  338]
Q_Learning:    38/  338]
Q_Learning:    39/  338]
Q_Learning:    40/  338]
Q_Learning:    41/  338]
Q_Learning:    42/  338]
Q_Learning:    43/  338]
Q_Learning:    44/  338]
Q_Learning:    45/  338]
Q_Learning:    46/  338]
Q_Learning:    47/  338]
Q_Learning:    48/  338]
Q_Learning:    49/  338]
Q_Learning:    50/  338]
Q_Learning:    51/  338]
Q_Learning:    52/  338]
Q_Learning:    53/  338]
Q_Learning:    54/  338]
Q_Learning:    55/  338]
Q_Learning:    56/  338]
Q_Learning:    57/  338]
Q_Learning:    58/  338]
Q_Learning:    59/  338]
Q_Learning:    60/  338]
Q_Learning:    61/  338]
Q_Learning:    62/  338]
Q_Learning:    63/  338]
Q_Learning:    64/  338]
Q_Learning:    65/  338]
Q_Learning:    66/  338]
Q_Learning:    67/  338]
Q_Learning:    68/  338]
Q_Learning:    69/  338]
Q_Learning:    70/  338]
Q_Learning:    71/  338]
Q_Learning:    72/  338]
Q_Learning:    73/  338]
Q_Learning:    74/  338]
Q_Learning:    75/  338]
Q_Learning:    76/  338]
Q_Learning:    77/  338]
Q_Learning:    78/  338]
Q_Learning:    79/  338]
Q_Learning:    80/  338]
Q_Learning:    81/  338]
Q_Learning:    82/  338]
Q_Learning:    83/  338]
Q_Learning:    84/  338]
Q_Learning:    85/  338]
Q_Learning:    86/  338]
Q_Learning:    87/  338]
Q_Learning:    88/  338]
Q_Learning:    89/  338]
Q_Learning:    90/  338]
Q_Learning:    91/  338]
Q_Learning:    92/  338]
Q_Learning:    93/  338]
Q_Learning:    94/  338]
Q_Learning:    95/  338]
Q_Learning:    96/  338]
Q_Learning:    97/  338]
Q_Learning:    98/  338]
Q_Learning:    99/  338]
Q_Learning:   100/  338]
Q_Learning:   101/  338]
Q_Learning:   102/  338]
Q_Learning:   103/  338]
Q_Learning:   104/  338]
Q_Learning:   105/  338]
Q_Learning:   106/  338]
Q_Learning:   107/  338]
Q_Learning:   108/  338]
Q_Learning:   109/  338]
Q_Learning:   110/  338]
Q_Learning:   111/  338]
Q_Learning:   112/  338]
Q_Learning:   113/  338]
Q_Learning:   114/  338]
Q_Learning:   115/  338]
Q_Learning:   116/  338]
Q_Learning:   117/  338]
Q_Learning:   118/  338]
Q_Learning:   119/  338]
Q_Learning:   120/  338]
Q_Learning:   121/  338]
Q_Learning:   122/  338]
Q_Learning:   123/  338]
Q_Learning:   124/  338]
Q_Learning:   125/  338]
Q_Learning:   126/  338]
Q_Learning:   127/  338]
Q_Learning:   128/  338]
Q_Learning:   129/  338]
Q_Learning:   130/  338]
Q_Learning:   131/  338]
Q_Learning:   132/  338]
Q_Learning:   133/  338]
Q_Learning:   134/  338]
Q_Learning:   135/  338]
Q_Learning:   136/  338]
Q_Learning:   137/  338]
Q_Learning:   138/  338]
Q_Learning:   139/  338]
Q_Learning:   140/  338]
Q_Learning:   141/  338]
Q_Learning:   142/  338]
Q_Learning:   143/  338]
Q_Learning:   144/  338]
Q_Learning:   145/  338]
Q_Learning:   146/  338]
Q_Learning:   147/  338]
Q_Learning:   148/  338]
Q_Learning:   149/  338]
Q_Learning:   150/  338]
Q_Learning:   151/  338]
Q_Learning:   152/  338]
Q_Learning:   153/  338]
Q_Learning:   154/  338]
Q_Learning:   155/  338]
Q_Learning:   156/  338]
Q_Learning:   157/  338]
Q_Learning:   158/  338]
Q_Learning:   159/  338]
Q_Learning:   160/  338]
Q_Learning:   161/  338]
Q_Learning:   162/  338]
Q_Learning:   163/  338]
Q_Learning:   164/  338]
Q_Learning:   165/  338]
Q_Learning:   166/  338]
Q_Learning:   167/  338]
Q_Learning:   168/  338]
Q_Learning:   169/  338]
Q_Learning:   170/  338]
Q_Learning:   171/  338]
Q_Learning:   172/  338]
Q_Learning:   173/  338]
Q_Learning:   174/  338]
Q_Learning:   175/  338]
Q_Learning:   176/  338]
Q_Learning:   177/  338]
Q_Learning:   178/  338]
Q_Learning:   179/  338]
Q_Learning:   180/  338]
Q_Learning:   181/  338]
Q_Learning:   182/  338]
Q_Learning:   183/  338]
Q_Learning:   184/  338]
Q_Learning:   185/  338]
Q_Learning:   186/  338]
Q_Learning:   187/  338]
Q_Learning:   188/  338]
Q_Learning:   189/  338]
Q_Learning:   190/  338]
Q_Learning:   191/  338]
Q_Learning:   192/  338]
Q_Learning:   193/  338]
Q_Learning:   194/  338]
Q_Learning:   195/  338]
Q_Learning:   196/  338]
Q_Learning:   197/  338]
Q_Learning:   198/  338]
Q_Learning:   199/  338]
Q_Learning:   200/  338]
Q_Learning:   201/  338]
Q_Learning:   202/  338]
Q_Learning:   203/  338]
Q_Learning:   204/  338]
Q_Learning:   205/  338]
Q_Learning:   206/  338]
Q_Learning:   207/  338]
Q_Learning:   208/  338]
Q_Learning:   209/  338]
Q_Learning:   210/  338]
Q_Learning:   211/  338]
Q_Learning:   212/  338]
Q_Learning:   213/  338]
Q_Learning:   214/  338]
Q_Learning:   215/  338]
Q_Learning:   216/  338]
Q_Learning:   217/  338]
Q_Learning:   218/  338]
Q_Learning:   219/  338]
Q_Learning:   220/  338]
Q_Learning:   221/  338]
Q_Learning:   222/  338]
Q_Learning:   223/  338]
Q_Learning:   224/  338]
Q_Learning:   225/  338]
Q_Learning:   226/  338]
Q_Learning:   227/  338]
Q_Learning:   228/  338]
Q_Learning:   229/  338]
Q_Learning:   230/  338]
Q_Learning:   231/  338]
Q_Learning:   232/  338]
Q_Learning:   233/  338]
Q_Learning:   234/  338]
Q_Learning:   235/  338]
Q_Learning:   236/  338]
Q_Learning:   237/  338]
Q_Learning:   238/  338]
Q_Learning:   239/  338]
Q_Learning:   240/  338]
Q_Learning:   241/  338]
Q_Learning:   242/  338]
Q_Learning:   243/  338]
Q_Learning:   244/  338]
Q_Learning:   245/  338]
Q_Learning:   246/  338]
Q_Learning:   247/  338]
Q_Learning:   248/  338]
Q_Learning:   249/  338]
Q_Learning:   250/  338]
Q_Learning:   251/  338]
Q_Learning:   252/  338]
Q_Learning:   253/  338]
Q_Learning:   254/  338]
Q_Learning:   255/  338]
Q_Learning:   256/  338]
Q_Learning:   257/  338]
Q_Learning:   258/  338]
Q_Learning:   259/  338]
Q_Learning:   260/  338]
Q_Learning:   261/  338]
Q_Learning:   262/  338]
Q_Learning:   263/  338]
Q_Learning:   264/  338]
Q_Learning:   265/  338]
Q_Learning:   266/  338]
Q_Learning:   267/  338]
Q_Learning:   268/  338]
Q_Learning:   269/  338]
Q_Learning:   270/  338]
Q_Learning:   271/  338]
Q_Learning:   272/  338]
Q_Learning:   273/  338]
Q_Learning:   274/  338]
Q_Learning:   275/  338]
Q_Learning:   276/  338]
Q_Learning:   277/  338]
Q_Learning:   278/  338]
Q_Learning:   279/  338]
Q_Learning:   280/  338]
Q_Learning:   281/  338]
Q_Learning:   282/  338]
Q_Learning:   283/  338]
Q_Learning:   284/  338]
Q_Learning:   285/  338]
Q_Learning:   286/  338]
Q_Learning:   287/  338]
Q_Learning:   288/  338]
Q_Learning:   289/  338]
Q_Learning:   290/  338]
Q_Learning:   291/  338]
Q_Learning:   292/  338]
Q_Learning:   293/  338]
Q_Learning:   294/  338]
Q_Learning:   295/  338]
Q_Learning:   296/  338]
Q_Learning:   297/  338]
Q_Learning:   298/  338]
Q_Learning:   299/  338]
Q_Learning:   300/  338]
Q_Learning:   301/  338]
Q_Learning:   302/  338]
Q_Learning:   303/  338]
Q_Learning:   304/  338]
Q_Learning:   305/  338]
Q_Learning:   306/  338]
Q_Learning:   307/  338]
Q_Learning:   308/  338]
Q_Learning:   309/  338]
Q_Learning:   310/  338]
Q_Learning:   311/  338]
Q_Learning:   312/  338]
Q_Learning:   313/  338]
Q_Learning:   314/  338]
Q_Learning:   315/  338]
Q_Learning:   316/  338]
Q_Learning:   317/  338]
Q_Learning:   318/  338]
Q_Learning:   319/  338]
Q_Learning:   320/  338]
Q_Learning:   321/  338]
Q_Learning:   322/  338]
Q_Learning:   323/  338]
Q_Learning:   324/  338]
Q_Learning:   325/  338]
Q_Learning:   326/  338]
Q_Learning:   327/  338]
Q_Learning:   328/  338]
Q_Learning:   329/  338]
Q_Learning:   330/  338]
Q_Learning:   331/  338]
Q_Learning:   332/  338]
Q_Learning:   333/  338]
Q_Learning:   334/  338]
Q_Learning:   335/  338]
Q_Learning:   336/  338]
Q_Learning:   337/  338]
Q_Learning:   338/  338]
Number of Samples after Autoencoder testing: 338
First Spike after testing: [ 0.05713964 -4.8911943 ]
[0, 1, 2, 2, 1, 1, 2, 0, 2, 2, 2, 0, 1, 1, 1, 2, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1, 2, 2, 2, 1, 2, 0, 2, 0, 2, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 1, 2, 2, 2, 0, 0, 2, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 2, 0, 2, 0, 2, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 2, 1, 1, 2, 0, 2, 1, 0, 0, 0, 2, 1, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 2, 2, 1, 0, 0, 2, 2, 0, 1, 0, 2, 2, 2, 0, 2, 0, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 1, 2, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 2, 1, 2, 1, 0, 2, 2, 0, 1, 2, 0, 1, 0, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 0, 1, 0, 1, 2, 2, 1, 2, 2, 1, 2, 0, 2, 2, 1, 2, 0, 1, 0, 2, 0, 1, 2]
[0, 1, 2, 2, 2, 3, 2, 4, 5, 6, 2, 7, 3, 8, 9, 6, 10, 2, 7, 4, 0, 9, 11, 10, 12, 7, 4, 0, 13, 13, 11, 0, 3, 5, 14, 11, 11, 13, 9, 6, 9, 0, 0, 4, 6, 9, 3, 14, 9, 1, 0, 0, 3, 9, 14, 14, 2, 1, 14, 13, 5, 15, 6, 7, 13, 14, 8, 0, 2, 0, 0, 16, 16, 6, 0, 11, 6, 9, 3, 7, 5, 4, 13, 6, 2, 9, 13, 9, 7, 7, 0, 9, 14, 2, 2, 7, 0, 14, 14, 17, 18, 3, 0, 3, 4, 14, 7, 9, 19, 17, 10, 2, 20, 2, 2, 0, 13, 0, 21, 5, 11, 3, 6, 7, 14, 13, 2, 6, 2, 0, 7, 3, 7, 4, 7, 0, 7, 14, 10, 11, 3, 22, 0, 0, 11, 8, 2, 11, 11, 6, 3, 9, 5, 7, 6, 19, 0, 23, 0, 6, 3, 8, 24, 6, 19, 14, 5, 0, 11, 3, 7, 3, 3, 6, 2, 5, 9, 11, 7, 2, 25, 26, 10, 20, 6, 19, 14, 7, 27, 27, 5, 6, 9, 6, 3, 9, 2, 10, 5, 6, 9, 2, 6, 6, 6, 5, 1, 9, 13, 12, 11, 11, 28, 9, 24, 8, 0, 29, 30, 3, 9, 1, 11, 7, 1, 9, 0, 7, 10, 23, 28, 11, 6, 6, 0, 4, 2, 14, 2, 14, 2, 9, 7, 18, 19, 3, 10, 2, 2, 9, 23, 7, 9, 4, 12, 1, 11, 3, 1, 4, 3, 9, 0, 23, 1, 1, 6, 2, 1, 7, 10, 6, 31, 31, 0, 2, 5, 14, 3, 23, 6, 14, 11, 13, 11, 9, 7, 11, 21, 32, 7, 6, 2, 32, 6, 9, 23, 28, 6, 0, 33, 26, 11, 3, 0, 6, 2, 15, 1, 17, 6, 28, 21, 2, 6, 23, 1, 11, 3, 6, 34, 10, 5, 34, 9, 14, 11, 2, 14, 9, 2, 7, 10, 29, 29, 13, 1, 2]
Centroids: [[-0.059004024, -5.039783], [-1.3960983, -5.5783286], [-2.372972, -10.245319]]
Centroids: [[-0.032785993, -4.868176], [-1.397435, -6.127766], [-2.3696356, -10.423248], [-1.3937832, -5.3313823], [0.41071302, -3.4400628], [-2.4590173, -11.065446], [-2.1880026, -9.785324], [-0.26012865, -5.658559], [-1.9128145, -6.226709], [-1.1856236, -4.669178], [-0.84594244, -3.9475763], [0.4393141, -4.2700496], [-1.6894597, -6.861515], [0.08748459, -3.999647], [-2.0951786, -9.035297], [-1.427918, -8.540562], [-4.620474, -14.598238], [-1.4507737, -6.9416394], [-2.9133792, -12.32736], [-1.6708626, -8.139608], [-0.67964464, -7.0425644], [0.83311945, -2.8898423], [-3.0046697, -11.593234], [-0.4156962, -5.150973], [0.4698997, -5.463091], [-3.4957151, -13.285144], [-1.5058091, -10.673157], [-2.6984425, -13.483609], [-2.9104867, -10.646221], [-4.001665, -14.253578], [-0.26547396, -10.517133], [-5.593522, -16.208776], [-0.9496758, -5.4459386], [-3.2110448, -9.7558775], [-2.689874, -10.189302]]
Contingency Matrix: 
[[31  0  0  0 10  0  0 26  1  0  0 23  0 12  1  1  0  1  0  0  2  3  0  7
   1  0  1  1  0  1  1  0  0  0  0]
 [ 0 14  1 24  0  0  0  0  4 29 11  0  3  0  0  0  1  2  0  1  0  0  0  0
   1  0  0  0  0  0  0  1  2  1  0]
 [ 0  0 33  0  0 13 34  0  0  0  0  0  0  0 19  1  1  0  2  4  0  0  1  0
   0  1  1  1  4  2  0  1  0  0  2]]
[[31, 0, 0, 0, 10, 0, -1, 26, 1, 0, 0, 23, 0, 12, 1, 1, 0, 1, 0, 0, 2, 3, 0, 7, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], [0, 14, 1, 24, 0, 0, -1, 0, 4, 29, 11, 0, 3, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, 14, 1, 24, 0, 0, -1, 0, 4, 29, 11, 0, 3, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {2: 6, 0: 0, 1: 9}
New Contingency Matrix: 
[[31  0  0  0  0  0 10  0 26  1  0 23  0 12  1  1  0  1  0  0  2  3  0  7
   1  0  1  1  0  1  1  0  0  0  0]
 [ 0 29  0 14  1 24  0  0  0  4 11  0  3  0  0  0  1  2  0  1  0  0  0  0
   1  0  0  0  0  0  0  1  2  1  0]
 [ 0  0 34  0 33  0  0 13  0  0  0  0  0  0 19  1  1  0  2  4  0  0  1  0
   0  1  1  1  4  2  0  1  0  0  2]]
New Clustered Label Sequence: [0, 9, 6, 1, 2, 3, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
Diagonal_Elements: [31, 29, 34], Sum: 94
All_Elements: [31, 0, 0, 0, 0, 0, 10, 0, 26, 1, 0, 23, 0, 12, 1, 1, 0, 1, 0, 0, 2, 3, 0, 7, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 29, 0, 14, 1, 24, 0, 0, 0, 4, 11, 0, 3, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 34, 0, 33, 0, 0, 13, 0, 0, 0, 0, 0, 0, 19, 1, 1, 0, 2, 4, 0, 0, 1, 0, 0, 1, 1, 1, 4, 2, 0, 1, 0, 0, 2], Sum: 338
Accuracy: 0.2781065088757396
Done!
