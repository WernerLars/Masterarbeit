Experiment_path: Random_Seeds//V4/Experiment_04_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Random_Seeds//V4/Experiment_04_6/C_Easy2_noise020.mat/Variant_04_Offline_Autoencoder_QLearning/2023_03_22-10_11_23
Punishment_Coefficient: 0.58
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000233B97FD940>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
Train Index: 3173
x_train: 3173
y_train: 3173
x_test: 353
y_test: 353
<torch.utils.data.dataloader.DataLoader object at 0x00000233BED56438>
<torch.utils.data.dataloader.DataLoader object at 0x00000233C16C06A0>
Epoch 1
-------------------------------
loss: 0.133752  [    0/ 3173]
loss: 0.247321  [  100/ 3173]
loss: 0.152257  [  200/ 3173]
loss: 0.106828  [  300/ 3173]
loss: 0.118780  [  400/ 3173]
loss: 0.101494  [  500/ 3173]
loss: 0.050766  [  600/ 3173]
loss: 0.043062  [  700/ 3173]
loss: 0.065749  [  800/ 3173]
loss: 0.022471  [  900/ 3173]
loss: 0.054722  [ 1000/ 3173]
loss: 0.032891  [ 1100/ 3173]
loss: 0.072592  [ 1200/ 3173]
loss: 0.096449  [ 1300/ 3173]
loss: 0.037445  [ 1400/ 3173]
loss: 0.054987  [ 1500/ 3173]
loss: 0.052668  [ 1600/ 3173]
loss: 0.038041  [ 1700/ 3173]
loss: 0.044118  [ 1800/ 3173]
loss: 0.029553  [ 1900/ 3173]
loss: 0.020435  [ 2000/ 3173]
loss: 0.190386  [ 2100/ 3173]
loss: 0.039968  [ 2200/ 3173]
loss: 0.017271  [ 2300/ 3173]
loss: 0.017762  [ 2400/ 3173]
loss: 0.014872  [ 2500/ 3173]
loss: 0.046512  [ 2600/ 3173]
loss: 0.033959  [ 2700/ 3173]
loss: 0.016820  [ 2800/ 3173]
loss: 0.020906  [ 2900/ 3173]
loss: 0.111471  [ 3000/ 3173]
loss: 0.166075  [ 3100/ 3173]
Epoch 2
-------------------------------
loss: 0.027353  [    0/ 3173]
loss: 0.028667  [  100/ 3173]
loss: 0.038856  [  200/ 3173]
loss: 0.016085  [  300/ 3173]
loss: 0.012850  [  400/ 3173]
loss: 0.049488  [  500/ 3173]
loss: 0.020863  [  600/ 3173]
loss: 0.015570  [  700/ 3173]
loss: 0.026100  [  800/ 3173]
loss: 0.013780  [  900/ 3173]
loss: 0.034399  [ 1000/ 3173]
loss: 0.025874  [ 1100/ 3173]
loss: 0.048550  [ 1200/ 3173]
loss: 0.026723  [ 1300/ 3173]
loss: 0.030389  [ 1400/ 3173]
loss: 0.014622  [ 1500/ 3173]
loss: 0.045466  [ 1600/ 3173]
loss: 0.022772  [ 1700/ 3173]
loss: 0.020245  [ 1800/ 3173]
loss: 0.021959  [ 1900/ 3173]
loss: 0.020132  [ 2000/ 3173]
loss: 0.087897  [ 2100/ 3173]
loss: 0.028703  [ 2200/ 3173]
loss: 0.017224  [ 2300/ 3173]
loss: 0.010665  [ 2400/ 3173]
loss: 0.014398  [ 2500/ 3173]
loss: 0.030158  [ 2600/ 3173]
loss: 0.019467  [ 2700/ 3173]
loss: 0.009275  [ 2800/ 3173]
loss: 0.012978  [ 2900/ 3173]
loss: 0.077043  [ 3000/ 3173]
loss: 0.149444  [ 3100/ 3173]
Epoch 3
-------------------------------
loss: 0.024493  [    0/ 3173]
loss: 0.034994  [  100/ 3173]
loss: 0.041435  [  200/ 3173]
loss: 0.009791  [  300/ 3173]
loss: 0.014551  [  400/ 3173]
loss: 0.037899  [  500/ 3173]
loss: 0.013373  [  600/ 3173]
loss: 0.016237  [  700/ 3173]
loss: 0.023466  [  800/ 3173]
loss: 0.012953  [  900/ 3173]
loss: 0.025679  [ 1000/ 3173]
loss: 0.021712  [ 1100/ 3173]
loss: 0.037794  [ 1200/ 3173]
loss: 0.024969  [ 1300/ 3173]
loss: 0.029233  [ 1400/ 3173]
loss: 0.013547  [ 1500/ 3173]
loss: 0.045004  [ 1600/ 3173]
loss: 0.020650  [ 1700/ 3173]
loss: 0.019760  [ 1800/ 3173]
loss: 0.014230  [ 1900/ 3173]
loss: 0.018124  [ 2000/ 3173]
loss: 0.114899  [ 2100/ 3173]
loss: 0.025357  [ 2200/ 3173]
loss: 0.016920  [ 2300/ 3173]
loss: 0.009629  [ 2400/ 3173]
loss: 0.015028  [ 2500/ 3173]
loss: 0.024308  [ 2600/ 3173]
loss: 0.015739  [ 2700/ 3173]
loss: 0.007168  [ 2800/ 3173]
loss: 0.009998  [ 2900/ 3173]
loss: 0.069545  [ 3000/ 3173]
loss: 0.152229  [ 3100/ 3173]
Epoch 4
-------------------------------
loss: 0.022537  [    0/ 3173]
loss: 0.039838  [  100/ 3173]
loss: 0.046625  [  200/ 3173]
loss: 0.010697  [  300/ 3173]
loss: 0.015327  [  400/ 3173]
loss: 0.035095  [  500/ 3173]
loss: 0.011312  [  600/ 3173]
loss: 0.016487  [  700/ 3173]
loss: 0.025589  [  800/ 3173]
loss: 0.012913  [  900/ 3173]
loss: 0.023120  [ 1000/ 3173]
loss: 0.020185  [ 1100/ 3173]
loss: 0.034149  [ 1200/ 3173]
loss: 0.021532  [ 1300/ 3173]
loss: 0.028710  [ 1400/ 3173]
loss: 0.014957  [ 1500/ 3173]
loss: 0.043809  [ 1600/ 3173]
loss: 0.019296  [ 1700/ 3173]
loss: 0.019719  [ 1800/ 3173]
loss: 0.012631  [ 1900/ 3173]
loss: 0.016561  [ 2000/ 3173]
loss: 0.107845  [ 2100/ 3173]
loss: 0.024297  [ 2200/ 3173]
loss: 0.016882  [ 2300/ 3173]
loss: 0.009260  [ 2400/ 3173]
loss: 0.015653  [ 2500/ 3173]
loss: 0.023167  [ 2600/ 3173]
loss: 0.015728  [ 2700/ 3173]
loss: 0.006710  [ 2800/ 3173]
loss: 0.009102  [ 2900/ 3173]
loss: 0.067674  [ 3000/ 3173]
loss: 0.154293  [ 3100/ 3173]
Epoch 5
-------------------------------
loss: 0.022402  [    0/ 3173]
loss: 0.040070  [  100/ 3173]
loss: 0.047876  [  200/ 3173]
loss: 0.006513  [  300/ 3173]
loss: 0.015907  [  400/ 3173]
loss: 0.034084  [  500/ 3173]
loss: 0.010884  [  600/ 3173]
loss: 0.016276  [  700/ 3173]
loss: 0.027106  [  800/ 3173]
loss: 0.012960  [  900/ 3173]
loss: 0.020897  [ 1000/ 3173]
loss: 0.019665  [ 1100/ 3173]
loss: 0.032151  [ 1200/ 3173]
loss: 0.019829  [ 1300/ 3173]
loss: 0.028386  [ 1400/ 3173]
loss: 0.015479  [ 1500/ 3173]
loss: 0.043475  [ 1600/ 3173]
loss: 0.018308  [ 1700/ 3173]
loss: 0.019498  [ 1800/ 3173]
loss: 0.012505  [ 1900/ 3173]
loss: 0.016005  [ 2000/ 3173]
loss: 0.104978  [ 2100/ 3173]
loss: 0.023505  [ 2200/ 3173]
loss: 0.016792  [ 2300/ 3173]
loss: 0.008905  [ 2400/ 3173]
loss: 0.015955  [ 2500/ 3173]
loss: 0.021922  [ 2600/ 3173]
loss: 0.015634  [ 2700/ 3173]
loss: 0.006684  [ 2800/ 3173]
loss: 0.009029  [ 2900/ 3173]
loss: 0.066140  [ 3000/ 3173]
loss: 0.154796  [ 3100/ 3173]
Epoch 6
-------------------------------
loss: 0.022078  [    0/ 3173]
loss: 0.041451  [  100/ 3173]
loss: 0.048332  [  200/ 3173]
loss: 0.006902  [  300/ 3173]
loss: 0.016332  [  400/ 3173]
loss: 0.033640  [  500/ 3173]
loss: 0.010640  [  600/ 3173]
loss: 0.016078  [  700/ 3173]
loss: 0.028012  [  800/ 3173]
loss: 0.013000  [  900/ 3173]
loss: 0.019338  [ 1000/ 3173]
loss: 0.019199  [ 1100/ 3173]
loss: 0.031198  [ 1200/ 3173]
loss: 0.019510  [ 1300/ 3173]
loss: 0.028325  [ 1400/ 3173]
loss: 0.015420  [ 1500/ 3173]
loss: 0.043622  [ 1600/ 3173]
loss: 0.017887  [ 1700/ 3173]
loss: 0.019062  [ 1800/ 3173]
loss: 0.012707  [ 1900/ 3173]
loss: 0.015875  [ 2000/ 3173]
loss: 0.105485  [ 2100/ 3173]
loss: 0.023062  [ 2200/ 3173]
loss: 0.016746  [ 2300/ 3173]
loss: 0.008759  [ 2400/ 3173]
loss: 0.016000  [ 2500/ 3173]
loss: 0.020826  [ 2600/ 3173]
loss: 0.015456  [ 2700/ 3173]
loss: 0.006757  [ 2800/ 3173]
loss: 0.008976  [ 2900/ 3173]
loss: 0.065196  [ 3000/ 3173]
loss: 0.154638  [ 3100/ 3173]
Epoch 7
-------------------------------
loss: 0.021685  [    0/ 3173]
loss: 0.041677  [  100/ 3173]
loss: 0.048463  [  200/ 3173]
loss: 0.013038  [  300/ 3173]
loss: 0.016482  [  400/ 3173]
loss: 0.034017  [  500/ 3173]
loss: 0.010405  [  600/ 3173]
loss: 0.016096  [  700/ 3173]
loss: 0.028460  [  800/ 3173]
loss: 0.012947  [  900/ 3173]
loss: 0.019994  [ 1000/ 3173]
loss: 0.018928  [ 1100/ 3173]
loss: 0.031044  [ 1200/ 3173]
loss: 0.020569  [ 1300/ 3173]
loss: 0.028151  [ 1400/ 3173]
loss: 0.015497  [ 1500/ 3173]
loss: 0.043262  [ 1600/ 3173]
loss: 0.018065  [ 1700/ 3173]
loss: 0.018909  [ 1800/ 3173]
loss: 0.012890  [ 1900/ 3173]
loss: 0.016031  [ 2000/ 3173]
loss: 0.106271  [ 2100/ 3173]
loss: 0.022999  [ 2200/ 3173]
loss: 0.016870  [ 2300/ 3173]
loss: 0.008685  [ 2400/ 3173]
loss: 0.016046  [ 2500/ 3173]
loss: 0.020500  [ 2600/ 3173]
loss: 0.015253  [ 2700/ 3173]
loss: 0.006930  [ 2800/ 3173]
loss: 0.008990  [ 2900/ 3173]
loss: 0.064222  [ 3000/ 3173]
loss: 0.154882  [ 3100/ 3173]
Epoch 8
-------------------------------
loss: 0.021834  [    0/ 3173]
loss: 0.041434  [  100/ 3173]
loss: 0.048272  [  200/ 3173]
loss: 0.005618  [  300/ 3173]
loss: 0.016777  [  400/ 3173]
loss: 0.033631  [  500/ 3173]
loss: 0.010150  [  600/ 3173]
loss: 0.016340  [  700/ 3173]
loss: 0.028969  [  800/ 3173]
loss: 0.012911  [  900/ 3173]
loss: 0.019170  [ 1000/ 3173]
loss: 0.018814  [ 1100/ 3173]
loss: 0.030750  [ 1200/ 3173]
loss: 0.019763  [ 1300/ 3173]
loss: 0.028008  [ 1400/ 3173]
loss: 0.015696  [ 1500/ 3173]
loss: 0.043382  [ 1600/ 3173]
loss: 0.017689  [ 1700/ 3173]
loss: 0.018779  [ 1800/ 3173]
loss: 0.013074  [ 1900/ 3173]
loss: 0.015849  [ 2000/ 3173]
loss: 0.105325  [ 2100/ 3173]
loss: 0.022689  [ 2200/ 3173]
loss: 0.016807  [ 2300/ 3173]
loss: 0.008619  [ 2400/ 3173]
loss: 0.016057  [ 2500/ 3173]
loss: 0.020130  [ 2600/ 3173]
loss: 0.015333  [ 2700/ 3173]
loss: 0.006994  [ 2800/ 3173]
loss: 0.008932  [ 2900/ 3173]
loss: 0.064072  [ 3000/ 3173]
loss: 0.154738  [ 3100/ 3173]
Number of Clusters: 3
Q_Learning:     1/  353]
Q_Learning:     2/  353]
Q_Learning:     3/  353]
Q_Learning:     4/  353]
Q_Learning:     5/  353]
Q_Learning:     6/  353]
Q_Learning:     7/  353]
Q_Learning:     8/  353]
Q_Learning:     9/  353]
Q_Learning:    10/  353]
Q_Learning:    11/  353]
Q_Learning:    12/  353]
Q_Learning:    13/  353]
Q_Learning:    14/  353]
Q_Learning:    15/  353]
Q_Learning:    16/  353]
Q_Learning:    17/  353]
Q_Learning:    18/  353]
Q_Learning:    19/  353]
Q_Learning:    20/  353]
Q_Learning:    21/  353]
Q_Learning:    22/  353]
Q_Learning:    23/  353]
Q_Learning:    24/  353]
Q_Learning:    25/  353]
Q_Learning:    26/  353]
Q_Learning:    27/  353]
Q_Learning:    28/  353]
Q_Learning:    29/  353]
Q_Learning:    30/  353]
Q_Learning:    31/  353]
Q_Learning:    32/  353]
Q_Learning:    33/  353]
Q_Learning:    34/  353]
Q_Learning:    35/  353]
Q_Learning:    36/  353]
Q_Learning:    37/  353]
Q_Learning:    38/  353]
Q_Learning:    39/  353]
Q_Learning:    40/  353]
Q_Learning:    41/  353]
Q_Learning:    42/  353]
Q_Learning:    43/  353]
Q_Learning:    44/  353]
Q_Learning:    45/  353]
Q_Learning:    46/  353]
Q_Learning:    47/  353]
Q_Learning:    48/  353]
Q_Learning:    49/  353]
Q_Learning:    50/  353]
Q_Learning:    51/  353]
Q_Learning:    52/  353]
Q_Learning:    53/  353]
Q_Learning:    54/  353]
Q_Learning:    55/  353]
Q_Learning:    56/  353]
Q_Learning:    57/  353]
Q_Learning:    58/  353]
Q_Learning:    59/  353]
Q_Learning:    60/  353]
Q_Learning:    61/  353]
Q_Learning:    62/  353]
Q_Learning:    63/  353]
Q_Learning:    64/  353]
Q_Learning:    65/  353]
Q_Learning:    66/  353]
Q_Learning:    67/  353]
Q_Learning:    68/  353]
Q_Learning:    69/  353]
Q_Learning:    70/  353]
Q_Learning:    71/  353]
Q_Learning:    72/  353]
Q_Learning:    73/  353]
Q_Learning:    74/  353]
Q_Learning:    75/  353]
Q_Learning:    76/  353]
Q_Learning:    77/  353]
Q_Learning:    78/  353]
Q_Learning:    79/  353]
Q_Learning:    80/  353]
Q_Learning:    81/  353]
Q_Learning:    82/  353]
Q_Learning:    83/  353]
Q_Learning:    84/  353]
Q_Learning:    85/  353]
Q_Learning:    86/  353]
Q_Learning:    87/  353]
Q_Learning:    88/  353]
Q_Learning:    89/  353]
Q_Learning:    90/  353]
Q_Learning:    91/  353]
Q_Learning:    92/  353]
Q_Learning:    93/  353]
Q_Learning:    94/  353]
Q_Learning:    95/  353]
Q_Learning:    96/  353]
Q_Learning:    97/  353]
Q_Learning:    98/  353]
Q_Learning:    99/  353]
Q_Learning:   100/  353]
Q_Learning:   101/  353]
Q_Learning:   102/  353]
Q_Learning:   103/  353]
Q_Learning:   104/  353]
Q_Learning:   105/  353]
Q_Learning:   106/  353]
Q_Learning:   107/  353]
Q_Learning:   108/  353]
Q_Learning:   109/  353]
Q_Learning:   110/  353]
Q_Learning:   111/  353]
Q_Learning:   112/  353]
Q_Learning:   113/  353]
Q_Learning:   114/  353]
Q_Learning:   115/  353]
Q_Learning:   116/  353]
Q_Learning:   117/  353]
Q_Learning:   118/  353]
Q_Learning:   119/  353]
Q_Learning:   120/  353]
Q_Learning:   121/  353]
Q_Learning:   122/  353]
Q_Learning:   123/  353]
Q_Learning:   124/  353]
Q_Learning:   125/  353]
Q_Learning:   126/  353]
Q_Learning:   127/  353]
Q_Learning:   128/  353]
Q_Learning:   129/  353]
Q_Learning:   130/  353]
Q_Learning:   131/  353]
Q_Learning:   132/  353]
Q_Learning:   133/  353]
Q_Learning:   134/  353]
Q_Learning:   135/  353]
Q_Learning:   136/  353]
Q_Learning:   137/  353]
Q_Learning:   138/  353]
Q_Learning:   139/  353]
Q_Learning:   140/  353]
Q_Learning:   141/  353]
Q_Learning:   142/  353]
Q_Learning:   143/  353]
Q_Learning:   144/  353]
Q_Learning:   145/  353]
Q_Learning:   146/  353]
Q_Learning:   147/  353]
Q_Learning:   148/  353]
Q_Learning:   149/  353]
Q_Learning:   150/  353]
Q_Learning:   151/  353]
Q_Learning:   152/  353]
Q_Learning:   153/  353]
Q_Learning:   154/  353]
Q_Learning:   155/  353]
Q_Learning:   156/  353]
Q_Learning:   157/  353]
Q_Learning:   158/  353]
Q_Learning:   159/  353]
Q_Learning:   160/  353]
Q_Learning:   161/  353]
Q_Learning:   162/  353]
Q_Learning:   163/  353]
Q_Learning:   164/  353]
Q_Learning:   165/  353]
Q_Learning:   166/  353]
Q_Learning:   167/  353]
Q_Learning:   168/  353]
Q_Learning:   169/  353]
Q_Learning:   170/  353]
Q_Learning:   171/  353]
Q_Learning:   172/  353]
Q_Learning:   173/  353]
Q_Learning:   174/  353]
Q_Learning:   175/  353]
Q_Learning:   176/  353]
Q_Learning:   177/  353]
Q_Learning:   178/  353]
Q_Learning:   179/  353]
Q_Learning:   180/  353]
Q_Learning:   181/  353]
Q_Learning:   182/  353]
Q_Learning:   183/  353]
Q_Learning:   184/  353]
Q_Learning:   185/  353]
Q_Learning:   186/  353]
Q_Learning:   187/  353]
Q_Learning:   188/  353]
Q_Learning:   189/  353]
Q_Learning:   190/  353]
Q_Learning:   191/  353]
Q_Learning:   192/  353]
Q_Learning:   193/  353]
Q_Learning:   194/  353]
Q_Learning:   195/  353]
Q_Learning:   196/  353]
Q_Learning:   197/  353]
Q_Learning:   198/  353]
Q_Learning:   199/  353]
Q_Learning:   200/  353]
Q_Learning:   201/  353]
Q_Learning:   202/  353]
Q_Learning:   203/  353]
Q_Learning:   204/  353]
Q_Learning:   205/  353]
Q_Learning:   206/  353]
Q_Learning:   207/  353]
Q_Learning:   208/  353]
Q_Learning:   209/  353]
Q_Learning:   210/  353]
Q_Learning:   211/  353]
Q_Learning:   212/  353]
Q_Learning:   213/  353]
Q_Learning:   214/  353]
Q_Learning:   215/  353]
Q_Learning:   216/  353]
Q_Learning:   217/  353]
Q_Learning:   218/  353]
Q_Learning:   219/  353]
Q_Learning:   220/  353]
Q_Learning:   221/  353]
Q_Learning:   222/  353]
Q_Learning:   223/  353]
Q_Learning:   224/  353]
Q_Learning:   225/  353]
Q_Learning:   226/  353]
Q_Learning:   227/  353]
Q_Learning:   228/  353]
Q_Learning:   229/  353]
Q_Learning:   230/  353]
Q_Learning:   231/  353]
Q_Learning:   232/  353]
Q_Learning:   233/  353]
Q_Learning:   234/  353]
Q_Learning:   235/  353]
Q_Learning:   236/  353]
Q_Learning:   237/  353]
Q_Learning:   238/  353]
Q_Learning:   239/  353]
Q_Learning:   240/  353]
Q_Learning:   241/  353]
Q_Learning:   242/  353]
Q_Learning:   243/  353]
Q_Learning:   244/  353]
Q_Learning:   245/  353]
Q_Learning:   246/  353]
Q_Learning:   247/  353]
Q_Learning:   248/  353]
Q_Learning:   249/  353]
Q_Learning:   250/  353]
Q_Learning:   251/  353]
Q_Learning:   252/  353]
Q_Learning:   253/  353]
Q_Learning:   254/  353]
Q_Learning:   255/  353]
Q_Learning:   256/  353]
Q_Learning:   257/  353]
Q_Learning:   258/  353]
Q_Learning:   259/  353]
Q_Learning:   260/  353]
Q_Learning:   261/  353]
Q_Learning:   262/  353]
Q_Learning:   263/  353]
Q_Learning:   264/  353]
Q_Learning:   265/  353]
Q_Learning:   266/  353]
Q_Learning:   267/  353]
Q_Learning:   268/  353]
Q_Learning:   269/  353]
Q_Learning:   270/  353]
Q_Learning:   271/  353]
Q_Learning:   272/  353]
Q_Learning:   273/  353]
Q_Learning:   274/  353]
Q_Learning:   275/  353]
Q_Learning:   276/  353]
Q_Learning:   277/  353]
Q_Learning:   278/  353]
Q_Learning:   279/  353]
Q_Learning:   280/  353]
Q_Learning:   281/  353]
Q_Learning:   282/  353]
Q_Learning:   283/  353]
Q_Learning:   284/  353]
Q_Learning:   285/  353]
Q_Learning:   286/  353]
Q_Learning:   287/  353]
Q_Learning:   288/  353]
Q_Learning:   289/  353]
Q_Learning:   290/  353]
Q_Learning:   291/  353]
Q_Learning:   292/  353]
Q_Learning:   293/  353]
Q_Learning:   294/  353]
Q_Learning:   295/  353]
Q_Learning:   296/  353]
Q_Learning:   297/  353]
Q_Learning:   298/  353]
Q_Learning:   299/  353]
Q_Learning:   300/  353]
Q_Learning:   301/  353]
Q_Learning:   302/  353]
Q_Learning:   303/  353]
Q_Learning:   304/  353]
Q_Learning:   305/  353]
Q_Learning:   306/  353]
Q_Learning:   307/  353]
Q_Learning:   308/  353]
Q_Learning:   309/  353]
Q_Learning:   310/  353]
Q_Learning:   311/  353]
Q_Learning:   312/  353]
Q_Learning:   313/  353]
Q_Learning:   314/  353]
Q_Learning:   315/  353]
Q_Learning:   316/  353]
Q_Learning:   317/  353]
Q_Learning:   318/  353]
Q_Learning:   319/  353]
Q_Learning:   320/  353]
Q_Learning:   321/  353]
Q_Learning:   322/  353]
Q_Learning:   323/  353]
Q_Learning:   324/  353]
Q_Learning:   325/  353]
Q_Learning:   326/  353]
Q_Learning:   327/  353]
Q_Learning:   328/  353]
Q_Learning:   329/  353]
Q_Learning:   330/  353]
Q_Learning:   331/  353]
Q_Learning:   332/  353]
Q_Learning:   333/  353]
Q_Learning:   334/  353]
Q_Learning:   335/  353]
Q_Learning:   336/  353]
Q_Learning:   337/  353]
Q_Learning:   338/  353]
Q_Learning:   339/  353]
Q_Learning:   340/  353]
Q_Learning:   341/  353]
Q_Learning:   342/  353]
Q_Learning:   343/  353]
Q_Learning:   344/  353]
Q_Learning:   345/  353]
Q_Learning:   346/  353]
Q_Learning:   347/  353]
Q_Learning:   348/  353]
Q_Learning:   349/  353]
Q_Learning:   350/  353]
Q_Learning:   351/  353]
Q_Learning:   352/  353]
Q_Learning:   353/  353]
Number of Samples after Autoencoder testing: 353
First Spike after testing: [-2.1341267 -4.697213 ]
[2, 0, 1, 1, 2, 0, 1, 2, 1, 1, 2, 1, 0, 0, 0, 2, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 0, 1, 2, 0, 2, 0, 1, 0, 1, 0, 2, 0, 2, 0, 2, 1, 2, 1, 1, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 2, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 0, 1, 1, 2, 1, 2, 2, 2, 0, 2, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 0, 0, 2, 0, 2, 0, 2, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 1, 1, 2, 0, 2, 0, 1, 0, 2, 0, 2, 1, 2, 0, 1, 2, 1, 0, 1, 2, 1, 2, 0, 1, 0, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 2, 0, 2, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 2, 2, 2, 0, 1, 0, 2, 2, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 2, 0, 0, 0, 2, 2]
[0, 1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 5, 11, 5, 0, 8, 12, 0, 13, 11, 3, 10, 2, 0, 3, 0, 8, 0, 14, 14, 0, 12, 9, 8, 5, 7, 8, 3, 15, 16, 9, 5, 4, 14, 15, 14, 8, 17, 18, 9, 18, 3, 11, 8, 4, 8, 8, 15, 0, 3, 14, 0, 11, 5, 8, 13, 13, 16, 13, 19, 5, 11, 8, 20, 11, 9, 20, 20, 17, 3, 20, 9, 3, 11, 19, 4, 6, 0, 12, 8, 2, 2, 3, 10, 14, 10, 2, 12, 14, 3, 16, 5, 13, 9, 1, 1, 16, 12, 11, 10, 21, 19, 11, 19, 9, 11, 2, 14, 15, 11, 20, 2, 18, 15, 3, 3, 20, 19, 2, 14, 2, 14, 9, 12, 0, 2, 10, 11, 19, 16, 15, 19, 1, 8, 16, 0, 11, 15, 22, 8, 8, 8, 14, 8, 15, 10, 18, 20, 9, 23, 9, 11, 3, 12, 3, 8, 14, 6, 12, 3, 3, 0, 3, 14, 14, 24, 5, 19, 3, 19, 25, 17, 2, 11, 10, 23, 9, 25, 24, 11, 26, 24, 23, 23, 2, 3, 20, 21, 11, 17, 5, 12, 12, 3, 14, 24, 3, 15, 5, 27, 15, 20, 3, 9, 2, 12, 10, 12, 2, 14, 3, 11, 14, 28, 23, 11, 29, 13, 5, 5, 12, 14, 0, 22, 11, 14, 3, 11, 11, 3, 11, 18, 8, 30, 8, 14, 14, 0, 14, 19, 8, 15, 11, 14, 17, 14, 12, 12, 22, 11, 18, 26, 3, 25, 12, 22, 11, 4, 0, 11, 15, 25, 11, 8, 19, 24, 20, 11, 13, 2, 2, 19, 15, 31, 26, 24, 14, 17, 29, 4, 18, 19, 8, 24, 20, 3, 12, 24, 8, 12, 11, 15, 14, 14, 11, 2, 32, 4, 14, 9, 33, 8, 11, 11, 5, 14, 14, 11, 15, 18, 4, 26, 24, 16, 21, 20, 2, 14, 15, 1, 13, 11, 5, 2, 8, 8, 33, 11, 3, 34, 1, 14, 30, 24, 29, 20, 4, 33, 11, 1, 19, 19]
Centroids: [[1.6897179, -6.8453636], [0.84531486, -5.3590035], [-2.5656247, -5.750015]]
Centroids: [[-2.4078116, -4.849415], [2.2325857, -11.0722475], [0.6387806, -4.9135427], [1.2727007, -7.6475782], [-3.2274094, -2.0559459], [1.7847238, -9.858929], [-1.4195362, -11.529479], [0.8504642, -11.127616], [1.0092962, -3.491815], [-2.3333735, -8.6591215], [0.26432312, -2.5109987], [1.0914447, -6.3881073], [1.8998901, -8.614086], [0.46690816, -0.46156392], [1.2778869, -5.086056], [-2.6900368, -5.7722683], [1.3378161, -3.0344217], [-3.4105618, -4.548933], [-2.862451, -2.8606253], [-2.7982144, -3.9526868], [-2.6523848, -6.8489428], [-0.6652851, -7.3176956], [-1.9899733, -9.98282], [1.2052976, -1.9221249], [1.8376652, -6.8902655], [1.92577, -12.156979], [2.0074608, -4.380353], [-2.177434, -13.432958], [-3.2360892, -7.9421763], [-1.1000823, -3.898962], [-1.5428854, -5.0805273], [-1.0457402, -12.980142], [1.4801699, -14.264541], [1.8453856, -7.8064036], [3.1997683, -9.087913]]
Contingency Matrix: 
[[ 0  6  1 13  0 10  0  1 11  0  0 17 14  0 15  0  5  0  0  0  0  0  0  3
   9  4  4  0  0  0  0  0  0  3  0]
 [ 0  1 19 15  0  5  0  1 16  0  9 20  4  8 17  0  2  0  0  0  0  1  0  2
   1  0  0  0  0  1  0  0  1  0  1]
 [16  0  0  0  9  0  3  0  0 14  0  0  0  0  0 16  0  6  8 15 13  2  4  0
   0  0  0  1  1  2  2  1  0  0  0]]
[[0, 6, 1, 13, 0, 10, 0, 1, 11, 0, 0, -1, 14, 0, 15, 0, 5, 0, 0, 0, 0, 0, 0, 3, 9, 4, 4, 0, 0, 0, 0, 0, 0, 3, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [16, 0, 0, 0, 9, 0, 3, 0, 0, 14, 0, -1, 0, 0, 0, 16, 0, 6, 8, 15, 13, 2, 4, 0, 0, 0, 0, 1, 1, 2, 2, 1, 0, 0, 0]]
[[-1, 6, 1, 13, 0, 10, 0, 1, 11, 0, 0, -1, 14, 0, 15, 0, 5, 0, 0, 0, 0, 0, 0, 3, 9, 4, 4, 0, 0, 0, 0, 0, 0, 3, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {1: 11, 2: 0, 0: 14}
New Contingency Matrix: 
[[15 17  0  6  1 13  0 10  0  1 11  0  0 14  0  0  5  0  0  0  0  0  0  3
   9  4  4  0  0  0  0  0  0  3  0]
 [17 20  0  1 19 15  0  5  0  1 16  0  9  4  8  0  2  0  0  0  0  1  0  2
   1  0  0  0  0  1  0  0  1  0  1]
 [ 0  0 16  0  0  0  9  0  3  0  0 14  0  0  0 16  0  6  8 15 13  2  4  0
   0  0  0  1  1  2  2  1  0  0  0]]
New Clustered Label Sequence: [14, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
Diagonal_Elements: [15, 20, 16], Sum: 51
All_Elements: [15, 17, 0, 6, 1, 13, 0, 10, 0, 1, 11, 0, 0, 14, 0, 0, 5, 0, 0, 0, 0, 0, 0, 3, 9, 4, 4, 0, 0, 0, 0, 0, 0, 3, 0, 17, 20, 0, 1, 19, 15, 0, 5, 0, 1, 16, 0, 9, 4, 8, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 16, 0, 0, 0, 9, 0, 3, 0, 0, 14, 0, 0, 0, 16, 0, 6, 8, 15, 13, 2, 4, 0, 0, 0, 0, 1, 1, 2, 2, 1, 0, 0, 0], Sum: 353
Accuracy: 0.14447592067988668
Done!
