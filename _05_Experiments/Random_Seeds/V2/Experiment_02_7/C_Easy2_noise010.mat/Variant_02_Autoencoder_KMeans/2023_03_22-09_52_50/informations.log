Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise010.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Easy2_noise010.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_52_50
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000017452F8C048>
Sampling rate: 24000.0
Raw: [-0.04397287 -0.05368168 -0.05753576 ... -0.17707654 -0.14968225
 -0.12084286]
Times: [   1077    1809    2216 ... 1439324 1439736 1439818]
Cluster: [1 2 3 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3520
First aligned Spike Frame: [-5.66507481e-02 -6.59320228e-02 -6.70701971e-02 -7.19520617e-02
 -7.89243788e-02 -8.44863120e-02 -9.23204981e-02 -9.75387283e-02
 -7.89589716e-02 -3.66949571e-02  2.34965171e-04 -2.60677777e-03
 -8.36059782e-02 -2.16751250e-01 -3.29544857e-01 -3.35165947e-01
 -2.03449552e-01  7.47840458e-02  4.22419255e-01  7.09409540e-01
  8.78002642e-01  9.55364309e-01  9.77809330e-01  9.55005143e-01
  8.85120577e-01  8.00574977e-01  7.20670596e-01  6.49598354e-01
  5.48520603e-01  4.27922886e-01  3.27637830e-01  2.50259973e-01
  1.79725440e-01  1.08182425e-01  5.15669298e-02  1.18971249e-02
 -1.33865595e-02 -3.45955406e-02 -6.81150537e-02 -1.12799097e-01
 -1.58924383e-01 -1.84417551e-01 -2.01640893e-01 -2.18864546e-01
 -2.16773696e-01 -2.09095391e-01 -1.81456244e-01]
Cluster 0, Occurrences: 1160
Cluster 1, Occurrences: 1146
Cluster 2, Occurrences: 1214
<torch.utils.data.dataloader.DataLoader object at 0x0000017452F74C18>
Epoch 1
-------------------------------
loss: 0.222562  [    0/ 3520]
loss: 0.061234  [  100/ 3520]
loss: 0.190531  [  200/ 3520]
loss: 0.017834  [  300/ 3520]
loss: 0.090653  [  400/ 3520]
loss: 0.164370  [  500/ 3520]
loss: 0.014677  [  600/ 3520]
loss: 0.007693  [  700/ 3520]
loss: 0.007172  [  800/ 3520]
loss: 0.018107  [  900/ 3520]
loss: 0.004355  [ 1000/ 3520]
loss: 0.015787  [ 1100/ 3520]
loss: 0.013487  [ 1200/ 3520]
loss: 0.020098  [ 1300/ 3520]
loss: 0.016088  [ 1400/ 3520]
loss: 0.003113  [ 1500/ 3520]
loss: 0.003202  [ 1600/ 3520]
loss: 0.003068  [ 1700/ 3520]
loss: 0.096320  [ 1800/ 3520]
loss: 0.007224  [ 1900/ 3520]
loss: 0.014447  [ 2000/ 3520]
loss: 0.008133  [ 2100/ 3520]
loss: 0.003360  [ 2200/ 3520]
loss: 0.063453  [ 2300/ 3520]
loss: 0.008195  [ 2400/ 3520]
loss: 0.007478  [ 2500/ 3520]
loss: 0.012732  [ 2600/ 3520]
loss: 0.003623  [ 2700/ 3520]
loss: 0.015616  [ 2800/ 3520]
loss: 0.005370  [ 2900/ 3520]
loss: 0.011883  [ 3000/ 3520]
loss: 0.014195  [ 3100/ 3520]
loss: 0.005692  [ 3200/ 3520]
loss: 0.003623  [ 3300/ 3520]
loss: 0.005401  [ 3400/ 3520]
loss: 0.010960  [ 3500/ 3520]
Epoch 2
-------------------------------
loss: 0.009516  [    0/ 3520]
loss: 0.002832  [  100/ 3520]
loss: 0.168087  [  200/ 3520]
loss: 0.006535  [  300/ 3520]
loss: 0.071557  [  400/ 3520]
loss: 0.108311  [  500/ 3520]
loss: 0.007115  [  600/ 3520]
loss: 0.006589  [  700/ 3520]
loss: 0.004492  [  800/ 3520]
loss: 0.004307  [  900/ 3520]
loss: 0.003465  [ 1000/ 3520]
loss: 0.008111  [ 1100/ 3520]
loss: 0.009490  [ 1200/ 3520]
loss: 0.004368  [ 1300/ 3520]
loss: 0.008067  [ 1400/ 3520]
loss: 0.001491  [ 1500/ 3520]
loss: 0.001783  [ 1600/ 3520]
loss: 0.003523  [ 1700/ 3520]
loss: 0.113304  [ 1800/ 3520]
loss: 0.006647  [ 1900/ 3520]
loss: 0.014777  [ 2000/ 3520]
loss: 0.007833  [ 2100/ 3520]
loss: 0.001875  [ 2200/ 3520]
loss: 0.053744  [ 2300/ 3520]
loss: 0.007240  [ 2400/ 3520]
loss: 0.008400  [ 2500/ 3520]
loss: 0.013626  [ 2600/ 3520]
loss: 0.003121  [ 2700/ 3520]
loss: 0.013089  [ 2800/ 3520]
loss: 0.002507  [ 2900/ 3520]
loss: 0.011972  [ 3000/ 3520]
loss: 0.007697  [ 3100/ 3520]
loss: 0.005384  [ 3200/ 3520]
loss: 0.003459  [ 3300/ 3520]
loss: 0.006594  [ 3400/ 3520]
loss: 0.010331  [ 3500/ 3520]
Epoch 3
-------------------------------
loss: 0.008901  [    0/ 3520]
loss: 0.003182  [  100/ 3520]
loss: 0.135216  [  200/ 3520]
loss: 0.005546  [  300/ 3520]
loss: 0.071309  [  400/ 3520]
loss: 0.079825  [  500/ 3520]
loss: 0.006367  [  600/ 3520]
loss: 0.007123  [  700/ 3520]
loss: 0.005472  [  800/ 3520]
loss: 0.002029  [  900/ 3520]
loss: 0.003499  [ 1000/ 3520]
loss: 0.007969  [ 1100/ 3520]
loss: 0.009557  [ 1200/ 3520]
loss: 0.003711  [ 1300/ 3520]
loss: 0.006223  [ 1400/ 3520]
loss: 0.001497  [ 1500/ 3520]
loss: 0.001278  [ 1600/ 3520]
loss: 0.003574  [ 1700/ 3520]
loss: 0.108816  [ 1800/ 3520]
loss: 0.006231  [ 1900/ 3520]
loss: 0.014841  [ 2000/ 3520]
loss: 0.007779  [ 2100/ 3520]
loss: 0.001749  [ 2200/ 3520]
loss: 0.057839  [ 2300/ 3520]
loss: 0.005449  [ 2400/ 3520]
loss: 0.008738  [ 2500/ 3520]
loss: 0.013171  [ 2600/ 3520]
loss: 0.002869  [ 2700/ 3520]
loss: 0.014666  [ 2800/ 3520]
loss: 0.003370  [ 2900/ 3520]
loss: 0.011703  [ 3000/ 3520]
loss: 0.005126  [ 3100/ 3520]
loss: 0.005580  [ 3200/ 3520]
loss: 0.002868  [ 3300/ 3520]
loss: 0.007026  [ 3400/ 3520]
loss: 0.010769  [ 3500/ 3520]
Epoch 4
-------------------------------
loss: 0.011181  [    0/ 3520]
loss: 0.003213  [  100/ 3520]
loss: 0.114017  [  200/ 3520]
loss: 0.004941  [  300/ 3520]
loss: 0.066056  [  400/ 3520]
loss: 0.068444  [  500/ 3520]
loss: 0.005981  [  600/ 3520]
loss: 0.007784  [  700/ 3520]
loss: 0.006183  [  800/ 3520]
loss: 0.002414  [  900/ 3520]
loss: 0.003463  [ 1000/ 3520]
loss: 0.007583  [ 1100/ 3520]
loss: 0.009553  [ 1200/ 3520]
loss: 0.005113  [ 1300/ 3520]
loss: 0.005045  [ 1400/ 3520]
loss: 0.001519  [ 1500/ 3520]
loss: 0.001218  [ 1600/ 3520]
loss: 0.003702  [ 1700/ 3520]
loss: 0.108156  [ 1800/ 3520]
loss: 0.006531  [ 1900/ 3520]
loss: 0.014552  [ 2000/ 3520]
loss: 0.007763  [ 2100/ 3520]
loss: 0.002190  [ 2200/ 3520]
loss: 0.057726  [ 2300/ 3520]
loss: 0.004162  [ 2400/ 3520]
loss: 0.008840  [ 2500/ 3520]
loss: 0.012601  [ 2600/ 3520]
loss: 0.002825  [ 2700/ 3520]
loss: 0.015209  [ 2800/ 3520]
loss: 0.003897  [ 2900/ 3520]
loss: 0.011540  [ 3000/ 3520]
loss: 0.005247  [ 3100/ 3520]
loss: 0.005642  [ 3200/ 3520]
loss: 0.002305  [ 3300/ 3520]
loss: 0.007080  [ 3400/ 3520]
loss: 0.011193  [ 3500/ 3520]
Epoch 5
-------------------------------
loss: 0.012475  [    0/ 3520]
loss: 0.003256  [  100/ 3520]
loss: 0.100898  [  200/ 3520]
loss: 0.004606  [  300/ 3520]
loss: 0.060175  [  400/ 3520]
loss: 0.058945  [  500/ 3520]
loss: 0.005660  [  600/ 3520]
loss: 0.008114  [  700/ 3520]
loss: 0.006576  [  800/ 3520]
loss: 0.002701  [  900/ 3520]
loss: 0.003482  [ 1000/ 3520]
loss: 0.007355  [ 1100/ 3520]
loss: 0.009334  [ 1200/ 3520]
loss: 0.005648  [ 1300/ 3520]
loss: 0.004817  [ 1400/ 3520]
loss: 0.001651  [ 1500/ 3520]
loss: 0.001448  [ 1600/ 3520]
loss: 0.003598  [ 1700/ 3520]
loss: 0.107463  [ 1800/ 3520]
loss: 0.006946  [ 1900/ 3520]
loss: 0.013381  [ 2000/ 3520]
loss: 0.007673  [ 2100/ 3520]
loss: 0.003031  [ 2200/ 3520]
loss: 0.058635  [ 2300/ 3520]
loss: 0.003412  [ 2400/ 3520]
loss: 0.009015  [ 2500/ 3520]
loss: 0.012395  [ 2600/ 3520]
loss: 0.002763  [ 2700/ 3520]
loss: 0.015385  [ 2800/ 3520]
loss: 0.003833  [ 2900/ 3520]
loss: 0.011535  [ 3000/ 3520]
loss: 0.005723  [ 3100/ 3520]
loss: 0.005723  [ 3200/ 3520]
loss: 0.001984  [ 3300/ 3520]
loss: 0.006899  [ 3400/ 3520]
loss: 0.011206  [ 3500/ 3520]
Epoch 6
-------------------------------
loss: 0.013025  [    0/ 3520]
loss: 0.003245  [  100/ 3520]
loss: 0.096356  [  200/ 3520]
loss: 0.004498  [  300/ 3520]
loss: 0.057543  [  400/ 3520]
loss: 0.053659  [  500/ 3520]
loss: 0.005511  [  600/ 3520]
loss: 0.008257  [  700/ 3520]
loss: 0.006587  [  800/ 3520]
loss: 0.002755  [  900/ 3520]
loss: 0.003635  [ 1000/ 3520]
loss: 0.007256  [ 1100/ 3520]
loss: 0.009024  [ 1200/ 3520]
loss: 0.005613  [ 1300/ 3520]
loss: 0.004773  [ 1400/ 3520]
loss: 0.001645  [ 1500/ 3520]
loss: 0.001671  [ 1600/ 3520]
loss: 0.003459  [ 1700/ 3520]
loss: 0.107502  [ 1800/ 3520]
loss: 0.006846  [ 1900/ 3520]
loss: 0.012611  [ 2000/ 3520]
loss: 0.007598  [ 2100/ 3520]
loss: 0.003473  [ 2200/ 3520]
loss: 0.059534  [ 2300/ 3520]
loss: 0.003105  [ 2400/ 3520]
loss: 0.009010  [ 2500/ 3520]
loss: 0.012002  [ 2600/ 3520]
loss: 0.002756  [ 2700/ 3520]
loss: 0.015482  [ 2800/ 3520]
loss: 0.003626  [ 2900/ 3520]
loss: 0.011412  [ 3000/ 3520]
loss: 0.005996  [ 3100/ 3520]
loss: 0.005704  [ 3200/ 3520]
loss: 0.001874  [ 3300/ 3520]
loss: 0.006907  [ 3400/ 3520]
loss: 0.011096  [ 3500/ 3520]
Epoch 7
-------------------------------
loss: 0.012948  [    0/ 3520]
loss: 0.003198  [  100/ 3520]
loss: 0.093527  [  200/ 3520]
loss: 0.004301  [  300/ 3520]
loss: 0.056369  [  400/ 3520]
loss: 0.049921  [  500/ 3520]
loss: 0.005431  [  600/ 3520]
loss: 0.008397  [  700/ 3520]
loss: 0.006502  [  800/ 3520]
loss: 0.002520  [  900/ 3520]
loss: 0.003763  [ 1000/ 3520]
loss: 0.007289  [ 1100/ 3520]
loss: 0.008780  [ 1200/ 3520]
loss: 0.005070  [ 1300/ 3520]
loss: 0.004848  [ 1400/ 3520]
loss: 0.001672  [ 1500/ 3520]
loss: 0.001768  [ 1600/ 3520]
loss: 0.003232  [ 1700/ 3520]
loss: 0.106181  [ 1800/ 3520]
loss: 0.006855  [ 1900/ 3520]
loss: 0.011980  [ 2000/ 3520]
loss: 0.007506  [ 2100/ 3520]
loss: 0.003682  [ 2200/ 3520]
loss: 0.061301  [ 2300/ 3520]
loss: 0.003010  [ 2400/ 3520]
loss: 0.008960  [ 2500/ 3520]
loss: 0.011834  [ 2600/ 3520]
loss: 0.002731  [ 2700/ 3520]
loss: 0.015412  [ 2800/ 3520]
loss: 0.003445  [ 2900/ 3520]
loss: 0.011440  [ 3000/ 3520]
loss: 0.005924  [ 3100/ 3520]
loss: 0.005687  [ 3200/ 3520]
loss: 0.001825  [ 3300/ 3520]
loss: 0.006926  [ 3400/ 3520]
loss: 0.010910  [ 3500/ 3520]
Epoch 8
-------------------------------
loss: 0.012521  [    0/ 3520]
loss: 0.003140  [  100/ 3520]
loss: 0.096249  [  200/ 3520]
loss: 0.004194  [  300/ 3520]
loss: 0.055129  [  400/ 3520]
loss: 0.047817  [  500/ 3520]
loss: 0.005543  [  600/ 3520]
loss: 0.008528  [  700/ 3520]
loss: 0.006333  [  800/ 3520]
loss: 0.002464  [  900/ 3520]
loss: 0.004393  [ 1000/ 3520]
loss: 0.007154  [ 1100/ 3520]
loss: 0.008336  [ 1200/ 3520]
loss: 0.004775  [ 1300/ 3520]
loss: 0.004972  [ 1400/ 3520]
loss: 0.001602  [ 1500/ 3520]
loss: 0.001837  [ 1600/ 3520]
loss: 0.003170  [ 1700/ 3520]
loss: 0.105124  [ 1800/ 3520]
loss: 0.006791  [ 1900/ 3520]
loss: 0.011511  [ 2000/ 3520]
loss: 0.007464  [ 2100/ 3520]
loss: 0.003787  [ 2200/ 3520]
loss: 0.062188  [ 2300/ 3520]
loss: 0.003280  [ 2400/ 3520]
loss: 0.009019  [ 2500/ 3520]
loss: 0.011564  [ 2600/ 3520]
loss: 0.002680  [ 2700/ 3520]
loss: 0.015107  [ 2800/ 3520]
loss: 0.003244  [ 2900/ 3520]
loss: 0.011588  [ 3000/ 3520]
loss: 0.005897  [ 3100/ 3520]
loss: 0.005667  [ 3200/ 3520]
loss: 0.001898  [ 3300/ 3520]
loss: 0.006862  [ 3400/ 3520]
loss: 0.010741  [ 3500/ 3520]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3520
First Spike after testing: [-0.6245533 -1.4488012]
[0 1 2 ... 0 1 2]
[2 0 1 ... 2 0 1]
Cluster 0 Occurrences: 1160; KMEANS: 1101
Cluster 1 Occurrences: 1146; KMEANS: 1211
Cluster 2 Occurrences: 1214; KMEANS: 1208
Centroids: [[-0.93025136, -1.633606], [-0.53539836, -0.6157797], [1.1472801, 0.93458784]]
Centroids: [[-0.50643826, -0.5700509], [1.1495417, 0.9390207], [-0.9390453, -1.6354346]]
Contingency Matrix: 
[[  11    0 1149]
 [1087    1   58]
 [   3 1210    1]]
[[11, -1, 1149], [1087, -1, 58], [-1, -1, -1]]
[[-1, -1, -1], [1087, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 2, 1: 0}
New Contingency Matrix: 
[[1149   11    0]
 [  58 1087    1]
 [   1    3 1210]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1149, 1087, 1210], Sum: 3446
All_Elements: [1149, 11, 0, 58, 1087, 1, 1, 3, 1210], Sum: 3520
Accuracy: 0.9789772727272728
Done!
