Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_43
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000017452CFEC18>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000017452F74F60>
Epoch 1
-------------------------------
loss: 0.288066  [    0/ 3475]
loss: 0.109073  [  100/ 3475]
loss: 0.225528  [  200/ 3475]
loss: 0.221398  [  300/ 3475]
loss: 0.203826  [  400/ 3475]
loss: 0.083143  [  500/ 3475]
loss: 0.078574  [  600/ 3475]
loss: 0.065466  [  700/ 3475]
loss: 0.138493  [  800/ 3475]
loss: 0.211154  [  900/ 3475]
loss: 0.057208  [ 1000/ 3475]
loss: 0.035915  [ 1100/ 3475]
loss: 0.099802  [ 1200/ 3475]
loss: 0.036113  [ 1300/ 3475]
loss: 0.076909  [ 1400/ 3475]
loss: 0.064640  [ 1500/ 3475]
loss: 0.040657  [ 1600/ 3475]
loss: 0.012346  [ 1700/ 3475]
loss: 0.149909  [ 1800/ 3475]
loss: 0.039011  [ 1900/ 3475]
loss: 0.021036  [ 2000/ 3475]
loss: 0.039081  [ 2100/ 3475]
loss: 0.083127  [ 2200/ 3475]
loss: 0.015734  [ 2300/ 3475]
loss: 0.015868  [ 2400/ 3475]
loss: 0.083431  [ 2500/ 3475]
loss: 0.040557  [ 2600/ 3475]
loss: 0.098130  [ 2700/ 3475]
loss: 0.043978  [ 2800/ 3475]
loss: 0.028298  [ 2900/ 3475]
loss: 0.051526  [ 3000/ 3475]
loss: 0.063279  [ 3100/ 3475]
loss: 0.045667  [ 3200/ 3475]
loss: 0.012792  [ 3300/ 3475]
loss: 0.024413  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.023670  [    0/ 3475]
loss: 0.033223  [  100/ 3475]
loss: 0.055064  [  200/ 3475]
loss: 0.224172  [  300/ 3475]
loss: 0.051627  [  400/ 3475]
loss: 0.028097  [  500/ 3475]
loss: 0.022522  [  600/ 3475]
loss: 0.034039  [  700/ 3475]
loss: 0.104207  [  800/ 3475]
loss: 0.075600  [  900/ 3475]
loss: 0.043910  [ 1000/ 3475]
loss: 0.026392  [ 1100/ 3475]
loss: 0.032824  [ 1200/ 3475]
loss: 0.046071  [ 1300/ 3475]
loss: 0.054943  [ 1400/ 3475]
loss: 0.038406  [ 1500/ 3475]
loss: 0.045615  [ 1600/ 3475]
loss: 0.012228  [ 1700/ 3475]
loss: 0.126534  [ 1800/ 3475]
loss: 0.040264  [ 1900/ 3475]
loss: 0.017352  [ 2000/ 3475]
loss: 0.039171  [ 2100/ 3475]
loss: 0.076459  [ 2200/ 3475]
loss: 0.015600  [ 2300/ 3475]
loss: 0.015601  [ 2400/ 3475]
loss: 0.085687  [ 2500/ 3475]
loss: 0.039301  [ 2600/ 3475]
loss: 0.094973  [ 2700/ 3475]
loss: 0.040710  [ 2800/ 3475]
loss: 0.028178  [ 2900/ 3475]
loss: 0.052223  [ 3000/ 3475]
loss: 0.063308  [ 3100/ 3475]
loss: 0.044321  [ 3200/ 3475]
loss: 0.012799  [ 3300/ 3475]
loss: 0.024477  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.024869  [    0/ 3475]
loss: 0.034231  [  100/ 3475]
loss: 0.054887  [  200/ 3475]
loss: 0.225932  [  300/ 3475]
loss: 0.042768  [  400/ 3475]
loss: 0.028837  [  500/ 3475]
loss: 0.020458  [  600/ 3475]
loss: 0.035592  [  700/ 3475]
loss: 0.103766  [  800/ 3475]
loss: 0.070629  [  900/ 3475]
loss: 0.042146  [ 1000/ 3475]
loss: 0.023989  [ 1100/ 3475]
loss: 0.030318  [ 1200/ 3475]
loss: 0.049463  [ 1300/ 3475]
loss: 0.055075  [ 1400/ 3475]
loss: 0.038399  [ 1500/ 3475]
loss: 0.045939  [ 1600/ 3475]
loss: 0.012050  [ 1700/ 3475]
loss: 0.124978  [ 1800/ 3475]
loss: 0.039982  [ 1900/ 3475]
loss: 0.016801  [ 2000/ 3475]
loss: 0.040503  [ 2100/ 3475]
loss: 0.076971  [ 2200/ 3475]
loss: 0.015938  [ 2300/ 3475]
loss: 0.015530  [ 2400/ 3475]
loss: 0.085599  [ 2500/ 3475]
loss: 0.038524  [ 2600/ 3475]
loss: 0.096227  [ 2700/ 3475]
loss: 0.039906  [ 2800/ 3475]
loss: 0.028111  [ 2900/ 3475]
loss: 0.053037  [ 3000/ 3475]
loss: 0.064889  [ 3100/ 3475]
loss: 0.046527  [ 3200/ 3475]
loss: 0.012879  [ 3300/ 3475]
loss: 0.025603  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.025709  [    0/ 3475]
loss: 0.034899  [  100/ 3475]
loss: 0.055222  [  200/ 3475]
loss: 0.227681  [  300/ 3475]
loss: 0.037854  [  400/ 3475]
loss: 0.029068  [  500/ 3475]
loss: 0.019098  [  600/ 3475]
loss: 0.037010  [  700/ 3475]
loss: 0.103873  [  800/ 3475]
loss: 0.069314  [  900/ 3475]
loss: 0.040657  [ 1000/ 3475]
loss: 0.023295  [ 1100/ 3475]
loss: 0.028732  [ 1200/ 3475]
loss: 0.050430  [ 1300/ 3475]
loss: 0.054553  [ 1400/ 3475]
loss: 0.038291  [ 1500/ 3475]
loss: 0.046037  [ 1600/ 3475]
loss: 0.012234  [ 1700/ 3475]
loss: 0.126426  [ 1800/ 3475]
loss: 0.040263  [ 1900/ 3475]
loss: 0.016909  [ 2000/ 3475]
loss: 0.041049  [ 2100/ 3475]
loss: 0.077672  [ 2200/ 3475]
loss: 0.015585  [ 2300/ 3475]
loss: 0.015212  [ 2400/ 3475]
loss: 0.083714  [ 2500/ 3475]
loss: 0.038162  [ 2600/ 3475]
loss: 0.096782  [ 2700/ 3475]
loss: 0.038828  [ 2800/ 3475]
loss: 0.028432  [ 2900/ 3475]
loss: 0.053408  [ 3000/ 3475]
loss: 0.065346  [ 3100/ 3475]
loss: 0.048052  [ 3200/ 3475]
loss: 0.012856  [ 3300/ 3475]
loss: 0.025739  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.025947  [    0/ 3475]
loss: 0.034370  [  100/ 3475]
loss: 0.055192  [  200/ 3475]
loss: 0.228963  [  300/ 3475]
loss: 0.033105  [  400/ 3475]
loss: 0.028912  [  500/ 3475]
loss: 0.019340  [  600/ 3475]
loss: 0.038023  [  700/ 3475]
loss: 0.103510  [  800/ 3475]
loss: 0.068027  [  900/ 3475]
loss: 0.040074  [ 1000/ 3475]
loss: 0.022676  [ 1100/ 3475]
loss: 0.027973  [ 1200/ 3475]
loss: 0.050392  [ 1300/ 3475]
loss: 0.053801  [ 1400/ 3475]
loss: 0.038211  [ 1500/ 3475]
loss: 0.045652  [ 1600/ 3475]
loss: 0.012358  [ 1700/ 3475]
loss: 0.127757  [ 1800/ 3475]
loss: 0.039733  [ 1900/ 3475]
loss: 0.016973  [ 2000/ 3475]
loss: 0.041507  [ 2100/ 3475]
loss: 0.077391  [ 2200/ 3475]
loss: 0.015520  [ 2300/ 3475]
loss: 0.015285  [ 2400/ 3475]
loss: 0.083228  [ 2500/ 3475]
loss: 0.037840  [ 2600/ 3475]
loss: 0.097063  [ 2700/ 3475]
loss: 0.037858  [ 2800/ 3475]
loss: 0.028643  [ 2900/ 3475]
loss: 0.053944  [ 3000/ 3475]
loss: 0.065659  [ 3100/ 3475]
loss: 0.050340  [ 3200/ 3475]
loss: 0.013913  [ 3300/ 3475]
loss: 0.025870  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.026367  [    0/ 3475]
loss: 0.034442  [  100/ 3475]
loss: 0.054473  [  200/ 3475]
loss: 0.227968  [  300/ 3475]
loss: 0.029259  [  400/ 3475]
loss: 0.028215  [  500/ 3475]
loss: 0.019407  [  600/ 3475]
loss: 0.039665  [  700/ 3475]
loss: 0.103170  [  800/ 3475]
loss: 0.064482  [  900/ 3475]
loss: 0.039769  [ 1000/ 3475]
loss: 0.020165  [ 1100/ 3475]
loss: 0.027968  [ 1200/ 3475]
loss: 0.050267  [ 1300/ 3475]
loss: 0.052778  [ 1400/ 3475]
loss: 0.038453  [ 1500/ 3475]
loss: 0.045478  [ 1600/ 3475]
loss: 0.012323  [ 1700/ 3475]
loss: 0.128022  [ 1800/ 3475]
loss: 0.039361  [ 1900/ 3475]
loss: 0.017086  [ 2000/ 3475]
loss: 0.041586  [ 2100/ 3475]
loss: 0.077597  [ 2200/ 3475]
loss: 0.016087  [ 2300/ 3475]
loss: 0.015225  [ 2400/ 3475]
loss: 0.082647  [ 2500/ 3475]
loss: 0.037965  [ 2600/ 3475]
loss: 0.097708  [ 2700/ 3475]
loss: 0.037136  [ 2800/ 3475]
loss: 0.028540  [ 2900/ 3475]
loss: 0.054190  [ 3000/ 3475]
loss: 0.065727  [ 3100/ 3475]
loss: 0.050881  [ 3200/ 3475]
loss: 0.014769  [ 3300/ 3475]
loss: 0.026385  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.025914  [    0/ 3475]
loss: 0.034116  [  100/ 3475]
loss: 0.053867  [  200/ 3475]
loss: 0.227071  [  300/ 3475]
loss: 0.024918  [  400/ 3475]
loss: 0.027667  [  500/ 3475]
loss: 0.019071  [  600/ 3475]
loss: 0.042171  [  700/ 3475]
loss: 0.102665  [  800/ 3475]
loss: 0.062106  [  900/ 3475]
loss: 0.038459  [ 1000/ 3475]
loss: 0.019622  [ 1100/ 3475]
loss: 0.027775  [ 1200/ 3475]
loss: 0.048725  [ 1300/ 3475]
loss: 0.051840  [ 1400/ 3475]
loss: 0.038663  [ 1500/ 3475]
loss: 0.044386  [ 1600/ 3475]
loss: 0.012232  [ 1700/ 3475]
loss: 0.129641  [ 1800/ 3475]
loss: 0.039105  [ 1900/ 3475]
loss: 0.017146  [ 2000/ 3475]
loss: 0.041563  [ 2100/ 3475]
loss: 0.078629  [ 2200/ 3475]
loss: 0.016641  [ 2300/ 3475]
loss: 0.015034  [ 2400/ 3475]
loss: 0.081139  [ 2500/ 3475]
loss: 0.038196  [ 2600/ 3475]
loss: 0.098710  [ 2700/ 3475]
loss: 0.035782  [ 2800/ 3475]
loss: 0.028912  [ 2900/ 3475]
loss: 0.053905  [ 3000/ 3475]
loss: 0.066047  [ 3100/ 3475]
loss: 0.051116  [ 3200/ 3475]
loss: 0.015187  [ 3300/ 3475]
loss: 0.026604  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.025354  [    0/ 3475]
loss: 0.033507  [  100/ 3475]
loss: 0.053294  [  200/ 3475]
loss: 0.226647  [  300/ 3475]
loss: 0.023742  [  400/ 3475]
loss: 0.027378  [  500/ 3475]
loss: 0.019009  [  600/ 3475]
loss: 0.046910  [  700/ 3475]
loss: 0.102451  [  800/ 3475]
loss: 0.061197  [  900/ 3475]
loss: 0.036235  [ 1000/ 3475]
loss: 0.019137  [ 1100/ 3475]
loss: 0.028431  [ 1200/ 3475]
loss: 0.048797  [ 1300/ 3475]
loss: 0.049962  [ 1400/ 3475]
loss: 0.039264  [ 1500/ 3475]
loss: 0.042425  [ 1600/ 3475]
loss: 0.012397  [ 1700/ 3475]
loss: 0.126341  [ 1800/ 3475]
loss: 0.038775  [ 1900/ 3475]
loss: 0.016963  [ 2000/ 3475]
loss: 0.040523  [ 2100/ 3475]
loss: 0.079891  [ 2200/ 3475]
loss: 0.017324  [ 2300/ 3475]
loss: 0.014824  [ 2400/ 3475]
loss: 0.080158  [ 2500/ 3475]
loss: 0.039033  [ 2600/ 3475]
loss: 0.101604  [ 2700/ 3475]
loss: 0.033702  [ 2800/ 3475]
loss: 0.029691  [ 2900/ 3475]
loss: 0.054328  [ 3000/ 3475]
loss: 0.067094  [ 3100/ 3475]
loss: 0.051138  [ 3200/ 3475]
loss: 0.013649  [ 3300/ 3475]
loss: 0.027290  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [1.2257279  0.36470598]
[2 1 2 ... 1 0 1]
[0 1 2 ... 1 0 1]
Cluster 0 Occurrences: 1162; KMEANS: 1224
Cluster 1 Occurrences: 1164; KMEANS: 1146
Cluster 2 Occurrences: 1149; KMEANS: 1105
Centroids: [[-0.025474338, 0.4140057], [-1.6562138, -1.5341344], [1.4271247, 1.529088]]
Centroids: [[-0.00811971, 0.36753616], [-1.6848125, -1.5640879], [1.4688382, 1.6242939]]
Contingency Matrix: 
[[1117    1   44]
 [  20 1141    3]
 [  87    4 1058]]
[[1117, -1, 44], [-1, -1, -1], [87, -1, 1058]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 1058]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 0, 2: 2}
New Contingency Matrix: 
[[1117    1   44]
 [  20 1141    3]
 [  87    4 1058]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1117, 1141, 1058], Sum: 3316
All_Elements: [1117, 1, 44, 20, 1141, 3, 87, 4, 1058], Sum: 3475
Accuracy: 0.9542446043165468
Done!
