Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_12
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000017452F2BA90>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x0000017452F74128>
Epoch 1
-------------------------------
loss: 0.213129  [    0/ 3477]
loss: 0.225936  [  100/ 3477]
loss: 0.086447  [  200/ 3477]
loss: 0.120273  [  300/ 3477]
loss: 0.041249  [  400/ 3477]
loss: 0.097657  [  500/ 3477]
loss: 0.020298  [  600/ 3477]
loss: 0.025494  [  700/ 3477]
loss: 0.021378  [  800/ 3477]
loss: 0.019925  [  900/ 3477]
loss: 0.022888  [ 1000/ 3477]
loss: 0.004953  [ 1100/ 3477]
loss: 0.023113  [ 1200/ 3477]
loss: 0.017073  [ 1300/ 3477]
loss: 0.011266  [ 1400/ 3477]
loss: 0.028587  [ 1500/ 3477]
loss: 0.013343  [ 1600/ 3477]
loss: 0.013283  [ 1700/ 3477]
loss: 0.008369  [ 1800/ 3477]
loss: 0.016401  [ 1900/ 3477]
loss: 0.026020  [ 2000/ 3477]
loss: 0.017816  [ 2100/ 3477]
loss: 0.022580  [ 2200/ 3477]
loss: 0.011742  [ 2300/ 3477]
loss: 0.010858  [ 2400/ 3477]
loss: 0.103913  [ 2500/ 3477]
loss: 0.086223  [ 2600/ 3477]
loss: 0.012392  [ 2700/ 3477]
loss: 0.175572  [ 2800/ 3477]
loss: 0.021448  [ 2900/ 3477]
loss: 0.017376  [ 3000/ 3477]
loss: 0.015119  [ 3100/ 3477]
loss: 0.088611  [ 3200/ 3477]
loss: 0.006295  [ 3300/ 3477]
loss: 0.007116  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.024485  [    0/ 3477]
loss: 0.028653  [  100/ 3477]
loss: 0.039329  [  200/ 3477]
loss: 0.030261  [  300/ 3477]
loss: 0.016338  [  400/ 3477]
loss: 0.005291  [  500/ 3477]
loss: 0.011100  [  600/ 3477]
loss: 0.021007  [  700/ 3477]
loss: 0.021941  [  800/ 3477]
loss: 0.017689  [  900/ 3477]
loss: 0.018864  [ 1000/ 3477]
loss: 0.002991  [ 1100/ 3477]
loss: 0.022645  [ 1200/ 3477]
loss: 0.009854  [ 1300/ 3477]
loss: 0.010536  [ 1400/ 3477]
loss: 0.013473  [ 1500/ 3477]
loss: 0.010180  [ 1600/ 3477]
loss: 0.009320  [ 1700/ 3477]
loss: 0.007371  [ 1800/ 3477]
loss: 0.009827  [ 1900/ 3477]
loss: 0.021126  [ 2000/ 3477]
loss: 0.014187  [ 2100/ 3477]
loss: 0.021383  [ 2200/ 3477]
loss: 0.013595  [ 2300/ 3477]
loss: 0.008596  [ 2400/ 3477]
loss: 0.088872  [ 2500/ 3477]
loss: 0.077150  [ 2600/ 3477]
loss: 0.010040  [ 2700/ 3477]
loss: 0.089926  [ 2800/ 3477]
loss: 0.019021  [ 2900/ 3477]
loss: 0.015294  [ 3000/ 3477]
loss: 0.013099  [ 3100/ 3477]
loss: 0.077009  [ 3200/ 3477]
loss: 0.005860  [ 3300/ 3477]
loss: 0.006725  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.026837  [    0/ 3477]
loss: 0.021210  [  100/ 3477]
loss: 0.039871  [  200/ 3477]
loss: 0.025726  [  300/ 3477]
loss: 0.014690  [  400/ 3477]
loss: 0.006116  [  500/ 3477]
loss: 0.012164  [  600/ 3477]
loss: 0.020338  [  700/ 3477]
loss: 0.021069  [  800/ 3477]
loss: 0.014286  [  900/ 3477]
loss: 0.019649  [ 1000/ 3477]
loss: 0.003290  [ 1100/ 3477]
loss: 0.024086  [ 1200/ 3477]
loss: 0.008110  [ 1300/ 3477]
loss: 0.012660  [ 1400/ 3477]
loss: 0.009851  [ 1500/ 3477]
loss: 0.009045  [ 1600/ 3477]
loss: 0.010395  [ 1700/ 3477]
loss: 0.008870  [ 1800/ 3477]
loss: 0.009065  [ 1900/ 3477]
loss: 0.020897  [ 2000/ 3477]
loss: 0.015505  [ 2100/ 3477]
loss: 0.020525  [ 2200/ 3477]
loss: 0.014615  [ 2300/ 3477]
loss: 0.009165  [ 2400/ 3477]
loss: 0.091000  [ 2500/ 3477]
loss: 0.075100  [ 2600/ 3477]
loss: 0.004473  [ 2700/ 3477]
loss: 0.062709  [ 2800/ 3477]
loss: 0.020000  [ 2900/ 3477]
loss: 0.011599  [ 3000/ 3477]
loss: 0.010848  [ 3100/ 3477]
loss: 0.075125  [ 3200/ 3477]
loss: 0.005332  [ 3300/ 3477]
loss: 0.006623  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.027282  [    0/ 3477]
loss: 0.019102  [  100/ 3477]
loss: 0.034154  [  200/ 3477]
loss: 0.024344  [  300/ 3477]
loss: 0.013762  [  400/ 3477]
loss: 0.006570  [  500/ 3477]
loss: 0.011865  [  600/ 3477]
loss: 0.018968  [  700/ 3477]
loss: 0.019196  [  800/ 3477]
loss: 0.011930  [  900/ 3477]
loss: 0.020511  [ 1000/ 3477]
loss: 0.002202  [ 1100/ 3477]
loss: 0.022372  [ 1200/ 3477]
loss: 0.008364  [ 1300/ 3477]
loss: 0.013327  [ 1400/ 3477]
loss: 0.010497  [ 1500/ 3477]
loss: 0.009329  [ 1600/ 3477]
loss: 0.010007  [ 1700/ 3477]
loss: 0.011084  [ 1800/ 3477]
loss: 0.009123  [ 1900/ 3477]
loss: 0.021637  [ 2000/ 3477]
loss: 0.013499  [ 2100/ 3477]
loss: 0.021175  [ 2200/ 3477]
loss: 0.014908  [ 2300/ 3477]
loss: 0.009145  [ 2400/ 3477]
loss: 0.089423  [ 2500/ 3477]
loss: 0.077324  [ 2600/ 3477]
loss: 0.003603  [ 2700/ 3477]
loss: 0.068889  [ 2800/ 3477]
loss: 0.020396  [ 2900/ 3477]
loss: 0.010544  [ 3000/ 3477]
loss: 0.011001  [ 3100/ 3477]
loss: 0.077470  [ 3200/ 3477]
loss: 0.005569  [ 3300/ 3477]
loss: 0.006639  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.026671  [    0/ 3477]
loss: 0.017657  [  100/ 3477]
loss: 0.033495  [  200/ 3477]
loss: 0.024937  [  300/ 3477]
loss: 0.013790  [  400/ 3477]
loss: 0.006905  [  500/ 3477]
loss: 0.010787  [  600/ 3477]
loss: 0.018310  [  700/ 3477]
loss: 0.018441  [  800/ 3477]
loss: 0.011774  [  900/ 3477]
loss: 0.020423  [ 1000/ 3477]
loss: 0.003155  [ 1100/ 3477]
loss: 0.021536  [ 1200/ 3477]
loss: 0.007596  [ 1300/ 3477]
loss: 0.014006  [ 1400/ 3477]
loss: 0.011938  [ 1500/ 3477]
loss: 0.009266  [ 1600/ 3477]
loss: 0.010242  [ 1700/ 3477]
loss: 0.011649  [ 1800/ 3477]
loss: 0.009240  [ 1900/ 3477]
loss: 0.022190  [ 2000/ 3477]
loss: 0.011254  [ 2100/ 3477]
loss: 0.021419  [ 2200/ 3477]
loss: 0.014618  [ 2300/ 3477]
loss: 0.009329  [ 2400/ 3477]
loss: 0.091649  [ 2500/ 3477]
loss: 0.078573  [ 2600/ 3477]
loss: 0.003176  [ 2700/ 3477]
loss: 0.068382  [ 2800/ 3477]
loss: 0.020788  [ 2900/ 3477]
loss: 0.010336  [ 3000/ 3477]
loss: 0.011123  [ 3100/ 3477]
loss: 0.076540  [ 3200/ 3477]
loss: 0.005617  [ 3300/ 3477]
loss: 0.006665  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.025659  [    0/ 3477]
loss: 0.017639  [  100/ 3477]
loss: 0.033770  [  200/ 3477]
loss: 0.024966  [  300/ 3477]
loss: 0.013542  [  400/ 3477]
loss: 0.007011  [  500/ 3477]
loss: 0.010879  [  600/ 3477]
loss: 0.018004  [  700/ 3477]
loss: 0.017386  [  800/ 3477]
loss: 0.012402  [  900/ 3477]
loss: 0.020641  [ 1000/ 3477]
loss: 0.003010  [ 1100/ 3477]
loss: 0.022145  [ 1200/ 3477]
loss: 0.006621  [ 1300/ 3477]
loss: 0.014818  [ 1400/ 3477]
loss: 0.013494  [ 1500/ 3477]
loss: 0.009450  [ 1600/ 3477]
loss: 0.010111  [ 1700/ 3477]
loss: 0.012034  [ 1800/ 3477]
loss: 0.009375  [ 1900/ 3477]
loss: 0.022725  [ 2000/ 3477]
loss: 0.010168  [ 2100/ 3477]
loss: 0.021633  [ 2200/ 3477]
loss: 0.014866  [ 2300/ 3477]
loss: 0.009148  [ 2400/ 3477]
loss: 0.093479  [ 2500/ 3477]
loss: 0.077899  [ 2600/ 3477]
loss: 0.003312  [ 2700/ 3477]
loss: 0.063367  [ 2800/ 3477]
loss: 0.021260  [ 2900/ 3477]
loss: 0.010359  [ 3000/ 3477]
loss: 0.011456  [ 3100/ 3477]
loss: 0.076570  [ 3200/ 3477]
loss: 0.005555  [ 3300/ 3477]
loss: 0.006568  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.023924  [    0/ 3477]
loss: 0.016948  [  100/ 3477]
loss: 0.031968  [  200/ 3477]
loss: 0.025012  [  300/ 3477]
loss: 0.014013  [  400/ 3477]
loss: 0.007164  [  500/ 3477]
loss: 0.010533  [  600/ 3477]
loss: 0.017702  [  700/ 3477]
loss: 0.017149  [  800/ 3477]
loss: 0.012979  [  900/ 3477]
loss: 0.021154  [ 1000/ 3477]
loss: 0.002645  [ 1100/ 3477]
loss: 0.022161  [ 1200/ 3477]
loss: 0.006453  [ 1300/ 3477]
loss: 0.014998  [ 1400/ 3477]
loss: 0.013934  [ 1500/ 3477]
loss: 0.009400  [ 1600/ 3477]
loss: 0.010078  [ 1700/ 3477]
loss: 0.012382  [ 1800/ 3477]
loss: 0.009299  [ 1900/ 3477]
loss: 0.022910  [ 2000/ 3477]
loss: 0.009666  [ 2100/ 3477]
loss: 0.021827  [ 2200/ 3477]
loss: 0.014914  [ 2300/ 3477]
loss: 0.009157  [ 2400/ 3477]
loss: 0.094315  [ 2500/ 3477]
loss: 0.077831  [ 2600/ 3477]
loss: 0.003522  [ 2700/ 3477]
loss: 0.057250  [ 2800/ 3477]
loss: 0.021573  [ 2900/ 3477]
loss: 0.010508  [ 3000/ 3477]
loss: 0.012136  [ 3100/ 3477]
loss: 0.076494  [ 3200/ 3477]
loss: 0.005666  [ 3300/ 3477]
loss: 0.006766  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.022638  [    0/ 3477]
loss: 0.016593  [  100/ 3477]
loss: 0.031369  [  200/ 3477]
loss: 0.025364  [  300/ 3477]
loss: 0.012828  [  400/ 3477]
loss: 0.007364  [  500/ 3477]
loss: 0.010348  [  600/ 3477]
loss: 0.017560  [  700/ 3477]
loss: 0.017154  [  800/ 3477]
loss: 0.013044  [  900/ 3477]
loss: 0.021753  [ 1000/ 3477]
loss: 0.002805  [ 1100/ 3477]
loss: 0.021177  [ 1200/ 3477]
loss: 0.006405  [ 1300/ 3477]
loss: 0.015069  [ 1400/ 3477]
loss: 0.014202  [ 1500/ 3477]
loss: 0.009165  [ 1600/ 3477]
loss: 0.010046  [ 1700/ 3477]
loss: 0.012928  [ 1800/ 3477]
loss: 0.009235  [ 1900/ 3477]
loss: 0.022864  [ 2000/ 3477]
loss: 0.009263  [ 2100/ 3477]
loss: 0.021860  [ 2200/ 3477]
loss: 0.014868  [ 2300/ 3477]
loss: 0.009091  [ 2400/ 3477]
loss: 0.094695  [ 2500/ 3477]
loss: 0.077278  [ 2600/ 3477]
loss: 0.003833  [ 2700/ 3477]
loss: 0.056124  [ 2800/ 3477]
loss: 0.021683  [ 2900/ 3477]
loss: 0.010587  [ 3000/ 3477]
loss: 0.012046  [ 3100/ 3477]
loss: 0.076970  [ 3200/ 3477]
loss: 0.005464  [ 3300/ 3477]
loss: 0.006724  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [-2.3704054 -1.1602514]
[1 0 2 ... 1 1 1]
[1 2 0 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1182
Cluster 1 Occurrences: 1188; KMEANS: 1173
Cluster 2 Occurrences: 1157; KMEANS: 1122
Centroids: [[1.4610108, -0.10447602], [-2.3959394, -0.9494541], [-0.035998456, 1.4427841]]
Centroids: [[-0.041137844, 1.4309167], [-2.418712, -0.96910965], [1.4720246, -0.11719691]]
Contingency Matrix: 
[[  13    1 1118]
 [  15 1169    4]
 [1154    3    0]]
[[13, -1, 1118], [-1, -1, -1], [1154, -1, 0]]
[[-1, -1, 1118], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1118    1   13]
 [   4 1169   15]
 [   0    3 1154]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1118, 1169, 1154], Sum: 3441
All_Elements: [1118, 1, 13, 4, 1169, 15, 0, 3, 1154], Sum: 3477
Accuracy: 0.9896462467644521
Done!
