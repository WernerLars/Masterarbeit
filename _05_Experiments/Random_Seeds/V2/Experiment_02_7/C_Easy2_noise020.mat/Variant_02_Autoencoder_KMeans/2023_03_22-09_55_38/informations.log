Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_55_38
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000174518A14A8>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x0000017452F740F0>
Epoch 1
-------------------------------
loss: 0.141320  [    0/ 3526]
loss: 0.194202  [  100/ 3526]
loss: 0.113222  [  200/ 3526]
loss: 0.055560  [  300/ 3526]
loss: 0.017584  [  400/ 3526]
loss: 0.060278  [  500/ 3526]
loss: 0.029961  [  600/ 3526]
loss: 0.020709  [  700/ 3526]
loss: 0.044098  [  800/ 3526]
loss: 0.012186  [  900/ 3526]
loss: 0.042535  [ 1000/ 3526]
loss: 0.025662  [ 1100/ 3526]
loss: 0.054186  [ 1200/ 3526]
loss: 0.034426  [ 1300/ 3526]
loss: 0.029717  [ 1400/ 3526]
loss: 0.016305  [ 1500/ 3526]
loss: 0.039972  [ 1600/ 3526]
loss: 0.025062  [ 1700/ 3526]
loss: 0.021143  [ 1800/ 3526]
loss: 0.021978  [ 1900/ 3526]
loss: 0.017353  [ 2000/ 3526]
loss: 0.095642  [ 2100/ 3526]
loss: 0.026031  [ 2200/ 3526]
loss: 0.017287  [ 2300/ 3526]
loss: 0.009267  [ 2400/ 3526]
loss: 0.015579  [ 2500/ 3526]
loss: 0.020053  [ 2600/ 3526]
loss: 0.024684  [ 2700/ 3526]
loss: 0.006514  [ 2800/ 3526]
loss: 0.012289  [ 2900/ 3526]
loss: 0.057150  [ 3000/ 3526]
loss: 0.166308  [ 3100/ 3526]
loss: 0.034875  [ 3200/ 3526]
loss: 0.026631  [ 3300/ 3526]
loss: 0.048294  [ 3400/ 3526]
loss: 0.045984  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.015471  [    0/ 3526]
loss: 0.044983  [  100/ 3526]
loss: 0.050332  [  200/ 3526]
loss: 0.004376  [  300/ 3526]
loss: 0.016634  [  400/ 3526]
loss: 0.036354  [  500/ 3526]
loss: 0.014085  [  600/ 3526]
loss: 0.014519  [  700/ 3526]
loss: 0.034973  [  800/ 3526]
loss: 0.013385  [  900/ 3526]
loss: 0.019072  [ 1000/ 3526]
loss: 0.016685  [ 1100/ 3526]
loss: 0.029437  [ 1200/ 3526]
loss: 0.023853  [ 1300/ 3526]
loss: 0.029023  [ 1400/ 3526]
loss: 0.019369  [ 1500/ 3526]
loss: 0.041377  [ 1600/ 3526]
loss: 0.019816  [ 1700/ 3526]
loss: 0.021679  [ 1800/ 3526]
loss: 0.015690  [ 1900/ 3526]
loss: 0.015367  [ 2000/ 3526]
loss: 0.136520  [ 2100/ 3526]
loss: 0.021723  [ 2200/ 3526]
loss: 0.016445  [ 2300/ 3526]
loss: 0.008598  [ 2400/ 3526]
loss: 0.016798  [ 2500/ 3526]
loss: 0.016202  [ 2600/ 3526]
loss: 0.016559  [ 2700/ 3526]
loss: 0.007078  [ 2800/ 3526]
loss: 0.009287  [ 2900/ 3526]
loss: 0.059376  [ 3000/ 3526]
loss: 0.160048  [ 3100/ 3526]
loss: 0.028126  [ 3200/ 3526]
loss: 0.025795  [ 3300/ 3526]
loss: 0.045415  [ 3400/ 3526]
loss: 0.040653  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.016116  [    0/ 3526]
loss: 0.045466  [  100/ 3526]
loss: 0.050303  [  200/ 3526]
loss: 0.004441  [  300/ 3526]
loss: 0.016420  [  400/ 3526]
loss: 0.034438  [  500/ 3526]
loss: 0.012134  [  600/ 3526]
loss: 0.015401  [  700/ 3526]
loss: 0.034907  [  800/ 3526]
loss: 0.013242  [  900/ 3526]
loss: 0.017104  [ 1000/ 3526]
loss: 0.016325  [ 1100/ 3526]
loss: 0.030018  [ 1200/ 3526]
loss: 0.022783  [ 1300/ 3526]
loss: 0.028393  [ 1400/ 3526]
loss: 0.018942  [ 1500/ 3526]
loss: 0.043406  [ 1600/ 3526]
loss: 0.019360  [ 1700/ 3526]
loss: 0.020587  [ 1800/ 3526]
loss: 0.014828  [ 1900/ 3526]
loss: 0.015160  [ 2000/ 3526]
loss: 0.120932  [ 2100/ 3526]
loss: 0.021845  [ 2200/ 3526]
loss: 0.016397  [ 2300/ 3526]
loss: 0.008588  [ 2400/ 3526]
loss: 0.016725  [ 2500/ 3526]
loss: 0.015953  [ 2600/ 3526]
loss: 0.015905  [ 2700/ 3526]
loss: 0.007053  [ 2800/ 3526]
loss: 0.009347  [ 2900/ 3526]
loss: 0.060677  [ 3000/ 3526]
loss: 0.157864  [ 3100/ 3526]
loss: 0.026710  [ 3200/ 3526]
loss: 0.025595  [ 3300/ 3526]
loss: 0.044574  [ 3400/ 3526]
loss: 0.039143  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.016301  [    0/ 3526]
loss: 0.045851  [  100/ 3526]
loss: 0.050028  [  200/ 3526]
loss: 0.004641  [  300/ 3526]
loss: 0.016146  [  400/ 3526]
loss: 0.033865  [  500/ 3526]
loss: 0.011433  [  600/ 3526]
loss: 0.015644  [  700/ 3526]
loss: 0.034088  [  800/ 3526]
loss: 0.013050  [  900/ 3526]
loss: 0.017622  [ 1000/ 3526]
loss: 0.016204  [ 1100/ 3526]
loss: 0.030735  [ 1200/ 3526]
loss: 0.022369  [ 1300/ 3526]
loss: 0.028121  [ 1400/ 3526]
loss: 0.018236  [ 1500/ 3526]
loss: 0.045463  [ 1600/ 3526]
loss: 0.019295  [ 1700/ 3526]
loss: 0.019902  [ 1800/ 3526]
loss: 0.014782  [ 1900/ 3526]
loss: 0.015064  [ 2000/ 3526]
loss: 0.117413  [ 2100/ 3526]
loss: 0.021841  [ 2200/ 3526]
loss: 0.016359  [ 2300/ 3526]
loss: 0.008558  [ 2400/ 3526]
loss: 0.016394  [ 2500/ 3526]
loss: 0.015873  [ 2600/ 3526]
loss: 0.015900  [ 2700/ 3526]
loss: 0.007204  [ 2800/ 3526]
loss: 0.009396  [ 2900/ 3526]
loss: 0.060798  [ 3000/ 3526]
loss: 0.158463  [ 3100/ 3526]
loss: 0.025794  [ 3200/ 3526]
loss: 0.025524  [ 3300/ 3526]
loss: 0.043896  [ 3400/ 3526]
loss: 0.038077  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.016355  [    0/ 3526]
loss: 0.045144  [  100/ 3526]
loss: 0.049647  [  200/ 3526]
loss: 0.004809  [  300/ 3526]
loss: 0.015920  [  400/ 3526]
loss: 0.033782  [  500/ 3526]
loss: 0.010847  [  600/ 3526]
loss: 0.015659  [  700/ 3526]
loss: 0.033287  [  800/ 3526]
loss: 0.012958  [  900/ 3526]
loss: 0.017697  [ 1000/ 3526]
loss: 0.016022  [ 1100/ 3526]
loss: 0.031078  [ 1200/ 3526]
loss: 0.022731  [ 1300/ 3526]
loss: 0.028213  [ 1400/ 3526]
loss: 0.017914  [ 1500/ 3526]
loss: 0.045085  [ 1600/ 3526]
loss: 0.019169  [ 1700/ 3526]
loss: 0.019503  [ 1800/ 3526]
loss: 0.014988  [ 1900/ 3526]
loss: 0.015025  [ 2000/ 3526]
loss: 0.117646  [ 2100/ 3526]
loss: 0.022145  [ 2200/ 3526]
loss: 0.016435  [ 2300/ 3526]
loss: 0.008589  [ 2400/ 3526]
loss: 0.016232  [ 2500/ 3526]
loss: 0.015810  [ 2600/ 3526]
loss: 0.015826  [ 2700/ 3526]
loss: 0.007208  [ 2800/ 3526]
loss: 0.009322  [ 2900/ 3526]
loss: 0.060216  [ 3000/ 3526]
loss: 0.159403  [ 3100/ 3526]
loss: 0.025282  [ 3200/ 3526]
loss: 0.025357  [ 3300/ 3526]
loss: 0.044156  [ 3400/ 3526]
loss: 0.038425  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.016287  [    0/ 3526]
loss: 0.044962  [  100/ 3526]
loss: 0.049207  [  200/ 3526]
loss: 0.004850  [  300/ 3526]
loss: 0.015662  [  400/ 3526]
loss: 0.034025  [  500/ 3526]
loss: 0.010277  [  600/ 3526]
loss: 0.015356  [  700/ 3526]
loss: 0.031985  [  800/ 3526]
loss: 0.012838  [  900/ 3526]
loss: 0.017341  [ 1000/ 3526]
loss: 0.015550  [ 1100/ 3526]
loss: 0.031240  [ 1200/ 3526]
loss: 0.022811  [ 1300/ 3526]
loss: 0.028427  [ 1400/ 3526]
loss: 0.017434  [ 1500/ 3526]
loss: 0.044687  [ 1600/ 3526]
loss: 0.019198  [ 1700/ 3526]
loss: 0.019627  [ 1800/ 3526]
loss: 0.015245  [ 1900/ 3526]
loss: 0.015007  [ 2000/ 3526]
loss: 0.122462  [ 2100/ 3526]
loss: 0.022556  [ 2200/ 3526]
loss: 0.016518  [ 2300/ 3526]
loss: 0.008674  [ 2400/ 3526]
loss: 0.016049  [ 2500/ 3526]
loss: 0.015791  [ 2600/ 3526]
loss: 0.015809  [ 2700/ 3526]
loss: 0.007252  [ 2800/ 3526]
loss: 0.009222  [ 2900/ 3526]
loss: 0.059065  [ 3000/ 3526]
loss: 0.159955  [ 3100/ 3526]
loss: 0.025352  [ 3200/ 3526]
loss: 0.025257  [ 3300/ 3526]
loss: 0.044745  [ 3400/ 3526]
loss: 0.039158  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.016017  [    0/ 3526]
loss: 0.046832  [  100/ 3526]
loss: 0.048690  [  200/ 3526]
loss: 0.004844  [  300/ 3526]
loss: 0.015290  [  400/ 3526]
loss: 0.034562  [  500/ 3526]
loss: 0.010405  [  600/ 3526]
loss: 0.015042  [  700/ 3526]
loss: 0.030577  [  800/ 3526]
loss: 0.012659  [  900/ 3526]
loss: 0.016731  [ 1000/ 3526]
loss: 0.015209  [ 1100/ 3526]
loss: 0.031023  [ 1200/ 3526]
loss: 0.023050  [ 1300/ 3526]
loss: 0.028885  [ 1400/ 3526]
loss: 0.017421  [ 1500/ 3526]
loss: 0.044595  [ 1600/ 3526]
loss: 0.019057  [ 1700/ 3526]
loss: 0.019770  [ 1800/ 3526]
loss: 0.015652  [ 1900/ 3526]
loss: 0.015035  [ 2000/ 3526]
loss: 0.124896  [ 2100/ 3526]
loss: 0.023203  [ 2200/ 3526]
loss: 0.016602  [ 2300/ 3526]
loss: 0.008872  [ 2400/ 3526]
loss: 0.015829  [ 2500/ 3526]
loss: 0.015746  [ 2600/ 3526]
loss: 0.015540  [ 2700/ 3526]
loss: 0.007342  [ 2800/ 3526]
loss: 0.009191  [ 2900/ 3526]
loss: 0.057398  [ 3000/ 3526]
loss: 0.159449  [ 3100/ 3526]
loss: 0.025677  [ 3200/ 3526]
loss: 0.025156  [ 3300/ 3526]
loss: 0.044490  [ 3400/ 3526]
loss: 0.039581  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.015594  [    0/ 3526]
loss: 0.045864  [  100/ 3526]
loss: 0.047768  [  200/ 3526]
loss: 0.004783  [  300/ 3526]
loss: 0.014866  [  400/ 3526]
loss: 0.035526  [  500/ 3526]
loss: 0.010657  [  600/ 3526]
loss: 0.014561  [  700/ 3526]
loss: 0.029105  [  800/ 3526]
loss: 0.012514  [  900/ 3526]
loss: 0.016433  [ 1000/ 3526]
loss: 0.014980  [ 1100/ 3526]
loss: 0.030868  [ 1200/ 3526]
loss: 0.023183  [ 1300/ 3526]
loss: 0.029469  [ 1400/ 3526]
loss: 0.017582  [ 1500/ 3526]
loss: 0.044502  [ 1600/ 3526]
loss: 0.019090  [ 1700/ 3526]
loss: 0.019984  [ 1800/ 3526]
loss: 0.016054  [ 1900/ 3526]
loss: 0.015089  [ 2000/ 3526]
loss: 0.126697  [ 2100/ 3526]
loss: 0.024603  [ 2200/ 3526]
loss: 0.016593  [ 2300/ 3526]
loss: 0.009054  [ 2400/ 3526]
loss: 0.015878  [ 2500/ 3526]
loss: 0.015817  [ 2600/ 3526]
loss: 0.015537  [ 2700/ 3526]
loss: 0.007435  [ 2800/ 3526]
loss: 0.009186  [ 2900/ 3526]
loss: 0.055642  [ 3000/ 3526]
loss: 0.160197  [ 3100/ 3526]
loss: 0.025901  [ 3200/ 3526]
loss: 0.025010  [ 3300/ 3526]
loss: 0.043649  [ 3400/ 3526]
loss: 0.039991  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [1.0057151  0.01820987]
[2 0 2 ... 0 2 2]
[1 2 1 ... 2 1 1]
Cluster 0 Occurrences: 1186; KMEANS: 1265
Cluster 1 Occurrences: 1188; KMEANS: 1141
Cluster 2 Occurrences: 1152; KMEANS: 1120
Centroids: [[-0.6983383, -1.1762646], [-0.2609132, -0.5854355], [1.0418582, 0.23876987]]
Centroids: [[-0.23205166, -0.57600206], [1.0498275, 0.24882331], [-0.75203663, -1.2238832]]
Contingency Matrix: 
[[ 163    0 1023]
 [1090    2   96]
 [  12 1139    1]]
[[163, -1, 1023], [1090, -1, 96], [-1, -1, -1]]
[[-1, -1, 1023], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 0, 0: 2}
New Contingency Matrix: 
[[1023  163    0]
 [  96 1090    2]
 [   1   12 1139]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1023, 1090, 1139], Sum: 3252
All_Elements: [1023, 163, 0, 96, 1090, 2, 1, 12, 1139], Sum: 3526
Accuracy: 0.9222915484968803
Done!
