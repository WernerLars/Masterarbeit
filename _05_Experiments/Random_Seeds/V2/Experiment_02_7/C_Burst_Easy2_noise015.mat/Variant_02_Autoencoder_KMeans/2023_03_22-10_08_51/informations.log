Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_51
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000017483EAA6A0>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x00000174870F2588>
Epoch 1
-------------------------------
loss: 0.202381  [    0/ 3442]
loss: 0.103033  [  100/ 3442]
loss: 0.056581  [  200/ 3442]
loss: 0.018984  [  300/ 3442]
loss: 0.034593  [  400/ 3442]
loss: 0.027155  [  500/ 3442]
loss: 0.030964  [  600/ 3442]
loss: 0.015868  [  700/ 3442]
loss: 0.024563  [  800/ 3442]
loss: 0.046109  [  900/ 3442]
loss: 0.029518  [ 1000/ 3442]
loss: 0.004162  [ 1100/ 3442]
loss: 0.024083  [ 1200/ 3442]
loss: 0.011032  [ 1300/ 3442]
loss: 0.023597  [ 1400/ 3442]
loss: 0.019032  [ 1500/ 3442]
loss: 0.015321  [ 1600/ 3442]
loss: 0.013278  [ 1700/ 3442]
loss: 0.018168  [ 1800/ 3442]
loss: 0.029173  [ 1900/ 3442]
loss: 0.020752  [ 2000/ 3442]
loss: 0.016075  [ 2100/ 3442]
loss: 0.017588  [ 2200/ 3442]
loss: 0.010452  [ 2300/ 3442]
loss: 0.019042  [ 2400/ 3442]
loss: 0.033615  [ 2500/ 3442]
loss: 0.136172  [ 2600/ 3442]
loss: 0.030469  [ 2700/ 3442]
loss: 0.012521  [ 2800/ 3442]
loss: 0.005992  [ 2900/ 3442]
loss: 0.008086  [ 3000/ 3442]
loss: 0.023223  [ 3100/ 3442]
loss: 0.105303  [ 3200/ 3442]
loss: 0.017030  [ 3300/ 3442]
loss: 0.013385  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.017579  [    0/ 3442]
loss: 0.011057  [  100/ 3442]
loss: 0.034120  [  200/ 3442]
loss: 0.006901  [  300/ 3442]
loss: 0.014568  [  400/ 3442]
loss: 0.010528  [  500/ 3442]
loss: 0.027682  [  600/ 3442]
loss: 0.012914  [  700/ 3442]
loss: 0.014701  [  800/ 3442]
loss: 0.024615  [  900/ 3442]
loss: 0.012237  [ 1000/ 3442]
loss: 0.003241  [ 1100/ 3442]
loss: 0.013322  [ 1200/ 3442]
loss: 0.009647  [ 1300/ 3442]
loss: 0.012045  [ 1400/ 3442]
loss: 0.018986  [ 1500/ 3442]
loss: 0.019888  [ 1600/ 3442]
loss: 0.012318  [ 1700/ 3442]
loss: 0.014799  [ 1800/ 3442]
loss: 0.021281  [ 1900/ 3442]
loss: 0.016137  [ 2000/ 3442]
loss: 0.013041  [ 2100/ 3442]
loss: 0.017133  [ 2200/ 3442]
loss: 0.009710  [ 2300/ 3442]
loss: 0.016577  [ 2400/ 3442]
loss: 0.033768  [ 2500/ 3442]
loss: 0.124345  [ 2600/ 3442]
loss: 0.030751  [ 2700/ 3442]
loss: 0.010667  [ 2800/ 3442]
loss: 0.009249  [ 2900/ 3442]
loss: 0.005581  [ 3000/ 3442]
loss: 0.019863  [ 3100/ 3442]
loss: 0.111159  [ 3200/ 3442]
loss: 0.015882  [ 3300/ 3442]
loss: 0.015788  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.011415  [    0/ 3442]
loss: 0.011560  [  100/ 3442]
loss: 0.031083  [  200/ 3442]
loss: 0.006463  [  300/ 3442]
loss: 0.018354  [  400/ 3442]
loss: 0.009404  [  500/ 3442]
loss: 0.024438  [  600/ 3442]
loss: 0.011993  [  700/ 3442]
loss: 0.017078  [  800/ 3442]
loss: 0.023830  [  900/ 3442]
loss: 0.009198  [ 1000/ 3442]
loss: 0.003339  [ 1100/ 3442]
loss: 0.012636  [ 1200/ 3442]
loss: 0.009773  [ 1300/ 3442]
loss: 0.012181  [ 1400/ 3442]
loss: 0.019002  [ 1500/ 3442]
loss: 0.019363  [ 1600/ 3442]
loss: 0.012161  [ 1700/ 3442]
loss: 0.015485  [ 1800/ 3442]
loss: 0.018490  [ 1900/ 3442]
loss: 0.015513  [ 2000/ 3442]
loss: 0.012934  [ 2100/ 3442]
loss: 0.016212  [ 2200/ 3442]
loss: 0.009623  [ 2300/ 3442]
loss: 0.015783  [ 2400/ 3442]
loss: 0.034666  [ 2500/ 3442]
loss: 0.124902  [ 2600/ 3442]
loss: 0.030164  [ 2700/ 3442]
loss: 0.010030  [ 2800/ 3442]
loss: 0.009988  [ 2900/ 3442]
loss: 0.005396  [ 3000/ 3442]
loss: 0.019019  [ 3100/ 3442]
loss: 0.109574  [ 3200/ 3442]
loss: 0.015713  [ 3300/ 3442]
loss: 0.016726  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.010353  [    0/ 3442]
loss: 0.011744  [  100/ 3442]
loss: 0.030547  [  200/ 3442]
loss: 0.006448  [  300/ 3442]
loss: 0.019426  [  400/ 3442]
loss: 0.009192  [  500/ 3442]
loss: 0.023406  [  600/ 3442]
loss: 0.011914  [  700/ 3442]
loss: 0.017605  [  800/ 3442]
loss: 0.023425  [  900/ 3442]
loss: 0.008567  [ 1000/ 3442]
loss: 0.003235  [ 1100/ 3442]
loss: 0.012508  [ 1200/ 3442]
loss: 0.009856  [ 1300/ 3442]
loss: 0.012488  [ 1400/ 3442]
loss: 0.018921  [ 1500/ 3442]
loss: 0.019168  [ 1600/ 3442]
loss: 0.012165  [ 1700/ 3442]
loss: 0.015521  [ 1800/ 3442]
loss: 0.017855  [ 1900/ 3442]
loss: 0.015314  [ 2000/ 3442]
loss: 0.012954  [ 2100/ 3442]
loss: 0.015936  [ 2200/ 3442]
loss: 0.009326  [ 2300/ 3442]
loss: 0.015583  [ 2400/ 3442]
loss: 0.034740  [ 2500/ 3442]
loss: 0.125364  [ 2600/ 3442]
loss: 0.030111  [ 2700/ 3442]
loss: 0.009780  [ 2800/ 3442]
loss: 0.010124  [ 2900/ 3442]
loss: 0.005290  [ 3000/ 3442]
loss: 0.018771  [ 3100/ 3442]
loss: 0.108592  [ 3200/ 3442]
loss: 0.015418  [ 3300/ 3442]
loss: 0.016933  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.010231  [    0/ 3442]
loss: 0.011720  [  100/ 3442]
loss: 0.030349  [  200/ 3442]
loss: 0.006484  [  300/ 3442]
loss: 0.019490  [  400/ 3442]
loss: 0.009188  [  500/ 3442]
loss: 0.022957  [  600/ 3442]
loss: 0.011873  [  700/ 3442]
loss: 0.017724  [  800/ 3442]
loss: 0.023256  [  900/ 3442]
loss: 0.008351  [ 1000/ 3442]
loss: 0.003199  [ 1100/ 3442]
loss: 0.012490  [ 1200/ 3442]
loss: 0.009693  [ 1300/ 3442]
loss: 0.012760  [ 1400/ 3442]
loss: 0.018886  [ 1500/ 3442]
loss: 0.018806  [ 1600/ 3442]
loss: 0.012167  [ 1700/ 3442]
loss: 0.015620  [ 1800/ 3442]
loss: 0.017511  [ 1900/ 3442]
loss: 0.015198  [ 2000/ 3442]
loss: 0.013021  [ 2100/ 3442]
loss: 0.015808  [ 2200/ 3442]
loss: 0.009069  [ 2300/ 3442]
loss: 0.015414  [ 2400/ 3442]
loss: 0.034686  [ 2500/ 3442]
loss: 0.126222  [ 2600/ 3442]
loss: 0.030229  [ 2700/ 3442]
loss: 0.009578  [ 2800/ 3442]
loss: 0.010153  [ 2900/ 3442]
loss: 0.005294  [ 3000/ 3442]
loss: 0.018691  [ 3100/ 3442]
loss: 0.108112  [ 3200/ 3442]
loss: 0.015133  [ 3300/ 3442]
loss: 0.017017  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.010271  [    0/ 3442]
loss: 0.011606  [  100/ 3442]
loss: 0.030278  [  200/ 3442]
loss: 0.006566  [  300/ 3442]
loss: 0.019442  [  400/ 3442]
loss: 0.009214  [  500/ 3442]
loss: 0.022688  [  600/ 3442]
loss: 0.011847  [  700/ 3442]
loss: 0.017776  [  800/ 3442]
loss: 0.023209  [  900/ 3442]
loss: 0.008157  [ 1000/ 3442]
loss: 0.003112  [ 1100/ 3442]
loss: 0.012507  [ 1200/ 3442]
loss: 0.009721  [ 1300/ 3442]
loss: 0.013047  [ 1400/ 3442]
loss: 0.018938  [ 1500/ 3442]
loss: 0.017711  [ 1600/ 3442]
loss: 0.012152  [ 1700/ 3442]
loss: 0.015946  [ 1800/ 3442]
loss: 0.017083  [ 1900/ 3442]
loss: 0.015152  [ 2000/ 3442]
loss: 0.013042  [ 2100/ 3442]
loss: 0.015663  [ 2200/ 3442]
loss: 0.008941  [ 2300/ 3442]
loss: 0.015106  [ 2400/ 3442]
loss: 0.034622  [ 2500/ 3442]
loss: 0.126274  [ 2600/ 3442]
loss: 0.030400  [ 2700/ 3442]
loss: 0.009556  [ 2800/ 3442]
loss: 0.010154  [ 2900/ 3442]
loss: 0.005241  [ 3000/ 3442]
loss: 0.018541  [ 3100/ 3442]
loss: 0.108079  [ 3200/ 3442]
loss: 0.015131  [ 3300/ 3442]
loss: 0.016900  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.010415  [    0/ 3442]
loss: 0.011633  [  100/ 3442]
loss: 0.030232  [  200/ 3442]
loss: 0.006590  [  300/ 3442]
loss: 0.019595  [  400/ 3442]
loss: 0.009255  [  500/ 3442]
loss: 0.022357  [  600/ 3442]
loss: 0.011801  [  700/ 3442]
loss: 0.017836  [  800/ 3442]
loss: 0.022975  [  900/ 3442]
loss: 0.007947  [ 1000/ 3442]
loss: 0.003097  [ 1100/ 3442]
loss: 0.012516  [ 1200/ 3442]
loss: 0.009707  [ 1300/ 3442]
loss: 0.013113  [ 1400/ 3442]
loss: 0.018940  [ 1500/ 3442]
loss: 0.017722  [ 1600/ 3442]
loss: 0.012126  [ 1700/ 3442]
loss: 0.016123  [ 1800/ 3442]
loss: 0.016941  [ 1900/ 3442]
loss: 0.015073  [ 2000/ 3442]
loss: 0.013134  [ 2100/ 3442]
loss: 0.015513  [ 2200/ 3442]
loss: 0.008860  [ 2300/ 3442]
loss: 0.015153  [ 2400/ 3442]
loss: 0.034659  [ 2500/ 3442]
loss: 0.126044  [ 2600/ 3442]
loss: 0.030349  [ 2700/ 3442]
loss: 0.009577  [ 2800/ 3442]
loss: 0.010186  [ 2900/ 3442]
loss: 0.005240  [ 3000/ 3442]
loss: 0.018452  [ 3100/ 3442]
loss: 0.107922  [ 3200/ 3442]
loss: 0.015234  [ 3300/ 3442]
loss: 0.016927  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.010433  [    0/ 3442]
loss: 0.011623  [  100/ 3442]
loss: 0.030171  [  200/ 3442]
loss: 0.006648  [  300/ 3442]
loss: 0.019511  [  400/ 3442]
loss: 0.009190  [  500/ 3442]
loss: 0.022123  [  600/ 3442]
loss: 0.011799  [  700/ 3442]
loss: 0.017855  [  800/ 3442]
loss: 0.022773  [  900/ 3442]
loss: 0.007762  [ 1000/ 3442]
loss: 0.003066  [ 1100/ 3442]
loss: 0.012536  [ 1200/ 3442]
loss: 0.009776  [ 1300/ 3442]
loss: 0.013315  [ 1400/ 3442]
loss: 0.018895  [ 1500/ 3442]
loss: 0.017642  [ 1600/ 3442]
loss: 0.012098  [ 1700/ 3442]
loss: 0.016222  [ 1800/ 3442]
loss: 0.016764  [ 1900/ 3442]
loss: 0.015046  [ 2000/ 3442]
loss: 0.013088  [ 2100/ 3442]
loss: 0.015335  [ 2200/ 3442]
loss: 0.008851  [ 2300/ 3442]
loss: 0.015010  [ 2400/ 3442]
loss: 0.034603  [ 2500/ 3442]
loss: 0.125955  [ 2600/ 3442]
loss: 0.030393  [ 2700/ 3442]
loss: 0.009547  [ 2800/ 3442]
loss: 0.010145  [ 2900/ 3442]
loss: 0.005231  [ 3000/ 3442]
loss: 0.018351  [ 3100/ 3442]
loss: 0.107838  [ 3200/ 3442]
loss: 0.015099  [ 3300/ 3442]
loss: 0.016882  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [ 0.17421699 -0.6392491 ]
[1 0 0 ... 1 2 0]
[0 2 0 ... 0 1 0]
Cluster 0 Occurrences: 1159; KMEANS: 1167
Cluster 1 Occurrences: 1156; KMEANS: 1122
Cluster 2 Occurrences: 1127; KMEANS: 1153
Centroids: [[-0.8102014, -1.1369948], [-0.25160667, -0.44692475], [1.0367463, 0.3985391]]
Centroids: [[-0.20992112, -0.4339524], [1.0440443, 0.40713298], [-0.8568148, -1.1584121]]
Contingency Matrix: 
[[  87    1 1071]
 [1071    5   80]
 [   9 1116    2]]
[[87, -1, 1071], [1071, -1, 80], [-1, -1, -1]]
[[-1, -1, -1], [1071, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 2, 1: 0}
New Contingency Matrix: 
[[1071   87    1]
 [  80 1071    5]
 [   2    9 1116]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1071, 1071, 1116], Sum: 3258
All_Elements: [1071, 87, 1, 80, 1071, 5, 2, 9, 1116], Sum: 3442
Accuracy: 0.946542707728065
Done!
