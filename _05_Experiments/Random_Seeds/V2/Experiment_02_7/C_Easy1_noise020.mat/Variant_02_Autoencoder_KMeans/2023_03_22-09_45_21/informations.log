Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_21
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000174525FD4A8>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x0000017452F74F60>
Epoch 1
-------------------------------
loss: 0.147008  [    0/ 3474]
loss: 0.082196  [  100/ 3474]
loss: 0.138111  [  200/ 3474]
loss: 0.108634  [  300/ 3474]
loss: 0.319879  [  400/ 3474]
loss: 0.018278  [  500/ 3474]
loss: 0.042725  [  600/ 3474]
loss: 0.017759  [  700/ 3474]
loss: 0.056307  [  800/ 3474]
loss: 0.216278  [  900/ 3474]
loss: 0.045379  [ 1000/ 3474]
loss: 0.209409  [ 1100/ 3474]
loss: 0.059013  [ 1200/ 3474]
loss: 0.080134  [ 1300/ 3474]
loss: 0.039593  [ 1400/ 3474]
loss: 0.096091  [ 1500/ 3474]
loss: 0.014768  [ 1600/ 3474]
loss: 0.113132  [ 1700/ 3474]
loss: 0.031393  [ 1800/ 3474]
loss: 0.022613  [ 1900/ 3474]
loss: 0.014149  [ 2000/ 3474]
loss: 0.014527  [ 2100/ 3474]
loss: 0.023959  [ 2200/ 3474]
loss: 0.082833  [ 2300/ 3474]
loss: 0.012537  [ 2400/ 3474]
loss: 0.021023  [ 2500/ 3474]
loss: 0.017343  [ 2600/ 3474]
loss: 0.068014  [ 2700/ 3474]
loss: 0.030120  [ 2800/ 3474]
loss: 0.018730  [ 2900/ 3474]
loss: 0.328950  [ 3000/ 3474]
loss: 0.010110  [ 3100/ 3474]
loss: 0.030995  [ 3200/ 3474]
loss: 0.033671  [ 3300/ 3474]
loss: 0.029978  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.053928  [    0/ 3474]
loss: 0.009426  [  100/ 3474]
loss: 0.030490  [  200/ 3474]
loss: 0.064239  [  300/ 3474]
loss: 0.073992  [  400/ 3474]
loss: 0.006491  [  500/ 3474]
loss: 0.033156  [  600/ 3474]
loss: 0.008996  [  700/ 3474]
loss: 0.014588  [  800/ 3474]
loss: 0.226670  [  900/ 3474]
loss: 0.013603  [ 1000/ 3474]
loss: 0.191185  [ 1100/ 3474]
loss: 0.048992  [ 1200/ 3474]
loss: 0.068327  [ 1300/ 3474]
loss: 0.033435  [ 1400/ 3474]
loss: 0.095058  [ 1500/ 3474]
loss: 0.017205  [ 1600/ 3474]
loss: 0.106836  [ 1700/ 3474]
loss: 0.027552  [ 1800/ 3474]
loss: 0.023180  [ 1900/ 3474]
loss: 0.008551  [ 2000/ 3474]
loss: 0.015397  [ 2100/ 3474]
loss: 0.024082  [ 2200/ 3474]
loss: 0.077668  [ 2300/ 3474]
loss: 0.007835  [ 2400/ 3474]
loss: 0.016348  [ 2500/ 3474]
loss: 0.018259  [ 2600/ 3474]
loss: 0.047333  [ 2700/ 3474]
loss: 0.023333  [ 2800/ 3474]
loss: 0.017342  [ 2900/ 3474]
loss: 0.271786  [ 3000/ 3474]
loss: 0.010202  [ 3100/ 3474]
loss: 0.018705  [ 3200/ 3474]
loss: 0.038772  [ 3300/ 3474]
loss: 0.018236  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.046341  [    0/ 3474]
loss: 0.009876  [  100/ 3474]
loss: 0.027140  [  200/ 3474]
loss: 0.064817  [  300/ 3474]
loss: 0.068750  [  400/ 3474]
loss: 0.005949  [  500/ 3474]
loss: 0.029526  [  600/ 3474]
loss: 0.008849  [  700/ 3474]
loss: 0.015424  [  800/ 3474]
loss: 0.216117  [  900/ 3474]
loss: 0.010732  [ 1000/ 3474]
loss: 0.187877  [ 1100/ 3474]
loss: 0.040549  [ 1200/ 3474]
loss: 0.053187  [ 1300/ 3474]
loss: 0.028241  [ 1400/ 3474]
loss: 0.094737  [ 1500/ 3474]
loss: 0.016693  [ 1600/ 3474]
loss: 0.069723  [ 1700/ 3474]
loss: 0.025192  [ 1800/ 3474]
loss: 0.023504  [ 1900/ 3474]
loss: 0.009415  [ 2000/ 3474]
loss: 0.014873  [ 2100/ 3474]
loss: 0.024148  [ 2200/ 3474]
loss: 0.074421  [ 2300/ 3474]
loss: 0.006666  [ 2400/ 3474]
loss: 0.014953  [ 2500/ 3474]
loss: 0.018184  [ 2600/ 3474]
loss: 0.039745  [ 2700/ 3474]
loss: 0.020377  [ 2800/ 3474]
loss: 0.016616  [ 2900/ 3474]
loss: 0.228736  [ 3000/ 3474]
loss: 0.010439  [ 3100/ 3474]
loss: 0.015961  [ 3200/ 3474]
loss: 0.041220  [ 3300/ 3474]
loss: 0.014370  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.037970  [    0/ 3474]
loss: 0.009260  [  100/ 3474]
loss: 0.023068  [  200/ 3474]
loss: 0.061597  [  300/ 3474]
loss: 0.068969  [  400/ 3474]
loss: 0.005823  [  500/ 3474]
loss: 0.027484  [  600/ 3474]
loss: 0.008707  [  700/ 3474]
loss: 0.016848  [  800/ 3474]
loss: 0.205225  [  900/ 3474]
loss: 0.009177  [ 1000/ 3474]
loss: 0.164998  [ 1100/ 3474]
loss: 0.038344  [ 1200/ 3474]
loss: 0.048001  [ 1300/ 3474]
loss: 0.025602  [ 1400/ 3474]
loss: 0.090531  [ 1500/ 3474]
loss: 0.015998  [ 1600/ 3474]
loss: 0.047528  [ 1700/ 3474]
loss: 0.024510  [ 1800/ 3474]
loss: 0.023433  [ 1900/ 3474]
loss: 0.010142  [ 2000/ 3474]
loss: 0.015281  [ 2100/ 3474]
loss: 0.025929  [ 2200/ 3474]
loss: 0.072021  [ 2300/ 3474]
loss: 0.006100  [ 2400/ 3474]
loss: 0.014098  [ 2500/ 3474]
loss: 0.018718  [ 2600/ 3474]
loss: 0.038251  [ 2700/ 3474]
loss: 0.018582  [ 2800/ 3474]
loss: 0.015903  [ 2900/ 3474]
loss: 0.210649  [ 3000/ 3474]
loss: 0.010454  [ 3100/ 3474]
loss: 0.013418  [ 3200/ 3474]
loss: 0.043434  [ 3300/ 3474]
loss: 0.011899  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.034624  [    0/ 3474]
loss: 0.009028  [  100/ 3474]
loss: 0.019674  [  200/ 3474]
loss: 0.060659  [  300/ 3474]
loss: 0.071493  [  400/ 3474]
loss: 0.005404  [  500/ 3474]
loss: 0.027733  [  600/ 3474]
loss: 0.008579  [  700/ 3474]
loss: 0.020351  [  800/ 3474]
loss: 0.202658  [  900/ 3474]
loss: 0.009183  [ 1000/ 3474]
loss: 0.151365  [ 1100/ 3474]
loss: 0.042570  [ 1200/ 3474]
loss: 0.036296  [ 1300/ 3474]
loss: 0.018627  [ 1400/ 3474]
loss: 0.087124  [ 1500/ 3474]
loss: 0.012843  [ 1600/ 3474]
loss: 0.040342  [ 1700/ 3474]
loss: 0.023868  [ 1800/ 3474]
loss: 0.021814  [ 1900/ 3474]
loss: 0.010331  [ 2000/ 3474]
loss: 0.015597  [ 2100/ 3474]
loss: 0.026732  [ 2200/ 3474]
loss: 0.069835  [ 2300/ 3474]
loss: 0.005894  [ 2400/ 3474]
loss: 0.013005  [ 2500/ 3474]
loss: 0.018881  [ 2600/ 3474]
loss: 0.035937  [ 2700/ 3474]
loss: 0.017675  [ 2800/ 3474]
loss: 0.015392  [ 2900/ 3474]
loss: 0.219007  [ 3000/ 3474]
loss: 0.010521  [ 3100/ 3474]
loss: 0.012354  [ 3200/ 3474]
loss: 0.041580  [ 3300/ 3474]
loss: 0.012161  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.034076  [    0/ 3474]
loss: 0.008895  [  100/ 3474]
loss: 0.020518  [  200/ 3474]
loss: 0.060841  [  300/ 3474]
loss: 0.067418  [  400/ 3474]
loss: 0.005636  [  500/ 3474]
loss: 0.025806  [  600/ 3474]
loss: 0.008298  [  700/ 3474]
loss: 0.021222  [  800/ 3474]
loss: 0.192697  [  900/ 3474]
loss: 0.010356  [ 1000/ 3474]
loss: 0.149422  [ 1100/ 3474]
loss: 0.035885  [ 1200/ 3474]
loss: 0.030797  [ 1300/ 3474]
loss: 0.013791  [ 1400/ 3474]
loss: 0.089995  [ 1500/ 3474]
loss: 0.011375  [ 1600/ 3474]
loss: 0.035128  [ 1700/ 3474]
loss: 0.023431  [ 1800/ 3474]
loss: 0.020932  [ 1900/ 3474]
loss: 0.010153  [ 2000/ 3474]
loss: 0.014736  [ 2100/ 3474]
loss: 0.028105  [ 2200/ 3474]
loss: 0.070051  [ 2300/ 3474]
loss: 0.006455  [ 2400/ 3474]
loss: 0.011894  [ 2500/ 3474]
loss: 0.018162  [ 2600/ 3474]
loss: 0.036032  [ 2700/ 3474]
loss: 0.017505  [ 2800/ 3474]
loss: 0.016276  [ 2900/ 3474]
loss: 0.220244  [ 3000/ 3474]
loss: 0.010430  [ 3100/ 3474]
loss: 0.011819  [ 3200/ 3474]
loss: 0.041676  [ 3300/ 3474]
loss: 0.011537  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.033227  [    0/ 3474]
loss: 0.008920  [  100/ 3474]
loss: 0.021823  [  200/ 3474]
loss: 0.061028  [  300/ 3474]
loss: 0.064228  [  400/ 3474]
loss: 0.005912  [  500/ 3474]
loss: 0.024236  [  600/ 3474]
loss: 0.009230  [  700/ 3474]
loss: 0.021931  [  800/ 3474]
loss: 0.188688  [  900/ 3474]
loss: 0.010183  [ 1000/ 3474]
loss: 0.149743  [ 1100/ 3474]
loss: 0.032746  [ 1200/ 3474]
loss: 0.027099  [ 1300/ 3474]
loss: 0.011787  [ 1400/ 3474]
loss: 0.087230  [ 1500/ 3474]
loss: 0.010689  [ 1600/ 3474]
loss: 0.033350  [ 1700/ 3474]
loss: 0.023231  [ 1800/ 3474]
loss: 0.020647  [ 1900/ 3474]
loss: 0.010278  [ 2000/ 3474]
loss: 0.014162  [ 2100/ 3474]
loss: 0.028694  [ 2200/ 3474]
loss: 0.070852  [ 2300/ 3474]
loss: 0.006234  [ 2400/ 3474]
loss: 0.011963  [ 2500/ 3474]
loss: 0.017845  [ 2600/ 3474]
loss: 0.036534  [ 2700/ 3474]
loss: 0.017174  [ 2800/ 3474]
loss: 0.016321  [ 2900/ 3474]
loss: 0.183221  [ 3000/ 3474]
loss: 0.010504  [ 3100/ 3474]
loss: 0.011773  [ 3200/ 3474]
loss: 0.040949  [ 3300/ 3474]
loss: 0.011024  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.032553  [    0/ 3474]
loss: 0.008933  [  100/ 3474]
loss: 0.022427  [  200/ 3474]
loss: 0.061095  [  300/ 3474]
loss: 0.064355  [  400/ 3474]
loss: 0.006027  [  500/ 3474]
loss: 0.023515  [  600/ 3474]
loss: 0.009985  [  700/ 3474]
loss: 0.021946  [  800/ 3474]
loss: 0.189539  [  900/ 3474]
loss: 0.009884  [ 1000/ 3474]
loss: 0.147083  [ 1100/ 3474]
loss: 0.035613  [ 1200/ 3474]
loss: 0.025843  [ 1300/ 3474]
loss: 0.011067  [ 1400/ 3474]
loss: 0.085688  [ 1500/ 3474]
loss: 0.010347  [ 1600/ 3474]
loss: 0.030566  [ 1700/ 3474]
loss: 0.022606  [ 1800/ 3474]
loss: 0.020672  [ 1900/ 3474]
loss: 0.009766  [ 2000/ 3474]
loss: 0.013806  [ 2100/ 3474]
loss: 0.028558  [ 2200/ 3474]
loss: 0.071263  [ 2300/ 3474]
loss: 0.006227  [ 2400/ 3474]
loss: 0.012070  [ 2500/ 3474]
loss: 0.016991  [ 2600/ 3474]
loss: 0.036270  [ 2700/ 3474]
loss: 0.017038  [ 2800/ 3474]
loss: 0.016418  [ 2900/ 3474]
loss: 0.170898  [ 3000/ 3474]
loss: 0.010612  [ 3100/ 3474]
loss: 0.011553  [ 3200/ 3474]
loss: 0.040425  [ 3300/ 3474]
loss: 0.010499  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [ 0.13437319 -1.284113  ]
[0 1 1 ... 1 1 2]
[0 1 1 ... 1 1 2]
Cluster 0 Occurrences: 1198; KMEANS: 1186
Cluster 1 Occurrences: 1128; KMEANS: 1117
Cluster 2 Occurrences: 1148; KMEANS: 1171
Centroids: [[0.781514, -1.6992018], [-2.1970804, 0.02383495], [0.19943024, 1.2593875]]
Centroids: [[0.7836515, -1.729549], [-2.2164378, 0.02355492], [0.19918284, 1.2484655]]
Contingency Matrix: 
[[1181    2   15]
 [   5 1115    8]
 [   0    0 1148]]
[[-1, -1, -1], [-1, 1115, 8], [-1, 0, 1148]]
[[-1, -1, -1], [-1, 1115, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 2, 1: 1}
New Contingency Matrix: 
[[1181    2   15]
 [   5 1115    8]
 [   0    0 1148]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1181, 1115, 1148], Sum: 3444
All_Elements: [1181, 2, 15, 5, 1115, 8, 0, 0, 1148], Sum: 3474
Accuracy: 0.9913644214162349
Done!
