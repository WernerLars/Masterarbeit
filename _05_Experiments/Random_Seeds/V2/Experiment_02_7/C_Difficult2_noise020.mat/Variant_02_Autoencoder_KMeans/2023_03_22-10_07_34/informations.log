Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_07_34
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001749998E588>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x00000174870F2630>
Epoch 1
-------------------------------
loss: 0.297198  [    0/ 3493]
loss: 0.108334  [  100/ 3493]
loss: 0.042893  [  200/ 3493]
loss: 0.057363  [  300/ 3493]
loss: 0.073247  [  400/ 3493]
loss: 0.022308  [  500/ 3493]
loss: 0.058023  [  600/ 3493]
loss: 0.185611  [  700/ 3493]
loss: 0.043734  [  800/ 3493]
loss: 0.018228  [  900/ 3493]
loss: 0.044886  [ 1000/ 3493]
loss: 0.059082  [ 1100/ 3493]
loss: 0.018629  [ 1200/ 3493]
loss: 0.013068  [ 1300/ 3493]
loss: 0.027997  [ 1400/ 3493]
loss: 0.042180  [ 1500/ 3493]
loss: 0.027299  [ 1600/ 3493]
loss: 0.042687  [ 1700/ 3493]
loss: 0.058347  [ 1800/ 3493]
loss: 0.014375  [ 1900/ 3493]
loss: 0.009598  [ 2000/ 3493]
loss: 0.016349  [ 2100/ 3493]
loss: 0.011992  [ 2200/ 3493]
loss: 0.024189  [ 2300/ 3493]
loss: 0.012787  [ 2400/ 3493]
loss: 0.078327  [ 2500/ 3493]
loss: 0.003947  [ 2600/ 3493]
loss: 0.013712  [ 2700/ 3493]
loss: 0.025015  [ 2800/ 3493]
loss: 0.008796  [ 2900/ 3493]
loss: 0.044352  [ 3000/ 3493]
loss: 0.013584  [ 3100/ 3493]
loss: 0.030234  [ 3200/ 3493]
loss: 0.022925  [ 3300/ 3493]
loss: 0.010015  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.067589  [    0/ 3493]
loss: 0.016690  [  100/ 3493]
loss: 0.025262  [  200/ 3493]
loss: 0.017225  [  300/ 3493]
loss: 0.056884  [  400/ 3493]
loss: 0.023328  [  500/ 3493]
loss: 0.052261  [  600/ 3493]
loss: 0.161325  [  700/ 3493]
loss: 0.010861  [  800/ 3493]
loss: 0.013850  [  900/ 3493]
loss: 0.030510  [ 1000/ 3493]
loss: 0.033060  [ 1100/ 3493]
loss: 0.017787  [ 1200/ 3493]
loss: 0.011120  [ 1300/ 3493]
loss: 0.032886  [ 1400/ 3493]
loss: 0.057915  [ 1500/ 3493]
loss: 0.031998  [ 1600/ 3493]
loss: 0.023489  [ 1700/ 3493]
loss: 0.040329  [ 1800/ 3493]
loss: 0.013966  [ 1900/ 3493]
loss: 0.012028  [ 2000/ 3493]
loss: 0.020274  [ 2100/ 3493]
loss: 0.013805  [ 2200/ 3493]
loss: 0.021633  [ 2300/ 3493]
loss: 0.013734  [ 2400/ 3493]
loss: 0.082573  [ 2500/ 3493]
loss: 0.003874  [ 2600/ 3493]
loss: 0.011742  [ 2700/ 3493]
loss: 0.021272  [ 2800/ 3493]
loss: 0.008525  [ 2900/ 3493]
loss: 0.042707  [ 3000/ 3493]
loss: 0.012809  [ 3100/ 3493]
loss: 0.025314  [ 3200/ 3493]
loss: 0.023762  [ 3300/ 3493]
loss: 0.010445  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.065029  [    0/ 3493]
loss: 0.017742  [  100/ 3493]
loss: 0.022491  [  200/ 3493]
loss: 0.015211  [  300/ 3493]
loss: 0.055101  [  400/ 3493]
loss: 0.023316  [  500/ 3493]
loss: 0.052033  [  600/ 3493]
loss: 0.156679  [  700/ 3493]
loss: 0.010349  [  800/ 3493]
loss: 0.013131  [  900/ 3493]
loss: 0.029836  [ 1000/ 3493]
loss: 0.035355  [ 1100/ 3493]
loss: 0.017691  [ 1200/ 3493]
loss: 0.012141  [ 1300/ 3493]
loss: 0.032815  [ 1400/ 3493]
loss: 0.058979  [ 1500/ 3493]
loss: 0.032194  [ 1600/ 3493]
loss: 0.023103  [ 1700/ 3493]
loss: 0.036030  [ 1800/ 3493]
loss: 0.014835  [ 1900/ 3493]
loss: 0.013324  [ 2000/ 3493]
loss: 0.022709  [ 2100/ 3493]
loss: 0.014139  [ 2200/ 3493]
loss: 0.021612  [ 2300/ 3493]
loss: 0.014784  [ 2400/ 3493]
loss: 0.083825  [ 2500/ 3493]
loss: 0.004920  [ 2600/ 3493]
loss: 0.011256  [ 2700/ 3493]
loss: 0.021165  [ 2800/ 3493]
loss: 0.008601  [ 2900/ 3493]
loss: 0.043265  [ 3000/ 3493]
loss: 0.012743  [ 3100/ 3493]
loss: 0.023390  [ 3200/ 3493]
loss: 0.023688  [ 3300/ 3493]
loss: 0.010089  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.068476  [    0/ 3493]
loss: 0.017967  [  100/ 3493]
loss: 0.022311  [  200/ 3493]
loss: 0.015094  [  300/ 3493]
loss: 0.054711  [  400/ 3493]
loss: 0.022261  [  500/ 3493]
loss: 0.051296  [  600/ 3493]
loss: 0.147619  [  700/ 3493]
loss: 0.010505  [  800/ 3493]
loss: 0.012645  [  900/ 3493]
loss: 0.029843  [ 1000/ 3493]
loss: 0.037547  [ 1100/ 3493]
loss: 0.017663  [ 1200/ 3493]
loss: 0.012163  [ 1300/ 3493]
loss: 0.032674  [ 1400/ 3493]
loss: 0.058886  [ 1500/ 3493]
loss: 0.031991  [ 1600/ 3493]
loss: 0.022639  [ 1700/ 3493]
loss: 0.033313  [ 1800/ 3493]
loss: 0.014786  [ 1900/ 3493]
loss: 0.013226  [ 2000/ 3493]
loss: 0.024609  [ 2100/ 3493]
loss: 0.013923  [ 2200/ 3493]
loss: 0.021596  [ 2300/ 3493]
loss: 0.015150  [ 2400/ 3493]
loss: 0.084250  [ 2500/ 3493]
loss: 0.005297  [ 2600/ 3493]
loss: 0.010873  [ 2700/ 3493]
loss: 0.020396  [ 2800/ 3493]
loss: 0.008619  [ 2900/ 3493]
loss: 0.042991  [ 3000/ 3493]
loss: 0.012896  [ 3100/ 3493]
loss: 0.022781  [ 3200/ 3493]
loss: 0.023673  [ 3300/ 3493]
loss: 0.009754  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.070773  [    0/ 3493]
loss: 0.017589  [  100/ 3493]
loss: 0.022210  [  200/ 3493]
loss: 0.014625  [  300/ 3493]
loss: 0.054438  [  400/ 3493]
loss: 0.021856  [  500/ 3493]
loss: 0.050980  [  600/ 3493]
loss: 0.138056  [  700/ 3493]
loss: 0.011451  [  800/ 3493]
loss: 0.012618  [  900/ 3493]
loss: 0.031277  [ 1000/ 3493]
loss: 0.037844  [ 1100/ 3493]
loss: 0.017557  [ 1200/ 3493]
loss: 0.012569  [ 1300/ 3493]
loss: 0.031981  [ 1400/ 3493]
loss: 0.058011  [ 1500/ 3493]
loss: 0.031993  [ 1600/ 3493]
loss: 0.022581  [ 1700/ 3493]
loss: 0.030889  [ 1800/ 3493]
loss: 0.014721  [ 1900/ 3493]
loss: 0.012660  [ 2000/ 3493]
loss: 0.025330  [ 2100/ 3493]
loss: 0.014008  [ 2200/ 3493]
loss: 0.021656  [ 2300/ 3493]
loss: 0.015185  [ 2400/ 3493]
loss: 0.084272  [ 2500/ 3493]
loss: 0.005966  [ 2600/ 3493]
loss: 0.010658  [ 2700/ 3493]
loss: 0.019767  [ 2800/ 3493]
loss: 0.008626  [ 2900/ 3493]
loss: 0.042886  [ 3000/ 3493]
loss: 0.012976  [ 3100/ 3493]
loss: 0.021973  [ 3200/ 3493]
loss: 0.023654  [ 3300/ 3493]
loss: 0.009462  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.072925  [    0/ 3493]
loss: 0.017141  [  100/ 3493]
loss: 0.021989  [  200/ 3493]
loss: 0.013735  [  300/ 3493]
loss: 0.053973  [  400/ 3493]
loss: 0.020996  [  500/ 3493]
loss: 0.051110  [  600/ 3493]
loss: 0.126964  [  700/ 3493]
loss: 0.011341  [  800/ 3493]
loss: 0.012477  [  900/ 3493]
loss: 0.032450  [ 1000/ 3493]
loss: 0.038629  [ 1100/ 3493]
loss: 0.017577  [ 1200/ 3493]
loss: 0.012710  [ 1300/ 3493]
loss: 0.031405  [ 1400/ 3493]
loss: 0.057252  [ 1500/ 3493]
loss: 0.031951  [ 1600/ 3493]
loss: 0.022416  [ 1700/ 3493]
loss: 0.028985  [ 1800/ 3493]
loss: 0.014722  [ 1900/ 3493]
loss: 0.011930  [ 2000/ 3493]
loss: 0.025942  [ 2100/ 3493]
loss: 0.013909  [ 2200/ 3493]
loss: 0.021829  [ 2300/ 3493]
loss: 0.015238  [ 2400/ 3493]
loss: 0.084460  [ 2500/ 3493]
loss: 0.006379  [ 2600/ 3493]
loss: 0.010348  [ 2700/ 3493]
loss: 0.018843  [ 2800/ 3493]
loss: 0.008743  [ 2900/ 3493]
loss: 0.042656  [ 3000/ 3493]
loss: 0.013067  [ 3100/ 3493]
loss: 0.021278  [ 3200/ 3493]
loss: 0.023516  [ 3300/ 3493]
loss: 0.009296  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.074352  [    0/ 3493]
loss: 0.016772  [  100/ 3493]
loss: 0.021952  [  200/ 3493]
loss: 0.013530  [  300/ 3493]
loss: 0.054055  [  400/ 3493]
loss: 0.020207  [  500/ 3493]
loss: 0.051313  [  600/ 3493]
loss: 0.113353  [  700/ 3493]
loss: 0.011713  [  800/ 3493]
loss: 0.012449  [  900/ 3493]
loss: 0.033832  [ 1000/ 3493]
loss: 0.039172  [ 1100/ 3493]
loss: 0.017657  [ 1200/ 3493]
loss: 0.013077  [ 1300/ 3493]
loss: 0.030774  [ 1400/ 3493]
loss: 0.056779  [ 1500/ 3493]
loss: 0.031886  [ 1600/ 3493]
loss: 0.022478  [ 1700/ 3493]
loss: 0.027302  [ 1800/ 3493]
loss: 0.015434  [ 1900/ 3493]
loss: 0.011280  [ 2000/ 3493]
loss: 0.026266  [ 2100/ 3493]
loss: 0.014033  [ 2200/ 3493]
loss: 0.022120  [ 2300/ 3493]
loss: 0.015319  [ 2400/ 3493]
loss: 0.083718  [ 2500/ 3493]
loss: 0.006848  [ 2600/ 3493]
loss: 0.010006  [ 2700/ 3493]
loss: 0.018119  [ 2800/ 3493]
loss: 0.008754  [ 2900/ 3493]
loss: 0.042338  [ 3000/ 3493]
loss: 0.013252  [ 3100/ 3493]
loss: 0.020734  [ 3200/ 3493]
loss: 0.023198  [ 3300/ 3493]
loss: 0.009113  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.075607  [    0/ 3493]
loss: 0.016446  [  100/ 3493]
loss: 0.022017  [  200/ 3493]
loss: 0.012909  [  300/ 3493]
loss: 0.054005  [  400/ 3493]
loss: 0.018293  [  500/ 3493]
loss: 0.050426  [  600/ 3493]
loss: 0.098630  [  700/ 3493]
loss: 0.011918  [  800/ 3493]
loss: 0.012394  [  900/ 3493]
loss: 0.036923  [ 1000/ 3493]
loss: 0.040317  [ 1100/ 3493]
loss: 0.017878  [ 1200/ 3493]
loss: 0.013460  [ 1300/ 3493]
loss: 0.029908  [ 1400/ 3493]
loss: 0.055590  [ 1500/ 3493]
loss: 0.031939  [ 1600/ 3493]
loss: 0.020788  [ 1700/ 3493]
loss: 0.026218  [ 1800/ 3493]
loss: 0.015613  [ 1900/ 3493]
loss: 0.010743  [ 2000/ 3493]
loss: 0.026725  [ 2100/ 3493]
loss: 0.014130  [ 2200/ 3493]
loss: 0.022406  [ 2300/ 3493]
loss: 0.015066  [ 2400/ 3493]
loss: 0.082701  [ 2500/ 3493]
loss: 0.007378  [ 2600/ 3493]
loss: 0.009460  [ 2700/ 3493]
loss: 0.017374  [ 2800/ 3493]
loss: 0.008798  [ 2900/ 3493]
loss: 0.043742  [ 3000/ 3493]
loss: 0.013360  [ 3100/ 3493]
loss: 0.019309  [ 3200/ 3493]
loss: 0.022960  [ 3300/ 3493]
loss: 0.009095  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [-0.96893144 -1.872628  ]
[1 0 0 ... 1 0 2]
[1 0 0 ... 1 0 2]
Cluster 0 Occurrences: 1151; KMEANS: 1058
Cluster 1 Occurrences: 1195; KMEANS: 1170
Cluster 2 Occurrences: 1147; KMEANS: 1265
Centroids: [[0.51229566, 0.34629887], [-1.2145866, -1.4203993], [0.4998009, -0.1441064]]
Centroids: [[0.51178455, 0.43481028], [-1.2485884, -1.4423528], [0.49871406, -0.18699913]]
Contingency Matrix: 
[[ 895    2  254]
 [   6 1164   25]
 [ 157    4  986]]
[[895, -1, 254], [-1, -1, -1], [157, -1, 986]]
[[895, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 2, 0: 0}
New Contingency Matrix: 
[[ 895    2  254]
 [   6 1164   25]
 [ 157    4  986]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [895, 1164, 986], Sum: 3045
All_Elements: [895, 2, 254, 6, 1164, 25, 157, 4, 986], Sum: 3493
Accuracy: 0.8717434869739479
Done!
