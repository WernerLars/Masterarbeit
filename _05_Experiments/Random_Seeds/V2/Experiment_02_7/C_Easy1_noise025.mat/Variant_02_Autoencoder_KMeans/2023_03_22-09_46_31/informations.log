Experiment_path: Random_Seeds//V2/Experiment_02_7
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_7/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_31
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000017452F2BA58>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x0000017452F74128>
Epoch 1
-------------------------------
loss: 0.205811  [    0/ 3298]
loss: 0.306366  [  100/ 3298]
loss: 0.151283  [  200/ 3298]
loss: 0.054079  [  300/ 3298]
loss: 0.033994  [  400/ 3298]
loss: 0.036000  [  500/ 3298]
loss: 0.047182  [  600/ 3298]
loss: 0.088389  [  700/ 3298]
loss: 0.028347  [  800/ 3298]
loss: 0.047783  [  900/ 3298]
loss: 0.028540  [ 1000/ 3298]
loss: 0.090728  [ 1100/ 3298]
loss: 0.078063  [ 1200/ 3298]
loss: 0.038427  [ 1300/ 3298]
loss: 0.010899  [ 1400/ 3298]
loss: 0.036037  [ 1500/ 3298]
loss: 0.014623  [ 1600/ 3298]
loss: 0.125712  [ 1700/ 3298]
loss: 0.025853  [ 1800/ 3298]
loss: 0.022369  [ 1900/ 3298]
loss: 0.009505  [ 2000/ 3298]
loss: 0.012524  [ 2100/ 3298]
loss: 0.026221  [ 2200/ 3298]
loss: 0.017579  [ 2300/ 3298]
loss: 0.028999  [ 2400/ 3298]
loss: 0.014608  [ 2500/ 3298]
loss: 0.029718  [ 2600/ 3298]
loss: 0.035589  [ 2700/ 3298]
loss: 0.377268  [ 2800/ 3298]
loss: 0.027607  [ 2900/ 3298]
loss: 0.035175  [ 3000/ 3298]
loss: 0.020163  [ 3100/ 3298]
loss: 0.045037  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.051421  [    0/ 3298]
loss: 0.020974  [  100/ 3298]
loss: 0.108339  [  200/ 3298]
loss: 0.016345  [  300/ 3298]
loss: 0.018312  [  400/ 3298]
loss: 0.020652  [  500/ 3298]
loss: 0.029192  [  600/ 3298]
loss: 0.064063  [  700/ 3298]
loss: 0.024760  [  800/ 3298]
loss: 0.080318  [  900/ 3298]
loss: 0.034456  [ 1000/ 3298]
loss: 0.104059  [ 1100/ 3298]
loss: 0.076993  [ 1200/ 3298]
loss: 0.027054  [ 1300/ 3298]
loss: 0.011301  [ 1400/ 3298]
loss: 0.026207  [ 1500/ 3298]
loss: 0.019753  [ 1600/ 3298]
loss: 0.130892  [ 1700/ 3298]
loss: 0.025610  [ 1800/ 3298]
loss: 0.022238  [ 1900/ 3298]
loss: 0.009704  [ 2000/ 3298]
loss: 0.017288  [ 2100/ 3298]
loss: 0.022612  [ 2200/ 3298]
loss: 0.016419  [ 2300/ 3298]
loss: 0.023442  [ 2400/ 3298]
loss: 0.012196  [ 2500/ 3298]
loss: 0.029405  [ 2600/ 3298]
loss: 0.044274  [ 2700/ 3298]
loss: 0.411877  [ 2800/ 3298]
loss: 0.029185  [ 2900/ 3298]
loss: 0.036103  [ 3000/ 3298]
loss: 0.021710  [ 3100/ 3298]
loss: 0.048124  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.051955  [    0/ 3298]
loss: 0.020341  [  100/ 3298]
loss: 0.105067  [  200/ 3298]
loss: 0.014418  [  300/ 3298]
loss: 0.019367  [  400/ 3298]
loss: 0.020565  [  500/ 3298]
loss: 0.027432  [  600/ 3298]
loss: 0.064648  [  700/ 3298]
loss: 0.024167  [  800/ 3298]
loss: 0.093023  [  900/ 3298]
loss: 0.036481  [ 1000/ 3298]
loss: 0.099303  [ 1100/ 3298]
loss: 0.076016  [ 1200/ 3298]
loss: 0.025457  [ 1300/ 3298]
loss: 0.013606  [ 1400/ 3298]
loss: 0.025428  [ 1500/ 3298]
loss: 0.017673  [ 1600/ 3298]
loss: 0.126042  [ 1700/ 3298]
loss: 0.024622  [ 1800/ 3298]
loss: 0.023099  [ 1900/ 3298]
loss: 0.009717  [ 2000/ 3298]
loss: 0.018413  [ 2100/ 3298]
loss: 0.021169  [ 2200/ 3298]
loss: 0.016170  [ 2300/ 3298]
loss: 0.022812  [ 2400/ 3298]
loss: 0.013864  [ 2500/ 3298]
loss: 0.028614  [ 2600/ 3298]
loss: 0.045211  [ 2700/ 3298]
loss: 0.347882  [ 2800/ 3298]
loss: 0.031390  [ 2900/ 3298]
loss: 0.036368  [ 3000/ 3298]
loss: 0.022172  [ 3100/ 3298]
loss: 0.048905  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.053658  [    0/ 3298]
loss: 0.019858  [  100/ 3298]
loss: 0.105494  [  200/ 3298]
loss: 0.016829  [  300/ 3298]
loss: 0.019595  [  400/ 3298]
loss: 0.021040  [  500/ 3298]
loss: 0.024978  [  600/ 3298]
loss: 0.064991  [  700/ 3298]
loss: 0.024296  [  800/ 3298]
loss: 0.092355  [  900/ 3298]
loss: 0.035633  [ 1000/ 3298]
loss: 0.093850  [ 1100/ 3298]
loss: 0.076007  [ 1200/ 3298]
loss: 0.024774  [ 1300/ 3298]
loss: 0.015102  [ 1400/ 3298]
loss: 0.024855  [ 1500/ 3298]
loss: 0.016624  [ 1600/ 3298]
loss: 0.117580  [ 1700/ 3298]
loss: 0.024177  [ 1800/ 3298]
loss: 0.023605  [ 1900/ 3298]
loss: 0.009844  [ 2000/ 3298]
loss: 0.016589  [ 2100/ 3298]
loss: 0.019420  [ 2200/ 3298]
loss: 0.016542  [ 2300/ 3298]
loss: 0.021312  [ 2400/ 3298]
loss: 0.014890  [ 2500/ 3298]
loss: 0.027273  [ 2600/ 3298]
loss: 0.047269  [ 2700/ 3298]
loss: 0.316715  [ 2800/ 3298]
loss: 0.032114  [ 2900/ 3298]
loss: 0.036537  [ 3000/ 3298]
loss: 0.024352  [ 3100/ 3298]
loss: 0.047126  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.050752  [    0/ 3298]
loss: 0.020600  [  100/ 3298]
loss: 0.108257  [  200/ 3298]
loss: 0.017359  [  300/ 3298]
loss: 0.017442  [  400/ 3298]
loss: 0.022404  [  500/ 3298]
loss: 0.022136  [  600/ 3298]
loss: 0.063470  [  700/ 3298]
loss: 0.024234  [  800/ 3298]
loss: 0.074198  [  900/ 3298]
loss: 0.033939  [ 1000/ 3298]
loss: 0.078026  [ 1100/ 3298]
loss: 0.075347  [ 1200/ 3298]
loss: 0.025724  [ 1300/ 3298]
loss: 0.016461  [ 1400/ 3298]
loss: 0.022659  [ 1500/ 3298]
loss: 0.016857  [ 1600/ 3298]
loss: 0.094762  [ 1700/ 3298]
loss: 0.024355  [ 1800/ 3298]
loss: 0.023182  [ 1900/ 3298]
loss: 0.009651  [ 2000/ 3298]
loss: 0.015325  [ 2100/ 3298]
loss: 0.016943  [ 2200/ 3298]
loss: 0.017464  [ 2300/ 3298]
loss: 0.019794  [ 2400/ 3298]
loss: 0.016840  [ 2500/ 3298]
loss: 0.027348  [ 2600/ 3298]
loss: 0.045633  [ 2700/ 3298]
loss: 0.287028  [ 2800/ 3298]
loss: 0.032390  [ 2900/ 3298]
loss: 0.035825  [ 3000/ 3298]
loss: 0.024190  [ 3100/ 3298]
loss: 0.045226  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.044978  [    0/ 3298]
loss: 0.020802  [  100/ 3298]
loss: 0.105411  [  200/ 3298]
loss: 0.016586  [  300/ 3298]
loss: 0.014827  [  400/ 3298]
loss: 0.025160  [  500/ 3298]
loss: 0.019513  [  600/ 3298]
loss: 0.060658  [  700/ 3298]
loss: 0.024072  [  800/ 3298]
loss: 0.051521  [  900/ 3298]
loss: 0.032835  [ 1000/ 3298]
loss: 0.062031  [ 1100/ 3298]
loss: 0.074900  [ 1200/ 3298]
loss: 0.027143  [ 1300/ 3298]
loss: 0.016833  [ 1400/ 3298]
loss: 0.023413  [ 1500/ 3298]
loss: 0.018372  [ 1600/ 3298]
loss: 0.074557  [ 1700/ 3298]
loss: 0.023589  [ 1800/ 3298]
loss: 0.022913  [ 1900/ 3298]
loss: 0.010170  [ 2000/ 3298]
loss: 0.015179  [ 2100/ 3298]
loss: 0.015034  [ 2200/ 3298]
loss: 0.018255  [ 2300/ 3298]
loss: 0.018184  [ 2400/ 3298]
loss: 0.016439  [ 2500/ 3298]
loss: 0.028130  [ 2600/ 3298]
loss: 0.045932  [ 2700/ 3298]
loss: 0.279263  [ 2800/ 3298]
loss: 0.033653  [ 2900/ 3298]
loss: 0.034520  [ 3000/ 3298]
loss: 0.023301  [ 3100/ 3298]
loss: 0.044713  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.040221  [    0/ 3298]
loss: 0.021236  [  100/ 3298]
loss: 0.103251  [  200/ 3298]
loss: 0.015859  [  300/ 3298]
loss: 0.012383  [  400/ 3298]
loss: 0.027300  [  500/ 3298]
loss: 0.017681  [  600/ 3298]
loss: 0.059730  [  700/ 3298]
loss: 0.024104  [  800/ 3298]
loss: 0.037676  [  900/ 3298]
loss: 0.031857  [ 1000/ 3298]
loss: 0.052064  [ 1100/ 3298]
loss: 0.073819  [ 1200/ 3298]
loss: 0.028526  [ 1300/ 3298]
loss: 0.016748  [ 1400/ 3298]
loss: 0.023969  [ 1500/ 3298]
loss: 0.018211  [ 1600/ 3298]
loss: 0.062909  [ 1700/ 3298]
loss: 0.023351  [ 1800/ 3298]
loss: 0.022542  [ 1900/ 3298]
loss: 0.011313  [ 2000/ 3298]
loss: 0.015118  [ 2100/ 3298]
loss: 0.013651  [ 2200/ 3298]
loss: 0.018777  [ 2300/ 3298]
loss: 0.017554  [ 2400/ 3298]
loss: 0.015905  [ 2500/ 3298]
loss: 0.028302  [ 2600/ 3298]
loss: 0.045628  [ 2700/ 3298]
loss: 0.259881  [ 2800/ 3298]
loss: 0.033516  [ 2900/ 3298]
loss: 0.033610  [ 3000/ 3298]
loss: 0.021737  [ 3100/ 3298]
loss: 0.044728  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.036778  [    0/ 3298]
loss: 0.020955  [  100/ 3298]
loss: 0.102492  [  200/ 3298]
loss: 0.016001  [  300/ 3298]
loss: 0.011285  [  400/ 3298]
loss: 0.028325  [  500/ 3298]
loss: 0.015990  [  600/ 3298]
loss: 0.059013  [  700/ 3298]
loss: 0.024278  [  800/ 3298]
loss: 0.028105  [  900/ 3298]
loss: 0.030920  [ 1000/ 3298]
loss: 0.045317  [ 1100/ 3298]
loss: 0.073338  [ 1200/ 3298]
loss: 0.029771  [ 1300/ 3298]
loss: 0.017022  [ 1400/ 3298]
loss: 0.024924  [ 1500/ 3298]
loss: 0.017604  [ 1600/ 3298]
loss: 0.055943  [ 1700/ 3298]
loss: 0.023726  [ 1800/ 3298]
loss: 0.022172  [ 1900/ 3298]
loss: 0.011124  [ 2000/ 3298]
loss: 0.015188  [ 2100/ 3298]
loss: 0.012601  [ 2200/ 3298]
loss: 0.018993  [ 2300/ 3298]
loss: 0.016790  [ 2400/ 3298]
loss: 0.014079  [ 2500/ 3298]
loss: 0.028420  [ 2600/ 3298]
loss: 0.045734  [ 2700/ 3298]
loss: 0.258355  [ 2800/ 3298]
loss: 0.033528  [ 2900/ 3298]
loss: 0.033693  [ 3000/ 3298]
loss: 0.020841  [ 3100/ 3298]
loss: 0.044978  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [-2.991033  -2.1599586]
[1 0 0 ... 0 1 2]
[0 2 2 ... 2 0 1]
Cluster 0 Occurrences: 1094; KMEANS: 1079
Cluster 1 Occurrences: 1089; KMEANS: 1118
Cluster 2 Occurrences: 1115; KMEANS: 1101
Centroids: [[0.97476935, -0.1770503], [-3.607701, -1.9796227], [0.43677333, 1.3334394]]
Centroids: [[-3.637969, -1.9971603], [0.44834653, 1.3475201], [0.9525256, -0.19464917]]
Contingency Matrix: 
[[   1   17 1076]
 [1076    2   11]
 [   2 1099   14]]
[[1, -1, 1076], [1076, -1, 11], [-1, -1, -1]]
[[-1, -1, -1], [1076, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 2, 1: 0}
New Contingency Matrix: 
[[1076    1   17]
 [  11 1076    2]
 [  14    2 1099]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1076, 1076, 1099], Sum: 3251
All_Elements: [1076, 1, 17, 11, 1076, 2, 14, 2, 1099], Sum: 3298
Accuracy: 0.9857489387507581
Done!
