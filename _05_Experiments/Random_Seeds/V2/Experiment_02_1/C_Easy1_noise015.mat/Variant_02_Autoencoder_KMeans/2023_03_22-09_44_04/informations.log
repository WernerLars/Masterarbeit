Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_04
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000171DAADB4E0>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81B208>
Epoch 1
-------------------------------
loss: 0.208744  [    0/ 3477]
loss: 0.187494  [  100/ 3477]
loss: 0.121335  [  200/ 3477]
loss: 0.103879  [  300/ 3477]
loss: 0.033971  [  400/ 3477]
loss: 0.094944  [  500/ 3477]
loss: 0.063539  [  600/ 3477]
loss: 0.041842  [  700/ 3477]
loss: 0.026076  [  800/ 3477]
loss: 0.026748  [  900/ 3477]
loss: 0.023616  [ 1000/ 3477]
loss: 0.005280  [ 1100/ 3477]
loss: 0.024012  [ 1200/ 3477]
loss: 0.020946  [ 1300/ 3477]
loss: 0.018303  [ 1400/ 3477]
loss: 0.023795  [ 1500/ 3477]
loss: 0.014076  [ 1600/ 3477]
loss: 0.013219  [ 1700/ 3477]
loss: 0.006524  [ 1800/ 3477]
loss: 0.017603  [ 1900/ 3477]
loss: 0.021155  [ 2000/ 3477]
loss: 0.027200  [ 2100/ 3477]
loss: 0.023327  [ 2200/ 3477]
loss: 0.012788  [ 2300/ 3477]
loss: 0.011871  [ 2400/ 3477]
loss: 0.094070  [ 2500/ 3477]
loss: 0.075839  [ 2600/ 3477]
loss: 0.009470  [ 2700/ 3477]
loss: 0.056363  [ 2800/ 3477]
loss: 0.018574  [ 2900/ 3477]
loss: 0.017866  [ 3000/ 3477]
loss: 0.014562  [ 3100/ 3477]
loss: 0.109432  [ 3200/ 3477]
loss: 0.010228  [ 3300/ 3477]
loss: 0.006081  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.017931  [    0/ 3477]
loss: 0.031551  [  100/ 3477]
loss: 0.032066  [  200/ 3477]
loss: 0.029887  [  300/ 3477]
loss: 0.017802  [  400/ 3477]
loss: 0.007309  [  500/ 3477]
loss: 0.016376  [  600/ 3477]
loss: 0.024234  [  700/ 3477]
loss: 0.021302  [  800/ 3477]
loss: 0.019385  [  900/ 3477]
loss: 0.022748  [ 1000/ 3477]
loss: 0.004015  [ 1100/ 3477]
loss: 0.021337  [ 1200/ 3477]
loss: 0.020529  [ 1300/ 3477]
loss: 0.014625  [ 1400/ 3477]
loss: 0.021703  [ 1500/ 3477]
loss: 0.013982  [ 1600/ 3477]
loss: 0.013146  [ 1700/ 3477]
loss: 0.005489  [ 1800/ 3477]
loss: 0.014467  [ 1900/ 3477]
loss: 0.019484  [ 2000/ 3477]
loss: 0.023798  [ 2100/ 3477]
loss: 0.022247  [ 2200/ 3477]
loss: 0.012361  [ 2300/ 3477]
loss: 0.009714  [ 2400/ 3477]
loss: 0.081101  [ 2500/ 3477]
loss: 0.066684  [ 2600/ 3477]
loss: 0.009347  [ 2700/ 3477]
loss: 0.040627  [ 2800/ 3477]
loss: 0.018512  [ 2900/ 3477]
loss: 0.017889  [ 3000/ 3477]
loss: 0.014429  [ 3100/ 3477]
loss: 0.102710  [ 3200/ 3477]
loss: 0.011045  [ 3300/ 3477]
loss: 0.006567  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.012355  [    0/ 3477]
loss: 0.019747  [  100/ 3477]
loss: 0.025325  [  200/ 3477]
loss: 0.029958  [  300/ 3477]
loss: 0.014130  [  400/ 3477]
loss: 0.006486  [  500/ 3477]
loss: 0.014317  [  600/ 3477]
loss: 0.026124  [  700/ 3477]
loss: 0.020184  [  800/ 3477]
loss: 0.017902  [  900/ 3477]
loss: 0.017437  [ 1000/ 3477]
loss: 0.007287  [ 1100/ 3477]
loss: 0.018650  [ 1200/ 3477]
loss: 0.013962  [ 1300/ 3477]
loss: 0.012374  [ 1400/ 3477]
loss: 0.018223  [ 1500/ 3477]
loss: 0.013668  [ 1600/ 3477]
loss: 0.011751  [ 1700/ 3477]
loss: 0.005319  [ 1800/ 3477]
loss: 0.010583  [ 1900/ 3477]
loss: 0.020402  [ 2000/ 3477]
loss: 0.017249  [ 2100/ 3477]
loss: 0.022811  [ 2200/ 3477]
loss: 0.013383  [ 2300/ 3477]
loss: 0.007786  [ 2400/ 3477]
loss: 0.077508  [ 2500/ 3477]
loss: 0.065580  [ 2600/ 3477]
loss: 0.009822  [ 2700/ 3477]
loss: 0.040895  [ 2800/ 3477]
loss: 0.019424  [ 2900/ 3477]
loss: 0.017147  [ 3000/ 3477]
loss: 0.014611  [ 3100/ 3477]
loss: 0.092501  [ 3200/ 3477]
loss: 0.009782  [ 3300/ 3477]
loss: 0.007202  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.009458  [    0/ 3477]
loss: 0.016115  [  100/ 3477]
loss: 0.019382  [  200/ 3477]
loss: 0.030915  [  300/ 3477]
loss: 0.014603  [  400/ 3477]
loss: 0.005649  [  500/ 3477]
loss: 0.011113  [  600/ 3477]
loss: 0.025052  [  700/ 3477]
loss: 0.020087  [  800/ 3477]
loss: 0.017272  [  900/ 3477]
loss: 0.013576  [ 1000/ 3477]
loss: 0.004658  [ 1100/ 3477]
loss: 0.016787  [ 1200/ 3477]
loss: 0.012125  [ 1300/ 3477]
loss: 0.012391  [ 1400/ 3477]
loss: 0.020075  [ 1500/ 3477]
loss: 0.012802  [ 1600/ 3477]
loss: 0.011451  [ 1700/ 3477]
loss: 0.005326  [ 1800/ 3477]
loss: 0.010430  [ 1900/ 3477]
loss: 0.021160  [ 2000/ 3477]
loss: 0.013066  [ 2100/ 3477]
loss: 0.023872  [ 2200/ 3477]
loss: 0.013627  [ 2300/ 3477]
loss: 0.008437  [ 2400/ 3477]
loss: 0.079751  [ 2500/ 3477]
loss: 0.064847  [ 2600/ 3477]
loss: 0.009531  [ 2700/ 3477]
loss: 0.038719  [ 2800/ 3477]
loss: 0.019688  [ 2900/ 3477]
loss: 0.016540  [ 3000/ 3477]
loss: 0.014891  [ 3100/ 3477]
loss: 0.088696  [ 3200/ 3477]
loss: 0.008729  [ 3300/ 3477]
loss: 0.007238  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.008330  [    0/ 3477]
loss: 0.015324  [  100/ 3477]
loss: 0.015884  [  200/ 3477]
loss: 0.028638  [  300/ 3477]
loss: 0.014145  [  400/ 3477]
loss: 0.006539  [  500/ 3477]
loss: 0.009119  [  600/ 3477]
loss: 0.024006  [  700/ 3477]
loss: 0.020343  [  800/ 3477]
loss: 0.016727  [  900/ 3477]
loss: 0.011145  [ 1000/ 3477]
loss: 0.003606  [ 1100/ 3477]
loss: 0.016148  [ 1200/ 3477]
loss: 0.011624  [ 1300/ 3477]
loss: 0.013294  [ 1400/ 3477]
loss: 0.018225  [ 1500/ 3477]
loss: 0.012360  [ 1600/ 3477]
loss: 0.010131  [ 1700/ 3477]
loss: 0.005673  [ 1800/ 3477]
loss: 0.010231  [ 1900/ 3477]
loss: 0.021055  [ 2000/ 3477]
loss: 0.010070  [ 2100/ 3477]
loss: 0.022838  [ 2200/ 3477]
loss: 0.013609  [ 2300/ 3477]
loss: 0.008645  [ 2400/ 3477]
loss: 0.079655  [ 2500/ 3477]
loss: 0.065968  [ 2600/ 3477]
loss: 0.009204  [ 2700/ 3477]
loss: 0.036781  [ 2800/ 3477]
loss: 0.019465  [ 2900/ 3477]
loss: 0.014035  [ 3000/ 3477]
loss: 0.015260  [ 3100/ 3477]
loss: 0.087707  [ 3200/ 3477]
loss: 0.007932  [ 3300/ 3477]
loss: 0.007656  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.008560  [    0/ 3477]
loss: 0.015389  [  100/ 3477]
loss: 0.014120  [  200/ 3477]
loss: 0.025581  [  300/ 3477]
loss: 0.015012  [  400/ 3477]
loss: 0.007289  [  500/ 3477]
loss: 0.007680  [  600/ 3477]
loss: 0.022616  [  700/ 3477]
loss: 0.020526  [  800/ 3477]
loss: 0.015579  [  900/ 3477]
loss: 0.009858  [ 1000/ 3477]
loss: 0.002990  [ 1100/ 3477]
loss: 0.016140  [ 1200/ 3477]
loss: 0.011354  [ 1300/ 3477]
loss: 0.014043  [ 1400/ 3477]
loss: 0.017360  [ 1500/ 3477]
loss: 0.011883  [ 1600/ 3477]
loss: 0.009706  [ 1700/ 3477]
loss: 0.006594  [ 1800/ 3477]
loss: 0.009841  [ 1900/ 3477]
loss: 0.020872  [ 2000/ 3477]
loss: 0.007846  [ 2100/ 3477]
loss: 0.024262  [ 2200/ 3477]
loss: 0.014284  [ 2300/ 3477]
loss: 0.009199  [ 2400/ 3477]
loss: 0.080267  [ 2500/ 3477]
loss: 0.066651  [ 2600/ 3477]
loss: 0.008638  [ 2700/ 3477]
loss: 0.037000  [ 2800/ 3477]
loss: 0.019919  [ 2900/ 3477]
loss: 0.012516  [ 3000/ 3477]
loss: 0.015456  [ 3100/ 3477]
loss: 0.086518  [ 3200/ 3477]
loss: 0.007301  [ 3300/ 3477]
loss: 0.007543  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.009250  [    0/ 3477]
loss: 0.015658  [  100/ 3477]
loss: 0.013230  [  200/ 3477]
loss: 0.024005  [  300/ 3477]
loss: 0.015020  [  400/ 3477]
loss: 0.008055  [  500/ 3477]
loss: 0.007246  [  600/ 3477]
loss: 0.021718  [  700/ 3477]
loss: 0.020551  [  800/ 3477]
loss: 0.014439  [  900/ 3477]
loss: 0.009988  [ 1000/ 3477]
loss: 0.002990  [ 1100/ 3477]
loss: 0.015808  [ 1200/ 3477]
loss: 0.011305  [ 1300/ 3477]
loss: 0.013948  [ 1400/ 3477]
loss: 0.017122  [ 1500/ 3477]
loss: 0.011574  [ 1600/ 3477]
loss: 0.009305  [ 1700/ 3477]
loss: 0.007617  [ 1800/ 3477]
loss: 0.009368  [ 1900/ 3477]
loss: 0.020462  [ 2000/ 3477]
loss: 0.006144  [ 2100/ 3477]
loss: 0.023574  [ 2200/ 3477]
loss: 0.014150  [ 2300/ 3477]
loss: 0.008986  [ 2400/ 3477]
loss: 0.080543  [ 2500/ 3477]
loss: 0.068160  [ 2600/ 3477]
loss: 0.008084  [ 2700/ 3477]
loss: 0.042131  [ 2800/ 3477]
loss: 0.020394  [ 2900/ 3477]
loss: 0.011953  [ 3000/ 3477]
loss: 0.015613  [ 3100/ 3477]
loss: 0.085470  [ 3200/ 3477]
loss: 0.006812  [ 3300/ 3477]
loss: 0.007551  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.009857  [    0/ 3477]
loss: 0.015723  [  100/ 3477]
loss: 0.013390  [  200/ 3477]
loss: 0.023673  [  300/ 3477]
loss: 0.015008  [  400/ 3477]
loss: 0.008701  [  500/ 3477]
loss: 0.007306  [  600/ 3477]
loss: 0.021299  [  700/ 3477]
loss: 0.020834  [  800/ 3477]
loss: 0.013992  [  900/ 3477]
loss: 0.011166  [ 1000/ 3477]
loss: 0.003023  [ 1100/ 3477]
loss: 0.015159  [ 1200/ 3477]
loss: 0.011217  [ 1300/ 3477]
loss: 0.013753  [ 1400/ 3477]
loss: 0.017063  [ 1500/ 3477]
loss: 0.011676  [ 1600/ 3477]
loss: 0.009307  [ 1700/ 3477]
loss: 0.008728  [ 1800/ 3477]
loss: 0.009065  [ 1900/ 3477]
loss: 0.019894  [ 2000/ 3477]
loss: 0.005059  [ 2100/ 3477]
loss: 0.022069  [ 2200/ 3477]
loss: 0.014118  [ 2300/ 3477]
loss: 0.009151  [ 2400/ 3477]
loss: 0.081085  [ 2500/ 3477]
loss: 0.069515  [ 2600/ 3477]
loss: 0.007532  [ 2700/ 3477]
loss: 0.048826  [ 2800/ 3477]
loss: 0.020970  [ 2900/ 3477]
loss: 0.011736  [ 3000/ 3477]
loss: 0.015536  [ 3100/ 3477]
loss: 0.084215  [ 3200/ 3477]
loss: 0.006908  [ 3300/ 3477]
loss: 0.007609  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [-2.0172043  1.3292062]
[1 0 2 ... 1 1 1]
[0 1 2 ... 0 0 0]
Cluster 0 Occurrences: 1132; KMEANS: 1190
Cluster 1 Occurrences: 1188; KMEANS: 1126
Cluster 2 Occurrences: 1157; KMEANS: 1161
Centroids: [[1.6468331, -0.18232493], [-1.5137792, 1.1894712], [1.2614385, 1.7545061]]
Centroids: [[-1.520377, 1.1995192], [1.64978, -0.19433226], [1.2721155, 1.7468162]]
Contingency Matrix: 
[[   1 1122    9]
 [1184    4    0]
 [   5    0 1152]]
[[-1, 1122, 9], [-1, -1, -1], [-1, 0, 1152]]
[[-1, 1122, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 2: 2, 0: 1}
New Contingency Matrix: 
[[1122    1    9]
 [   4 1184    0]
 [   0    5 1152]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1122, 1184, 1152], Sum: 3458
All_Elements: [1122, 1, 9, 4, 1184, 0, 0, 5, 1152], Sum: 3477
Accuracy: 0.994535519125683
Done!
