Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_54_59
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000171DA14E128>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81B390>
Epoch 1
-------------------------------
loss: 0.168609  [    0/ 3526]
loss: 0.243650  [  100/ 3526]
loss: 0.152561  [  200/ 3526]
loss: 0.133711  [  300/ 3526]
loss: 0.027022  [  400/ 3526]
loss: 0.053207  [  500/ 3526]
loss: 0.043870  [  600/ 3526]
loss: 0.015587  [  700/ 3526]
loss: 0.069673  [  800/ 3526]
loss: 0.016559  [  900/ 3526]
loss: 0.044033  [ 1000/ 3526]
loss: 0.018377  [ 1100/ 3526]
loss: 0.064853  [ 1200/ 3526]
loss: 0.049825  [ 1300/ 3526]
loss: 0.028139  [ 1400/ 3526]
loss: 0.033586  [ 1500/ 3526]
loss: 0.038240  [ 1600/ 3526]
loss: 0.032281  [ 1700/ 3526]
loss: 0.031803  [ 1800/ 3526]
loss: 0.018628  [ 1900/ 3526]
loss: 0.017767  [ 2000/ 3526]
loss: 0.107651  [ 2100/ 3526]
loss: 0.022833  [ 2200/ 3526]
loss: 0.016459  [ 2300/ 3526]
loss: 0.008956  [ 2400/ 3526]
loss: 0.014840  [ 2500/ 3526]
loss: 0.020120  [ 2600/ 3526]
loss: 0.038772  [ 2700/ 3526]
loss: 0.006245  [ 2800/ 3526]
loss: 0.020100  [ 2900/ 3526]
loss: 0.069179  [ 3000/ 3526]
loss: 0.186947  [ 3100/ 3526]
loss: 0.036884  [ 3200/ 3526]
loss: 0.027749  [ 3300/ 3526]
loss: 0.040029  [ 3400/ 3526]
loss: 0.050405  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.016529  [    0/ 3526]
loss: 0.031139  [  100/ 3526]
loss: 0.040139  [  200/ 3526]
loss: 0.007559  [  300/ 3526]
loss: 0.017733  [  400/ 3526]
loss: 0.057003  [  500/ 3526]
loss: 0.019538  [  600/ 3526]
loss: 0.011475  [  700/ 3526]
loss: 0.033386  [  800/ 3526]
loss: 0.011605  [  900/ 3526]
loss: 0.032949  [ 1000/ 3526]
loss: 0.018243  [ 1100/ 3526]
loss: 0.036498  [ 1200/ 3526]
loss: 0.032077  [ 1300/ 3526]
loss: 0.028942  [ 1400/ 3526]
loss: 0.021983  [ 1500/ 3526]
loss: 0.036891  [ 1600/ 3526]
loss: 0.026285  [ 1700/ 3526]
loss: 0.022224  [ 1800/ 3526]
loss: 0.019033  [ 1900/ 3526]
loss: 0.018333  [ 2000/ 3526]
loss: 0.121095  [ 2100/ 3526]
loss: 0.024275  [ 2200/ 3526]
loss: 0.016791  [ 2300/ 3526]
loss: 0.007863  [ 2400/ 3526]
loss: 0.016079  [ 2500/ 3526]
loss: 0.017986  [ 2600/ 3526]
loss: 0.018923  [ 2700/ 3526]
loss: 0.008670  [ 2800/ 3526]
loss: 0.010417  [ 2900/ 3526]
loss: 0.052516  [ 3000/ 3526]
loss: 0.171778  [ 3100/ 3526]
loss: 0.028923  [ 3200/ 3526]
loss: 0.025881  [ 3300/ 3526]
loss: 0.050045  [ 3400/ 3526]
loss: 0.043145  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.016471  [    0/ 3526]
loss: 0.045387  [  100/ 3526]
loss: 0.050606  [  200/ 3526]
loss: 0.004911  [  300/ 3526]
loss: 0.016048  [  400/ 3526]
loss: 0.043010  [  500/ 3526]
loss: 0.012362  [  600/ 3526]
loss: 0.013871  [  700/ 3526]
loss: 0.032957  [  800/ 3526]
loss: 0.012489  [  900/ 3526]
loss: 0.017143  [ 1000/ 3526]
loss: 0.016614  [ 1100/ 3526]
loss: 0.028745  [ 1200/ 3526]
loss: 0.026297  [ 1300/ 3526]
loss: 0.028013  [ 1400/ 3526]
loss: 0.020038  [ 1500/ 3526]
loss: 0.042744  [ 1600/ 3526]
loss: 0.021501  [ 1700/ 3526]
loss: 0.020273  [ 1800/ 3526]
loss: 0.017135  [ 1900/ 3526]
loss: 0.016856  [ 2000/ 3526]
loss: 0.134150  [ 2100/ 3526]
loss: 0.023934  [ 2200/ 3526]
loss: 0.016363  [ 2300/ 3526]
loss: 0.008187  [ 2400/ 3526]
loss: 0.016358  [ 2500/ 3526]
loss: 0.016586  [ 2600/ 3526]
loss: 0.016443  [ 2700/ 3526]
loss: 0.008638  [ 2800/ 3526]
loss: 0.010436  [ 2900/ 3526]
loss: 0.052281  [ 3000/ 3526]
loss: 0.167562  [ 3100/ 3526]
loss: 0.027534  [ 3200/ 3526]
loss: 0.025618  [ 3300/ 3526]
loss: 0.049249  [ 3400/ 3526]
loss: 0.040007  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.015967  [    0/ 3526]
loss: 0.044547  [  100/ 3526]
loss: 0.049306  [  200/ 3526]
loss: 0.004885  [  300/ 3526]
loss: 0.016120  [  400/ 3526]
loss: 0.039423  [  500/ 3526]
loss: 0.011286  [  600/ 3526]
loss: 0.014197  [  700/ 3526]
loss: 0.032669  [  800/ 3526]
loss: 0.012678  [  900/ 3526]
loss: 0.016430  [ 1000/ 3526]
loss: 0.017414  [ 1100/ 3526]
loss: 0.030006  [ 1200/ 3526]
loss: 0.024584  [ 1300/ 3526]
loss: 0.027284  [ 1400/ 3526]
loss: 0.019716  [ 1500/ 3526]
loss: 0.044495  [ 1600/ 3526]
loss: 0.021230  [ 1700/ 3526]
loss: 0.020237  [ 1800/ 3526]
loss: 0.016566  [ 1900/ 3526]
loss: 0.016337  [ 2000/ 3526]
loss: 0.139913  [ 2100/ 3526]
loss: 0.023714  [ 2200/ 3526]
loss: 0.016380  [ 2300/ 3526]
loss: 0.008407  [ 2400/ 3526]
loss: 0.016088  [ 2500/ 3526]
loss: 0.016413  [ 2600/ 3526]
loss: 0.015939  [ 2700/ 3526]
loss: 0.008158  [ 2800/ 3526]
loss: 0.010169  [ 2900/ 3526]
loss: 0.052660  [ 3000/ 3526]
loss: 0.165715  [ 3100/ 3526]
loss: 0.026850  [ 3200/ 3526]
loss: 0.025626  [ 3300/ 3526]
loss: 0.048213  [ 3400/ 3526]
loss: 0.039255  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.016108  [    0/ 3526]
loss: 0.044896  [  100/ 3526]
loss: 0.048841  [  200/ 3526]
loss: 0.005106  [  300/ 3526]
loss: 0.016628  [  400/ 3526]
loss: 0.038708  [  500/ 3526]
loss: 0.011077  [  600/ 3526]
loss: 0.014112  [  700/ 3526]
loss: 0.032346  [  800/ 3526]
loss: 0.012674  [  900/ 3526]
loss: 0.016193  [ 1000/ 3526]
loss: 0.017282  [ 1100/ 3526]
loss: 0.029824  [ 1200/ 3526]
loss: 0.024259  [ 1300/ 3526]
loss: 0.027447  [ 1400/ 3526]
loss: 0.019429  [ 1500/ 3526]
loss: 0.045216  [ 1600/ 3526]
loss: 0.020938  [ 1700/ 3526]
loss: 0.020285  [ 1800/ 3526]
loss: 0.016456  [ 1900/ 3526]
loss: 0.016122  [ 2000/ 3526]
loss: 0.138096  [ 2100/ 3526]
loss: 0.023854  [ 2200/ 3526]
loss: 0.016393  [ 2300/ 3526]
loss: 0.008512  [ 2400/ 3526]
loss: 0.016068  [ 2500/ 3526]
loss: 0.016618  [ 2600/ 3526]
loss: 0.015832  [ 2700/ 3526]
loss: 0.007975  [ 2800/ 3526]
loss: 0.009966  [ 2900/ 3526]
loss: 0.052748  [ 3000/ 3526]
loss: 0.165496  [ 3100/ 3526]
loss: 0.026491  [ 3200/ 3526]
loss: 0.025637  [ 3300/ 3526]
loss: 0.047774  [ 3400/ 3526]
loss: 0.038981  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.016055  [    0/ 3526]
loss: 0.044948  [  100/ 3526]
loss: 0.048446  [  200/ 3526]
loss: 0.005139  [  300/ 3526]
loss: 0.016796  [  400/ 3526]
loss: 0.038379  [  500/ 3526]
loss: 0.010982  [  600/ 3526]
loss: 0.014191  [  700/ 3526]
loss: 0.031810  [  800/ 3526]
loss: 0.012674  [  900/ 3526]
loss: 0.015972  [ 1000/ 3526]
loss: 0.017384  [ 1100/ 3526]
loss: 0.029554  [ 1200/ 3526]
loss: 0.024191  [ 1300/ 3526]
loss: 0.027673  [ 1400/ 3526]
loss: 0.019218  [ 1500/ 3526]
loss: 0.045338  [ 1600/ 3526]
loss: 0.020537  [ 1700/ 3526]
loss: 0.020328  [ 1800/ 3526]
loss: 0.016507  [ 1900/ 3526]
loss: 0.016043  [ 2000/ 3526]
loss: 0.135393  [ 2100/ 3526]
loss: 0.024077  [ 2200/ 3526]
loss: 0.016473  [ 2300/ 3526]
loss: 0.008583  [ 2400/ 3526]
loss: 0.015981  [ 2500/ 3526]
loss: 0.016849  [ 2600/ 3526]
loss: 0.015801  [ 2700/ 3526]
loss: 0.007916  [ 2800/ 3526]
loss: 0.009762  [ 2900/ 3526]
loss: 0.051870  [ 3000/ 3526]
loss: 0.165967  [ 3100/ 3526]
loss: 0.026497  [ 3200/ 3526]
loss: 0.025702  [ 3300/ 3526]
loss: 0.047964  [ 3400/ 3526]
loss: 0.038953  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.016025  [    0/ 3526]
loss: 0.045555  [  100/ 3526]
loss: 0.047984  [  200/ 3526]
loss: 0.005265  [  300/ 3526]
loss: 0.016452  [  400/ 3526]
loss: 0.038236  [  500/ 3526]
loss: 0.011006  [  600/ 3526]
loss: 0.014093  [  700/ 3526]
loss: 0.031056  [  800/ 3526]
loss: 0.012600  [  900/ 3526]
loss: 0.015651  [ 1000/ 3526]
loss: 0.017526  [ 1100/ 3526]
loss: 0.029289  [ 1200/ 3526]
loss: 0.024044  [ 1300/ 3526]
loss: 0.027865  [ 1400/ 3526]
loss: 0.019116  [ 1500/ 3526]
loss: 0.045520  [ 1600/ 3526]
loss: 0.020375  [ 1700/ 3526]
loss: 0.020460  [ 1800/ 3526]
loss: 0.016770  [ 1900/ 3526]
loss: 0.015880  [ 2000/ 3526]
loss: 0.138583  [ 2100/ 3526]
loss: 0.024277  [ 2200/ 3526]
loss: 0.016618  [ 2300/ 3526]
loss: 0.008717  [ 2400/ 3526]
loss: 0.015853  [ 2500/ 3526]
loss: 0.017026  [ 2600/ 3526]
loss: 0.015729  [ 2700/ 3526]
loss: 0.007926  [ 2800/ 3526]
loss: 0.009650  [ 2900/ 3526]
loss: 0.051356  [ 3000/ 3526]
loss: 0.165891  [ 3100/ 3526]
loss: 0.026478  [ 3200/ 3526]
loss: 0.025803  [ 3300/ 3526]
loss: 0.047886  [ 3400/ 3526]
loss: 0.038816  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.016068  [    0/ 3526]
loss: 0.044987  [  100/ 3526]
loss: 0.047617  [  200/ 3526]
loss: 0.005308  [  300/ 3526]
loss: 0.016304  [  400/ 3526]
loss: 0.038325  [  500/ 3526]
loss: 0.010959  [  600/ 3526]
loss: 0.014035  [  700/ 3526]
loss: 0.030453  [  800/ 3526]
loss: 0.012539  [  900/ 3526]
loss: 0.015549  [ 1000/ 3526]
loss: 0.017584  [ 1100/ 3526]
loss: 0.029235  [ 1200/ 3526]
loss: 0.024089  [ 1300/ 3526]
loss: 0.028281  [ 1400/ 3526]
loss: 0.018892  [ 1500/ 3526]
loss: 0.045716  [ 1600/ 3526]
loss: 0.020206  [ 1700/ 3526]
loss: 0.020499  [ 1800/ 3526]
loss: 0.016926  [ 1900/ 3526]
loss: 0.015838  [ 2000/ 3526]
loss: 0.140192  [ 2100/ 3526]
loss: 0.024509  [ 2200/ 3526]
loss: 0.016732  [ 2300/ 3526]
loss: 0.008860  [ 2400/ 3526]
loss: 0.015745  [ 2500/ 3526]
loss: 0.017276  [ 2600/ 3526]
loss: 0.015600  [ 2700/ 3526]
loss: 0.007927  [ 2800/ 3526]
loss: 0.009555  [ 2900/ 3526]
loss: 0.050766  [ 3000/ 3526]
loss: 0.165911  [ 3100/ 3526]
loss: 0.026368  [ 3200/ 3526]
loss: 0.025849  [ 3300/ 3526]
loss: 0.048182  [ 3400/ 3526]
loss: 0.038892  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [ 0.7202956 -0.4604096]
[2 0 2 ... 0 2 2]
[1 2 1 ... 2 1 1]
Cluster 0 Occurrences: 1186; KMEANS: 1189
Cluster 1 Occurrences: 1188; KMEANS: 1150
Cluster 2 Occurrences: 1152; KMEANS: 1187
Centroids: [[0.40083146, 1.1680661], [0.30393764, 0.6457359], [0.61120474, -0.53315884]]
Centroids: [[0.30732694, 1.2323527], [0.6122295, -0.5369541], [0.3968797, 0.5817114]]
Contingency Matrix: 
[[ 947    0  239]
 [ 242    9  937]
 [   0 1141   11]]
[[947, -1, 239], [242, -1, 937], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, 937], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 0, 1: 2}
New Contingency Matrix: 
[[ 947  239    0]
 [ 242  937    9]
 [   0   11 1141]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [947, 937, 1141], Sum: 3025
All_Elements: [947, 239, 0, 242, 937, 9, 0, 11, 1141], Sum: 3526
Accuracy: 0.8579126488939308
Done!
