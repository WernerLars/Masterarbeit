Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_06
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001720138FC18>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81B128>
Epoch 1
-------------------------------
loss: 0.264464  [    0/ 3442]
loss: 0.148369  [  100/ 3442]
loss: 0.062515  [  200/ 3442]
loss: 0.077767  [  300/ 3442]
loss: 0.028060  [  400/ 3442]
loss: 0.030730  [  500/ 3442]
loss: 0.031518  [  600/ 3442]
loss: 0.021403  [  700/ 3442]
loss: 0.023158  [  800/ 3442]
loss: 0.039002  [  900/ 3442]
loss: 0.013584  [ 1000/ 3442]
loss: 0.004065  [ 1100/ 3442]
loss: 0.017847  [ 1200/ 3442]
loss: 0.013105  [ 1300/ 3442]
loss: 0.017380  [ 1400/ 3442]
loss: 0.019759  [ 1500/ 3442]
loss: 0.016448  [ 1600/ 3442]
loss: 0.019463  [ 1700/ 3442]
loss: 0.018722  [ 1800/ 3442]
loss: 0.021047  [ 1900/ 3442]
loss: 0.021286  [ 2000/ 3442]
loss: 0.024164  [ 2100/ 3442]
loss: 0.016922  [ 2200/ 3442]
loss: 0.012559  [ 2300/ 3442]
loss: 0.015098  [ 2400/ 3442]
loss: 0.039302  [ 2500/ 3442]
loss: 0.124912  [ 2600/ 3442]
loss: 0.032653  [ 2700/ 3442]
loss: 0.011106  [ 2800/ 3442]
loss: 0.012177  [ 2900/ 3442]
loss: 0.005763  [ 3000/ 3442]
loss: 0.019771  [ 3100/ 3442]
loss: 0.098917  [ 3200/ 3442]
loss: 0.016262  [ 3300/ 3442]
loss: 0.019041  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.010806  [    0/ 3442]
loss: 0.012550  [  100/ 3442]
loss: 0.031681  [  200/ 3442]
loss: 0.006951  [  300/ 3442]
loss: 0.019753  [  400/ 3442]
loss: 0.008898  [  500/ 3442]
loss: 0.025178  [  600/ 3442]
loss: 0.011288  [  700/ 3442]
loss: 0.018152  [  800/ 3442]
loss: 0.024345  [  900/ 3442]
loss: 0.007882  [ 1000/ 3442]
loss: 0.003623  [ 1100/ 3442]
loss: 0.013097  [ 1200/ 3442]
loss: 0.010377  [ 1300/ 3442]
loss: 0.014872  [ 1400/ 3442]
loss: 0.019125  [ 1500/ 3442]
loss: 0.019218  [ 1600/ 3442]
loss: 0.011461  [ 1700/ 3442]
loss: 0.015697  [ 1800/ 3442]
loss: 0.017938  [ 1900/ 3442]
loss: 0.017032  [ 2000/ 3442]
loss: 0.013916  [ 2100/ 3442]
loss: 0.015943  [ 2200/ 3442]
loss: 0.011192  [ 2300/ 3442]
loss: 0.015905  [ 2400/ 3442]
loss: 0.036136  [ 2500/ 3442]
loss: 0.125971  [ 2600/ 3442]
loss: 0.030145  [ 2700/ 3442]
loss: 0.009700  [ 2800/ 3442]
loss: 0.009687  [ 2900/ 3442]
loss: 0.005499  [ 3000/ 3442]
loss: 0.018448  [ 3100/ 3442]
loss: 0.112685  [ 3200/ 3442]
loss: 0.015534  [ 3300/ 3442]
loss: 0.018420  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.010045  [    0/ 3442]
loss: 0.012244  [  100/ 3442]
loss: 0.030339  [  200/ 3442]
loss: 0.006848  [  300/ 3442]
loss: 0.019458  [  400/ 3442]
loss: 0.008536  [  500/ 3442]
loss: 0.024167  [  600/ 3442]
loss: 0.011431  [  700/ 3442]
loss: 0.018161  [  800/ 3442]
loss: 0.024096  [  900/ 3442]
loss: 0.008274  [ 1000/ 3442]
loss: 0.003393  [ 1100/ 3442]
loss: 0.013111  [ 1200/ 3442]
loss: 0.010234  [ 1300/ 3442]
loss: 0.013585  [ 1400/ 3442]
loss: 0.018785  [ 1500/ 3442]
loss: 0.017657  [ 1600/ 3442]
loss: 0.011331  [ 1700/ 3442]
loss: 0.015866  [ 1800/ 3442]
loss: 0.016801  [ 1900/ 3442]
loss: 0.016604  [ 2000/ 3442]
loss: 0.013484  [ 2100/ 3442]
loss: 0.015134  [ 2200/ 3442]
loss: 0.011096  [ 2300/ 3442]
loss: 0.015586  [ 2400/ 3442]
loss: 0.035344  [ 2500/ 3442]
loss: 0.126533  [ 2600/ 3442]
loss: 0.030256  [ 2700/ 3442]
loss: 0.009531  [ 2800/ 3442]
loss: 0.009551  [ 2900/ 3442]
loss: 0.005428  [ 3000/ 3442]
loss: 0.018330  [ 3100/ 3442]
loss: 0.113813  [ 3200/ 3442]
loss: 0.015367  [ 3300/ 3442]
loss: 0.018115  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.010061  [    0/ 3442]
loss: 0.012041  [  100/ 3442]
loss: 0.030391  [  200/ 3442]
loss: 0.006896  [  300/ 3442]
loss: 0.019192  [  400/ 3442]
loss: 0.008572  [  500/ 3442]
loss: 0.023647  [  600/ 3442]
loss: 0.011613  [  700/ 3442]
loss: 0.018242  [  800/ 3442]
loss: 0.024117  [  900/ 3442]
loss: 0.008435  [ 1000/ 3442]
loss: 0.003393  [ 1100/ 3442]
loss: 0.013149  [ 1200/ 3442]
loss: 0.010255  [ 1300/ 3442]
loss: 0.013011  [ 1400/ 3442]
loss: 0.018689  [ 1500/ 3442]
loss: 0.017651  [ 1600/ 3442]
loss: 0.011363  [ 1700/ 3442]
loss: 0.015953  [ 1800/ 3442]
loss: 0.016317  [ 1900/ 3442]
loss: 0.016326  [ 2000/ 3442]
loss: 0.013337  [ 2100/ 3442]
loss: 0.014861  [ 2200/ 3442]
loss: 0.010698  [ 2300/ 3442]
loss: 0.015479  [ 2400/ 3442]
loss: 0.035128  [ 2500/ 3442]
loss: 0.126470  [ 2600/ 3442]
loss: 0.029888  [ 2700/ 3442]
loss: 0.009363  [ 2800/ 3442]
loss: 0.009575  [ 2900/ 3442]
loss: 0.005422  [ 3000/ 3442]
loss: 0.018334  [ 3100/ 3442]
loss: 0.113505  [ 3200/ 3442]
loss: 0.015601  [ 3300/ 3442]
loss: 0.017888  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.010047  [    0/ 3442]
loss: 0.011897  [  100/ 3442]
loss: 0.030493  [  200/ 3442]
loss: 0.007039  [  300/ 3442]
loss: 0.019154  [  400/ 3442]
loss: 0.008797  [  500/ 3442]
loss: 0.023083  [  600/ 3442]
loss: 0.011808  [  700/ 3442]
loss: 0.018233  [  800/ 3442]
loss: 0.024507  [  900/ 3442]
loss: 0.008669  [ 1000/ 3442]
loss: 0.003319  [ 1100/ 3442]
loss: 0.013498  [ 1200/ 3442]
loss: 0.010246  [ 1300/ 3442]
loss: 0.013238  [ 1400/ 3442]
loss: 0.018767  [ 1500/ 3442]
loss: 0.017544  [ 1600/ 3442]
loss: 0.011305  [ 1700/ 3442]
loss: 0.015695  [ 1800/ 3442]
loss: 0.016075  [ 1900/ 3442]
loss: 0.016133  [ 2000/ 3442]
loss: 0.013319  [ 2100/ 3442]
loss: 0.014718  [ 2200/ 3442]
loss: 0.010428  [ 2300/ 3442]
loss: 0.015499  [ 2400/ 3442]
loss: 0.034641  [ 2500/ 3442]
loss: 0.126339  [ 2600/ 3442]
loss: 0.029934  [ 2700/ 3442]
loss: 0.009232  [ 2800/ 3442]
loss: 0.009551  [ 2900/ 3442]
loss: 0.005420  [ 3000/ 3442]
loss: 0.018222  [ 3100/ 3442]
loss: 0.113026  [ 3200/ 3442]
loss: 0.015832  [ 3300/ 3442]
loss: 0.017757  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.010105  [    0/ 3442]
loss: 0.011830  [  100/ 3442]
loss: 0.030262  [  200/ 3442]
loss: 0.007143  [  300/ 3442]
loss: 0.019017  [  400/ 3442]
loss: 0.009054  [  500/ 3442]
loss: 0.022575  [  600/ 3442]
loss: 0.012101  [  700/ 3442]
loss: 0.018153  [  800/ 3442]
loss: 0.024892  [  900/ 3442]
loss: 0.008861  [ 1000/ 3442]
loss: 0.003368  [ 1100/ 3442]
loss: 0.013865  [ 1200/ 3442]
loss: 0.010115  [ 1300/ 3442]
loss: 0.013436  [ 1400/ 3442]
loss: 0.018739  [ 1500/ 3442]
loss: 0.017533  [ 1600/ 3442]
loss: 0.011282  [ 1700/ 3442]
loss: 0.015798  [ 1800/ 3442]
loss: 0.015804  [ 1900/ 3442]
loss: 0.015876  [ 2000/ 3442]
loss: 0.013384  [ 2100/ 3442]
loss: 0.014006  [ 2200/ 3442]
loss: 0.010125  [ 2300/ 3442]
loss: 0.015461  [ 2400/ 3442]
loss: 0.033950  [ 2500/ 3442]
loss: 0.126345  [ 2600/ 3442]
loss: 0.030113  [ 2700/ 3442]
loss: 0.009180  [ 2800/ 3442]
loss: 0.009559  [ 2900/ 3442]
loss: 0.005430  [ 3000/ 3442]
loss: 0.017982  [ 3100/ 3442]
loss: 0.112865  [ 3200/ 3442]
loss: 0.016025  [ 3300/ 3442]
loss: 0.018004  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.010413  [    0/ 3442]
loss: 0.011816  [  100/ 3442]
loss: 0.029946  [  200/ 3442]
loss: 0.006970  [  300/ 3442]
loss: 0.018688  [  400/ 3442]
loss: 0.009267  [  500/ 3442]
loss: 0.022245  [  600/ 3442]
loss: 0.012267  [  700/ 3442]
loss: 0.018134  [  800/ 3442]
loss: 0.025309  [  900/ 3442]
loss: 0.008995  [ 1000/ 3442]
loss: 0.003295  [ 1100/ 3442]
loss: 0.014113  [ 1200/ 3442]
loss: 0.009937  [ 1300/ 3442]
loss: 0.013601  [ 1400/ 3442]
loss: 0.018668  [ 1500/ 3442]
loss: 0.017390  [ 1600/ 3442]
loss: 0.011254  [ 1700/ 3442]
loss: 0.016082  [ 1800/ 3442]
loss: 0.015399  [ 1900/ 3442]
loss: 0.015661  [ 2000/ 3442]
loss: 0.013338  [ 2100/ 3442]
loss: 0.014083  [ 2200/ 3442]
loss: 0.009957  [ 2300/ 3442]
loss: 0.015513  [ 2400/ 3442]
loss: 0.033407  [ 2500/ 3442]
loss: 0.126174  [ 2600/ 3442]
loss: 0.030433  [ 2700/ 3442]
loss: 0.008751  [ 2800/ 3442]
loss: 0.009396  [ 2900/ 3442]
loss: 0.005539  [ 3000/ 3442]
loss: 0.017968  [ 3100/ 3442]
loss: 0.113983  [ 3200/ 3442]
loss: 0.016156  [ 3300/ 3442]
loss: 0.018315  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.010849  [    0/ 3442]
loss: 0.011836  [  100/ 3442]
loss: 0.029579  [  200/ 3442]
loss: 0.007040  [  300/ 3442]
loss: 0.018101  [  400/ 3442]
loss: 0.009505  [  500/ 3442]
loss: 0.022303  [  600/ 3442]
loss: 0.012352  [  700/ 3442]
loss: 0.017970  [  800/ 3442]
loss: 0.026056  [  900/ 3442]
loss: 0.009048  [ 1000/ 3442]
loss: 0.003334  [ 1100/ 3442]
loss: 0.014266  [ 1200/ 3442]
loss: 0.009894  [ 1300/ 3442]
loss: 0.013293  [ 1400/ 3442]
loss: 0.018641  [ 1500/ 3442]
loss: 0.017365  [ 1600/ 3442]
loss: 0.011030  [ 1700/ 3442]
loss: 0.015866  [ 1800/ 3442]
loss: 0.015154  [ 1900/ 3442]
loss: 0.015514  [ 2000/ 3442]
loss: 0.013054  [ 2100/ 3442]
loss: 0.013954  [ 2200/ 3442]
loss: 0.009806  [ 2300/ 3442]
loss: 0.015483  [ 2400/ 3442]
loss: 0.032754  [ 2500/ 3442]
loss: 0.126079  [ 2600/ 3442]
loss: 0.030820  [ 2700/ 3442]
loss: 0.008462  [ 2800/ 3442]
loss: 0.009291  [ 2900/ 3442]
loss: 0.005470  [ 3000/ 3442]
loss: 0.017889  [ 3100/ 3442]
loss: 0.114273  [ 3200/ 3442]
loss: 0.016397  [ 3300/ 3442]
loss: 0.018357  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [0.58079565 0.68819183]
[1 0 0 ... 1 2 0]
[0 2 0 ... 0 1 0]
Cluster 0 Occurrences: 1159; KMEANS: 1208
Cluster 1 Occurrences: 1156; KMEANS: 1136
Cluster 2 Occurrences: 1127; KMEANS: 1098
Centroids: [[-0.40094733, 1.7002808], [0.04903298, 0.7972905], [1.0162946, -0.3608643]]
Centroids: [[0.09432786, 0.79494363], [1.0185663, -0.36028472], [-0.48605746, 1.7619225]]
Contingency Matrix: 
[[ 147    2 1010]
 [1053   16   87]
 [   8 1118    1]]
[[147, -1, 1010], [1053, -1, 87], [-1, -1, -1]]
[[-1, -1, 1010], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 0, 0: 2}
New Contingency Matrix: 
[[1010  147    2]
 [  87 1053   16]
 [   1    8 1118]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1010, 1053, 1118], Sum: 3181
All_Elements: [1010, 147, 2, 87, 1053, 16, 1, 8, 1118], Sum: 3442
Accuracy: 0.9241719930273097
Done!
