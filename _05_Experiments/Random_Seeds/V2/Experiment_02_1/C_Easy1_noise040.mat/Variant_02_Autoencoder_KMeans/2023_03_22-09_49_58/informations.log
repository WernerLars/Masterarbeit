Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_49_58
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000171E2BE9C18>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81B588>
Epoch 1
-------------------------------
loss: 0.120814  [    0/ 3386]
loss: 0.273340  [  100/ 3386]
loss: 0.211309  [  200/ 3386]
loss: 0.179508  [  300/ 3386]
loss: 0.282664  [  400/ 3386]
loss: 0.075866  [  500/ 3386]
loss: 0.179138  [  600/ 3386]
loss: 0.129214  [  700/ 3386]
loss: 0.201951  [  800/ 3386]
loss: 0.089280  [  900/ 3386]
loss: 0.309368  [ 1000/ 3386]
loss: 0.249377  [ 1100/ 3386]
loss: 0.243045  [ 1200/ 3386]
loss: 0.030672  [ 1300/ 3386]
loss: 0.082585  [ 1400/ 3386]
loss: 0.033295  [ 1500/ 3386]
loss: 0.040620  [ 1600/ 3386]
loss: 0.052591  [ 1700/ 3386]
loss: 0.334628  [ 1800/ 3386]
loss: 0.269077  [ 1900/ 3386]
loss: 0.101157  [ 2000/ 3386]
loss: 0.174931  [ 2100/ 3386]
loss: 0.176116  [ 2200/ 3386]
loss: 0.171997  [ 2300/ 3386]
loss: 0.093634  [ 2400/ 3386]
loss: 0.046134  [ 2500/ 3386]
loss: 0.095089  [ 2600/ 3386]
loss: 0.243337  [ 2700/ 3386]
loss: 0.084677  [ 2800/ 3386]
loss: 0.084763  [ 2900/ 3386]
loss: 0.017085  [ 3000/ 3386]
loss: 0.111887  [ 3100/ 3386]
loss: 0.078535  [ 3200/ 3386]
loss: 0.121433  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.056279  [    0/ 3386]
loss: 0.121746  [  100/ 3386]
loss: 0.089019  [  200/ 3386]
loss: 0.043609  [  300/ 3386]
loss: 0.319387  [  400/ 3386]
loss: 0.068042  [  500/ 3386]
loss: 0.168271  [  600/ 3386]
loss: 0.049842  [  700/ 3386]
loss: 0.117208  [  800/ 3386]
loss: 0.061917  [  900/ 3386]
loss: 0.106247  [ 1000/ 3386]
loss: 0.201060  [ 1100/ 3386]
loss: 0.177520  [ 1200/ 3386]
loss: 0.026771  [ 1300/ 3386]
loss: 0.092982  [ 1400/ 3386]
loss: 0.031597  [ 1500/ 3386]
loss: 0.039476  [ 1600/ 3386]
loss: 0.029471  [ 1700/ 3386]
loss: 0.361370  [ 1800/ 3386]
loss: 0.234216  [ 1900/ 3386]
loss: 0.124971  [ 2000/ 3386]
loss: 0.103033  [ 2100/ 3386]
loss: 0.157429  [ 2200/ 3386]
loss: 0.095323  [ 2300/ 3386]
loss: 0.070443  [ 2400/ 3386]
loss: 0.049100  [ 2500/ 3386]
loss: 0.069353  [ 2600/ 3386]
loss: 0.213071  [ 2700/ 3386]
loss: 0.084620  [ 2800/ 3386]
loss: 0.060859  [ 2900/ 3386]
loss: 0.017122  [ 3000/ 3386]
loss: 0.110707  [ 3100/ 3386]
loss: 0.060636  [ 3200/ 3386]
loss: 0.103613  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.056195  [    0/ 3386]
loss: 0.120117  [  100/ 3386]
loss: 0.098641  [  200/ 3386]
loss: 0.047625  [  300/ 3386]
loss: 0.306917  [  400/ 3386]
loss: 0.071006  [  500/ 3386]
loss: 0.161809  [  600/ 3386]
loss: 0.049051  [  700/ 3386]
loss: 0.119656  [  800/ 3386]
loss: 0.050800  [  900/ 3386]
loss: 0.118829  [ 1000/ 3386]
loss: 0.170861  [ 1100/ 3386]
loss: 0.157485  [ 1200/ 3386]
loss: 0.029299  [ 1300/ 3386]
loss: 0.088611  [ 1400/ 3386]
loss: 0.043706  [ 1500/ 3386]
loss: 0.040952  [ 1600/ 3386]
loss: 0.032443  [ 1700/ 3386]
loss: 0.237219  [ 1800/ 3386]
loss: 0.232131  [ 1900/ 3386]
loss: 0.128583  [ 2000/ 3386]
loss: 0.089873  [ 2100/ 3386]
loss: 0.098482  [ 2200/ 3386]
loss: 0.080084  [ 2300/ 3386]
loss: 0.071460  [ 2400/ 3386]
loss: 0.053189  [ 2500/ 3386]
loss: 0.070421  [ 2600/ 3386]
loss: 0.208661  [ 2700/ 3386]
loss: 0.087891  [ 2800/ 3386]
loss: 0.058864  [ 2900/ 3386]
loss: 0.017691  [ 3000/ 3386]
loss: 0.106819  [ 3100/ 3386]
loss: 0.052910  [ 3200/ 3386]
loss: 0.102302  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.056790  [    0/ 3386]
loss: 0.119966  [  100/ 3386]
loss: 0.095816  [  200/ 3386]
loss: 0.052195  [  300/ 3386]
loss: 0.282163  [  400/ 3386]
loss: 0.066409  [  500/ 3386]
loss: 0.159911  [  600/ 3386]
loss: 0.048843  [  700/ 3386]
loss: 0.110670  [  800/ 3386]
loss: 0.049916  [  900/ 3386]
loss: 0.131785  [ 1000/ 3386]
loss: 0.157688  [ 1100/ 3386]
loss: 0.155217  [ 1200/ 3386]
loss: 0.030585  [ 1300/ 3386]
loss: 0.073229  [ 1400/ 3386]
loss: 0.068150  [ 1500/ 3386]
loss: 0.041003  [ 1600/ 3386]
loss: 0.035191  [ 1700/ 3386]
loss: 0.150088  [ 1800/ 3386]
loss: 0.242371  [ 1900/ 3386]
loss: 0.097822  [ 2000/ 3386]
loss: 0.085877  [ 2100/ 3386]
loss: 0.077111  [ 2200/ 3386]
loss: 0.071878  [ 2300/ 3386]
loss: 0.072008  [ 2400/ 3386]
loss: 0.057225  [ 2500/ 3386]
loss: 0.089376  [ 2600/ 3386]
loss: 0.206799  [ 2700/ 3386]
loss: 0.092615  [ 2800/ 3386]
loss: 0.058065  [ 2900/ 3386]
loss: 0.017588  [ 3000/ 3386]
loss: 0.104591  [ 3100/ 3386]
loss: 0.049083  [ 3200/ 3386]
loss: 0.102024  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.059041  [    0/ 3386]
loss: 0.119099  [  100/ 3386]
loss: 0.090692  [  200/ 3386]
loss: 0.052953  [  300/ 3386]
loss: 0.235532  [  400/ 3386]
loss: 0.064838  [  500/ 3386]
loss: 0.159465  [  600/ 3386]
loss: 0.049437  [  700/ 3386]
loss: 0.109438  [  800/ 3386]
loss: 0.048862  [  900/ 3386]
loss: 0.135285  [ 1000/ 3386]
loss: 0.163479  [ 1100/ 3386]
loss: 0.151440  [ 1200/ 3386]
loss: 0.032941  [ 1300/ 3386]
loss: 0.065089  [ 1400/ 3386]
loss: 0.071977  [ 1500/ 3386]
loss: 0.041209  [ 1600/ 3386]
loss: 0.032991  [ 1700/ 3386]
loss: 0.140180  [ 1800/ 3386]
loss: 0.252675  [ 1900/ 3386]
loss: 0.073465  [ 2000/ 3386]
loss: 0.083299  [ 2100/ 3386]
loss: 0.079318  [ 2200/ 3386]
loss: 0.064538  [ 2300/ 3386]
loss: 0.070127  [ 2400/ 3386]
loss: 0.057444  [ 2500/ 3386]
loss: 0.095997  [ 2600/ 3386]
loss: 0.206170  [ 2700/ 3386]
loss: 0.097180  [ 2800/ 3386]
loss: 0.057914  [ 2900/ 3386]
loss: 0.017468  [ 3000/ 3386]
loss: 0.102940  [ 3100/ 3386]
loss: 0.045585  [ 3200/ 3386]
loss: 0.101204  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.062882  [    0/ 3386]
loss: 0.118152  [  100/ 3386]
loss: 0.088526  [  200/ 3386]
loss: 0.050087  [  300/ 3386]
loss: 0.224760  [  400/ 3386]
loss: 0.065500  [  500/ 3386]
loss: 0.159717  [  600/ 3386]
loss: 0.049193  [  700/ 3386]
loss: 0.105708  [  800/ 3386]
loss: 0.048996  [  900/ 3386]
loss: 0.139305  [ 1000/ 3386]
loss: 0.165789  [ 1100/ 3386]
loss: 0.151685  [ 1200/ 3386]
loss: 0.034264  [ 1300/ 3386]
loss: 0.059110  [ 1400/ 3386]
loss: 0.074813  [ 1500/ 3386]
loss: 0.039045  [ 1600/ 3386]
loss: 0.031251  [ 1700/ 3386]
loss: 0.144084  [ 1800/ 3386]
loss: 0.254180  [ 1900/ 3386]
loss: 0.059572  [ 2000/ 3386]
loss: 0.082537  [ 2100/ 3386]
loss: 0.074036  [ 2200/ 3386]
loss: 0.060049  [ 2300/ 3386]
loss: 0.071080  [ 2400/ 3386]
loss: 0.059245  [ 2500/ 3386]
loss: 0.092933  [ 2600/ 3386]
loss: 0.206065  [ 2700/ 3386]
loss: 0.098272  [ 2800/ 3386]
loss: 0.057916  [ 2900/ 3386]
loss: 0.017373  [ 3000/ 3386]
loss: 0.101621  [ 3100/ 3386]
loss: 0.043618  [ 3200/ 3386]
loss: 0.100975  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.063479  [    0/ 3386]
loss: 0.117666  [  100/ 3386]
loss: 0.089189  [  200/ 3386]
loss: 0.043805  [  300/ 3386]
loss: 0.222366  [  400/ 3386]
loss: 0.064925  [  500/ 3386]
loss: 0.158982  [  600/ 3386]
loss: 0.049635  [  700/ 3386]
loss: 0.103996  [  800/ 3386]
loss: 0.049554  [  900/ 3386]
loss: 0.136362  [ 1000/ 3386]
loss: 0.164396  [ 1100/ 3386]
loss: 0.153191  [ 1200/ 3386]
loss: 0.035295  [ 1300/ 3386]
loss: 0.056434  [ 1400/ 3386]
loss: 0.072876  [ 1500/ 3386]
loss: 0.037477  [ 1600/ 3386]
loss: 0.029774  [ 1700/ 3386]
loss: 0.143069  [ 1800/ 3386]
loss: 0.252620  [ 1900/ 3386]
loss: 0.051700  [ 2000/ 3386]
loss: 0.082593  [ 2100/ 3386]
loss: 0.071020  [ 2200/ 3386]
loss: 0.058619  [ 2300/ 3386]
loss: 0.070618  [ 2400/ 3386]
loss: 0.060489  [ 2500/ 3386]
loss: 0.087435  [ 2600/ 3386]
loss: 0.206228  [ 2700/ 3386]
loss: 0.100156  [ 2800/ 3386]
loss: 0.056201  [ 2900/ 3386]
loss: 0.017121  [ 3000/ 3386]
loss: 0.101750  [ 3100/ 3386]
loss: 0.044707  [ 3200/ 3386]
loss: 0.099723  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.062768  [    0/ 3386]
loss: 0.117326  [  100/ 3386]
loss: 0.086847  [  200/ 3386]
loss: 0.038294  [  300/ 3386]
loss: 0.221490  [  400/ 3386]
loss: 0.064460  [  500/ 3386]
loss: 0.157377  [  600/ 3386]
loss: 0.046924  [  700/ 3386]
loss: 0.103267  [  800/ 3386]
loss: 0.050364  [  900/ 3386]
loss: 0.140045  [ 1000/ 3386]
loss: 0.161935  [ 1100/ 3386]
loss: 0.152170  [ 1200/ 3386]
loss: 0.035947  [ 1300/ 3386]
loss: 0.054980  [ 1400/ 3386]
loss: 0.071095  [ 1500/ 3386]
loss: 0.037018  [ 1600/ 3386]
loss: 0.028215  [ 1700/ 3386]
loss: 0.143436  [ 1800/ 3386]
loss: 0.248992  [ 1900/ 3386]
loss: 0.047411  [ 2000/ 3386]
loss: 0.083057  [ 2100/ 3386]
loss: 0.065370  [ 2200/ 3386]
loss: 0.056978  [ 2300/ 3386]
loss: 0.070470  [ 2400/ 3386]
loss: 0.061241  [ 2500/ 3386]
loss: 0.084409  [ 2600/ 3386]
loss: 0.206067  [ 2700/ 3386]
loss: 0.099179  [ 2800/ 3386]
loss: 0.055240  [ 2900/ 3386]
loss: 0.016830  [ 3000/ 3386]
loss: 0.103660  [ 3100/ 3386]
loss: 0.047692  [ 3200/ 3386]
loss: 0.098447  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [ 1.087971   -0.37991875]
[0 2 0 ... 2 0 1]
[2 0 2 ... 0 2 1]
Cluster 0 Occurrences: 1079; KMEANS: 1102
Cluster 1 Occurrences: 1158; KMEANS: 1148
Cluster 2 Occurrences: 1149; KMEANS: 1136
Centroids: [[1.4359391, -0.24508601], [-2.7456994, 1.058486], [1.3186214, 1.5533603]]
Centroids: [[1.4020785, 1.5922222], [-2.8155832, 1.1234576], [1.3839376, -0.26256007]]
Contingency Matrix: 
[[  70    0 1009]
 [  26 1104   28]
 [1006   44   99]]
[[70, -1, 1009], [-1, -1, -1], [1006, -1, 99]]
[[-1, -1, -1], [-1, -1, -1], [1006, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 2, 2: 0}
New Contingency Matrix: 
[[1009    0   70]
 [  28 1104   26]
 [  99   44 1006]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1009, 1104, 1006], Sum: 3119
All_Elements: [1009, 0, 70, 28, 1104, 26, 99, 44, 1006], Sum: 3386
Accuracy: 0.9211458948611931
Done!
