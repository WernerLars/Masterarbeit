Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_30
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000171FFC3B0F0>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81B3C8>
Epoch 1
-------------------------------
loss: 0.248311  [    0/ 3298]
loss: 0.269187  [  100/ 3298]
loss: 0.108043  [  200/ 3298]
loss: 0.085529  [  300/ 3298]
loss: 0.089391  [  400/ 3298]
loss: 0.073842  [  500/ 3298]
loss: 0.041887  [  600/ 3298]
loss: 0.107613  [  700/ 3298]
loss: 0.034830  [  800/ 3298]
loss: 0.067119  [  900/ 3298]
loss: 0.037884  [ 1000/ 3298]
loss: 0.052694  [ 1100/ 3298]
loss: 0.080131  [ 1200/ 3298]
loss: 0.049466  [ 1300/ 3298]
loss: 0.014681  [ 1400/ 3298]
loss: 0.042045  [ 1500/ 3298]
loss: 0.010418  [ 1600/ 3298]
loss: 0.020810  [ 1700/ 3298]
loss: 0.049118  [ 1800/ 3298]
loss: 0.025313  [ 1900/ 3298]
loss: 0.010495  [ 2000/ 3298]
loss: 0.014046  [ 2100/ 3298]
loss: 0.009253  [ 2200/ 3298]
loss: 0.026282  [ 2300/ 3298]
loss: 0.015969  [ 2400/ 3298]
loss: 0.084524  [ 2500/ 3298]
loss: 0.040933  [ 2600/ 3298]
loss: 0.031188  [ 2700/ 3298]
loss: 0.100563  [ 2800/ 3298]
loss: 0.027848  [ 2900/ 3298]
loss: 0.038908  [ 3000/ 3298]
loss: 0.019802  [ 3100/ 3298]
loss: 0.086747  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.036721  [    0/ 3298]
loss: 0.024521  [  100/ 3298]
loss: 0.106637  [  200/ 3298]
loss: 0.014251  [  300/ 3298]
loss: 0.011335  [  400/ 3298]
loss: 0.024426  [  500/ 3298]
loss: 0.021236  [  600/ 3298]
loss: 0.061493  [  700/ 3298]
loss: 0.023763  [  800/ 3298]
loss: 0.035992  [  900/ 3298]
loss: 0.035183  [ 1000/ 3298]
loss: 0.047449  [ 1100/ 3298]
loss: 0.074368  [ 1200/ 3298]
loss: 0.023942  [ 1300/ 3298]
loss: 0.011794  [ 1400/ 3298]
loss: 0.021294  [ 1500/ 3298]
loss: 0.013843  [ 1600/ 3298]
loss: 0.015215  [ 1700/ 3298]
loss: 0.032151  [ 1800/ 3298]
loss: 0.021433  [ 1900/ 3298]
loss: 0.010990  [ 2000/ 3298]
loss: 0.015245  [ 2100/ 3298]
loss: 0.015867  [ 2200/ 3298]
loss: 0.022417  [ 2300/ 3298]
loss: 0.018479  [ 2400/ 3298]
loss: 0.018352  [ 2500/ 3298]
loss: 0.034013  [ 2600/ 3298]
loss: 0.039156  [ 2700/ 3298]
loss: 0.141599  [ 2800/ 3298]
loss: 0.021619  [ 2900/ 3298]
loss: 0.036759  [ 3000/ 3298]
loss: 0.016310  [ 3100/ 3298]
loss: 0.062781  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.057630  [    0/ 3298]
loss: 0.022933  [  100/ 3298]
loss: 0.106852  [  200/ 3298]
loss: 0.013506  [  300/ 3298]
loss: 0.007956  [  400/ 3298]
loss: 0.023711  [  500/ 3298]
loss: 0.022016  [  600/ 3298]
loss: 0.064632  [  700/ 3298]
loss: 0.025370  [  800/ 3298]
loss: 0.020887  [  900/ 3298]
loss: 0.036923  [ 1000/ 3298]
loss: 0.065911  [ 1100/ 3298]
loss: 0.071972  [ 1200/ 3298]
loss: 0.023809  [ 1300/ 3298]
loss: 0.013317  [ 1400/ 3298]
loss: 0.021802  [ 1500/ 3298]
loss: 0.013157  [ 1600/ 3298]
loss: 0.018312  [ 1700/ 3298]
loss: 0.030823  [ 1800/ 3298]
loss: 0.021679  [ 1900/ 3298]
loss: 0.010072  [ 2000/ 3298]
loss: 0.014713  [ 2100/ 3298]
loss: 0.014815  [ 2200/ 3298]
loss: 0.020539  [ 2300/ 3298]
loss: 0.018286  [ 2400/ 3298]
loss: 0.016317  [ 2500/ 3298]
loss: 0.032055  [ 2600/ 3298]
loss: 0.041045  [ 2700/ 3298]
loss: 0.147061  [ 2800/ 3298]
loss: 0.022401  [ 2900/ 3298]
loss: 0.034239  [ 3000/ 3298]
loss: 0.016278  [ 3100/ 3298]
loss: 0.056328  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.063266  [    0/ 3298]
loss: 0.023094  [  100/ 3298]
loss: 0.105464  [  200/ 3298]
loss: 0.012677  [  300/ 3298]
loss: 0.007186  [  400/ 3298]
loss: 0.021820  [  500/ 3298]
loss: 0.022571  [  600/ 3298]
loss: 0.063439  [  700/ 3298]
loss: 0.025953  [  800/ 3298]
loss: 0.026885  [  900/ 3298]
loss: 0.037694  [ 1000/ 3298]
loss: 0.076892  [ 1100/ 3298]
loss: 0.071538  [ 1200/ 3298]
loss: 0.024962  [ 1300/ 3298]
loss: 0.016263  [ 1400/ 3298]
loss: 0.022535  [ 1500/ 3298]
loss: 0.011890  [ 1600/ 3298]
loss: 0.022249  [ 1700/ 3298]
loss: 0.029931  [ 1800/ 3298]
loss: 0.022594  [ 1900/ 3298]
loss: 0.010090  [ 2000/ 3298]
loss: 0.014703  [ 2100/ 3298]
loss: 0.013349  [ 2200/ 3298]
loss: 0.019009  [ 2300/ 3298]
loss: 0.017518  [ 2400/ 3298]
loss: 0.016758  [ 2500/ 3298]
loss: 0.032730  [ 2600/ 3298]
loss: 0.042459  [ 2700/ 3298]
loss: 0.155112  [ 2800/ 3298]
loss: 0.023117  [ 2900/ 3298]
loss: 0.032735  [ 3000/ 3298]
loss: 0.016936  [ 3100/ 3298]
loss: 0.054182  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.062014  [    0/ 3298]
loss: 0.023694  [  100/ 3298]
loss: 0.104616  [  200/ 3298]
loss: 0.012309  [  300/ 3298]
loss: 0.007363  [  400/ 3298]
loss: 0.021621  [  500/ 3298]
loss: 0.023029  [  600/ 3298]
loss: 0.062340  [  700/ 3298]
loss: 0.026959  [  800/ 3298]
loss: 0.034193  [  900/ 3298]
loss: 0.038209  [ 1000/ 3298]
loss: 0.078169  [ 1100/ 3298]
loss: 0.071220  [ 1200/ 3298]
loss: 0.025960  [ 1300/ 3298]
loss: 0.017267  [ 1400/ 3298]
loss: 0.022867  [ 1500/ 3298]
loss: 0.011587  [ 1600/ 3298]
loss: 0.025543  [ 1700/ 3298]
loss: 0.029461  [ 1800/ 3298]
loss: 0.022925  [ 1900/ 3298]
loss: 0.010097  [ 2000/ 3298]
loss: 0.014422  [ 2100/ 3298]
loss: 0.012913  [ 2200/ 3298]
loss: 0.017905  [ 2300/ 3298]
loss: 0.017147  [ 2400/ 3298]
loss: 0.017369  [ 2500/ 3298]
loss: 0.033786  [ 2600/ 3298]
loss: 0.043144  [ 2700/ 3298]
loss: 0.173357  [ 2800/ 3298]
loss: 0.023935  [ 2900/ 3298]
loss: 0.031824  [ 3000/ 3298]
loss: 0.017602  [ 3100/ 3298]
loss: 0.053689  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.059893  [    0/ 3298]
loss: 0.024551  [  100/ 3298]
loss: 0.104251  [  200/ 3298]
loss: 0.012264  [  300/ 3298]
loss: 0.007323  [  400/ 3298]
loss: 0.022070  [  500/ 3298]
loss: 0.023313  [  600/ 3298]
loss: 0.061501  [  700/ 3298]
loss: 0.027948  [  800/ 3298]
loss: 0.038121  [  900/ 3298]
loss: 0.038562  [ 1000/ 3298]
loss: 0.075059  [ 1100/ 3298]
loss: 0.070523  [ 1200/ 3298]
loss: 0.026630  [ 1300/ 3298]
loss: 0.019352  [ 1400/ 3298]
loss: 0.023077  [ 1500/ 3298]
loss: 0.011519  [ 1600/ 3298]
loss: 0.026495  [ 1700/ 3298]
loss: 0.029285  [ 1800/ 3298]
loss: 0.022807  [ 1900/ 3298]
loss: 0.010171  [ 2000/ 3298]
loss: 0.014395  [ 2100/ 3298]
loss: 0.011881  [ 2200/ 3298]
loss: 0.017312  [ 2300/ 3298]
loss: 0.016530  [ 2400/ 3298]
loss: 0.017943  [ 2500/ 3298]
loss: 0.035342  [ 2600/ 3298]
loss: 0.043317  [ 2700/ 3298]
loss: 0.185646  [ 2800/ 3298]
loss: 0.024576  [ 2900/ 3298]
loss: 0.030905  [ 3000/ 3298]
loss: 0.017789  [ 3100/ 3298]
loss: 0.053967  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.057541  [    0/ 3298]
loss: 0.025642  [  100/ 3298]
loss: 0.104020  [  200/ 3298]
loss: 0.012226  [  300/ 3298]
loss: 0.007804  [  400/ 3298]
loss: 0.022123  [  500/ 3298]
loss: 0.023474  [  600/ 3298]
loss: 0.060768  [  700/ 3298]
loss: 0.027451  [  800/ 3298]
loss: 0.039795  [  900/ 3298]
loss: 0.038696  [ 1000/ 3298]
loss: 0.071209  [ 1100/ 3298]
loss: 0.070728  [ 1200/ 3298]
loss: 0.027148  [ 1300/ 3298]
loss: 0.020096  [ 1400/ 3298]
loss: 0.023433  [ 1500/ 3298]
loss: 0.011597  [ 1600/ 3298]
loss: 0.027696  [ 1700/ 3298]
loss: 0.029358  [ 1800/ 3298]
loss: 0.022765  [ 1900/ 3298]
loss: 0.010289  [ 2000/ 3298]
loss: 0.014246  [ 2100/ 3298]
loss: 0.011977  [ 2200/ 3298]
loss: 0.017136  [ 2300/ 3298]
loss: 0.016536  [ 2400/ 3298]
loss: 0.018236  [ 2500/ 3298]
loss: 0.035541  [ 2600/ 3298]
loss: 0.043676  [ 2700/ 3298]
loss: 0.194862  [ 2800/ 3298]
loss: 0.025073  [ 2900/ 3298]
loss: 0.030568  [ 3000/ 3298]
loss: 0.018267  [ 3100/ 3298]
loss: 0.054136  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.055903  [    0/ 3298]
loss: 0.026001  [  100/ 3298]
loss: 0.103866  [  200/ 3298]
loss: 0.012477  [  300/ 3298]
loss: 0.008135  [  400/ 3298]
loss: 0.022316  [  500/ 3298]
loss: 0.023404  [  600/ 3298]
loss: 0.060709  [  700/ 3298]
loss: 0.027325  [  800/ 3298]
loss: 0.039291  [  900/ 3298]
loss: 0.038878  [ 1000/ 3298]
loss: 0.067240  [ 1100/ 3298]
loss: 0.070486  [ 1200/ 3298]
loss: 0.027506  [ 1300/ 3298]
loss: 0.020179  [ 1400/ 3298]
loss: 0.023443  [ 1500/ 3298]
loss: 0.011909  [ 1600/ 3298]
loss: 0.026408  [ 1700/ 3298]
loss: 0.029443  [ 1800/ 3298]
loss: 0.022778  [ 1900/ 3298]
loss: 0.010421  [ 2000/ 3298]
loss: 0.014188  [ 2100/ 3298]
loss: 0.011704  [ 2200/ 3298]
loss: 0.016931  [ 2300/ 3298]
loss: 0.016248  [ 2400/ 3298]
loss: 0.018131  [ 2500/ 3298]
loss: 0.036143  [ 2600/ 3298]
loss: 0.043639  [ 2700/ 3298]
loss: 0.207859  [ 2800/ 3298]
loss: 0.025561  [ 2900/ 3298]
loss: 0.030202  [ 3000/ 3298]
loss: 0.018870  [ 3100/ 3298]
loss: 0.054354  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [-2.1332135   0.41878438]
[1 0 0 ... 0 1 2]
[0 1 1 ... 1 0 2]
Cluster 0 Occurrences: 1094; KMEANS: 1087
Cluster 1 Occurrences: 1089; KMEANS: 1096
Cluster 2 Occurrences: 1115; KMEANS: 1115
Centroids: [[1.2146546, -0.015771868], [-2.6358755, 0.8853223], [1.1976343, 1.5239614]]
Centroids: [[-2.648128, 0.8908812], [1.209977, -0.028056633], [1.2072704, 1.532234]]
Contingency Matrix: 
[[   0 1078   16]
 [1082    5    2]
 [   5   13 1097]]
[[0, 1078, -1], [1082, 5, -1], [-1, -1, -1]]
[[-1, 1078, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 2, 1: 0, 0: 1}
New Contingency Matrix: 
[[1078    0   16]
 [   5 1082    2]
 [  13    5 1097]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1078, 1082, 1097], Sum: 3257
All_Elements: [1078, 0, 16, 5, 1082, 2, 13, 5, 1097], Sum: 3298
Accuracy: 0.9875682231655549
Done!
