Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_14
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000171DAADBB70>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81BF28>
Epoch 1
-------------------------------
loss: 0.145214  [    0/ 3474]
loss: 0.074076  [  100/ 3474]
loss: 0.103016  [  200/ 3474]
loss: 0.107285  [  300/ 3474]
loss: 0.276096  [  400/ 3474]
loss: 0.010067  [  500/ 3474]
loss: 0.074930  [  600/ 3474]
loss: 0.024655  [  700/ 3474]
loss: 0.046987  [  800/ 3474]
loss: 0.215877  [  900/ 3474]
loss: 0.043249  [ 1000/ 3474]
loss: 0.096356  [ 1100/ 3474]
loss: 0.060815  [ 1200/ 3474]
loss: 0.082190  [ 1300/ 3474]
loss: 0.041542  [ 1400/ 3474]
loss: 0.083241  [ 1500/ 3474]
loss: 0.012598  [ 1600/ 3474]
loss: 0.119172  [ 1700/ 3474]
loss: 0.025454  [ 1800/ 3474]
loss: 0.023789  [ 1900/ 3474]
loss: 0.013443  [ 2000/ 3474]
loss: 0.018467  [ 2100/ 3474]
loss: 0.025573  [ 2200/ 3474]
loss: 0.072454  [ 2300/ 3474]
loss: 0.036341  [ 2400/ 3474]
loss: 0.044358  [ 2500/ 3474]
loss: 0.027952  [ 2600/ 3474]
loss: 0.092525  [ 2700/ 3474]
loss: 0.047099  [ 2800/ 3474]
loss: 0.017994  [ 2900/ 3474]
loss: 0.149191  [ 3000/ 3474]
loss: 0.010401  [ 3100/ 3474]
loss: 0.030831  [ 3200/ 3474]
loss: 0.020051  [ 3300/ 3474]
loss: 0.088177  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.048198  [    0/ 3474]
loss: 0.009602  [  100/ 3474]
loss: 0.017265  [  200/ 3474]
loss: 0.061199  [  300/ 3474]
loss: 0.088614  [  400/ 3474]
loss: 0.005147  [  500/ 3474]
loss: 0.047014  [  600/ 3474]
loss: 0.019210  [  700/ 3474]
loss: 0.029552  [  800/ 3474]
loss: 0.203321  [  900/ 3474]
loss: 0.018159  [ 1000/ 3474]
loss: 0.068499  [ 1100/ 3474]
loss: 0.030443  [ 1200/ 3474]
loss: 0.044545  [ 1300/ 3474]
loss: 0.031466  [ 1400/ 3474]
loss: 0.075031  [ 1500/ 3474]
loss: 0.010118  [ 1600/ 3474]
loss: 0.080430  [ 1700/ 3474]
loss: 0.031319  [ 1800/ 3474]
loss: 0.017650  [ 1900/ 3474]
loss: 0.007374  [ 2000/ 3474]
loss: 0.019537  [ 2100/ 3474]
loss: 0.025520  [ 2200/ 3474]
loss: 0.069766  [ 2300/ 3474]
loss: 0.013564  [ 2400/ 3474]
loss: 0.042479  [ 2500/ 3474]
loss: 0.020159  [ 2600/ 3474]
loss: 0.039957  [ 2700/ 3474]
loss: 0.046134  [ 2800/ 3474]
loss: 0.016662  [ 2900/ 3474]
loss: 0.183760  [ 3000/ 3474]
loss: 0.009878  [ 3100/ 3474]
loss: 0.015481  [ 3200/ 3474]
loss: 0.019647  [ 3300/ 3474]
loss: 0.061356  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.026567  [    0/ 3474]
loss: 0.009490  [  100/ 3474]
loss: 0.018827  [  200/ 3474]
loss: 0.062463  [  300/ 3474]
loss: 0.038904  [  400/ 3474]
loss: 0.005768  [  500/ 3474]
loss: 0.040396  [  600/ 3474]
loss: 0.012641  [  700/ 3474]
loss: 0.019027  [  800/ 3474]
loss: 0.187587  [  900/ 3474]
loss: 0.007667  [ 1000/ 3474]
loss: 0.099643  [ 1100/ 3474]
loss: 0.029070  [ 1200/ 3474]
loss: 0.036762  [ 1300/ 3474]
loss: 0.027373  [ 1400/ 3474]
loss: 0.081557  [ 1500/ 3474]
loss: 0.011938  [ 1600/ 3474]
loss: 0.045078  [ 1700/ 3474]
loss: 0.029862  [ 1800/ 3474]
loss: 0.016432  [ 1900/ 3474]
loss: 0.008322  [ 2000/ 3474]
loss: 0.018363  [ 2100/ 3474]
loss: 0.025714  [ 2200/ 3474]
loss: 0.070869  [ 2300/ 3474]
loss: 0.008514  [ 2400/ 3474]
loss: 0.030725  [ 2500/ 3474]
loss: 0.020665  [ 2600/ 3474]
loss: 0.039889  [ 2700/ 3474]
loss: 0.037315  [ 2800/ 3474]
loss: 0.017221  [ 2900/ 3474]
loss: 0.211595  [ 3000/ 3474]
loss: 0.010004  [ 3100/ 3474]
loss: 0.014295  [ 3200/ 3474]
loss: 0.021776  [ 3300/ 3474]
loss: 0.041777  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.028054  [    0/ 3474]
loss: 0.009258  [  100/ 3474]
loss: 0.020300  [  200/ 3474]
loss: 0.061520  [  300/ 3474]
loss: 0.036334  [  400/ 3474]
loss: 0.005756  [  500/ 3474]
loss: 0.036607  [  600/ 3474]
loss: 0.011890  [  700/ 3474]
loss: 0.014098  [  800/ 3474]
loss: 0.182215  [  900/ 3474]
loss: 0.007351  [ 1000/ 3474]
loss: 0.127000  [ 1100/ 3474]
loss: 0.030722  [ 1200/ 3474]
loss: 0.033432  [ 1300/ 3474]
loss: 0.024476  [ 1400/ 3474]
loss: 0.083360  [ 1500/ 3474]
loss: 0.012051  [ 1600/ 3474]
loss: 0.036798  [ 1700/ 3474]
loss: 0.028149  [ 1800/ 3474]
loss: 0.016471  [ 1900/ 3474]
loss: 0.007793  [ 2000/ 3474]
loss: 0.017885  [ 2100/ 3474]
loss: 0.025740  [ 2200/ 3474]
loss: 0.071742  [ 2300/ 3474]
loss: 0.007151  [ 2400/ 3474]
loss: 0.022444  [ 2500/ 3474]
loss: 0.020973  [ 2600/ 3474]
loss: 0.040778  [ 2700/ 3474]
loss: 0.031552  [ 2800/ 3474]
loss: 0.017638  [ 2900/ 3474]
loss: 0.219299  [ 3000/ 3474]
loss: 0.009850  [ 3100/ 3474]
loss: 0.014480  [ 3200/ 3474]
loss: 0.026751  [ 3300/ 3474]
loss: 0.027915  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.029389  [    0/ 3474]
loss: 0.008991  [  100/ 3474]
loss: 0.021194  [  200/ 3474]
loss: 0.061097  [  300/ 3474]
loss: 0.041449  [  400/ 3474]
loss: 0.005579  [  500/ 3474]
loss: 0.033295  [  600/ 3474]
loss: 0.011636  [  700/ 3474]
loss: 0.014439  [  800/ 3474]
loss: 0.183450  [  900/ 3474]
loss: 0.007097  [ 1000/ 3474]
loss: 0.146586  [ 1100/ 3474]
loss: 0.029987  [ 1200/ 3474]
loss: 0.029483  [ 1300/ 3474]
loss: 0.021080  [ 1400/ 3474]
loss: 0.081620  [ 1500/ 3474]
loss: 0.011363  [ 1600/ 3474]
loss: 0.034942  [ 1700/ 3474]
loss: 0.026524  [ 1800/ 3474]
loss: 0.017069  [ 1900/ 3474]
loss: 0.007775  [ 2000/ 3474]
loss: 0.017607  [ 2100/ 3474]
loss: 0.026581  [ 2200/ 3474]
loss: 0.071616  [ 2300/ 3474]
loss: 0.006615  [ 2400/ 3474]
loss: 0.017936  [ 2500/ 3474]
loss: 0.021116  [ 2600/ 3474]
loss: 0.041089  [ 2700/ 3474]
loss: 0.028244  [ 2800/ 3474]
loss: 0.017860  [ 2900/ 3474]
loss: 0.227501  [ 3000/ 3474]
loss: 0.010544  [ 3100/ 3474]
loss: 0.013843  [ 3200/ 3474]
loss: 0.031139  [ 3300/ 3474]
loss: 0.020001  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.030607  [    0/ 3474]
loss: 0.008592  [  100/ 3474]
loss: 0.022301  [  200/ 3474]
loss: 0.061514  [  300/ 3474]
loss: 0.047317  [  400/ 3474]
loss: 0.005098  [  500/ 3474]
loss: 0.030675  [  600/ 3474]
loss: 0.011268  [  700/ 3474]
loss: 0.015645  [  800/ 3474]
loss: 0.185934  [  900/ 3474]
loss: 0.007548  [ 1000/ 3474]
loss: 0.168798  [ 1100/ 3474]
loss: 0.028205  [ 1200/ 3474]
loss: 0.027968  [ 1300/ 3474]
loss: 0.017818  [ 1400/ 3474]
loss: 0.080756  [ 1500/ 3474]
loss: 0.010657  [ 1600/ 3474]
loss: 0.036529  [ 1700/ 3474]
loss: 0.025437  [ 1800/ 3474]
loss: 0.017719  [ 1900/ 3474]
loss: 0.007316  [ 2000/ 3474]
loss: 0.017208  [ 2100/ 3474]
loss: 0.026822  [ 2200/ 3474]
loss: 0.071390  [ 2300/ 3474]
loss: 0.006048  [ 2400/ 3474]
loss: 0.015098  [ 2500/ 3474]
loss: 0.020646  [ 2600/ 3474]
loss: 0.041446  [ 2700/ 3474]
loss: 0.026048  [ 2800/ 3474]
loss: 0.017633  [ 2900/ 3474]
loss: 0.229228  [ 3000/ 3474]
loss: 0.010487  [ 3100/ 3474]
loss: 0.012751  [ 3200/ 3474]
loss: 0.034451  [ 3300/ 3474]
loss: 0.015471  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.032413  [    0/ 3474]
loss: 0.008371  [  100/ 3474]
loss: 0.021506  [  200/ 3474]
loss: 0.062598  [  300/ 3474]
loss: 0.052658  [  400/ 3474]
loss: 0.004778  [  500/ 3474]
loss: 0.028611  [  600/ 3474]
loss: 0.011430  [  700/ 3474]
loss: 0.017418  [  800/ 3474]
loss: 0.187013  [  900/ 3474]
loss: 0.008632  [ 1000/ 3474]
loss: 0.165142  [ 1100/ 3474]
loss: 0.027605  [ 1200/ 3474]
loss: 0.025884  [ 1300/ 3474]
loss: 0.016336  [ 1400/ 3474]
loss: 0.083116  [ 1500/ 3474]
loss: 0.010276  [ 1600/ 3474]
loss: 0.038611  [ 1700/ 3474]
loss: 0.024899  [ 1800/ 3474]
loss: 0.018400  [ 1900/ 3474]
loss: 0.007150  [ 2000/ 3474]
loss: 0.016500  [ 2100/ 3474]
loss: 0.027733  [ 2200/ 3474]
loss: 0.071128  [ 2300/ 3474]
loss: 0.006159  [ 2400/ 3474]
loss: 0.013805  [ 2500/ 3474]
loss: 0.020183  [ 2600/ 3474]
loss: 0.039340  [ 2700/ 3474]
loss: 0.024315  [ 2800/ 3474]
loss: 0.017278  [ 2900/ 3474]
loss: 0.232739  [ 3000/ 3474]
loss: 0.011606  [ 3100/ 3474]
loss: 0.013295  [ 3200/ 3474]
loss: 0.036561  [ 3300/ 3474]
loss: 0.013369  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.034179  [    0/ 3474]
loss: 0.008264  [  100/ 3474]
loss: 0.021724  [  200/ 3474]
loss: 0.062399  [  300/ 3474]
loss: 0.057547  [  400/ 3474]
loss: 0.005004  [  500/ 3474]
loss: 0.026626  [  600/ 3474]
loss: 0.010916  [  700/ 3474]
loss: 0.018111  [  800/ 3474]
loss: 0.187816  [  900/ 3474]
loss: 0.009210  [ 1000/ 3474]
loss: 0.173285  [ 1100/ 3474]
loss: 0.026332  [ 1200/ 3474]
loss: 0.024266  [ 1300/ 3474]
loss: 0.015020  [ 1400/ 3474]
loss: 0.084763  [ 1500/ 3474]
loss: 0.010040  [ 1600/ 3474]
loss: 0.037410  [ 1700/ 3474]
loss: 0.023907  [ 1800/ 3474]
loss: 0.019028  [ 1900/ 3474]
loss: 0.007137  [ 2000/ 3474]
loss: 0.016299  [ 2100/ 3474]
loss: 0.028463  [ 2200/ 3474]
loss: 0.071678  [ 2300/ 3474]
loss: 0.006130  [ 2400/ 3474]
loss: 0.012924  [ 2500/ 3474]
loss: 0.019861  [ 2600/ 3474]
loss: 0.039358  [ 2700/ 3474]
loss: 0.023093  [ 2800/ 3474]
loss: 0.017193  [ 2900/ 3474]
loss: 0.231642  [ 3000/ 3474]
loss: 0.011062  [ 3100/ 3474]
loss: 0.013708  [ 3200/ 3474]
loss: 0.038245  [ 3300/ 3474]
loss: 0.011032  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [0.9830097  0.02465101]
[0 1 1 ... 1 1 2]
[0 1 1 ... 1 1 2]
Cluster 0 Occurrences: 1198; KMEANS: 1188
Cluster 1 Occurrences: 1128; KMEANS: 1123
Cluster 2 Occurrences: 1148; KMEANS: 1163
Centroids: [[1.5185018, -0.1672636], [-1.9531858, 1.1168201], [1.2244409, 1.6404619]]
Centroids: [[1.519102, -0.18808863], [-1.969365, 1.1242118], [1.2283175, 1.6368022]]
Contingency Matrix: 
[[1181    0   17]
 [   6 1122    0]
 [   1    1 1146]]
[[-1, -1, -1], [-1, 1122, 0], [-1, 1, 1146]]
[[-1, -1, -1], [-1, 1122, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 2, 1: 1}
New Contingency Matrix: 
[[1181    0   17]
 [   6 1122    0]
 [   1    1 1146]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1181, 1122, 1146], Sum: 3449
All_Elements: [1181, 0, 17, 6, 1122, 0, 1, 1, 1146], Sum: 3474
Accuracy: 0.9928036845135291
Done!
