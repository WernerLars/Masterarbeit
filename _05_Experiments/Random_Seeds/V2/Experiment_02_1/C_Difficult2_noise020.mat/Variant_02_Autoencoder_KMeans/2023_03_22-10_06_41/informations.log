Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_06_41
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000172057CB9B0>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81B128>
Epoch 1
-------------------------------
loss: 0.384452  [    0/ 3493]
loss: 0.123371  [  100/ 3493]
loss: 0.044237  [  200/ 3493]
loss: 0.089890  [  300/ 3493]
loss: 0.096868  [  400/ 3493]
loss: 0.057224  [  500/ 3493]
loss: 0.058655  [  600/ 3493]
loss: 0.186349  [  700/ 3493]
loss: 0.055172  [  800/ 3493]
loss: 0.024266  [  900/ 3493]
loss: 0.043223  [ 1000/ 3493]
loss: 0.055358  [ 1100/ 3493]
loss: 0.019324  [ 1200/ 3493]
loss: 0.013538  [ 1300/ 3493]
loss: 0.028052  [ 1400/ 3493]
loss: 0.038380  [ 1500/ 3493]
loss: 0.029703  [ 1600/ 3493]
loss: 0.032702  [ 1700/ 3493]
loss: 0.077385  [ 1800/ 3493]
loss: 0.007492  [ 1900/ 3493]
loss: 0.010901  [ 2000/ 3493]
loss: 0.012242  [ 2100/ 3493]
loss: 0.010178  [ 2200/ 3493]
loss: 0.028357  [ 2300/ 3493]
loss: 0.011410  [ 2400/ 3493]
loss: 0.077537  [ 2500/ 3493]
loss: 0.006565  [ 2600/ 3493]
loss: 0.017710  [ 2700/ 3493]
loss: 0.031021  [ 2800/ 3493]
loss: 0.008231  [ 2900/ 3493]
loss: 0.041553  [ 3000/ 3493]
loss: 0.014215  [ 3100/ 3493]
loss: 0.034592  [ 3200/ 3493]
loss: 0.024152  [ 3300/ 3493]
loss: 0.010857  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.055702  [    0/ 3493]
loss: 0.020614  [  100/ 3493]
loss: 0.030260  [  200/ 3493]
loss: 0.016675  [  300/ 3493]
loss: 0.059040  [  400/ 3493]
loss: 0.027872  [  500/ 3493]
loss: 0.051173  [  600/ 3493]
loss: 0.198545  [  700/ 3493]
loss: 0.014684  [  800/ 3493]
loss: 0.016616  [  900/ 3493]
loss: 0.036297  [ 1000/ 3493]
loss: 0.027361  [ 1100/ 3493]
loss: 0.017824  [ 1200/ 3493]
loss: 0.010699  [ 1300/ 3493]
loss: 0.034624  [ 1400/ 3493]
loss: 0.060193  [ 1500/ 3493]
loss: 0.031639  [ 1600/ 3493]
loss: 0.017117  [ 1700/ 3493]
loss: 0.051345  [ 1800/ 3493]
loss: 0.013923  [ 1900/ 3493]
loss: 0.009916  [ 2000/ 3493]
loss: 0.014688  [ 2100/ 3493]
loss: 0.012357  [ 2200/ 3493]
loss: 0.025397  [ 2300/ 3493]
loss: 0.012025  [ 2400/ 3493]
loss: 0.083641  [ 2500/ 3493]
loss: 0.004038  [ 2600/ 3493]
loss: 0.014157  [ 2700/ 3493]
loss: 0.023611  [ 2800/ 3493]
loss: 0.007940  [ 2900/ 3493]
loss: 0.043538  [ 3000/ 3493]
loss: 0.013233  [ 3100/ 3493]
loss: 0.029650  [ 3200/ 3493]
loss: 0.023366  [ 3300/ 3493]
loss: 0.011877  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.046401  [    0/ 3493]
loss: 0.017712  [  100/ 3493]
loss: 0.024010  [  200/ 3493]
loss: 0.015785  [  300/ 3493]
loss: 0.057380  [  400/ 3493]
loss: 0.027340  [  500/ 3493]
loss: 0.052616  [  600/ 3493]
loss: 0.177919  [  700/ 3493]
loss: 0.010297  [  800/ 3493]
loss: 0.015161  [  900/ 3493]
loss: 0.033294  [ 1000/ 3493]
loss: 0.030299  [ 1100/ 3493]
loss: 0.018286  [ 1200/ 3493]
loss: 0.012207  [ 1300/ 3493]
loss: 0.035161  [ 1400/ 3493]
loss: 0.063357  [ 1500/ 3493]
loss: 0.032777  [ 1600/ 3493]
loss: 0.017651  [ 1700/ 3493]
loss: 0.046505  [ 1800/ 3493]
loss: 0.015220  [ 1900/ 3493]
loss: 0.011898  [ 2000/ 3493]
loss: 0.017944  [ 2100/ 3493]
loss: 0.012579  [ 2200/ 3493]
loss: 0.023927  [ 2300/ 3493]
loss: 0.014436  [ 2400/ 3493]
loss: 0.083786  [ 2500/ 3493]
loss: 0.004539  [ 2600/ 3493]
loss: 0.014709  [ 2700/ 3493]
loss: 0.022495  [ 2800/ 3493]
loss: 0.008270  [ 2900/ 3493]
loss: 0.043326  [ 3000/ 3493]
loss: 0.013361  [ 3100/ 3493]
loss: 0.027947  [ 3200/ 3493]
loss: 0.023666  [ 3300/ 3493]
loss: 0.012414  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.048144  [    0/ 3493]
loss: 0.018621  [  100/ 3493]
loss: 0.022666  [  200/ 3493]
loss: 0.015168  [  300/ 3493]
loss: 0.056605  [  400/ 3493]
loss: 0.027319  [  500/ 3493]
loss: 0.053199  [  600/ 3493]
loss: 0.169747  [  700/ 3493]
loss: 0.009429  [  800/ 3493]
loss: 0.014399  [  900/ 3493]
loss: 0.031721  [ 1000/ 3493]
loss: 0.033448  [ 1100/ 3493]
loss: 0.018159  [ 1200/ 3493]
loss: 0.012185  [ 1300/ 3493]
loss: 0.034974  [ 1400/ 3493]
loss: 0.063851  [ 1500/ 3493]
loss: 0.032230  [ 1600/ 3493]
loss: 0.019093  [ 1700/ 3493]
loss: 0.044331  [ 1800/ 3493]
loss: 0.015903  [ 1900/ 3493]
loss: 0.012761  [ 2000/ 3493]
loss: 0.019650  [ 2100/ 3493]
loss: 0.012152  [ 2200/ 3493]
loss: 0.023271  [ 2300/ 3493]
loss: 0.014884  [ 2400/ 3493]
loss: 0.083135  [ 2500/ 3493]
loss: 0.004568  [ 2600/ 3493]
loss: 0.015296  [ 2700/ 3493]
loss: 0.023026  [ 2800/ 3493]
loss: 0.008619  [ 2900/ 3493]
loss: 0.042745  [ 3000/ 3493]
loss: 0.013295  [ 3100/ 3493]
loss: 0.026851  [ 3200/ 3493]
loss: 0.023884  [ 3300/ 3493]
loss: 0.012097  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.050581  [    0/ 3493]
loss: 0.018971  [  100/ 3493]
loss: 0.021851  [  200/ 3493]
loss: 0.015314  [  300/ 3493]
loss: 0.056323  [  400/ 3493]
loss: 0.027211  [  500/ 3493]
loss: 0.053637  [  600/ 3493]
loss: 0.165327  [  700/ 3493]
loss: 0.009186  [  800/ 3493]
loss: 0.013969  [  900/ 3493]
loss: 0.031052  [ 1000/ 3493]
loss: 0.035037  [ 1100/ 3493]
loss: 0.017891  [ 1200/ 3493]
loss: 0.011519  [ 1300/ 3493]
loss: 0.034722  [ 1400/ 3493]
loss: 0.064103  [ 1500/ 3493]
loss: 0.032247  [ 1600/ 3493]
loss: 0.020012  [ 1700/ 3493]
loss: 0.043523  [ 1800/ 3493]
loss: 0.016427  [ 1900/ 3493]
loss: 0.013077  [ 2000/ 3493]
loss: 0.020492  [ 2100/ 3493]
loss: 0.011789  [ 2200/ 3493]
loss: 0.022893  [ 2300/ 3493]
loss: 0.015308  [ 2400/ 3493]
loss: 0.082897  [ 2500/ 3493]
loss: 0.004753  [ 2600/ 3493]
loss: 0.015605  [ 2700/ 3493]
loss: 0.023371  [ 2800/ 3493]
loss: 0.008902  [ 2900/ 3493]
loss: 0.042355  [ 3000/ 3493]
loss: 0.013229  [ 3100/ 3493]
loss: 0.026089  [ 3200/ 3493]
loss: 0.023926  [ 3300/ 3493]
loss: 0.011801  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.052310  [    0/ 3493]
loss: 0.019035  [  100/ 3493]
loss: 0.021386  [  200/ 3493]
loss: 0.015386  [  300/ 3493]
loss: 0.055721  [  400/ 3493]
loss: 0.026863  [  500/ 3493]
loss: 0.053831  [  600/ 3493]
loss: 0.162702  [  700/ 3493]
loss: 0.009390  [  800/ 3493]
loss: 0.013579  [  900/ 3493]
loss: 0.030439  [ 1000/ 3493]
loss: 0.036283  [ 1100/ 3493]
loss: 0.017755  [ 1200/ 3493]
loss: 0.010364  [ 1300/ 3493]
loss: 0.034465  [ 1400/ 3493]
loss: 0.063748  [ 1500/ 3493]
loss: 0.032052  [ 1600/ 3493]
loss: 0.020288  [ 1700/ 3493]
loss: 0.042863  [ 1800/ 3493]
loss: 0.016783  [ 1900/ 3493]
loss: 0.013316  [ 2000/ 3493]
loss: 0.021198  [ 2100/ 3493]
loss: 0.011547  [ 2200/ 3493]
loss: 0.022638  [ 2300/ 3493]
loss: 0.015781  [ 2400/ 3493]
loss: 0.083384  [ 2500/ 3493]
loss: 0.004735  [ 2600/ 3493]
loss: 0.015951  [ 2700/ 3493]
loss: 0.023967  [ 2800/ 3493]
loss: 0.008822  [ 2900/ 3493]
loss: 0.041495  [ 3000/ 3493]
loss: 0.013251  [ 3100/ 3493]
loss: 0.025044  [ 3200/ 3493]
loss: 0.024172  [ 3300/ 3493]
loss: 0.011712  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.054179  [    0/ 3493]
loss: 0.019050  [  100/ 3493]
loss: 0.020894  [  200/ 3493]
loss: 0.015505  [  300/ 3493]
loss: 0.055515  [  400/ 3493]
loss: 0.026709  [  500/ 3493]
loss: 0.053586  [  600/ 3493]
loss: 0.161447  [  700/ 3493]
loss: 0.009900  [  800/ 3493]
loss: 0.013251  [  900/ 3493]
loss: 0.030717  [ 1000/ 3493]
loss: 0.037003  [ 1100/ 3493]
loss: 0.017642  [ 1200/ 3493]
loss: 0.010112  [ 1300/ 3493]
loss: 0.034242  [ 1400/ 3493]
loss: 0.063464  [ 1500/ 3493]
loss: 0.032022  [ 1600/ 3493]
loss: 0.020157  [ 1700/ 3493]
loss: 0.041668  [ 1800/ 3493]
loss: 0.016198  [ 1900/ 3493]
loss: 0.013252  [ 2000/ 3493]
loss: 0.021939  [ 2100/ 3493]
loss: 0.011147  [ 2200/ 3493]
loss: 0.022558  [ 2300/ 3493]
loss: 0.016248  [ 2400/ 3493]
loss: 0.082775  [ 2500/ 3493]
loss: 0.004974  [ 2600/ 3493]
loss: 0.016064  [ 2700/ 3493]
loss: 0.023971  [ 2800/ 3493]
loss: 0.008576  [ 2900/ 3493]
loss: 0.041000  [ 3000/ 3493]
loss: 0.013208  [ 3100/ 3493]
loss: 0.023445  [ 3200/ 3493]
loss: 0.024359  [ 3300/ 3493]
loss: 0.011428  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.056487  [    0/ 3493]
loss: 0.019026  [  100/ 3493]
loss: 0.020956  [  200/ 3493]
loss: 0.015546  [  300/ 3493]
loss: 0.054689  [  400/ 3493]
loss: 0.026342  [  500/ 3493]
loss: 0.052921  [  600/ 3493]
loss: 0.159191  [  700/ 3493]
loss: 0.010853  [  800/ 3493]
loss: 0.012692  [  900/ 3493]
loss: 0.031537  [ 1000/ 3493]
loss: 0.037991  [ 1100/ 3493]
loss: 0.017547  [ 1200/ 3493]
loss: 0.009867  [ 1300/ 3493]
loss: 0.034123  [ 1400/ 3493]
loss: 0.062802  [ 1500/ 3493]
loss: 0.032252  [ 1600/ 3493]
loss: 0.019552  [ 1700/ 3493]
loss: 0.040359  [ 1800/ 3493]
loss: 0.014959  [ 1900/ 3493]
loss: 0.012961  [ 2000/ 3493]
loss: 0.023018  [ 2100/ 3493]
loss: 0.010814  [ 2200/ 3493]
loss: 0.022300  [ 2300/ 3493]
loss: 0.016537  [ 2400/ 3493]
loss: 0.082791  [ 2500/ 3493]
loss: 0.005032  [ 2600/ 3493]
loss: 0.015627  [ 2700/ 3493]
loss: 0.023025  [ 2800/ 3493]
loss: 0.007962  [ 2900/ 3493]
loss: 0.040731  [ 3000/ 3493]
loss: 0.013231  [ 3100/ 3493]
loss: 0.022021  [ 3200/ 3493]
loss: 0.023945  [ 3300/ 3493]
loss: 0.010622  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [-1.0055912  1.8821864]
[1 0 0 ... 1 0 2]
[1 2 2 ... 1 2 0]
Cluster 0 Occurrences: 1151; KMEANS: 1283
Cluster 1 Occurrences: 1195; KMEANS: 1184
Cluster 2 Occurrences: 1147; KMEANS: 1026
Centroids: [[1.0093925, 0.13296863], [-0.90329325, 1.2789079], [0.5668671, 0.5231253]]
Centroids: [[0.51729906, 0.50999576], [-0.9227429, 1.2887893], [1.1319735, 0.09855302]]
Contingency Matrix: 
[[ 280    2  869]
 [  13 1177    5]
 [ 990    5  152]]
[[280, -1, 869], [-1, -1, -1], [990, -1, 152]]
[[-1, -1, 869], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[ 869    2  280]
 [   5 1177   13]
 [ 152    5  990]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [869, 1177, 990], Sum: 3036
All_Elements: [869, 2, 280, 5, 1177, 13, 152, 5, 990], Sum: 3493
Accuracy: 0.8691669052390495
Done!
