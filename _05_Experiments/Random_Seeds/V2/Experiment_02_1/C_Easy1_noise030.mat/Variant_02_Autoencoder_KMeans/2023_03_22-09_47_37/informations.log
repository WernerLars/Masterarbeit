Experiment_path: Random_Seeds//V2/Experiment_02_1
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_1/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000171DAADBF98>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x00000171DB81BFD0>
Epoch 1
-------------------------------
loss: 0.357440  [    0/ 3475]
loss: 0.116017  [  100/ 3475]
loss: 0.112149  [  200/ 3475]
loss: 0.225127  [  300/ 3475]
loss: 0.158830  [  400/ 3475]
loss: 0.041994  [  500/ 3475]
loss: 0.076825  [  600/ 3475]
loss: 0.119841  [  700/ 3475]
loss: 0.133762  [  800/ 3475]
loss: 0.179647  [  900/ 3475]
loss: 0.043628  [ 1000/ 3475]
loss: 0.098729  [ 1100/ 3475]
loss: 0.057940  [ 1200/ 3475]
loss: 0.026674  [ 1300/ 3475]
loss: 0.062674  [ 1400/ 3475]
loss: 0.051036  [ 1500/ 3475]
loss: 0.039407  [ 1600/ 3475]
loss: 0.026636  [ 1700/ 3475]
loss: 0.071379  [ 1800/ 3475]
loss: 0.035571  [ 1900/ 3475]
loss: 0.016965  [ 2000/ 3475]
loss: 0.047081  [ 2100/ 3475]
loss: 0.089469  [ 2200/ 3475]
loss: 0.051317  [ 2300/ 3475]
loss: 0.019572  [ 2400/ 3475]
loss: 0.074734  [ 2500/ 3475]
loss: 0.023282  [ 2600/ 3475]
loss: 0.065407  [ 2700/ 3475]
loss: 0.105008  [ 2800/ 3475]
loss: 0.050963  [ 2900/ 3475]
loss: 0.050293  [ 3000/ 3475]
loss: 0.107652  [ 3100/ 3475]
loss: 0.101805  [ 3200/ 3475]
loss: 0.064072  [ 3300/ 3475]
loss: 0.048728  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.076392  [    0/ 3475]
loss: 0.051213  [  100/ 3475]
loss: 0.056818  [  200/ 3475]
loss: 0.222986  [  300/ 3475]
loss: 0.053026  [  400/ 3475]
loss: 0.026852  [  500/ 3475]
loss: 0.021089  [  600/ 3475]
loss: 0.025251  [  700/ 3475]
loss: 0.110598  [  800/ 3475]
loss: 0.044462  [  900/ 3475]
loss: 0.053987  [ 1000/ 3475]
loss: 0.029877  [ 1100/ 3475]
loss: 0.026430  [ 1200/ 3475]
loss: 0.036218  [ 1300/ 3475]
loss: 0.048059  [ 1400/ 3475]
loss: 0.043163  [ 1500/ 3475]
loss: 0.042382  [ 1600/ 3475]
loss: 0.018874  [ 1700/ 3475]
loss: 0.065585  [ 1800/ 3475]
loss: 0.037443  [ 1900/ 3475]
loss: 0.014392  [ 2000/ 3475]
loss: 0.040317  [ 2100/ 3475]
loss: 0.073033  [ 2200/ 3475]
loss: 0.020545  [ 2300/ 3475]
loss: 0.015844  [ 2400/ 3475]
loss: 0.078683  [ 2500/ 3475]
loss: 0.030288  [ 2600/ 3475]
loss: 0.077499  [ 2700/ 3475]
loss: 0.104848  [ 2800/ 3475]
loss: 0.034158  [ 2900/ 3475]
loss: 0.050840  [ 3000/ 3475]
loss: 0.079267  [ 3100/ 3475]
loss: 0.058973  [ 3200/ 3475]
loss: 0.048245  [ 3300/ 3475]
loss: 0.031966  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.046978  [    0/ 3475]
loss: 0.032025  [  100/ 3475]
loss: 0.055719  [  200/ 3475]
loss: 0.229130  [  300/ 3475]
loss: 0.030006  [  400/ 3475]
loss: 0.026434  [  500/ 3475]
loss: 0.017111  [  600/ 3475]
loss: 0.019251  [  700/ 3475]
loss: 0.105486  [  800/ 3475]
loss: 0.042916  [  900/ 3475]
loss: 0.043331  [ 1000/ 3475]
loss: 0.027977  [ 1100/ 3475]
loss: 0.026533  [ 1200/ 3475]
loss: 0.042313  [ 1300/ 3475]
loss: 0.045948  [ 1400/ 3475]
loss: 0.039790  [ 1500/ 3475]
loss: 0.038802  [ 1600/ 3475]
loss: 0.017594  [ 1700/ 3475]
loss: 0.071469  [ 1800/ 3475]
loss: 0.036753  [ 1900/ 3475]
loss: 0.015838  [ 2000/ 3475]
loss: 0.036181  [ 2100/ 3475]
loss: 0.074382  [ 2200/ 3475]
loss: 0.020561  [ 2300/ 3475]
loss: 0.016192  [ 2400/ 3475]
loss: 0.078839  [ 2500/ 3475]
loss: 0.028509  [ 2600/ 3475]
loss: 0.084634  [ 2700/ 3475]
loss: 0.094590  [ 2800/ 3475]
loss: 0.033824  [ 2900/ 3475]
loss: 0.050452  [ 3000/ 3475]
loss: 0.073129  [ 3100/ 3475]
loss: 0.049287  [ 3200/ 3475]
loss: 0.042049  [ 3300/ 3475]
loss: 0.030398  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.037486  [    0/ 3475]
loss: 0.027619  [  100/ 3475]
loss: 0.055265  [  200/ 3475]
loss: 0.227869  [  300/ 3475]
loss: 0.022650  [  400/ 3475]
loss: 0.025226  [  500/ 3475]
loss: 0.017648  [  600/ 3475]
loss: 0.020261  [  700/ 3475]
loss: 0.104839  [  800/ 3475]
loss: 0.044043  [  900/ 3475]
loss: 0.039823  [ 1000/ 3475]
loss: 0.029548  [ 1100/ 3475]
loss: 0.026348  [ 1200/ 3475]
loss: 0.048100  [ 1300/ 3475]
loss: 0.042926  [ 1400/ 3475]
loss: 0.038874  [ 1500/ 3475]
loss: 0.037403  [ 1600/ 3475]
loss: 0.016600  [ 1700/ 3475]
loss: 0.073333  [ 1800/ 3475]
loss: 0.036475  [ 1900/ 3475]
loss: 0.016801  [ 2000/ 3475]
loss: 0.036712  [ 2100/ 3475]
loss: 0.076997  [ 2200/ 3475]
loss: 0.021870  [ 2300/ 3475]
loss: 0.015590  [ 2400/ 3475]
loss: 0.077464  [ 2500/ 3475]
loss: 0.027620  [ 2600/ 3475]
loss: 0.091853  [ 2700/ 3475]
loss: 0.087415  [ 2800/ 3475]
loss: 0.034143  [ 2900/ 3475]
loss: 0.050487  [ 3000/ 3475]
loss: 0.069306  [ 3100/ 3475]
loss: 0.046331  [ 3200/ 3475]
loss: 0.037697  [ 3300/ 3475]
loss: 0.029664  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.032415  [    0/ 3475]
loss: 0.025528  [  100/ 3475]
loss: 0.054415  [  200/ 3475]
loss: 0.226462  [  300/ 3475]
loss: 0.019049  [  400/ 3475]
loss: 0.024526  [  500/ 3475]
loss: 0.018228  [  600/ 3475]
loss: 0.025719  [  700/ 3475]
loss: 0.104832  [  800/ 3475]
loss: 0.041963  [  900/ 3475]
loss: 0.038276  [ 1000/ 3475]
loss: 0.030309  [ 1100/ 3475]
loss: 0.025853  [ 1200/ 3475]
loss: 0.051888  [ 1300/ 3475]
loss: 0.038822  [ 1400/ 3475]
loss: 0.037967  [ 1500/ 3475]
loss: 0.037071  [ 1600/ 3475]
loss: 0.016232  [ 1700/ 3475]
loss: 0.074147  [ 1800/ 3475]
loss: 0.036448  [ 1900/ 3475]
loss: 0.017772  [ 2000/ 3475]
loss: 0.037531  [ 2100/ 3475]
loss: 0.079520  [ 2200/ 3475]
loss: 0.023236  [ 2300/ 3475]
loss: 0.014897  [ 2400/ 3475]
loss: 0.077057  [ 2500/ 3475]
loss: 0.027953  [ 2600/ 3475]
loss: 0.097318  [ 2700/ 3475]
loss: 0.080181  [ 2800/ 3475]
loss: 0.033862  [ 2900/ 3475]
loss: 0.050331  [ 3000/ 3475]
loss: 0.067904  [ 3100/ 3475]
loss: 0.045084  [ 3200/ 3475]
loss: 0.038349  [ 3300/ 3475]
loss: 0.029398  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.028360  [    0/ 3475]
loss: 0.024038  [  100/ 3475]
loss: 0.053704  [  200/ 3475]
loss: 0.225433  [  300/ 3475]
loss: 0.018514  [  400/ 3475]
loss: 0.024253  [  500/ 3475]
loss: 0.015243  [  600/ 3475]
loss: 0.033269  [  700/ 3475]
loss: 0.104250  [  800/ 3475]
loss: 0.039618  [  900/ 3475]
loss: 0.037089  [ 1000/ 3475]
loss: 0.030815  [ 1100/ 3475]
loss: 0.025618  [ 1200/ 3475]
loss: 0.052650  [ 1300/ 3475]
loss: 0.034189  [ 1400/ 3475]
loss: 0.037139  [ 1500/ 3475]
loss: 0.037359  [ 1600/ 3475]
loss: 0.016789  [ 1700/ 3475]
loss: 0.072144  [ 1800/ 3475]
loss: 0.036448  [ 1900/ 3475]
loss: 0.018860  [ 2000/ 3475]
loss: 0.038107  [ 2100/ 3475]
loss: 0.081283  [ 2200/ 3475]
loss: 0.024421  [ 2300/ 3475]
loss: 0.014679  [ 2400/ 3475]
loss: 0.077058  [ 2500/ 3475]
loss: 0.028843  [ 2600/ 3475]
loss: 0.098356  [ 2700/ 3475]
loss: 0.077155  [ 2800/ 3475]
loss: 0.032612  [ 2900/ 3475]
loss: 0.050066  [ 3000/ 3475]
loss: 0.065870  [ 3100/ 3475]
loss: 0.045569  [ 3200/ 3475]
loss: 0.040464  [ 3300/ 3475]
loss: 0.029094  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.026989  [    0/ 3475]
loss: 0.023253  [  100/ 3475]
loss: 0.053405  [  200/ 3475]
loss: 0.224771  [  300/ 3475]
loss: 0.018796  [  400/ 3475]
loss: 0.024083  [  500/ 3475]
loss: 0.014669  [  600/ 3475]
loss: 0.035706  [  700/ 3475]
loss: 0.103292  [  800/ 3475]
loss: 0.039315  [  900/ 3475]
loss: 0.036277  [ 1000/ 3475]
loss: 0.030753  [ 1100/ 3475]
loss: 0.025387  [ 1200/ 3475]
loss: 0.052287  [ 1300/ 3475]
loss: 0.030884  [ 1400/ 3475]
loss: 0.036540  [ 1500/ 3475]
loss: 0.037736  [ 1600/ 3475]
loss: 0.016969  [ 1700/ 3475]
loss: 0.070071  [ 1800/ 3475]
loss: 0.036818  [ 1900/ 3475]
loss: 0.019273  [ 2000/ 3475]
loss: 0.037295  [ 2100/ 3475]
loss: 0.082256  [ 2200/ 3475]
loss: 0.025809  [ 2300/ 3475]
loss: 0.014689  [ 2400/ 3475]
loss: 0.077253  [ 2500/ 3475]
loss: 0.029644  [ 2600/ 3475]
loss: 0.101411  [ 2700/ 3475]
loss: 0.075722  [ 2800/ 3475]
loss: 0.032067  [ 2900/ 3475]
loss: 0.049938  [ 3000/ 3475]
loss: 0.064923  [ 3100/ 3475]
loss: 0.045993  [ 3200/ 3475]
loss: 0.039174  [ 3300/ 3475]
loss: 0.029412  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.026442  [    0/ 3475]
loss: 0.022108  [  100/ 3475]
loss: 0.053036  [  200/ 3475]
loss: 0.224447  [  300/ 3475]
loss: 0.018823  [  400/ 3475]
loss: 0.023870  [  500/ 3475]
loss: 0.014350  [  600/ 3475]
loss: 0.037107  [  700/ 3475]
loss: 0.102970  [  800/ 3475]
loss: 0.038870  [  900/ 3475]
loss: 0.035742  [ 1000/ 3475]
loss: 0.030424  [ 1100/ 3475]
loss: 0.025232  [ 1200/ 3475]
loss: 0.052607  [ 1300/ 3475]
loss: 0.029046  [ 1400/ 3475]
loss: 0.036198  [ 1500/ 3475]
loss: 0.037936  [ 1600/ 3475]
loss: 0.016649  [ 1700/ 3475]
loss: 0.069354  [ 1800/ 3475]
loss: 0.036749  [ 1900/ 3475]
loss: 0.019765  [ 2000/ 3475]
loss: 0.036738  [ 2100/ 3475]
loss: 0.082912  [ 2200/ 3475]
loss: 0.026464  [ 2300/ 3475]
loss: 0.014406  [ 2400/ 3475]
loss: 0.077487  [ 2500/ 3475]
loss: 0.030153  [ 2600/ 3475]
loss: 0.104559  [ 2700/ 3475]
loss: 0.073515  [ 2800/ 3475]
loss: 0.031832  [ 2900/ 3475]
loss: 0.050040  [ 3000/ 3475]
loss: 0.065193  [ 3100/ 3475]
loss: 0.046331  [ 3200/ 3475]
loss: 0.035271  [ 3300/ 3475]
loss: 0.029348  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [0.3712935 1.1123374]
[2 1 2 ... 1 0 1]
[2 1 2 ... 1 0 1]
Cluster 0 Occurrences: 1162; KMEANS: 1172
Cluster 1 Occurrences: 1164; KMEANS: 1137
Cluster 2 Occurrences: 1149; KMEANS: 1166
Centroids: [[1.1055691, -0.11206069], [-2.7235322, 1.0784472], [1.0697815, 1.509178]]
Centroids: [[1.0776876, -0.12871942], [-2.7805972, 1.0976485], [1.0653068, 1.5111291]]
Contingency Matrix: 
[[1125    0   37]
 [  21 1130   13]
 [  26    7 1116]]
[[1125, -1, 37], [-1, -1, -1], [26, -1, 1116]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 1116]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 0, 2: 2}
New Contingency Matrix: 
[[1125    0   37]
 [  21 1130   13]
 [  26    7 1116]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1125, 1130, 1116], Sum: 3371
All_Elements: [1125, 0, 37, 21, 1130, 13, 26, 7, 1116], Sum: 3475
Accuracy: 0.9700719424460431
Done!
