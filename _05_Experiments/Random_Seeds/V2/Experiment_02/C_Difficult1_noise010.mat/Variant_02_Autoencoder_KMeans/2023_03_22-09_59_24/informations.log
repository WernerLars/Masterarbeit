Experiment_path: Random_Seeds//V2/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise010.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02/C_Difficult1_noise010.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_59_24
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002DC03FB0518>
Sampling rate: 24000.0
Raw: [-0.08093593 -0.05766541 -0.01986255 ... -0.05525448 -0.03568967
 -0.02908211]
Times: [   1054    1166    1249 ... 1439165 1439229 1439456]
Cluster: [3 3 3 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3448
First aligned Spike Frame: [-4.79707202e-02 -8.83141277e-02 -1.14477415e-01 -1.20070620e-01
 -1.07771810e-01 -8.38316152e-02 -5.31944576e-02 -2.65798100e-02
 -6.71143520e-03  2.42590968e-02  6.29914810e-02  1.33638321e-01
  3.34948892e-01  7.06515114e-01  1.07966210e+00  1.10962398e+00
  6.55362247e-01 -7.19265760e-04 -4.64310305e-01 -6.23655437e-01
 -6.27154873e-01 -6.07058236e-01 -5.79298390e-01 -5.10136794e-01
 -4.12813198e-01 -3.36153681e-01 -2.86889156e-01 -2.38391657e-01
 -1.98571332e-01 -1.83172365e-01 -1.85707124e-01 -1.94134424e-01
 -2.08249204e-01 -2.33590971e-01 -2.49276546e-01 -2.46246400e-01
 -2.44939654e-01 -2.59799331e-01 -2.74706204e-01 -2.68658955e-01
 -2.53059716e-01 -2.46742300e-01 -2.46057086e-01 -2.38944634e-01
 -2.31254619e-01 -2.20748865e-01 -1.79535377e-01]
Cluster 0, Occurrences: 1164
Cluster 1, Occurrences: 1155
Cluster 2, Occurrences: 1129
<torch.utils.data.dataloader.DataLoader object at 0x000002DBE3E659B0>
Epoch 1
-------------------------------
loss: 0.186214  [    0/ 3448]
loss: 0.032820  [  100/ 3448]
loss: 0.021471  [  200/ 3448]
loss: 0.032784  [  300/ 3448]
loss: 0.021426  [  400/ 3448]
loss: 0.013535  [  500/ 3448]
loss: 0.019179  [  600/ 3448]
loss: 0.010744  [  700/ 3448]
loss: 0.013810  [  800/ 3448]
loss: 0.028079  [  900/ 3448]
loss: 0.080861  [ 1000/ 3448]
loss: 0.011823  [ 1100/ 3448]
loss: 0.012927  [ 1200/ 3448]
loss: 0.128039  [ 1300/ 3448]
loss: 0.009089  [ 1400/ 3448]
loss: 0.023645  [ 1500/ 3448]
loss: 0.022939  [ 1600/ 3448]
loss: 0.008288  [ 1700/ 3448]
loss: 0.008119  [ 1800/ 3448]
loss: 0.018542  [ 1900/ 3448]
loss: 0.006861  [ 2000/ 3448]
loss: 0.004216  [ 2100/ 3448]
loss: 0.012131  [ 2200/ 3448]
loss: 0.008335  [ 2300/ 3448]
loss: 0.020597  [ 2400/ 3448]
loss: 0.007254  [ 2500/ 3448]
loss: 0.012050  [ 2600/ 3448]
loss: 0.016614  [ 2700/ 3448]
loss: 0.006277  [ 2800/ 3448]
loss: 0.011009  [ 2900/ 3448]
loss: 0.004177  [ 3000/ 3448]
loss: 0.009766  [ 3100/ 3448]
loss: 0.009574  [ 3200/ 3448]
loss: 0.011648  [ 3300/ 3448]
loss: 0.006635  [ 3400/ 3448]
Epoch 2
-------------------------------
loss: 0.018256  [    0/ 3448]
loss: 0.008278  [  100/ 3448]
loss: 0.004546  [  200/ 3448]
loss: 0.013284  [  300/ 3448]
loss: 0.015377  [  400/ 3448]
loss: 0.014610  [  500/ 3448]
loss: 0.009422  [  600/ 3448]
loss: 0.008301  [  700/ 3448]
loss: 0.008810  [  800/ 3448]
loss: 0.016686  [  900/ 3448]
loss: 0.073472  [ 1000/ 3448]
loss: 0.008800  [ 1100/ 3448]
loss: 0.010704  [ 1200/ 3448]
loss: 0.126716  [ 1300/ 3448]
loss: 0.008953  [ 1400/ 3448]
loss: 0.010627  [ 1500/ 3448]
loss: 0.018715  [ 1600/ 3448]
loss: 0.008593  [ 1700/ 3448]
loss: 0.009874  [ 1800/ 3448]
loss: 0.018015  [ 1900/ 3448]
loss: 0.006348  [ 2000/ 3448]
loss: 0.004349  [ 2100/ 3448]
loss: 0.013357  [ 2200/ 3448]
loss: 0.008303  [ 2300/ 3448]
loss: 0.020098  [ 2400/ 3448]
loss: 0.005653  [ 2500/ 3448]
loss: 0.012492  [ 2600/ 3448]
loss: 0.014017  [ 2700/ 3448]
loss: 0.007219  [ 2800/ 3448]
loss: 0.008103  [ 2900/ 3448]
loss: 0.004221  [ 3000/ 3448]
loss: 0.014590  [ 3100/ 3448]
loss: 0.009210  [ 3200/ 3448]
loss: 0.010352  [ 3300/ 3448]
loss: 0.006347  [ 3400/ 3448]
Epoch 3
-------------------------------
loss: 0.020510  [    0/ 3448]
loss: 0.009159  [  100/ 3448]
loss: 0.003141  [  200/ 3448]
loss: 0.005917  [  300/ 3448]
loss: 0.008912  [  400/ 3448]
loss: 0.015405  [  500/ 3448]
loss: 0.005553  [  600/ 3448]
loss: 0.009448  [  700/ 3448]
loss: 0.005740  [  800/ 3448]
loss: 0.011383  [  900/ 3448]
loss: 0.075940  [ 1000/ 3448]
loss: 0.011389  [ 1100/ 3448]
loss: 0.011147  [ 1200/ 3448]
loss: 0.124310  [ 1300/ 3448]
loss: 0.008639  [ 1400/ 3448]
loss: 0.011055  [ 1500/ 3448]
loss: 0.008872  [ 1600/ 3448]
loss: 0.013325  [ 1700/ 3448]
loss: 0.008218  [ 1800/ 3448]
loss: 0.017607  [ 1900/ 3448]
loss: 0.007590  [ 2000/ 3448]
loss: 0.003933  [ 2100/ 3448]
loss: 0.008348  [ 2200/ 3448]
loss: 0.006456  [ 2300/ 3448]
loss: 0.010846  [ 2400/ 3448]
loss: 0.007089  [ 2500/ 3448]
loss: 0.012580  [ 2600/ 3448]
loss: 0.008588  [ 2700/ 3448]
loss: 0.007693  [ 2800/ 3448]
loss: 0.002402  [ 2900/ 3448]
loss: 0.004309  [ 3000/ 3448]
loss: 0.016036  [ 3100/ 3448]
loss: 0.011847  [ 3200/ 3448]
loss: 0.010206  [ 3300/ 3448]
loss: 0.006480  [ 3400/ 3448]
Epoch 4
-------------------------------
loss: 0.021115  [    0/ 3448]
loss: 0.007856  [  100/ 3448]
loss: 0.001784  [  200/ 3448]
loss: 0.003113  [  300/ 3448]
loss: 0.005145  [  400/ 3448]
loss: 0.014163  [  500/ 3448]
loss: 0.005990  [  600/ 3448]
loss: 0.007948  [  700/ 3448]
loss: 0.004076  [  800/ 3448]
loss: 0.005842  [  900/ 3448]
loss: 0.085478  [ 1000/ 3448]
loss: 0.014291  [ 1100/ 3448]
loss: 0.008879  [ 1200/ 3448]
loss: 0.124983  [ 1300/ 3448]
loss: 0.006596  [ 1400/ 3448]
loss: 0.020912  [ 1500/ 3448]
loss: 0.004142  [ 1600/ 3448]
loss: 0.010522  [ 1700/ 3448]
loss: 0.007052  [ 1800/ 3448]
loss: 0.017809  [ 1900/ 3448]
loss: 0.008126  [ 2000/ 3448]
loss: 0.003399  [ 2100/ 3448]
loss: 0.005447  [ 2200/ 3448]
loss: 0.005885  [ 2300/ 3448]
loss: 0.009053  [ 2400/ 3448]
loss: 0.008131  [ 2500/ 3448]
loss: 0.012655  [ 2600/ 3448]
loss: 0.008998  [ 2700/ 3448]
loss: 0.008163  [ 2800/ 3448]
loss: 0.002371  [ 2900/ 3448]
loss: 0.004093  [ 3000/ 3448]
loss: 0.015088  [ 3100/ 3448]
loss: 0.012636  [ 3200/ 3448]
loss: 0.010477  [ 3300/ 3448]
loss: 0.006607  [ 3400/ 3448]
Epoch 5
-------------------------------
loss: 0.019748  [    0/ 3448]
loss: 0.006858  [  100/ 3448]
loss: 0.002793  [  200/ 3448]
loss: 0.004538  [  300/ 3448]
loss: 0.003656  [  400/ 3448]
loss: 0.014259  [  500/ 3448]
loss: 0.006560  [  600/ 3448]
loss: 0.006782  [  700/ 3448]
loss: 0.004571  [  800/ 3448]
loss: 0.006297  [  900/ 3448]
loss: 0.084350  [ 1000/ 3448]
loss: 0.013619  [ 1100/ 3448]
loss: 0.007957  [ 1200/ 3448]
loss: 0.125329  [ 1300/ 3448]
loss: 0.006004  [ 1400/ 3448]
loss: 0.021983  [ 1500/ 3448]
loss: 0.004306  [ 1600/ 3448]
loss: 0.009223  [ 1700/ 3448]
loss: 0.006849  [ 1800/ 3448]
loss: 0.017976  [ 1900/ 3448]
loss: 0.007691  [ 2000/ 3448]
loss: 0.003344  [ 2100/ 3448]
loss: 0.004820  [ 2200/ 3448]
loss: 0.006113  [ 2300/ 3448]
loss: 0.009237  [ 2400/ 3448]
loss: 0.008245  [ 2500/ 3448]
loss: 0.012847  [ 2600/ 3448]
loss: 0.007400  [ 2700/ 3448]
loss: 0.008629  [ 2800/ 3448]
loss: 0.002421  [ 2900/ 3448]
loss: 0.004658  [ 3000/ 3448]
loss: 0.016299  [ 3100/ 3448]
loss: 0.011134  [ 3200/ 3448]
loss: 0.011798  [ 3300/ 3448]
loss: 0.006677  [ 3400/ 3448]
Epoch 6
-------------------------------
loss: 0.017069  [    0/ 3448]
loss: 0.005507  [  100/ 3448]
loss: 0.004051  [  200/ 3448]
loss: 0.007581  [  300/ 3448]
loss: 0.003866  [  400/ 3448]
loss: 0.014347  [  500/ 3448]
loss: 0.006964  [  600/ 3448]
loss: 0.006072  [  700/ 3448]
loss: 0.004544  [  800/ 3448]
loss: 0.006779  [  900/ 3448]
loss: 0.084148  [ 1000/ 3448]
loss: 0.013030  [ 1100/ 3448]
loss: 0.007438  [ 1200/ 3448]
loss: 0.125506  [ 1300/ 3448]
loss: 0.005728  [ 1400/ 3448]
loss: 0.021710  [ 1500/ 3448]
loss: 0.004616  [ 1600/ 3448]
loss: 0.009228  [ 1700/ 3448]
loss: 0.007290  [ 1800/ 3448]
loss: 0.018650  [ 1900/ 3448]
loss: 0.007070  [ 2000/ 3448]
loss: 0.003351  [ 2100/ 3448]
loss: 0.004555  [ 2200/ 3448]
loss: 0.006110  [ 2300/ 3448]
loss: 0.009240  [ 2400/ 3448]
loss: 0.007705  [ 2500/ 3448]
loss: 0.012966  [ 2600/ 3448]
loss: 0.007731  [ 2700/ 3448]
loss: 0.008644  [ 2800/ 3448]
loss: 0.002059  [ 2900/ 3448]
loss: 0.005531  [ 3000/ 3448]
loss: 0.014582  [ 3100/ 3448]
loss: 0.009809  [ 3200/ 3448]
loss: 0.011330  [ 3300/ 3448]
loss: 0.006903  [ 3400/ 3448]
Epoch 7
-------------------------------
loss: 0.015840  [    0/ 3448]
loss: 0.005292  [  100/ 3448]
loss: 0.004721  [  200/ 3448]
loss: 0.008540  [  300/ 3448]
loss: 0.004214  [  400/ 3448]
loss: 0.013894  [  500/ 3448]
loss: 0.007161  [  600/ 3448]
loss: 0.005921  [  700/ 3448]
loss: 0.004874  [  800/ 3448]
loss: 0.006676  [  900/ 3448]
loss: 0.083475  [ 1000/ 3448]
loss: 0.012597  [ 1100/ 3448]
loss: 0.007456  [ 1200/ 3448]
loss: 0.125678  [ 1300/ 3448]
loss: 0.005684  [ 1400/ 3448]
loss: 0.021504  [ 1500/ 3448]
loss: 0.004687  [ 1600/ 3448]
loss: 0.009288  [ 1700/ 3448]
loss: 0.007356  [ 1800/ 3448]
loss: 0.018722  [ 1900/ 3448]
loss: 0.006746  [ 2000/ 3448]
loss: 0.003237  [ 2100/ 3448]
loss: 0.004457  [ 2200/ 3448]
loss: 0.006122  [ 2300/ 3448]
loss: 0.009257  [ 2400/ 3448]
loss: 0.007655  [ 2500/ 3448]
loss: 0.012809  [ 2600/ 3448]
loss: 0.007666  [ 2700/ 3448]
loss: 0.008498  [ 2800/ 3448]
loss: 0.001969  [ 2900/ 3448]
loss: 0.005437  [ 3000/ 3448]
loss: 0.014005  [ 3100/ 3448]
loss: 0.009322  [ 3200/ 3448]
loss: 0.011664  [ 3300/ 3448]
loss: 0.007273  [ 3400/ 3448]
Epoch 8
-------------------------------
loss: 0.015770  [    0/ 3448]
loss: 0.005067  [  100/ 3448]
loss: 0.005306  [  200/ 3448]
loss: 0.008551  [  300/ 3448]
loss: 0.004404  [  400/ 3448]
loss: 0.013512  [  500/ 3448]
loss: 0.007105  [  600/ 3448]
loss: 0.006018  [  700/ 3448]
loss: 0.005122  [  800/ 3448]
loss: 0.005768  [  900/ 3448]
loss: 0.083106  [ 1000/ 3448]
loss: 0.012424  [ 1100/ 3448]
loss: 0.007526  [ 1200/ 3448]
loss: 0.125628  [ 1300/ 3448]
loss: 0.005790  [ 1400/ 3448]
loss: 0.021172  [ 1500/ 3448]
loss: 0.004996  [ 1600/ 3448]
loss: 0.009501  [ 1700/ 3448]
loss: 0.007587  [ 1800/ 3448]
loss: 0.018403  [ 1900/ 3448]
loss: 0.006601  [ 2000/ 3448]
loss: 0.003011  [ 2100/ 3448]
loss: 0.004306  [ 2200/ 3448]
loss: 0.006162  [ 2300/ 3448]
loss: 0.009305  [ 2400/ 3448]
loss: 0.007665  [ 2500/ 3448]
loss: 0.012395  [ 2600/ 3448]
loss: 0.007450  [ 2700/ 3448]
loss: 0.007899  [ 2800/ 3448]
loss: 0.001681  [ 2900/ 3448]
loss: 0.005151  [ 3000/ 3448]
loss: 0.013039  [ 3100/ 3448]
loss: 0.009110  [ 3200/ 3448]
loss: 0.011192  [ 3300/ 3448]
loss: 0.007748  [ 3400/ 3448]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3448
First Spike after testing: [0.35340995 0.21009643]
[2 2 2 ... 1 0 2]
[1 1 1 ... 0 2 1]
Cluster 0 Occurrences: 1164; KMEANS: 1137
Cluster 1 Occurrences: 1155; KMEANS: 1179
Cluster 2 Occurrences: 1129; KMEANS: 1132
Centroids: [[0.25643238, -0.03395186], [-0.10272425, 0.36546746], [0.48265088, 0.34581366]]
Centroids: [[-0.13951276, 0.3755658], [0.51972055, 0.3302781], [0.23907164, -0.038337175]]
Contingency Matrix: 
[[  26   75 1063]
 [1059   61   35]
 [  52 1043   34]]
[[-1, -1, -1], [1059, 61, -1], [52, 1043, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, 1043, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 1: 0, 2: 1}
New Contingency Matrix: 
[[1063   26   75]
 [  35 1059   61]
 [  34   52 1043]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1063, 1059, 1043], Sum: 3165
All_Elements: [1063, 26, 75, 35, 1059, 61, 34, 52, 1043], Sum: 3448
Accuracy: 0.91792343387471
Done!
