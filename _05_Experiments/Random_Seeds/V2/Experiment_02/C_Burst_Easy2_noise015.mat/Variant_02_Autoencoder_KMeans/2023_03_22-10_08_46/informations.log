Experiment_path: Random_Seeds//V2/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_46
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002DC10185860>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x000002DBE3E65D30>
Epoch 1
-------------------------------
loss: 0.218619  [    0/ 3442]
loss: 0.127295  [  100/ 3442]
loss: 0.048806  [  200/ 3442]
loss: 0.015555  [  300/ 3442]
loss: 0.034692  [  400/ 3442]
loss: 0.030481  [  500/ 3442]
loss: 0.030841  [  600/ 3442]
loss: 0.015963  [  700/ 3442]
loss: 0.016874  [  800/ 3442]
loss: 0.043415  [  900/ 3442]
loss: 0.019696  [ 1000/ 3442]
loss: 0.004338  [ 1100/ 3442]
loss: 0.018925  [ 1200/ 3442]
loss: 0.012299  [ 1300/ 3442]
loss: 0.017590  [ 1400/ 3442]
loss: 0.020341  [ 1500/ 3442]
loss: 0.016069  [ 1600/ 3442]
loss: 0.012211  [ 1700/ 3442]
loss: 0.020505  [ 1800/ 3442]
loss: 0.029909  [ 1900/ 3442]
loss: 0.020277  [ 2000/ 3442]
loss: 0.015865  [ 2100/ 3442]
loss: 0.016772  [ 2200/ 3442]
loss: 0.012414  [ 2300/ 3442]
loss: 0.012172  [ 2400/ 3442]
loss: 0.033913  [ 2500/ 3442]
loss: 0.127583  [ 2600/ 3442]
loss: 0.029436  [ 2700/ 3442]
loss: 0.009590  [ 2800/ 3442]
loss: 0.007168  [ 2900/ 3442]
loss: 0.006154  [ 3000/ 3442]
loss: 0.018079  [ 3100/ 3442]
loss: 0.124279  [ 3200/ 3442]
loss: 0.012830  [ 3300/ 3442]
loss: 0.021835  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.012864  [    0/ 3442]
loss: 0.011810  [  100/ 3442]
loss: 0.029939  [  200/ 3442]
loss: 0.006702  [  300/ 3442]
loss: 0.015577  [  400/ 3442]
loss: 0.008710  [  500/ 3442]
loss: 0.025897  [  600/ 3442]
loss: 0.012812  [  700/ 3442]
loss: 0.015717  [  800/ 3442]
loss: 0.029068  [  900/ 3442]
loss: 0.006656  [ 1000/ 3442]
loss: 0.003895  [ 1100/ 3442]
loss: 0.014411  [ 1200/ 3442]
loss: 0.009461  [ 1300/ 3442]
loss: 0.014971  [ 1400/ 3442]
loss: 0.019010  [ 1500/ 3442]
loss: 0.018210  [ 1600/ 3442]
loss: 0.010667  [ 1700/ 3442]
loss: 0.016291  [ 1800/ 3442]
loss: 0.019281  [ 1900/ 3442]
loss: 0.016067  [ 2000/ 3442]
loss: 0.012156  [ 2100/ 3442]
loss: 0.014842  [ 2200/ 3442]
loss: 0.011595  [ 2300/ 3442]
loss: 0.012263  [ 2400/ 3442]
loss: 0.033423  [ 2500/ 3442]
loss: 0.125304  [ 2600/ 3442]
loss: 0.030857  [ 2700/ 3442]
loss: 0.008667  [ 2800/ 3442]
loss: 0.008465  [ 2900/ 3442]
loss: 0.005168  [ 3000/ 3442]
loss: 0.017510  [ 3100/ 3442]
loss: 0.121372  [ 3200/ 3442]
loss: 0.014132  [ 3300/ 3442]
loss: 0.019054  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.011628  [    0/ 3442]
loss: 0.011248  [  100/ 3442]
loss: 0.030044  [  200/ 3442]
loss: 0.006735  [  300/ 3442]
loss: 0.017949  [  400/ 3442]
loss: 0.008927  [  500/ 3442]
loss: 0.024404  [  600/ 3442]
loss: 0.012810  [  700/ 3442]
loss: 0.017129  [  800/ 3442]
loss: 0.026818  [  900/ 3442]
loss: 0.007126  [ 1000/ 3442]
loss: 0.003967  [ 1100/ 3442]
loss: 0.014695  [ 1200/ 3442]
loss: 0.009107  [ 1300/ 3442]
loss: 0.013304  [ 1400/ 3442]
loss: 0.018538  [ 1500/ 3442]
loss: 0.018530  [ 1600/ 3442]
loss: 0.010902  [ 1700/ 3442]
loss: 0.015311  [ 1800/ 3442]
loss: 0.016808  [ 1900/ 3442]
loss: 0.015671  [ 2000/ 3442]
loss: 0.011895  [ 2100/ 3442]
loss: 0.014172  [ 2200/ 3442]
loss: 0.011492  [ 2300/ 3442]
loss: 0.012247  [ 2400/ 3442]
loss: 0.034066  [ 2500/ 3442]
loss: 0.125852  [ 2600/ 3442]
loss: 0.030625  [ 2700/ 3442]
loss: 0.008371  [ 2800/ 3442]
loss: 0.009391  [ 2900/ 3442]
loss: 0.005199  [ 3000/ 3442]
loss: 0.017545  [ 3100/ 3442]
loss: 0.120467  [ 3200/ 3442]
loss: 0.014290  [ 3300/ 3442]
loss: 0.018263  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.010735  [    0/ 3442]
loss: 0.011183  [  100/ 3442]
loss: 0.030358  [  200/ 3442]
loss: 0.006722  [  300/ 3442]
loss: 0.018550  [  400/ 3442]
loss: 0.009207  [  500/ 3442]
loss: 0.023528  [  600/ 3442]
loss: 0.012898  [  700/ 3442]
loss: 0.017551  [  800/ 3442]
loss: 0.025869  [  900/ 3442]
loss: 0.008163  [ 1000/ 3442]
loss: 0.003787  [ 1100/ 3442]
loss: 0.015294  [ 1200/ 3442]
loss: 0.008774  [ 1300/ 3442]
loss: 0.013059  [ 1400/ 3442]
loss: 0.018351  [ 1500/ 3442]
loss: 0.018082  [ 1600/ 3442]
loss: 0.010910  [ 1700/ 3442]
loss: 0.014677  [ 1800/ 3442]
loss: 0.015366  [ 1900/ 3442]
loss: 0.015729  [ 2000/ 3442]
loss: 0.011682  [ 2100/ 3442]
loss: 0.013690  [ 2200/ 3442]
loss: 0.011385  [ 2300/ 3442]
loss: 0.012074  [ 2400/ 3442]
loss: 0.034517  [ 2500/ 3442]
loss: 0.126241  [ 2600/ 3442]
loss: 0.030226  [ 2700/ 3442]
loss: 0.008040  [ 2800/ 3442]
loss: 0.009649  [ 2900/ 3442]
loss: 0.005241  [ 3000/ 3442]
loss: 0.017463  [ 3100/ 3442]
loss: 0.120756  [ 3200/ 3442]
loss: 0.014116  [ 3300/ 3442]
loss: 0.018116  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.010266  [    0/ 3442]
loss: 0.011203  [  100/ 3442]
loss: 0.030391  [  200/ 3442]
loss: 0.006795  [  300/ 3442]
loss: 0.018129  [  400/ 3442]
loss: 0.009339  [  500/ 3442]
loss: 0.023139  [  600/ 3442]
loss: 0.013016  [  700/ 3442]
loss: 0.017442  [  800/ 3442]
loss: 0.026177  [  900/ 3442]
loss: 0.009139  [ 1000/ 3442]
loss: 0.003914  [ 1100/ 3442]
loss: 0.015879  [ 1200/ 3442]
loss: 0.008446  [ 1300/ 3442]
loss: 0.012931  [ 1400/ 3442]
loss: 0.018176  [ 1500/ 3442]
loss: 0.017835  [ 1600/ 3442]
loss: 0.010679  [ 1700/ 3442]
loss: 0.014284  [ 1800/ 3442]
loss: 0.014972  [ 1900/ 3442]
loss: 0.015574  [ 2000/ 3442]
loss: 0.011523  [ 2100/ 3442]
loss: 0.013282  [ 2200/ 3442]
loss: 0.011219  [ 2300/ 3442]
loss: 0.011663  [ 2400/ 3442]
loss: 0.033955  [ 2500/ 3442]
loss: 0.126436  [ 2600/ 3442]
loss: 0.030380  [ 2700/ 3442]
loss: 0.007727  [ 2800/ 3442]
loss: 0.009669  [ 2900/ 3442]
loss: 0.005279  [ 3000/ 3442]
loss: 0.017332  [ 3100/ 3442]
loss: 0.120994  [ 3200/ 3442]
loss: 0.013752  [ 3300/ 3442]
loss: 0.018005  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.010240  [    0/ 3442]
loss: 0.011302  [  100/ 3442]
loss: 0.030350  [  200/ 3442]
loss: 0.006832  [  300/ 3442]
loss: 0.017112  [  400/ 3442]
loss: 0.009375  [  500/ 3442]
loss: 0.022948  [  600/ 3442]
loss: 0.013035  [  700/ 3442]
loss: 0.017319  [  800/ 3442]
loss: 0.027111  [  900/ 3442]
loss: 0.009504  [ 1000/ 3442]
loss: 0.003850  [ 1100/ 3442]
loss: 0.016339  [ 1200/ 3442]
loss: 0.008026  [ 1300/ 3442]
loss: 0.013320  [ 1400/ 3442]
loss: 0.017961  [ 1500/ 3442]
loss: 0.017538  [ 1600/ 3442]
loss: 0.010379  [ 1700/ 3442]
loss: 0.013842  [ 1800/ 3442]
loss: 0.014538  [ 1900/ 3442]
loss: 0.015498  [ 2000/ 3442]
loss: 0.011180  [ 2100/ 3442]
loss: 0.012914  [ 2200/ 3442]
loss: 0.011142  [ 2300/ 3442]
loss: 0.011059  [ 2400/ 3442]
loss: 0.033167  [ 2500/ 3442]
loss: 0.126383  [ 2600/ 3442]
loss: 0.030987  [ 2700/ 3442]
loss: 0.007527  [ 2800/ 3442]
loss: 0.009561  [ 2900/ 3442]
loss: 0.005307  [ 3000/ 3442]
loss: 0.017221  [ 3100/ 3442]
loss: 0.120954  [ 3200/ 3442]
loss: 0.012928  [ 3300/ 3442]
loss: 0.018372  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.011797  [    0/ 3442]
loss: 0.011562  [  100/ 3442]
loss: 0.030092  [  200/ 3442]
loss: 0.006696  [  300/ 3442]
loss: 0.016272  [  400/ 3442]
loss: 0.009365  [  500/ 3442]
loss: 0.022789  [  600/ 3442]
loss: 0.013078  [  700/ 3442]
loss: 0.017233  [  800/ 3442]
loss: 0.028121  [  900/ 3442]
loss: 0.009753  [ 1000/ 3442]
loss: 0.003900  [ 1100/ 3442]
loss: 0.016863  [ 1200/ 3442]
loss: 0.007394  [ 1300/ 3442]
loss: 0.012994  [ 1400/ 3442]
loss: 0.018004  [ 1500/ 3442]
loss: 0.017117  [ 1600/ 3442]
loss: 0.010113  [ 1700/ 3442]
loss: 0.013464  [ 1800/ 3442]
loss: 0.014207  [ 1900/ 3442]
loss: 0.015314  [ 2000/ 3442]
loss: 0.010852  [ 2100/ 3442]
loss: 0.012485  [ 2200/ 3442]
loss: 0.010918  [ 2300/ 3442]
loss: 0.010379  [ 2400/ 3442]
loss: 0.032302  [ 2500/ 3442]
loss: 0.126229  [ 2600/ 3442]
loss: 0.031831  [ 2700/ 3442]
loss: 0.007368  [ 2800/ 3442]
loss: 0.009328  [ 2900/ 3442]
loss: 0.005360  [ 3000/ 3442]
loss: 0.017118  [ 3100/ 3442]
loss: 0.121522  [ 3200/ 3442]
loss: 0.012246  [ 3300/ 3442]
loss: 0.019156  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.011378  [    0/ 3442]
loss: 0.011829  [  100/ 3442]
loss: 0.029133  [  200/ 3442]
loss: 0.006649  [  300/ 3442]
loss: 0.015438  [  400/ 3442]
loss: 0.009408  [  500/ 3442]
loss: 0.022654  [  600/ 3442]
loss: 0.013090  [  700/ 3442]
loss: 0.017119  [  800/ 3442]
loss: 0.029195  [  900/ 3442]
loss: 0.009891  [ 1000/ 3442]
loss: 0.003986  [ 1100/ 3442]
loss: 0.017359  [ 1200/ 3442]
loss: 0.007042  [ 1300/ 3442]
loss: 0.012779  [ 1400/ 3442]
loss: 0.017804  [ 1500/ 3442]
loss: 0.016727  [ 1600/ 3442]
loss: 0.009899  [ 1700/ 3442]
loss: 0.013655  [ 1800/ 3442]
loss: 0.014068  [ 1900/ 3442]
loss: 0.015222  [ 2000/ 3442]
loss: 0.010533  [ 2100/ 3442]
loss: 0.012138  [ 2200/ 3442]
loss: 0.010897  [ 2300/ 3442]
loss: 0.009694  [ 2400/ 3442]
loss: 0.031641  [ 2500/ 3442]
loss: 0.125525  [ 2600/ 3442]
loss: 0.032772  [ 2700/ 3442]
loss: 0.007197  [ 2800/ 3442]
loss: 0.009018  [ 2900/ 3442]
loss: 0.005448  [ 3000/ 3442]
loss: 0.016934  [ 3100/ 3442]
loss: 0.123034  [ 3200/ 3442]
loss: 0.011667  [ 3300/ 3442]
loss: 0.020154  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [ 0.7364748  -0.09857929]
[1 0 0 ... 1 2 0]
[1 2 2 ... 1 0 1]
Cluster 0 Occurrences: 1159; KMEANS: 1118
Cluster 1 Occurrences: 1156; KMEANS: 1097
Cluster 2 Occurrences: 1127; KMEANS: 1227
Centroids: [[0.7752794, -0.92007935], [0.27935728, -0.45938173], [-0.16080508, 2.0629961]]
Centroids: [[-0.16937742, 2.0817287], [0.24458385, -0.43054393], [0.78346694, -0.91889685]]
Contingency Matrix: 
[[   1   18 1140]
 [   6 1067   83]
 [1111   12    4]]
[[-1, -1, -1], [6, 1067, -1], [1111, 12, -1]]
[[-1, -1, -1], [-1, 1067, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 0, 1: 1}
New Contingency Matrix: 
[[1140   18    1]
 [  83 1067    6]
 [   4   12 1111]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1140, 1067, 1111], Sum: 3318
All_Elements: [1140, 18, 1, 83, 1067, 6, 4, 12, 1111], Sum: 3442
Accuracy: 0.9639744334689134
Done!
