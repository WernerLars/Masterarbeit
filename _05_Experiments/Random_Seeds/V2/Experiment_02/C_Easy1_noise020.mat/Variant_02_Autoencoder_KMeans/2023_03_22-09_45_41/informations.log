Experiment_path: Random_Seeds//V2/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_41
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002DBE34ED2E8>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x000002DBE3E65F60>
Epoch 1
-------------------------------
loss: 0.135251  [    0/ 3474]
loss: 0.095816  [  100/ 3474]
loss: 0.114658  [  200/ 3474]
loss: 0.105717  [  300/ 3474]
loss: 0.270443  [  400/ 3474]
loss: 0.011009  [  500/ 3474]
loss: 0.046057  [  600/ 3474]
loss: 0.018310  [  700/ 3474]
loss: 0.048975  [  800/ 3474]
loss: 0.215990  [  900/ 3474]
loss: 0.045304  [ 1000/ 3474]
loss: 0.143976  [ 1100/ 3474]
loss: 0.066057  [ 1200/ 3474]
loss: 0.082686  [ 1300/ 3474]
loss: 0.038983  [ 1400/ 3474]
loss: 0.095644  [ 1500/ 3474]
loss: 0.012398  [ 1600/ 3474]
loss: 0.108390  [ 1700/ 3474]
loss: 0.029944  [ 1800/ 3474]
loss: 0.018449  [ 1900/ 3474]
loss: 0.021129  [ 2000/ 3474]
loss: 0.017789  [ 2100/ 3474]
loss: 0.022802  [ 2200/ 3474]
loss: 0.076687  [ 2300/ 3474]
loss: 0.027637  [ 2400/ 3474]
loss: 0.018713  [ 2500/ 3474]
loss: 0.028544  [ 2600/ 3474]
loss: 0.100197  [ 2700/ 3474]
loss: 0.025144  [ 2800/ 3474]
loss: 0.019565  [ 2900/ 3474]
loss: 0.316619  [ 3000/ 3474]
loss: 0.010496  [ 3100/ 3474]
loss: 0.040422  [ 3200/ 3474]
loss: 0.027373  [ 3300/ 3474]
loss: 0.040943  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.043252  [    0/ 3474]
loss: 0.010225  [  100/ 3474]
loss: 0.018092  [  200/ 3474]
loss: 0.056779  [  300/ 3474]
loss: 0.109606  [  400/ 3474]
loss: 0.008185  [  500/ 3474]
loss: 0.028219  [  600/ 3474]
loss: 0.011251  [  700/ 3474]
loss: 0.022491  [  800/ 3474]
loss: 0.188096  [  900/ 3474]
loss: 0.019315  [ 1000/ 3474]
loss: 0.194645  [ 1100/ 3474]
loss: 0.037695  [ 1200/ 3474]
loss: 0.079860  [ 1300/ 3474]
loss: 0.033023  [ 1400/ 3474]
loss: 0.107682  [ 1500/ 3474]
loss: 0.010755  [ 1600/ 3474]
loss: 0.049659  [ 1700/ 3474]
loss: 0.032716  [ 1800/ 3474]
loss: 0.016478  [ 1900/ 3474]
loss: 0.015229  [ 2000/ 3474]
loss: 0.017436  [ 2100/ 3474]
loss: 0.026670  [ 2200/ 3474]
loss: 0.071837  [ 2300/ 3474]
loss: 0.010357  [ 2400/ 3474]
loss: 0.015535  [ 2500/ 3474]
loss: 0.021549  [ 2600/ 3474]
loss: 0.050340  [ 2700/ 3474]
loss: 0.020417  [ 2800/ 3474]
loss: 0.020566  [ 2900/ 3474]
loss: 0.311016  [ 3000/ 3474]
loss: 0.010517  [ 3100/ 3474]
loss: 0.018684  [ 3200/ 3474]
loss: 0.022473  [ 3300/ 3474]
loss: 0.030986  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.030618  [    0/ 3474]
loss: 0.010935  [  100/ 3474]
loss: 0.018736  [  200/ 3474]
loss: 0.056908  [  300/ 3474]
loss: 0.074075  [  400/ 3474]
loss: 0.010791  [  500/ 3474]
loss: 0.024243  [  600/ 3474]
loss: 0.009505  [  700/ 3474]
loss: 0.032520  [  800/ 3474]
loss: 0.188794  [  900/ 3474]
loss: 0.013934  [ 1000/ 3474]
loss: 0.177507  [ 1100/ 3474]
loss: 0.030095  [ 1200/ 3474]
loss: 0.061426  [ 1300/ 3474]
loss: 0.025534  [ 1400/ 3474]
loss: 0.102739  [ 1500/ 3474]
loss: 0.008564  [ 1600/ 3474]
loss: 0.032193  [ 1700/ 3474]
loss: 0.034272  [ 1800/ 3474]
loss: 0.014812  [ 1900/ 3474]
loss: 0.013102  [ 2000/ 3474]
loss: 0.016085  [ 2100/ 3474]
loss: 0.026704  [ 2200/ 3474]
loss: 0.071705  [ 2300/ 3474]
loss: 0.010370  [ 2400/ 3474]
loss: 0.014488  [ 2500/ 3474]
loss: 0.019113  [ 2600/ 3474]
loss: 0.046386  [ 2700/ 3474]
loss: 0.019227  [ 2800/ 3474]
loss: 0.021423  [ 2900/ 3474]
loss: 0.300328  [ 3000/ 3474]
loss: 0.010260  [ 3100/ 3474]
loss: 0.015804  [ 3200/ 3474]
loss: 0.025904  [ 3300/ 3474]
loss: 0.024482  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.032561  [    0/ 3474]
loss: 0.010562  [  100/ 3474]
loss: 0.019010  [  200/ 3474]
loss: 0.057584  [  300/ 3474]
loss: 0.060646  [  400/ 3474]
loss: 0.012489  [  500/ 3474]
loss: 0.022506  [  600/ 3474]
loss: 0.009843  [  700/ 3474]
loss: 0.036305  [  800/ 3474]
loss: 0.184208  [  900/ 3474]
loss: 0.016419  [ 1000/ 3474]
loss: 0.171180  [ 1100/ 3474]
loss: 0.029003  [ 1200/ 3474]
loss: 0.045758  [ 1300/ 3474]
loss: 0.019494  [ 1400/ 3474]
loss: 0.097866  [ 1500/ 3474]
loss: 0.008642  [ 1600/ 3474]
loss: 0.028636  [ 1700/ 3474]
loss: 0.034518  [ 1800/ 3474]
loss: 0.015597  [ 1900/ 3474]
loss: 0.012342  [ 2000/ 3474]
loss: 0.015563  [ 2100/ 3474]
loss: 0.026261  [ 2200/ 3474]
loss: 0.072537  [ 2300/ 3474]
loss: 0.010129  [ 2400/ 3474]
loss: 0.013587  [ 2500/ 3474]
loss: 0.017168  [ 2600/ 3474]
loss: 0.043394  [ 2700/ 3474]
loss: 0.018214  [ 2800/ 3474]
loss: 0.021218  [ 2900/ 3474]
loss: 0.306547  [ 3000/ 3474]
loss: 0.010134  [ 3100/ 3474]
loss: 0.015669  [ 3200/ 3474]
loss: 0.031289  [ 3300/ 3474]
loss: 0.020125  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.034239  [    0/ 3474]
loss: 0.009704  [  100/ 3474]
loss: 0.019228  [  200/ 3474]
loss: 0.057520  [  300/ 3474]
loss: 0.053556  [  400/ 3474]
loss: 0.012175  [  500/ 3474]
loss: 0.022614  [  600/ 3474]
loss: 0.009621  [  700/ 3474]
loss: 0.035872  [  800/ 3474]
loss: 0.177565  [  900/ 3474]
loss: 0.014457  [ 1000/ 3474]
loss: 0.182396  [ 1100/ 3474]
loss: 0.029975  [ 1200/ 3474]
loss: 0.036318  [ 1300/ 3474]
loss: 0.015268  [ 1400/ 3474]
loss: 0.096744  [ 1500/ 3474]
loss: 0.009373  [ 1600/ 3474]
loss: 0.026285  [ 1700/ 3474]
loss: 0.033012  [ 1800/ 3474]
loss: 0.016810  [ 1900/ 3474]
loss: 0.011626  [ 2000/ 3474]
loss: 0.015372  [ 2100/ 3474]
loss: 0.025617  [ 2200/ 3474]
loss: 0.074351  [ 2300/ 3474]
loss: 0.009781  [ 2400/ 3474]
loss: 0.012757  [ 2500/ 3474]
loss: 0.016717  [ 2600/ 3474]
loss: 0.040207  [ 2700/ 3474]
loss: 0.017262  [ 2800/ 3474]
loss: 0.020370  [ 2900/ 3474]
loss: 0.311065  [ 3000/ 3474]
loss: 0.010110  [ 3100/ 3474]
loss: 0.015186  [ 3200/ 3474]
loss: 0.035529  [ 3300/ 3474]
loss: 0.018211  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.038335  [    0/ 3474]
loss: 0.009211  [  100/ 3474]
loss: 0.020981  [  200/ 3474]
loss: 0.057356  [  300/ 3474]
loss: 0.050724  [  400/ 3474]
loss: 0.011034  [  500/ 3474]
loss: 0.022919  [  600/ 3474]
loss: 0.009439  [  700/ 3474]
loss: 0.033776  [  800/ 3474]
loss: 0.174721  [  900/ 3474]
loss: 0.016579  [ 1000/ 3474]
loss: 0.192701  [ 1100/ 3474]
loss: 0.030030  [ 1200/ 3474]
loss: 0.032963  [ 1300/ 3474]
loss: 0.012861  [ 1400/ 3474]
loss: 0.096405  [ 1500/ 3474]
loss: 0.010337  [ 1600/ 3474]
loss: 0.023899  [ 1700/ 3474]
loss: 0.029581  [ 1800/ 3474]
loss: 0.018417  [ 1900/ 3474]
loss: 0.011045  [ 2000/ 3474]
loss: 0.015051  [ 2100/ 3474]
loss: 0.025684  [ 2200/ 3474]
loss: 0.074362  [ 2300/ 3474]
loss: 0.009575  [ 2400/ 3474]
loss: 0.012307  [ 2500/ 3474]
loss: 0.016568  [ 2600/ 3474]
loss: 0.036793  [ 2700/ 3474]
loss: 0.016885  [ 2800/ 3474]
loss: 0.019593  [ 2900/ 3474]
loss: 0.305475  [ 3000/ 3474]
loss: 0.010178  [ 3100/ 3474]
loss: 0.014636  [ 3200/ 3474]
loss: 0.039159  [ 3300/ 3474]
loss: 0.015816  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.041207  [    0/ 3474]
loss: 0.008580  [  100/ 3474]
loss: 0.022819  [  200/ 3474]
loss: 0.057722  [  300/ 3474]
loss: 0.055255  [  400/ 3474]
loss: 0.010227  [  500/ 3474]
loss: 0.023188  [  600/ 3474]
loss: 0.009523  [  700/ 3474]
loss: 0.032497  [  800/ 3474]
loss: 0.174894  [  900/ 3474]
loss: 0.016026  [ 1000/ 3474]
loss: 0.184831  [ 1100/ 3474]
loss: 0.029615  [ 1200/ 3474]
loss: 0.031276  [ 1300/ 3474]
loss: 0.011310  [ 1400/ 3474]
loss: 0.097704  [ 1500/ 3474]
loss: 0.010915  [ 1600/ 3474]
loss: 0.024557  [ 1700/ 3474]
loss: 0.025965  [ 1800/ 3474]
loss: 0.019646  [ 1900/ 3474]
loss: 0.010831  [ 2000/ 3474]
loss: 0.014997  [ 2100/ 3474]
loss: 0.026339  [ 2200/ 3474]
loss: 0.074748  [ 2300/ 3474]
loss: 0.009433  [ 2400/ 3474]
loss: 0.012359  [ 2500/ 3474]
loss: 0.016481  [ 2600/ 3474]
loss: 0.033315  [ 2700/ 3474]
loss: 0.016594  [ 2800/ 3474]
loss: 0.019173  [ 2900/ 3474]
loss: 0.305925  [ 3000/ 3474]
loss: 0.010501  [ 3100/ 3474]
loss: 0.014211  [ 3200/ 3474]
loss: 0.042196  [ 3300/ 3474]
loss: 0.013922  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.041833  [    0/ 3474]
loss: 0.008413  [  100/ 3474]
loss: 0.024167  [  200/ 3474]
loss: 0.057625  [  300/ 3474]
loss: 0.061296  [  400/ 3474]
loss: 0.009744  [  500/ 3474]
loss: 0.022135  [  600/ 3474]
loss: 0.009656  [  700/ 3474]
loss: 0.032412  [  800/ 3474]
loss: 0.178916  [  900/ 3474]
loss: 0.016066  [ 1000/ 3474]
loss: 0.187022  [ 1100/ 3474]
loss: 0.028715  [ 1200/ 3474]
loss: 0.031028  [ 1300/ 3474]
loss: 0.010703  [ 1400/ 3474]
loss: 0.098328  [ 1500/ 3474]
loss: 0.010917  [ 1600/ 3474]
loss: 0.022641  [ 1700/ 3474]
loss: 0.023042  [ 1800/ 3474]
loss: 0.019698  [ 1900/ 3474]
loss: 0.010654  [ 2000/ 3474]
loss: 0.014757  [ 2100/ 3474]
loss: 0.027107  [ 2200/ 3474]
loss: 0.071157  [ 2300/ 3474]
loss: 0.009410  [ 2400/ 3474]
loss: 0.011894  [ 2500/ 3474]
loss: 0.017285  [ 2600/ 3474]
loss: 0.032020  [ 2700/ 3474]
loss: 0.017518  [ 2800/ 3474]
loss: 0.019300  [ 2900/ 3474]
loss: 0.306632  [ 3000/ 3474]
loss: 0.010861  [ 3100/ 3474]
loss: 0.013680  [ 3200/ 3474]
loss: 0.043916  [ 3300/ 3474]
loss: 0.011679  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [-0.4166937  1.4946873]
[0 1 1 ... 1 1 2]
[0 1 1 ... 1 1 2]
Cluster 0 Occurrences: 1198; KMEANS: 1179
Cluster 1 Occurrences: 1128; KMEANS: 1115
Cluster 2 Occurrences: 1148; KMEANS: 1180
Centroids: [[-0.31055513, 0.8504174], [0.20200062, -2.0477152], [1.6104518, 0.67868596]]
Centroids: [[-0.34295344, 0.8526224], [0.19708285, -2.0698173], [1.6010213, 0.67009556]]
Contingency Matrix: 
[[1172    0   26]
 [   6 1115    7]
 [   1    0 1147]]
[[-1, -1, -1], [-1, 1115, 7], [-1, 0, 1147]]
[[-1, -1, -1], [-1, 1115, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 2, 1: 1}
New Contingency Matrix: 
[[1172    0   26]
 [   6 1115    7]
 [   1    0 1147]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1172, 1115, 1147], Sum: 3434
All_Elements: [1172, 0, 26, 6, 1115, 7, 1, 0, 1147], Sum: 3474
Accuracy: 0.9884858952216465
Done!
