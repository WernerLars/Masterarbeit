Experiment_path: Random_Seeds//V2/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_24
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002DBE3E1D358>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x000002DBE3E65CF8>
Epoch 1
-------------------------------
loss: 0.171753  [    0/ 3477]
loss: 0.150032  [  100/ 3477]
loss: 0.076695  [  200/ 3477]
loss: 0.096359  [  300/ 3477]
loss: 0.033896  [  400/ 3477]
loss: 0.081787  [  500/ 3477]
loss: 0.018013  [  600/ 3477]
loss: 0.023452  [  700/ 3477]
loss: 0.022500  [  800/ 3477]
loss: 0.023547  [  900/ 3477]
loss: 0.024371  [ 1000/ 3477]
loss: 0.004520  [ 1100/ 3477]
loss: 0.023876  [ 1200/ 3477]
loss: 0.018596  [ 1300/ 3477]
loss: 0.014313  [ 1400/ 3477]
loss: 0.023831  [ 1500/ 3477]
loss: 0.014892  [ 1600/ 3477]
loss: 0.011738  [ 1700/ 3477]
loss: 0.004713  [ 1800/ 3477]
loss: 0.019458  [ 1900/ 3477]
loss: 0.024982  [ 2000/ 3477]
loss: 0.022425  [ 2100/ 3477]
loss: 0.021221  [ 2200/ 3477]
loss: 0.012914  [ 2300/ 3477]
loss: 0.012337  [ 2400/ 3477]
loss: 0.103803  [ 2500/ 3477]
loss: 0.076656  [ 2600/ 3477]
loss: 0.010542  [ 2700/ 3477]
loss: 0.040026  [ 2800/ 3477]
loss: 0.019388  [ 2900/ 3477]
loss: 0.019446  [ 3000/ 3477]
loss: 0.014122  [ 3100/ 3477]
loss: 0.102956  [ 3200/ 3477]
loss: 0.008873  [ 3300/ 3477]
loss: 0.007061  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.025056  [    0/ 3477]
loss: 0.032080  [  100/ 3477]
loss: 0.035649  [  200/ 3477]
loss: 0.028991  [  300/ 3477]
loss: 0.017281  [  400/ 3477]
loss: 0.006741  [  500/ 3477]
loss: 0.012186  [  600/ 3477]
loss: 0.021439  [  700/ 3477]
loss: 0.020642  [  800/ 3477]
loss: 0.022514  [  900/ 3477]
loss: 0.020368  [ 1000/ 3477]
loss: 0.004205  [ 1100/ 3477]
loss: 0.021644  [ 1200/ 3477]
loss: 0.013110  [ 1300/ 3477]
loss: 0.013932  [ 1400/ 3477]
loss: 0.018183  [ 1500/ 3477]
loss: 0.010463  [ 1600/ 3477]
loss: 0.010674  [ 1700/ 3477]
loss: 0.005556  [ 1800/ 3477]
loss: 0.017324  [ 1900/ 3477]
loss: 0.026081  [ 2000/ 3477]
loss: 0.014491  [ 2100/ 3477]
loss: 0.021652  [ 2200/ 3477]
loss: 0.012815  [ 2300/ 3477]
loss: 0.011326  [ 2400/ 3477]
loss: 0.106061  [ 2500/ 3477]
loss: 0.074633  [ 2600/ 3477]
loss: 0.009333  [ 2700/ 3477]
loss: 0.049179  [ 2800/ 3477]
loss: 0.021061  [ 2900/ 3477]
loss: 0.017431  [ 3000/ 3477]
loss: 0.012811  [ 3100/ 3477]
loss: 0.078311  [ 3200/ 3477]
loss: 0.008814  [ 3300/ 3477]
loss: 0.008502  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.029529  [    0/ 3477]
loss: 0.022383  [  100/ 3477]
loss: 0.033095  [  200/ 3477]
loss: 0.028268  [  300/ 3477]
loss: 0.017950  [  400/ 3477]
loss: 0.005543  [  500/ 3477]
loss: 0.011343  [  600/ 3477]
loss: 0.020102  [  700/ 3477]
loss: 0.018601  [  800/ 3477]
loss: 0.020945  [  900/ 3477]
loss: 0.019508  [ 1000/ 3477]
loss: 0.003024  [ 1100/ 3477]
loss: 0.020685  [ 1200/ 3477]
loss: 0.005531  [ 1300/ 3477]
loss: 0.012346  [ 1400/ 3477]
loss: 0.011165  [ 1500/ 3477]
loss: 0.010462  [ 1600/ 3477]
loss: 0.010932  [ 1700/ 3477]
loss: 0.007785  [ 1800/ 3477]
loss: 0.012995  [ 1900/ 3477]
loss: 0.023934  [ 2000/ 3477]
loss: 0.012551  [ 2100/ 3477]
loss: 0.021166  [ 2200/ 3477]
loss: 0.014150  [ 2300/ 3477]
loss: 0.009967  [ 2400/ 3477]
loss: 0.097210  [ 2500/ 3477]
loss: 0.074794  [ 2600/ 3477]
loss: 0.006136  [ 2700/ 3477]
loss: 0.040089  [ 2800/ 3477]
loss: 0.021234  [ 2900/ 3477]
loss: 0.014568  [ 3000/ 3477]
loss: 0.012921  [ 3100/ 3477]
loss: 0.077008  [ 3200/ 3477]
loss: 0.008061  [ 3300/ 3477]
loss: 0.007925  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.026253  [    0/ 3477]
loss: 0.015882  [  100/ 3477]
loss: 0.028747  [  200/ 3477]
loss: 0.023923  [  300/ 3477]
loss: 0.017644  [  400/ 3477]
loss: 0.006872  [  500/ 3477]
loss: 0.010756  [  600/ 3477]
loss: 0.019340  [  700/ 3477]
loss: 0.017722  [  800/ 3477]
loss: 0.019442  [  900/ 3477]
loss: 0.020180  [ 1000/ 3477]
loss: 0.003791  [ 1100/ 3477]
loss: 0.020883  [ 1200/ 3477]
loss: 0.006649  [ 1300/ 3477]
loss: 0.012481  [ 1400/ 3477]
loss: 0.011966  [ 1500/ 3477]
loss: 0.011206  [ 1600/ 3477]
loss: 0.010939  [ 1700/ 3477]
loss: 0.010712  [ 1800/ 3477]
loss: 0.011192  [ 1900/ 3477]
loss: 0.023232  [ 2000/ 3477]
loss: 0.011518  [ 2100/ 3477]
loss: 0.021726  [ 2200/ 3477]
loss: 0.014131  [ 2300/ 3477]
loss: 0.009229  [ 2400/ 3477]
loss: 0.095755  [ 2500/ 3477]
loss: 0.073530  [ 2600/ 3477]
loss: 0.005207  [ 2700/ 3477]
loss: 0.038157  [ 2800/ 3477]
loss: 0.021265  [ 2900/ 3477]
loss: 0.013487  [ 3000/ 3477]
loss: 0.013045  [ 3100/ 3477]
loss: 0.078455  [ 3200/ 3477]
loss: 0.007681  [ 3300/ 3477]
loss: 0.008100  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.022615  [    0/ 3477]
loss: 0.015630  [  100/ 3477]
loss: 0.026117  [  200/ 3477]
loss: 0.023964  [  300/ 3477]
loss: 0.017884  [  400/ 3477]
loss: 0.007083  [  500/ 3477]
loss: 0.010187  [  600/ 3477]
loss: 0.018962  [  700/ 3477]
loss: 0.018053  [  800/ 3477]
loss: 0.019881  [  900/ 3477]
loss: 0.018747  [ 1000/ 3477]
loss: 0.002620  [ 1100/ 3477]
loss: 0.020101  [ 1200/ 3477]
loss: 0.006907  [ 1300/ 3477]
loss: 0.012393  [ 1400/ 3477]
loss: 0.012659  [ 1500/ 3477]
loss: 0.011370  [ 1600/ 3477]
loss: 0.010895  [ 1700/ 3477]
loss: 0.011142  [ 1800/ 3477]
loss: 0.010399  [ 1900/ 3477]
loss: 0.023002  [ 2000/ 3477]
loss: 0.010389  [ 2100/ 3477]
loss: 0.022113  [ 2200/ 3477]
loss: 0.014076  [ 2300/ 3477]
loss: 0.008913  [ 2400/ 3477]
loss: 0.094325  [ 2500/ 3477]
loss: 0.074169  [ 2600/ 3477]
loss: 0.004170  [ 2700/ 3477]
loss: 0.035548  [ 2800/ 3477]
loss: 0.021167  [ 2900/ 3477]
loss: 0.013250  [ 3000/ 3477]
loss: 0.013215  [ 3100/ 3477]
loss: 0.078942  [ 3200/ 3477]
loss: 0.007296  [ 3300/ 3477]
loss: 0.008603  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.020104  [    0/ 3477]
loss: 0.016078  [  100/ 3477]
loss: 0.024678  [  200/ 3477]
loss: 0.024735  [  300/ 3477]
loss: 0.018463  [  400/ 3477]
loss: 0.007635  [  500/ 3477]
loss: 0.009623  [  600/ 3477]
loss: 0.019339  [  700/ 3477]
loss: 0.018282  [  800/ 3477]
loss: 0.019474  [  900/ 3477]
loss: 0.018148  [ 1000/ 3477]
loss: 0.002756  [ 1100/ 3477]
loss: 0.019454  [ 1200/ 3477]
loss: 0.006682  [ 1300/ 3477]
loss: 0.012583  [ 1400/ 3477]
loss: 0.012920  [ 1500/ 3477]
loss: 0.011155  [ 1600/ 3477]
loss: 0.010812  [ 1700/ 3477]
loss: 0.011437  [ 1800/ 3477]
loss: 0.010018  [ 1900/ 3477]
loss: 0.022929  [ 2000/ 3477]
loss: 0.009947  [ 2100/ 3477]
loss: 0.022176  [ 2200/ 3477]
loss: 0.014035  [ 2300/ 3477]
loss: 0.008882  [ 2400/ 3477]
loss: 0.094449  [ 2500/ 3477]
loss: 0.074348  [ 2600/ 3477]
loss: 0.004226  [ 2700/ 3477]
loss: 0.037275  [ 2800/ 3477]
loss: 0.021114  [ 2900/ 3477]
loss: 0.012992  [ 3000/ 3477]
loss: 0.013508  [ 3100/ 3477]
loss: 0.078194  [ 3200/ 3477]
loss: 0.007180  [ 3300/ 3477]
loss: 0.008377  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.019441  [    0/ 3477]
loss: 0.015973  [  100/ 3477]
loss: 0.023818  [  200/ 3477]
loss: 0.024657  [  300/ 3477]
loss: 0.018083  [  400/ 3477]
loss: 0.007694  [  500/ 3477]
loss: 0.009582  [  600/ 3477]
loss: 0.019237  [  700/ 3477]
loss: 0.018317  [  800/ 3477]
loss: 0.019413  [  900/ 3477]
loss: 0.018026  [ 1000/ 3477]
loss: 0.002591  [ 1100/ 3477]
loss: 0.018813  [ 1200/ 3477]
loss: 0.006448  [ 1300/ 3477]
loss: 0.012786  [ 1400/ 3477]
loss: 0.013057  [ 1500/ 3477]
loss: 0.011036  [ 1600/ 3477]
loss: 0.010816  [ 1700/ 3477]
loss: 0.011625  [ 1800/ 3477]
loss: 0.009777  [ 1900/ 3477]
loss: 0.022840  [ 2000/ 3477]
loss: 0.009171  [ 2100/ 3477]
loss: 0.022342  [ 2200/ 3477]
loss: 0.013916  [ 2300/ 3477]
loss: 0.008740  [ 2400/ 3477]
loss: 0.094337  [ 2500/ 3477]
loss: 0.075132  [ 2600/ 3477]
loss: 0.004352  [ 2700/ 3477]
loss: 0.035670  [ 2800/ 3477]
loss: 0.021000  [ 2900/ 3477]
loss: 0.012722  [ 3000/ 3477]
loss: 0.013709  [ 3100/ 3477]
loss: 0.079633  [ 3200/ 3477]
loss: 0.007068  [ 3300/ 3477]
loss: 0.008246  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.019064  [    0/ 3477]
loss: 0.015835  [  100/ 3477]
loss: 0.023882  [  200/ 3477]
loss: 0.025148  [  300/ 3477]
loss: 0.017092  [  400/ 3477]
loss: 0.007626  [  500/ 3477]
loss: 0.009476  [  600/ 3477]
loss: 0.018970  [  700/ 3477]
loss: 0.018276  [  800/ 3477]
loss: 0.019001  [  900/ 3477]
loss: 0.017416  [ 1000/ 3477]
loss: 0.002297  [ 1100/ 3477]
loss: 0.018280  [ 1200/ 3477]
loss: 0.006286  [ 1300/ 3477]
loss: 0.012835  [ 1400/ 3477]
loss: 0.013163  [ 1500/ 3477]
loss: 0.010735  [ 1600/ 3477]
loss: 0.010714  [ 1700/ 3477]
loss: 0.011846  [ 1800/ 3477]
loss: 0.009493  [ 1900/ 3477]
loss: 0.022746  [ 2000/ 3477]
loss: 0.008908  [ 2100/ 3477]
loss: 0.022360  [ 2200/ 3477]
loss: 0.013868  [ 2300/ 3477]
loss: 0.008799  [ 2400/ 3477]
loss: 0.094638  [ 2500/ 3477]
loss: 0.075675  [ 2600/ 3477]
loss: 0.004573  [ 2700/ 3477]
loss: 0.031474  [ 2800/ 3477]
loss: 0.021141  [ 2900/ 3477]
loss: 0.012679  [ 3000/ 3477]
loss: 0.013846  [ 3100/ 3477]
loss: 0.080670  [ 3200/ 3477]
loss: 0.007116  [ 3300/ 3477]
loss: 0.008037  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [ 0.3875663 -2.1509225]
[1 0 2 ... 1 1 1]
[1 2 0 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1174
Cluster 1 Occurrences: 1188; KMEANS: 1180
Cluster 2 Occurrences: 1157; KMEANS: 1123
Centroids: [[-0.341046, 0.8236055], [0.54869896, -2.1042838], [1.4875802, 0.7491338]]
Centroids: [[1.4810824, 0.74680763], [0.548975, -2.1204886], [-0.3558867, 0.8233342]]
Contingency Matrix: 
[[  13    0 1119]
 [   9 1175    4]
 [1152    5    0]]
[[13, -1, 1119], [-1, -1, -1], [1152, -1, 0]]
[[-1, -1, 1119], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1119    0   13]
 [   4 1175    9]
 [   0    5 1152]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1119, 1175, 1152], Sum: 3446
All_Elements: [1119, 0, 13, 4, 1175, 9, 0, 5, 1152], Sum: 3477
Accuracy: 0.9910842680471671
Done!
