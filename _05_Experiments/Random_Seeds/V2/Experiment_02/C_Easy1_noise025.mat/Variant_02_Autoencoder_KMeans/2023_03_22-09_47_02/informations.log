Experiment_path: Random_Seeds//V2/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_02
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002DBE3E1DC88>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x000002DBE3E65128>
Epoch 1
-------------------------------
loss: 0.192681  [    0/ 3298]
loss: 0.194900  [  100/ 3298]
loss: 0.122405  [  200/ 3298]
loss: 0.039382  [  300/ 3298]
loss: 0.020009  [  400/ 3298]
loss: 0.028346  [  500/ 3298]
loss: 0.043965  [  600/ 3298]
loss: 0.093295  [  700/ 3298]
loss: 0.026451  [  800/ 3298]
loss: 0.090250  [  900/ 3298]
loss: 0.033093  [ 1000/ 3298]
loss: 0.085355  [ 1100/ 3298]
loss: 0.073536  [ 1200/ 3298]
loss: 0.031360  [ 1300/ 3298]
loss: 0.011072  [ 1400/ 3298]
loss: 0.037176  [ 1500/ 3298]
loss: 0.025214  [ 1600/ 3298]
loss: 0.173563  [ 1700/ 3298]
loss: 0.022703  [ 1800/ 3298]
loss: 0.024798  [ 1900/ 3298]
loss: 0.011469  [ 2000/ 3298]
loss: 0.014284  [ 2100/ 3298]
loss: 0.027342  [ 2200/ 3298]
loss: 0.018792  [ 2300/ 3298]
loss: 0.028929  [ 2400/ 3298]
loss: 0.017714  [ 2500/ 3298]
loss: 0.023258  [ 2600/ 3298]
loss: 0.045631  [ 2700/ 3298]
loss: 0.375058  [ 2800/ 3298]
loss: 0.030249  [ 2900/ 3298]
loss: 0.036220  [ 3000/ 3298]
loss: 0.019104  [ 3100/ 3298]
loss: 0.047280  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.049206  [    0/ 3298]
loss: 0.016063  [  100/ 3298]
loss: 0.106195  [  200/ 3298]
loss: 0.019928  [  300/ 3298]
loss: 0.016297  [  400/ 3298]
loss: 0.022028  [  500/ 3298]
loss: 0.030357  [  600/ 3298]
loss: 0.061374  [  700/ 3298]
loss: 0.022643  [  800/ 3298]
loss: 0.089863  [  900/ 3298]
loss: 0.034084  [ 1000/ 3298]
loss: 0.097790  [ 1100/ 3298]
loss: 0.075130  [ 1200/ 3298]
loss: 0.025686  [ 1300/ 3298]
loss: 0.009769  [ 1400/ 3298]
loss: 0.028201  [ 1500/ 3298]
loss: 0.024998  [ 1600/ 3298]
loss: 0.139052  [ 1700/ 3298]
loss: 0.024927  [ 1800/ 3298]
loss: 0.021474  [ 1900/ 3298]
loss: 0.009840  [ 2000/ 3298]
loss: 0.014433  [ 2100/ 3298]
loss: 0.020095  [ 2200/ 3298]
loss: 0.018470  [ 2300/ 3298]
loss: 0.021770  [ 2400/ 3298]
loss: 0.011176  [ 2500/ 3298]
loss: 0.025178  [ 2600/ 3298]
loss: 0.046806  [ 2700/ 3298]
loss: 0.358336  [ 2800/ 3298]
loss: 0.027872  [ 2900/ 3298]
loss: 0.036683  [ 3000/ 3298]
loss: 0.019852  [ 3100/ 3298]
loss: 0.051473  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.045032  [    0/ 3298]
loss: 0.017438  [  100/ 3298]
loss: 0.106317  [  200/ 3298]
loss: 0.019445  [  300/ 3298]
loss: 0.017087  [  400/ 3298]
loss: 0.022378  [  500/ 3298]
loss: 0.028923  [  600/ 3298]
loss: 0.061912  [  700/ 3298]
loss: 0.023618  [  800/ 3298]
loss: 0.091211  [  900/ 3298]
loss: 0.034992  [ 1000/ 3298]
loss: 0.099179  [ 1100/ 3298]
loss: 0.075504  [ 1200/ 3298]
loss: 0.024462  [ 1300/ 3298]
loss: 0.009287  [ 1400/ 3298]
loss: 0.026805  [ 1500/ 3298]
loss: 0.022397  [ 1600/ 3298]
loss: 0.123020  [ 1700/ 3298]
loss: 0.026287  [ 1800/ 3298]
loss: 0.021443  [ 1900/ 3298]
loss: 0.009722  [ 2000/ 3298]
loss: 0.014270  [ 2100/ 3298]
loss: 0.019360  [ 2200/ 3298]
loss: 0.017951  [ 2300/ 3298]
loss: 0.021557  [ 2400/ 3298]
loss: 0.011051  [ 2500/ 3298]
loss: 0.026320  [ 2600/ 3298]
loss: 0.046281  [ 2700/ 3298]
loss: 0.504195  [ 2800/ 3298]
loss: 0.027536  [ 2900/ 3298]
loss: 0.036100  [ 3000/ 3298]
loss: 0.020106  [ 3100/ 3298]
loss: 0.051936  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.042783  [    0/ 3298]
loss: 0.018065  [  100/ 3298]
loss: 0.105652  [  200/ 3298]
loss: 0.018673  [  300/ 3298]
loss: 0.019770  [  400/ 3298]
loss: 0.021333  [  500/ 3298]
loss: 0.029245  [  600/ 3298]
loss: 0.062104  [  700/ 3298]
loss: 0.024660  [  800/ 3298]
loss: 0.097479  [  900/ 3298]
loss: 0.035737  [ 1000/ 3298]
loss: 0.099171  [ 1100/ 3298]
loss: 0.076937  [ 1200/ 3298]
loss: 0.023976  [ 1300/ 3298]
loss: 0.009133  [ 1400/ 3298]
loss: 0.025937  [ 1500/ 3298]
loss: 0.020579  [ 1600/ 3298]
loss: 0.120924  [ 1700/ 3298]
loss: 0.026508  [ 1800/ 3298]
loss: 0.021314  [ 1900/ 3298]
loss: 0.009128  [ 2000/ 3298]
loss: 0.013982  [ 2100/ 3298]
loss: 0.019044  [ 2200/ 3298]
loss: 0.017225  [ 2300/ 3298]
loss: 0.021769  [ 2400/ 3298]
loss: 0.012072  [ 2500/ 3298]
loss: 0.026673  [ 2600/ 3298]
loss: 0.044211  [ 2700/ 3298]
loss: 0.545982  [ 2800/ 3298]
loss: 0.026653  [ 2900/ 3298]
loss: 0.035039  [ 3000/ 3298]
loss: 0.020536  [ 3100/ 3298]
loss: 0.051152  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.041996  [    0/ 3298]
loss: 0.017973  [  100/ 3298]
loss: 0.105118  [  200/ 3298]
loss: 0.016304  [  300/ 3298]
loss: 0.020864  [  400/ 3298]
loss: 0.019582  [  500/ 3298]
loss: 0.028742  [  600/ 3298]
loss: 0.062356  [  700/ 3298]
loss: 0.024033  [  800/ 3298]
loss: 0.097372  [  900/ 3298]
loss: 0.035724  [ 1000/ 3298]
loss: 0.095817  [ 1100/ 3298]
loss: 0.078056  [ 1200/ 3298]
loss: 0.023674  [ 1300/ 3298]
loss: 0.009574  [ 1400/ 3298]
loss: 0.025093  [ 1500/ 3298]
loss: 0.019333  [ 1600/ 3298]
loss: 0.119246  [ 1700/ 3298]
loss: 0.026435  [ 1800/ 3298]
loss: 0.021117  [ 1900/ 3298]
loss: 0.009055  [ 2000/ 3298]
loss: 0.013309  [ 2100/ 3298]
loss: 0.018771  [ 2200/ 3298]
loss: 0.016731  [ 2300/ 3298]
loss: 0.021596  [ 2400/ 3298]
loss: 0.013137  [ 2500/ 3298]
loss: 0.026591  [ 2600/ 3298]
loss: 0.043110  [ 2700/ 3298]
loss: 0.530434  [ 2800/ 3298]
loss: 0.026144  [ 2900/ 3298]
loss: 0.035075  [ 3000/ 3298]
loss: 0.020582  [ 3100/ 3298]
loss: 0.050169  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.039710  [    0/ 3298]
loss: 0.018805  [  100/ 3298]
loss: 0.104039  [  200/ 3298]
loss: 0.015908  [  300/ 3298]
loss: 0.020274  [  400/ 3298]
loss: 0.018945  [  500/ 3298]
loss: 0.028841  [  600/ 3298]
loss: 0.062634  [  700/ 3298]
loss: 0.023353  [  800/ 3298]
loss: 0.093988  [  900/ 3298]
loss: 0.035479  [ 1000/ 3298]
loss: 0.089437  [ 1100/ 3298]
loss: 0.077829  [ 1200/ 3298]
loss: 0.023814  [ 1300/ 3298]
loss: 0.009711  [ 1400/ 3298]
loss: 0.022494  [ 1500/ 3298]
loss: 0.017781  [ 1600/ 3298]
loss: 0.103285  [ 1700/ 3298]
loss: 0.028711  [ 1800/ 3298]
loss: 0.021294  [ 1900/ 3298]
loss: 0.009463  [ 2000/ 3298]
loss: 0.012986  [ 2100/ 3298]
loss: 0.018568  [ 2200/ 3298]
loss: 0.016814  [ 2300/ 3298]
loss: 0.021424  [ 2400/ 3298]
loss: 0.013929  [ 2500/ 3298]
loss: 0.025457  [ 2600/ 3298]
loss: 0.042799  [ 2700/ 3298]
loss: 0.485709  [ 2800/ 3298]
loss: 0.027066  [ 2900/ 3298]
loss: 0.035615  [ 3000/ 3298]
loss: 0.019703  [ 3100/ 3298]
loss: 0.050058  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.035314  [    0/ 3298]
loss: 0.020085  [  100/ 3298]
loss: 0.103716  [  200/ 3298]
loss: 0.017265  [  300/ 3298]
loss: 0.019325  [  400/ 3298]
loss: 0.018109  [  500/ 3298]
loss: 0.028272  [  600/ 3298]
loss: 0.062705  [  700/ 3298]
loss: 0.022907  [  800/ 3298]
loss: 0.090505  [  900/ 3298]
loss: 0.035371  [ 1000/ 3298]
loss: 0.080207  [ 1100/ 3298]
loss: 0.077546  [ 1200/ 3298]
loss: 0.023616  [ 1300/ 3298]
loss: 0.009990  [ 1400/ 3298]
loss: 0.022686  [ 1500/ 3298]
loss: 0.016355  [ 1600/ 3298]
loss: 0.091466  [ 1700/ 3298]
loss: 0.027881  [ 1800/ 3298]
loss: 0.021123  [ 1900/ 3298]
loss: 0.009663  [ 2000/ 3298]
loss: 0.013647  [ 2100/ 3298]
loss: 0.018433  [ 2200/ 3298]
loss: 0.017310  [ 2300/ 3298]
loss: 0.021538  [ 2400/ 3298]
loss: 0.014512  [ 2500/ 3298]
loss: 0.026071  [ 2600/ 3298]
loss: 0.042312  [ 2700/ 3298]
loss: 0.392913  [ 2800/ 3298]
loss: 0.028275  [ 2900/ 3298]
loss: 0.034594  [ 3000/ 3298]
loss: 0.019159  [ 3100/ 3298]
loss: 0.048847  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.027131  [    0/ 3298]
loss: 0.021100  [  100/ 3298]
loss: 0.104033  [  200/ 3298]
loss: 0.017738  [  300/ 3298]
loss: 0.017491  [  400/ 3298]
loss: 0.017877  [  500/ 3298]
loss: 0.027685  [  600/ 3298]
loss: 0.062709  [  700/ 3298]
loss: 0.022876  [  800/ 3298]
loss: 0.081343  [  900/ 3298]
loss: 0.033926  [ 1000/ 3298]
loss: 0.066868  [ 1100/ 3298]
loss: 0.076713  [ 1200/ 3298]
loss: 0.023658  [ 1300/ 3298]
loss: 0.010834  [ 1400/ 3298]
loss: 0.023282  [ 1500/ 3298]
loss: 0.015036  [ 1600/ 3298]
loss: 0.071240  [ 1700/ 3298]
loss: 0.026699  [ 1800/ 3298]
loss: 0.020924  [ 1900/ 3298]
loss: 0.009607  [ 2000/ 3298]
loss: 0.014227  [ 2100/ 3298]
loss: 0.018409  [ 2200/ 3298]
loss: 0.017033  [ 2300/ 3298]
loss: 0.021532  [ 2400/ 3298]
loss: 0.014135  [ 2500/ 3298]
loss: 0.027265  [ 2600/ 3298]
loss: 0.042056  [ 2700/ 3298]
loss: 0.330287  [ 2800/ 3298]
loss: 0.027742  [ 2900/ 3298]
loss: 0.034755  [ 3000/ 3298]
loss: 0.018746  [ 3100/ 3298]
loss: 0.048547  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [-0.3245209 -3.0176122]
[1 0 0 ... 0 1 2]
[0 2 2 ... 2 0 1]
Cluster 0 Occurrences: 1094; KMEANS: 1093
Cluster 1 Occurrences: 1089; KMEANS: 1099
Cluster 2 Occurrences: 1115; KMEANS: 1106
Centroids: [[0.04658852, 0.24652004], [-0.10440485, -2.3141618], [0.7479528, 1.1183202]]
Centroids: [[-0.10453921, -2.3318098], [0.7695624, 1.1643275], [0.035940893, 0.24011733]]
Contingency Matrix: 
[[   0   20 1074]
 [1080    1    8]
 [  13 1078   24]]
[[-1, 20, 1074], [-1, -1, -1], [-1, 1078, 24]]
[[-1, -1, 1074], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 2: 1, 0: 2}
New Contingency Matrix: 
[[1074    0   20]
 [   8 1080    1]
 [  24   13 1078]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1074, 1080, 1078], Sum: 3232
All_Elements: [1074, 0, 20, 8, 1080, 1, 24, 13, 1078], Sum: 3298
Accuracy: 0.9799878714372346
Done!
