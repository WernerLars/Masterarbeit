Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_07_07
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000212101B9080>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x0000021209B12AC8>
Epoch 1
-------------------------------
loss: 0.354792  [    0/ 3493]
loss: 0.122699  [  100/ 3493]
loss: 0.043961  [  200/ 3493]
loss: 0.100995  [  300/ 3493]
loss: 0.099970  [  400/ 3493]
loss: 0.058451  [  500/ 3493]
loss: 0.055497  [  600/ 3493]
loss: 0.191719  [  700/ 3493]
loss: 0.051211  [  800/ 3493]
loss: 0.027643  [  900/ 3493]
loss: 0.041627  [ 1000/ 3493]
loss: 0.054831  [ 1100/ 3493]
loss: 0.023479  [ 1200/ 3493]
loss: 0.013244  [ 1300/ 3493]
loss: 0.030972  [ 1400/ 3493]
loss: 0.025754  [ 1500/ 3493]
loss: 0.025614  [ 1600/ 3493]
loss: 0.028415  [ 1700/ 3493]
loss: 0.084618  [ 1800/ 3493]
loss: 0.007954  [ 1900/ 3493]
loss: 0.009853  [ 2000/ 3493]
loss: 0.012485  [ 2100/ 3493]
loss: 0.011979  [ 2200/ 3493]
loss: 0.027801  [ 2300/ 3493]
loss: 0.012088  [ 2400/ 3493]
loss: 0.070900  [ 2500/ 3493]
loss: 0.004530  [ 2600/ 3493]
loss: 0.010688  [ 2700/ 3493]
loss: 0.024004  [ 2800/ 3493]
loss: 0.007386  [ 2900/ 3493]
loss: 0.037506  [ 3000/ 3493]
loss: 0.014627  [ 3100/ 3493]
loss: 0.034914  [ 3200/ 3493]
loss: 0.021241  [ 3300/ 3493]
loss: 0.008808  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.059874  [    0/ 3493]
loss: 0.015973  [  100/ 3493]
loss: 0.025959  [  200/ 3493]
loss: 0.019777  [  300/ 3493]
loss: 0.060005  [  400/ 3493]
loss: 0.019248  [  500/ 3493]
loss: 0.050455  [  600/ 3493]
loss: 0.176428  [  700/ 3493]
loss: 0.015144  [  800/ 3493]
loss: 0.017119  [  900/ 3493]
loss: 0.035047  [ 1000/ 3493]
loss: 0.021847  [ 1100/ 3493]
loss: 0.019046  [ 1200/ 3493]
loss: 0.011962  [ 1300/ 3493]
loss: 0.026038  [ 1400/ 3493]
loss: 0.054691  [ 1500/ 3493]
loss: 0.030841  [ 1600/ 3493]
loss: 0.021064  [ 1700/ 3493]
loss: 0.050336  [ 1800/ 3493]
loss: 0.012604  [ 1900/ 3493]
loss: 0.010339  [ 2000/ 3493]
loss: 0.019429  [ 2100/ 3493]
loss: 0.016903  [ 2200/ 3493]
loss: 0.024534  [ 2300/ 3493]
loss: 0.014373  [ 2400/ 3493]
loss: 0.085911  [ 2500/ 3493]
loss: 0.004096  [ 2600/ 3493]
loss: 0.010836  [ 2700/ 3493]
loss: 0.020548  [ 2800/ 3493]
loss: 0.007313  [ 2900/ 3493]
loss: 0.038909  [ 3000/ 3493]
loss: 0.013617  [ 3100/ 3493]
loss: 0.030496  [ 3200/ 3493]
loss: 0.021976  [ 3300/ 3493]
loss: 0.010693  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.052804  [    0/ 3493]
loss: 0.017492  [  100/ 3493]
loss: 0.023072  [  200/ 3493]
loss: 0.018319  [  300/ 3493]
loss: 0.059217  [  400/ 3493]
loss: 0.014671  [  500/ 3493]
loss: 0.050999  [  600/ 3493]
loss: 0.161970  [  700/ 3493]
loss: 0.011984  [  800/ 3493]
loss: 0.016406  [  900/ 3493]
loss: 0.032552  [ 1000/ 3493]
loss: 0.021261  [ 1100/ 3493]
loss: 0.019190  [ 1200/ 3493]
loss: 0.012516  [ 1300/ 3493]
loss: 0.022137  [ 1400/ 3493]
loss: 0.059309  [ 1500/ 3493]
loss: 0.028868  [ 1600/ 3493]
loss: 0.023299  [ 1700/ 3493]
loss: 0.047731  [ 1800/ 3493]
loss: 0.013363  [ 1900/ 3493]
loss: 0.011593  [ 2000/ 3493]
loss: 0.019758  [ 2100/ 3493]
loss: 0.015777  [ 2200/ 3493]
loss: 0.023358  [ 2300/ 3493]
loss: 0.014698  [ 2400/ 3493]
loss: 0.087466  [ 2500/ 3493]
loss: 0.005842  [ 2600/ 3493]
loss: 0.011638  [ 2700/ 3493]
loss: 0.019751  [ 2800/ 3493]
loss: 0.007606  [ 2900/ 3493]
loss: 0.037366  [ 3000/ 3493]
loss: 0.013750  [ 3100/ 3493]
loss: 0.030500  [ 3200/ 3493]
loss: 0.022585  [ 3300/ 3493]
loss: 0.011295  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.054004  [    0/ 3493]
loss: 0.018796  [  100/ 3493]
loss: 0.021528  [  200/ 3493]
loss: 0.017987  [  300/ 3493]
loss: 0.058636  [  400/ 3493]
loss: 0.012646  [  500/ 3493]
loss: 0.051519  [  600/ 3493]
loss: 0.155046  [  700/ 3493]
loss: 0.010943  [  800/ 3493]
loss: 0.015903  [  900/ 3493]
loss: 0.031557  [ 1000/ 3493]
loss: 0.022058  [ 1100/ 3493]
loss: 0.019015  [ 1200/ 3493]
loss: 0.013385  [ 1300/ 3493]
loss: 0.019990  [ 1400/ 3493]
loss: 0.059651  [ 1500/ 3493]
loss: 0.028584  [ 1600/ 3493]
loss: 0.024024  [ 1700/ 3493]
loss: 0.045322  [ 1800/ 3493]
loss: 0.013879  [ 1900/ 3493]
loss: 0.012115  [ 2000/ 3493]
loss: 0.019742  [ 2100/ 3493]
loss: 0.015366  [ 2200/ 3493]
loss: 0.022778  [ 2300/ 3493]
loss: 0.014913  [ 2400/ 3493]
loss: 0.087641  [ 2500/ 3493]
loss: 0.006736  [ 2600/ 3493]
loss: 0.012435  [ 2700/ 3493]
loss: 0.020011  [ 2800/ 3493]
loss: 0.007657  [ 2900/ 3493]
loss: 0.037098  [ 3000/ 3493]
loss: 0.014041  [ 3100/ 3493]
loss: 0.030556  [ 3200/ 3493]
loss: 0.023055  [ 3300/ 3493]
loss: 0.011203  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.056069  [    0/ 3493]
loss: 0.019115  [  100/ 3493]
loss: 0.020868  [  200/ 3493]
loss: 0.017315  [  300/ 3493]
loss: 0.059067  [  400/ 3493]
loss: 0.011546  [  500/ 3493]
loss: 0.051760  [  600/ 3493]
loss: 0.151911  [  700/ 3493]
loss: 0.010152  [  800/ 3493]
loss: 0.015975  [  900/ 3493]
loss: 0.031028  [ 1000/ 3493]
loss: 0.022145  [ 1100/ 3493]
loss: 0.018889  [ 1200/ 3493]
loss: 0.013665  [ 1300/ 3493]
loss: 0.018875  [ 1400/ 3493]
loss: 0.055859  [ 1500/ 3493]
loss: 0.028335  [ 1600/ 3493]
loss: 0.024080  [ 1700/ 3493]
loss: 0.043160  [ 1800/ 3493]
loss: 0.014130  [ 1900/ 3493]
loss: 0.012277  [ 2000/ 3493]
loss: 0.020392  [ 2100/ 3493]
loss: 0.015299  [ 2200/ 3493]
loss: 0.022405  [ 2300/ 3493]
loss: 0.014911  [ 2400/ 3493]
loss: 0.086523  [ 2500/ 3493]
loss: 0.006765  [ 2600/ 3493]
loss: 0.013131  [ 2700/ 3493]
loss: 0.019983  [ 2800/ 3493]
loss: 0.007779  [ 2900/ 3493]
loss: 0.037303  [ 3000/ 3493]
loss: 0.013917  [ 3100/ 3493]
loss: 0.030126  [ 3200/ 3493]
loss: 0.023283  [ 3300/ 3493]
loss: 0.011258  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.057999  [    0/ 3493]
loss: 0.018874  [  100/ 3493]
loss: 0.020475  [  200/ 3493]
loss: 0.016432  [  300/ 3493]
loss: 0.058944  [  400/ 3493]
loss: 0.011105  [  500/ 3493]
loss: 0.052199  [  600/ 3493]
loss: 0.151071  [  700/ 3493]
loss: 0.009781  [  800/ 3493]
loss: 0.015880  [  900/ 3493]
loss: 0.030809  [ 1000/ 3493]
loss: 0.022436  [ 1100/ 3493]
loss: 0.018880  [ 1200/ 3493]
loss: 0.013534  [ 1300/ 3493]
loss: 0.018306  [ 1400/ 3493]
loss: 0.055176  [ 1500/ 3493]
loss: 0.028252  [ 1600/ 3493]
loss: 0.023872  [ 1700/ 3493]
loss: 0.042201  [ 1800/ 3493]
loss: 0.014427  [ 1900/ 3493]
loss: 0.012484  [ 2000/ 3493]
loss: 0.020764  [ 2100/ 3493]
loss: 0.015509  [ 2200/ 3493]
loss: 0.021968  [ 2300/ 3493]
loss: 0.014813  [ 2400/ 3493]
loss: 0.086332  [ 2500/ 3493]
loss: 0.006523  [ 2600/ 3493]
loss: 0.013600  [ 2700/ 3493]
loss: 0.019745  [ 2800/ 3493]
loss: 0.007895  [ 2900/ 3493]
loss: 0.039298  [ 3000/ 3493]
loss: 0.013843  [ 3100/ 3493]
loss: 0.029603  [ 3200/ 3493]
loss: 0.023465  [ 3300/ 3493]
loss: 0.011612  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.059084  [    0/ 3493]
loss: 0.018678  [  100/ 3493]
loss: 0.020186  [  200/ 3493]
loss: 0.015841  [  300/ 3493]
loss: 0.058688  [  400/ 3493]
loss: 0.011674  [  500/ 3493]
loss: 0.051995  [  600/ 3493]
loss: 0.149530  [  700/ 3493]
loss: 0.009549  [  800/ 3493]
loss: 0.015639  [  900/ 3493]
loss: 0.030469  [ 1000/ 3493]
loss: 0.022632  [ 1100/ 3493]
loss: 0.018809  [ 1200/ 3493]
loss: 0.013448  [ 1300/ 3493]
loss: 0.017605  [ 1400/ 3493]
loss: 0.054705  [ 1500/ 3493]
loss: 0.028196  [ 1600/ 3493]
loss: 0.023917  [ 1700/ 3493]
loss: 0.041553  [ 1800/ 3493]
loss: 0.014483  [ 1900/ 3493]
loss: 0.012875  [ 2000/ 3493]
loss: 0.021185  [ 2100/ 3493]
loss: 0.015229  [ 2200/ 3493]
loss: 0.021678  [ 2300/ 3493]
loss: 0.014759  [ 2400/ 3493]
loss: 0.085327  [ 2500/ 3493]
loss: 0.006523  [ 2600/ 3493]
loss: 0.013932  [ 2700/ 3493]
loss: 0.019145  [ 2800/ 3493]
loss: 0.008043  [ 2900/ 3493]
loss: 0.038762  [ 3000/ 3493]
loss: 0.013852  [ 3100/ 3493]
loss: 0.029477  [ 3200/ 3493]
loss: 0.023583  [ 3300/ 3493]
loss: 0.011601  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.059784  [    0/ 3493]
loss: 0.018664  [  100/ 3493]
loss: 0.020386  [  200/ 3493]
loss: 0.015457  [  300/ 3493]
loss: 0.058539  [  400/ 3493]
loss: 0.011499  [  500/ 3493]
loss: 0.051855  [  600/ 3493]
loss: 0.148680  [  700/ 3493]
loss: 0.009353  [  800/ 3493]
loss: 0.015414  [  900/ 3493]
loss: 0.030353  [ 1000/ 3493]
loss: 0.022651  [ 1100/ 3493]
loss: 0.018615  [ 1200/ 3493]
loss: 0.013514  [ 1300/ 3493]
loss: 0.017033  [ 1400/ 3493]
loss: 0.054675  [ 1500/ 3493]
loss: 0.028431  [ 1600/ 3493]
loss: 0.023758  [ 1700/ 3493]
loss: 0.041519  [ 1800/ 3493]
loss: 0.014705  [ 1900/ 3493]
loss: 0.012802  [ 2000/ 3493]
loss: 0.021488  [ 2100/ 3493]
loss: 0.014867  [ 2200/ 3493]
loss: 0.021308  [ 2300/ 3493]
loss: 0.014730  [ 2400/ 3493]
loss: 0.085733  [ 2500/ 3493]
loss: 0.006521  [ 2600/ 3493]
loss: 0.014048  [ 2700/ 3493]
loss: 0.018863  [ 2800/ 3493]
loss: 0.008041  [ 2900/ 3493]
loss: 0.038566  [ 3000/ 3493]
loss: 0.013866  [ 3100/ 3493]
loss: 0.029201  [ 3200/ 3493]
loss: 0.023680  [ 3300/ 3493]
loss: 0.011535  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [0.16080566 2.7262676 ]
[1 0 0 ... 1 0 2]
[2 1 1 ... 2 1 1]
Cluster 0 Occurrences: 1151; KMEANS: 661
Cluster 1 Occurrences: 1195; KMEANS: 2301
Cluster 2 Occurrences: 1147; KMEANS: 531
Centroids: [[-0.9088486, -0.09485038], [0.73162276, 2.7527504], [-0.831128, 0.32912648]]
Centroids: [[1.0047774, 3.1467407], [-0.87429327, 0.10974723], [0.41900486, 2.3076231]]
Contingency Matrix: 
[[   0 1149    2]
 [ 659   13  523]
 [   2 1139    6]]
[[-1, -1, -1], [659, -1, 523], [2, -1, 6]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 6]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 1, 1: 0, 2: 2}
New Contingency Matrix: 
[[1149    0    2]
 [  13  659  523]
 [1139    2    6]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1149, 659, 6], Sum: 1814
All_Elements: [1149, 0, 2, 13, 659, 523, 1139, 2, 6], Sum: 3493
Accuracy: 0.5193243630117378
Done!
