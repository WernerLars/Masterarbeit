Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_55_13
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211E8804BE0>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DA208>
Epoch 1
-------------------------------
loss: 0.180814  [    0/ 3526]
loss: 0.214440  [  100/ 3526]
loss: 0.129257  [  200/ 3526]
loss: 0.079299  [  300/ 3526]
loss: 0.018816  [  400/ 3526]
loss: 0.057413  [  500/ 3526]
loss: 0.036754  [  600/ 3526]
loss: 0.014669  [  700/ 3526]
loss: 0.048089  [  800/ 3526]
loss: 0.013275  [  900/ 3526]
loss: 0.035140  [ 1000/ 3526]
loss: 0.019891  [ 1100/ 3526]
loss: 0.048784  [ 1200/ 3526]
loss: 0.031942  [ 1300/ 3526]
loss: 0.028486  [ 1400/ 3526]
loss: 0.025547  [ 1500/ 3526]
loss: 0.043094  [ 1600/ 3526]
loss: 0.026127  [ 1700/ 3526]
loss: 0.027568  [ 1800/ 3526]
loss: 0.012752  [ 1900/ 3526]
loss: 0.016011  [ 2000/ 3526]
loss: 0.101822  [ 2100/ 3526]
loss: 0.019559  [ 2200/ 3526]
loss: 0.015110  [ 2300/ 3526]
loss: 0.008928  [ 2400/ 3526]
loss: 0.015515  [ 2500/ 3526]
loss: 0.016253  [ 2600/ 3526]
loss: 0.016767  [ 2700/ 3526]
loss: 0.008567  [ 2800/ 3526]
loss: 0.010400  [ 2900/ 3526]
loss: 0.060132  [ 3000/ 3526]
loss: 0.159113  [ 3100/ 3526]
loss: 0.030228  [ 3200/ 3526]
loss: 0.023270  [ 3300/ 3526]
loss: 0.041974  [ 3400/ 3526]
loss: 0.040603  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.015889  [    0/ 3526]
loss: 0.050323  [  100/ 3526]
loss: 0.051825  [  200/ 3526]
loss: 0.004659  [  300/ 3526]
loss: 0.015589  [  400/ 3526]
loss: 0.040477  [  500/ 3526]
loss: 0.009856  [  600/ 3526]
loss: 0.014528  [  700/ 3526]
loss: 0.032454  [  800/ 3526]
loss: 0.013071  [  900/ 3526]
loss: 0.018988  [ 1000/ 3526]
loss: 0.011920  [ 1100/ 3526]
loss: 0.031950  [ 1200/ 3526]
loss: 0.023644  [ 1300/ 3526]
loss: 0.026393  [ 1400/ 3526]
loss: 0.020618  [ 1500/ 3526]
loss: 0.044002  [ 1600/ 3526]
loss: 0.021757  [ 1700/ 3526]
loss: 0.020417  [ 1800/ 3526]
loss: 0.015247  [ 1900/ 3526]
loss: 0.015580  [ 2000/ 3526]
loss: 0.117776  [ 2100/ 3526]
loss: 0.019233  [ 2200/ 3526]
loss: 0.014859  [ 2300/ 3526]
loss: 0.008463  [ 2400/ 3526]
loss: 0.016474  [ 2500/ 3526]
loss: 0.013883  [ 2600/ 3526]
loss: 0.015398  [ 2700/ 3526]
loss: 0.008667  [ 2800/ 3526]
loss: 0.009599  [ 2900/ 3526]
loss: 0.051359  [ 3000/ 3526]
loss: 0.159466  [ 3100/ 3526]
loss: 0.027886  [ 3200/ 3526]
loss: 0.022194  [ 3300/ 3526]
loss: 0.043156  [ 3400/ 3526]
loss: 0.039489  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.014779  [    0/ 3526]
loss: 0.050285  [  100/ 3526]
loss: 0.051835  [  200/ 3526]
loss: 0.004405  [  300/ 3526]
loss: 0.015911  [  400/ 3526]
loss: 0.038651  [  500/ 3526]
loss: 0.009537  [  600/ 3526]
loss: 0.014256  [  700/ 3526]
loss: 0.033028  [  800/ 3526]
loss: 0.012645  [  900/ 3526]
loss: 0.018157  [ 1000/ 3526]
loss: 0.011799  [ 1100/ 3526]
loss: 0.030681  [ 1200/ 3526]
loss: 0.025088  [ 1300/ 3526]
loss: 0.026603  [ 1400/ 3526]
loss: 0.020090  [ 1500/ 3526]
loss: 0.043829  [ 1600/ 3526]
loss: 0.021504  [ 1700/ 3526]
loss: 0.020131  [ 1800/ 3526]
loss: 0.015696  [ 1900/ 3526]
loss: 0.015230  [ 2000/ 3526]
loss: 0.112565  [ 2100/ 3526]
loss: 0.019249  [ 2200/ 3526]
loss: 0.015529  [ 2300/ 3526]
loss: 0.008517  [ 2400/ 3526]
loss: 0.016377  [ 2500/ 3526]
loss: 0.013283  [ 2600/ 3526]
loss: 0.014759  [ 2700/ 3526]
loss: 0.008895  [ 2800/ 3526]
loss: 0.009607  [ 2900/ 3526]
loss: 0.048571  [ 3000/ 3526]
loss: 0.156909  [ 3100/ 3526]
loss: 0.027263  [ 3200/ 3526]
loss: 0.021323  [ 3300/ 3526]
loss: 0.042689  [ 3400/ 3526]
loss: 0.039428  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.013951  [    0/ 3526]
loss: 0.051101  [  100/ 3526]
loss: 0.051561  [  200/ 3526]
loss: 0.004407  [  300/ 3526]
loss: 0.015560  [  400/ 3526]
loss: 0.038466  [  500/ 3526]
loss: 0.009313  [  600/ 3526]
loss: 0.013518  [  700/ 3526]
loss: 0.033231  [  800/ 3526]
loss: 0.012321  [  900/ 3526]
loss: 0.018587  [ 1000/ 3526]
loss: 0.011545  [ 1100/ 3526]
loss: 0.029768  [ 1200/ 3526]
loss: 0.030089  [ 1300/ 3526]
loss: 0.025292  [ 1400/ 3526]
loss: 0.020008  [ 1500/ 3526]
loss: 0.043706  [ 1600/ 3526]
loss: 0.020582  [ 1700/ 3526]
loss: 0.020258  [ 1800/ 3526]
loss: 0.015491  [ 1900/ 3526]
loss: 0.014945  [ 2000/ 3526]
loss: 0.115928  [ 2100/ 3526]
loss: 0.019189  [ 2200/ 3526]
loss: 0.015560  [ 2300/ 3526]
loss: 0.008484  [ 2400/ 3526]
loss: 0.016322  [ 2500/ 3526]
loss: 0.013076  [ 2600/ 3526]
loss: 0.014518  [ 2700/ 3526]
loss: 0.009510  [ 2800/ 3526]
loss: 0.009714  [ 2900/ 3526]
loss: 0.047375  [ 3000/ 3526]
loss: 0.159095  [ 3100/ 3526]
loss: 0.027216  [ 3200/ 3526]
loss: 0.020704  [ 3300/ 3526]
loss: 0.040409  [ 3400/ 3526]
loss: 0.038843  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.013241  [    0/ 3526]
loss: 0.050230  [  100/ 3526]
loss: 0.051029  [  200/ 3526]
loss: 0.004603  [  300/ 3526]
loss: 0.014981  [  400/ 3526]
loss: 0.038214  [  500/ 3526]
loss: 0.009182  [  600/ 3526]
loss: 0.013240  [  700/ 3526]
loss: 0.033121  [  800/ 3526]
loss: 0.012176  [  900/ 3526]
loss: 0.018902  [ 1000/ 3526]
loss: 0.011466  [ 1100/ 3526]
loss: 0.029957  [ 1200/ 3526]
loss: 0.031159  [ 1300/ 3526]
loss: 0.024906  [ 1400/ 3526]
loss: 0.020194  [ 1500/ 3526]
loss: 0.043560  [ 1600/ 3526]
loss: 0.020364  [ 1700/ 3526]
loss: 0.020426  [ 1800/ 3526]
loss: 0.015499  [ 1900/ 3526]
loss: 0.014688  [ 2000/ 3526]
loss: 0.114982  [ 2100/ 3526]
loss: 0.019387  [ 2200/ 3526]
loss: 0.015432  [ 2300/ 3526]
loss: 0.008692  [ 2400/ 3526]
loss: 0.016112  [ 2500/ 3526]
loss: 0.012909  [ 2600/ 3526]
loss: 0.014255  [ 2700/ 3526]
loss: 0.008848  [ 2800/ 3526]
loss: 0.009918  [ 2900/ 3526]
loss: 0.046760  [ 3000/ 3526]
loss: 0.159392  [ 3100/ 3526]
loss: 0.027274  [ 3200/ 3526]
loss: 0.020410  [ 3300/ 3526]
loss: 0.037718  [ 3400/ 3526]
loss: 0.038419  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.012501  [    0/ 3526]
loss: 0.049014  [  100/ 3526]
loss: 0.050317  [  200/ 3526]
loss: 0.004923  [  300/ 3526]
loss: 0.014354  [  400/ 3526]
loss: 0.038369  [  500/ 3526]
loss: 0.009155  [  600/ 3526]
loss: 0.012859  [  700/ 3526]
loss: 0.032753  [  800/ 3526]
loss: 0.012137  [  900/ 3526]
loss: 0.019426  [ 1000/ 3526]
loss: 0.011698  [ 1100/ 3526]
loss: 0.029954  [ 1200/ 3526]
loss: 0.031310  [ 1300/ 3526]
loss: 0.024616  [ 1400/ 3526]
loss: 0.020072  [ 1500/ 3526]
loss: 0.044149  [ 1600/ 3526]
loss: 0.020458  [ 1700/ 3526]
loss: 0.020645  [ 1800/ 3526]
loss: 0.015587  [ 1900/ 3526]
loss: 0.014548  [ 2000/ 3526]
loss: 0.113250  [ 2100/ 3526]
loss: 0.020001  [ 2200/ 3526]
loss: 0.015347  [ 2300/ 3526]
loss: 0.008896  [ 2400/ 3526]
loss: 0.015845  [ 2500/ 3526]
loss: 0.013190  [ 2600/ 3526]
loss: 0.013997  [ 2700/ 3526]
loss: 0.008549  [ 2800/ 3526]
loss: 0.009833  [ 2900/ 3526]
loss: 0.046663  [ 3000/ 3526]
loss: 0.159793  [ 3100/ 3526]
loss: 0.027442  [ 3200/ 3526]
loss: 0.020319  [ 3300/ 3526]
loss: 0.035507  [ 3400/ 3526]
loss: 0.038296  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.011887  [    0/ 3526]
loss: 0.047477  [  100/ 3526]
loss: 0.049529  [  200/ 3526]
loss: 0.005321  [  300/ 3526]
loss: 0.013933  [  400/ 3526]
loss: 0.038992  [  500/ 3526]
loss: 0.009270  [  600/ 3526]
loss: 0.012221  [  700/ 3526]
loss: 0.032417  [  800/ 3526]
loss: 0.012109  [  900/ 3526]
loss: 0.019720  [ 1000/ 3526]
loss: 0.012024  [ 1100/ 3526]
loss: 0.030115  [ 1200/ 3526]
loss: 0.031543  [ 1300/ 3526]
loss: 0.024807  [ 1400/ 3526]
loss: 0.020024  [ 1500/ 3526]
loss: 0.044890  [ 1600/ 3526]
loss: 0.020341  [ 1700/ 3526]
loss: 0.020982  [ 1800/ 3526]
loss: 0.015624  [ 1900/ 3526]
loss: 0.014612  [ 2000/ 3526]
loss: 0.106325  [ 2100/ 3526]
loss: 0.020841  [ 2200/ 3526]
loss: 0.015393  [ 2300/ 3526]
loss: 0.008954  [ 2400/ 3526]
loss: 0.015550  [ 2500/ 3526]
loss: 0.013573  [ 2600/ 3526]
loss: 0.013810  [ 2700/ 3526]
loss: 0.008441  [ 2800/ 3526]
loss: 0.009814  [ 2900/ 3526]
loss: 0.046867  [ 3000/ 3526]
loss: 0.160975  [ 3100/ 3526]
loss: 0.027531  [ 3200/ 3526]
loss: 0.020377  [ 3300/ 3526]
loss: 0.033353  [ 3400/ 3526]
loss: 0.038456  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.011455  [    0/ 3526]
loss: 0.046259  [  100/ 3526]
loss: 0.048367  [  200/ 3526]
loss: 0.005581  [  300/ 3526]
loss: 0.013676  [  400/ 3526]
loss: 0.038963  [  500/ 3526]
loss: 0.009223  [  600/ 3526]
loss: 0.011794  [  700/ 3526]
loss: 0.032267  [  800/ 3526]
loss: 0.012152  [  900/ 3526]
loss: 0.019466  [ 1000/ 3526]
loss: 0.012437  [ 1100/ 3526]
loss: 0.030128  [ 1200/ 3526]
loss: 0.032002  [ 1300/ 3526]
loss: 0.024903  [ 1400/ 3526]
loss: 0.020047  [ 1500/ 3526]
loss: 0.045572  [ 1600/ 3526]
loss: 0.020248  [ 1700/ 3526]
loss: 0.021581  [ 1800/ 3526]
loss: 0.015583  [ 1900/ 3526]
loss: 0.014344  [ 2000/ 3526]
loss: 0.104234  [ 2100/ 3526]
loss: 0.021766  [ 2200/ 3526]
loss: 0.015576  [ 2300/ 3526]
loss: 0.009176  [ 2400/ 3526]
loss: 0.015189  [ 2500/ 3526]
loss: 0.013986  [ 2600/ 3526]
loss: 0.013760  [ 2700/ 3526]
loss: 0.008280  [ 2800/ 3526]
loss: 0.009779  [ 2900/ 3526]
loss: 0.047422  [ 3000/ 3526]
loss: 0.161549  [ 3100/ 3526]
loss: 0.027589  [ 3200/ 3526]
loss: 0.020685  [ 3300/ 3526]
loss: 0.030315  [ 3400/ 3526]
loss: 0.038758  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [-2.0766268  -0.79750043]
[2 0 2 ... 0 2 2]
[2 0 1 ... 0 1 1]
Cluster 0 Occurrences: 1186; KMEANS: 2385
Cluster 1 Occurrences: 1188; KMEANS: 587
Cluster 2 Occurrences: 1152; KMEANS: 554
Centroids: [[0.32630244, 1.7025826], [0.0062835435, 0.95722216], [-2.5475264, -1.4641411]]
Centroids: [[0.16483498, 1.3300878], [-3.031559, -1.9902668], [-2.0828469, -0.9642964]]
Contingency Matrix: 
[[1186    0    0]
 [1185    0    3]
 [  14  587  551]]
[[-1, -1, -1], [-1, 0, 3], [-1, 587, 551]]
[[-1, -1, -1], [-1, -1, 3], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 1, 1: 2}
New Contingency Matrix: 
[[1186    0    0]
 [1185    3    0]
 [  14  551  587]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [1186, 3, 587], Sum: 1776
All_Elements: [1186, 0, 0, 1185, 3, 0, 14, 551, 587], Sum: 3526
Accuracy: 0.5036868973340897
Done!
