Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_42
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211EE996F60>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DAC18>
Epoch 1
-------------------------------
loss: 0.293452  [    0/ 3475]
loss: 0.118045  [  100/ 3475]
loss: 0.210093  [  200/ 3475]
loss: 0.234836  [  300/ 3475]
loss: 0.176700  [  400/ 3475]
loss: 0.049454  [  500/ 3475]
loss: 0.040148  [  600/ 3475]
loss: 0.100222  [  700/ 3475]
loss: 0.129538  [  800/ 3475]
loss: 0.102963  [  900/ 3475]
loss: 0.061925  [ 1000/ 3475]
loss: 0.072250  [ 1100/ 3475]
loss: 0.067407  [ 1200/ 3475]
loss: 0.032755  [ 1300/ 3475]
loss: 0.044172  [ 1400/ 3475]
loss: 0.060681  [ 1500/ 3475]
loss: 0.049719  [ 1600/ 3475]
loss: 0.020211  [ 1700/ 3475]
loss: 0.073226  [ 1800/ 3475]
loss: 0.036201  [ 1900/ 3475]
loss: 0.019510  [ 2000/ 3475]
loss: 0.056308  [ 2100/ 3475]
loss: 0.077165  [ 2200/ 3475]
loss: 0.023234  [ 2300/ 3475]
loss: 0.016973  [ 2400/ 3475]
loss: 0.073471  [ 2500/ 3475]
loss: 0.032462  [ 2600/ 3475]
loss: 0.095531  [ 2700/ 3475]
loss: 0.095464  [ 2800/ 3475]
loss: 0.027501  [ 2900/ 3475]
loss: 0.051915  [ 3000/ 3475]
loss: 0.078243  [ 3100/ 3475]
loss: 0.085937  [ 3200/ 3475]
loss: 0.040136  [ 3300/ 3475]
loss: 0.030350  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.035948  [    0/ 3475]
loss: 0.037965  [  100/ 3475]
loss: 0.054127  [  200/ 3475]
loss: 0.224411  [  300/ 3475]
loss: 0.031048  [  400/ 3475]
loss: 0.029283  [  500/ 3475]
loss: 0.016327  [  600/ 3475]
loss: 0.063495  [  700/ 3475]
loss: 0.107207  [  800/ 3475]
loss: 0.056967  [  900/ 3475]
loss: 0.045658  [ 1000/ 3475]
loss: 0.025245  [ 1100/ 3475]
loss: 0.026241  [ 1200/ 3475]
loss: 0.038352  [ 1300/ 3475]
loss: 0.033677  [ 1400/ 3475]
loss: 0.037910  [ 1500/ 3475]
loss: 0.057421  [ 1600/ 3475]
loss: 0.013387  [ 1700/ 3475]
loss: 0.077253  [ 1800/ 3475]
loss: 0.038881  [ 1900/ 3475]
loss: 0.018211  [ 2000/ 3475]
loss: 0.054966  [ 2100/ 3475]
loss: 0.080485  [ 2200/ 3475]
loss: 0.018335  [ 2300/ 3475]
loss: 0.015825  [ 2400/ 3475]
loss: 0.079175  [ 2500/ 3475]
loss: 0.035146  [ 2600/ 3475]
loss: 0.076045  [ 2700/ 3475]
loss: 0.086785  [ 2800/ 3475]
loss: 0.028434  [ 2900/ 3475]
loss: 0.051774  [ 3000/ 3475]
loss: 0.066395  [ 3100/ 3475]
loss: 0.068375  [ 3200/ 3475]
loss: 0.032742  [ 3300/ 3475]
loss: 0.028042  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.031923  [    0/ 3475]
loss: 0.034802  [  100/ 3475]
loss: 0.054326  [  200/ 3475]
loss: 0.225713  [  300/ 3475]
loss: 0.027751  [  400/ 3475]
loss: 0.031188  [  500/ 3475]
loss: 0.017286  [  600/ 3475]
loss: 0.056631  [  700/ 3475]
loss: 0.106616  [  800/ 3475]
loss: 0.056626  [  900/ 3475]
loss: 0.045109  [ 1000/ 3475]
loss: 0.023099  [ 1100/ 3475]
loss: 0.024194  [ 1200/ 3475]
loss: 0.037903  [ 1300/ 3475]
loss: 0.032940  [ 1400/ 3475]
loss: 0.036711  [ 1500/ 3475]
loss: 0.056086  [ 1600/ 3475]
loss: 0.012590  [ 1700/ 3475]
loss: 0.086277  [ 1800/ 3475]
loss: 0.038589  [ 1900/ 3475]
loss: 0.017720  [ 2000/ 3475]
loss: 0.051381  [ 2100/ 3475]
loss: 0.078914  [ 2200/ 3475]
loss: 0.017449  [ 2300/ 3475]
loss: 0.015720  [ 2400/ 3475]
loss: 0.080240  [ 2500/ 3475]
loss: 0.033797  [ 2600/ 3475]
loss: 0.060298  [ 2700/ 3475]
loss: 0.082382  [ 2800/ 3475]
loss: 0.028730  [ 2900/ 3475]
loss: 0.051307  [ 3000/ 3475]
loss: 0.063528  [ 3100/ 3475]
loss: 0.061459  [ 3200/ 3475]
loss: 0.023302  [ 3300/ 3475]
loss: 0.027329  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.030145  [    0/ 3475]
loss: 0.034323  [  100/ 3475]
loss: 0.054500  [  200/ 3475]
loss: 0.224554  [  300/ 3475]
loss: 0.026080  [  400/ 3475]
loss: 0.031367  [  500/ 3475]
loss: 0.018195  [  600/ 3475]
loss: 0.059279  [  700/ 3475]
loss: 0.105403  [  800/ 3475]
loss: 0.057182  [  900/ 3475]
loss: 0.045831  [ 1000/ 3475]
loss: 0.022013  [ 1100/ 3475]
loss: 0.023496  [ 1200/ 3475]
loss: 0.039950  [ 1300/ 3475]
loss: 0.035101  [ 1400/ 3475]
loss: 0.037835  [ 1500/ 3475]
loss: 0.054103  [ 1600/ 3475]
loss: 0.012281  [ 1700/ 3475]
loss: 0.093447  [ 1800/ 3475]
loss: 0.038302  [ 1900/ 3475]
loss: 0.016881  [ 2000/ 3475]
loss: 0.048543  [ 2100/ 3475]
loss: 0.076430  [ 2200/ 3475]
loss: 0.017343  [ 2300/ 3475]
loss: 0.015649  [ 2400/ 3475]
loss: 0.081240  [ 2500/ 3475]
loss: 0.032055  [ 2600/ 3475]
loss: 0.056278  [ 2700/ 3475]
loss: 0.077002  [ 2800/ 3475]
loss: 0.028972  [ 2900/ 3475]
loss: 0.051390  [ 3000/ 3475]
loss: 0.061479  [ 3100/ 3475]
loss: 0.058066  [ 3200/ 3475]
loss: 0.016471  [ 3300/ 3475]
loss: 0.026860  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.030193  [    0/ 3475]
loss: 0.033345  [  100/ 3475]
loss: 0.054125  [  200/ 3475]
loss: 0.222955  [  300/ 3475]
loss: 0.026354  [  400/ 3475]
loss: 0.030689  [  500/ 3475]
loss: 0.018676  [  600/ 3475]
loss: 0.069131  [  700/ 3475]
loss: 0.104978  [  800/ 3475]
loss: 0.057094  [  900/ 3475]
loss: 0.045870  [ 1000/ 3475]
loss: 0.021178  [ 1100/ 3475]
loss: 0.023657  [ 1200/ 3475]
loss: 0.040221  [ 1300/ 3475]
loss: 0.034210  [ 1400/ 3475]
loss: 0.038560  [ 1500/ 3475]
loss: 0.053554  [ 1600/ 3475]
loss: 0.012309  [ 1700/ 3475]
loss: 0.097890  [ 1800/ 3475]
loss: 0.038186  [ 1900/ 3475]
loss: 0.016066  [ 2000/ 3475]
loss: 0.048591  [ 2100/ 3475]
loss: 0.073741  [ 2200/ 3475]
loss: 0.017629  [ 2300/ 3475]
loss: 0.015552  [ 2400/ 3475]
loss: 0.082036  [ 2500/ 3475]
loss: 0.030509  [ 2600/ 3475]
loss: 0.067635  [ 2700/ 3475]
loss: 0.067765  [ 2800/ 3475]
loss: 0.029367  [ 2900/ 3475]
loss: 0.051631  [ 3000/ 3475]
loss: 0.060779  [ 3100/ 3475]
loss: 0.057549  [ 3200/ 3475]
loss: 0.010762  [ 3300/ 3475]
loss: 0.026834  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.031007  [    0/ 3475]
loss: 0.032780  [  100/ 3475]
loss: 0.053471  [  200/ 3475]
loss: 0.221895  [  300/ 3475]
loss: 0.025694  [  400/ 3475]
loss: 0.029784  [  500/ 3475]
loss: 0.018856  [  600/ 3475]
loss: 0.077014  [  700/ 3475]
loss: 0.104927  [  800/ 3475]
loss: 0.055578  [  900/ 3475]
loss: 0.045694  [ 1000/ 3475]
loss: 0.020295  [ 1100/ 3475]
loss: 0.022588  [ 1200/ 3475]
loss: 0.051903  [ 1300/ 3475]
loss: 0.033684  [ 1400/ 3475]
loss: 0.039915  [ 1500/ 3475]
loss: 0.052408  [ 1600/ 3475]
loss: 0.012214  [ 1700/ 3475]
loss: 0.099120  [ 1800/ 3475]
loss: 0.037892  [ 1900/ 3475]
loss: 0.015572  [ 2000/ 3475]
loss: 0.049983  [ 2100/ 3475]
loss: 0.072278  [ 2200/ 3475]
loss: 0.018387  [ 2300/ 3475]
loss: 0.015622  [ 2400/ 3475]
loss: 0.082741  [ 2500/ 3475]
loss: 0.029498  [ 2600/ 3475]
loss: 0.087480  [ 2700/ 3475]
loss: 0.056986  [ 2800/ 3475]
loss: 0.030826  [ 2900/ 3475]
loss: 0.051473  [ 3000/ 3475]
loss: 0.060837  [ 3100/ 3475]
loss: 0.056232  [ 3200/ 3475]
loss: 0.007298  [ 3300/ 3475]
loss: 0.026738  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.031537  [    0/ 3475]
loss: 0.031893  [  100/ 3475]
loss: 0.052784  [  200/ 3475]
loss: 0.221298  [  300/ 3475]
loss: 0.023935  [  400/ 3475]
loss: 0.028561  [  500/ 3475]
loss: 0.019355  [  600/ 3475]
loss: 0.075424  [  700/ 3475]
loss: 0.104381  [  800/ 3475]
loss: 0.053660  [  900/ 3475]
loss: 0.043652  [ 1000/ 3475]
loss: 0.020600  [ 1100/ 3475]
loss: 0.021684  [ 1200/ 3475]
loss: 0.047147  [ 1300/ 3475]
loss: 0.033693  [ 1400/ 3475]
loss: 0.039909  [ 1500/ 3475]
loss: 0.051240  [ 1600/ 3475]
loss: 0.011946  [ 1700/ 3475]
loss: 0.094079  [ 1800/ 3475]
loss: 0.037249  [ 1900/ 3475]
loss: 0.015331  [ 2000/ 3475]
loss: 0.048781  [ 2100/ 3475]
loss: 0.073397  [ 2200/ 3475]
loss: 0.019255  [ 2300/ 3475]
loss: 0.015709  [ 2400/ 3475]
loss: 0.083548  [ 2500/ 3475]
loss: 0.029142  [ 2600/ 3475]
loss: 0.103192  [ 2700/ 3475]
loss: 0.048990  [ 2800/ 3475]
loss: 0.034484  [ 2900/ 3475]
loss: 0.051084  [ 3000/ 3475]
loss: 0.060725  [ 3100/ 3475]
loss: 0.054737  [ 3200/ 3475]
loss: 0.007144  [ 3300/ 3475]
loss: 0.027166  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.032477  [    0/ 3475]
loss: 0.030550  [  100/ 3475]
loss: 0.051988  [  200/ 3475]
loss: 0.220556  [  300/ 3475]
loss: 0.021119  [  400/ 3475]
loss: 0.027558  [  500/ 3475]
loss: 0.019769  [  600/ 3475]
loss: 0.070121  [  700/ 3475]
loss: 0.104753  [  800/ 3475]
loss: 0.050919  [  900/ 3475]
loss: 0.041837  [ 1000/ 3475]
loss: 0.020016  [ 1100/ 3475]
loss: 0.021601  [ 1200/ 3475]
loss: 0.045129  [ 1300/ 3475]
loss: 0.032415  [ 1400/ 3475]
loss: 0.039611  [ 1500/ 3475]
loss: 0.048855  [ 1600/ 3475]
loss: 0.012071  [ 1700/ 3475]
loss: 0.084819  [ 1800/ 3475]
loss: 0.036798  [ 1900/ 3475]
loss: 0.016402  [ 2000/ 3475]
loss: 0.045429  [ 2100/ 3475]
loss: 0.075511  [ 2200/ 3475]
loss: 0.021006  [ 2300/ 3475]
loss: 0.015976  [ 2400/ 3475]
loss: 0.083813  [ 2500/ 3475]
loss: 0.028118  [ 2600/ 3475]
loss: 0.114850  [ 2700/ 3475]
loss: 0.043317  [ 2800/ 3475]
loss: 0.038546  [ 2900/ 3475]
loss: 0.051378  [ 3000/ 3475]
loss: 0.061204  [ 3100/ 3475]
loss: 0.051361  [ 3200/ 3475]
loss: 0.009610  [ 3300/ 3475]
loss: 0.027748  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [-1.6067752  0.2680087]
[2 1 2 ... 1 0 1]
[2 1 2 ... 1 0 1]
Cluster 0 Occurrences: 1162; KMEANS: 1193
Cluster 1 Occurrences: 1164; KMEANS: 1135
Cluster 2 Occurrences: 1149; KMEANS: 1147
Centroids: [[-0.46628857, -0.20516217], [0.71946764, 3.886655], [-1.9538935, -0.27149376]]
Centroids: [[-0.4365472, -0.15192124], [0.739176, 3.9771402], [-1.9769437, -0.31306937]]
Contingency Matrix: 
[[1124    0   38]
 [  30 1128    6]
 [  39    7 1103]]
[[1124, -1, 38], [-1, -1, -1], [39, -1, 1103]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 1103]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 0, 2: 2}
New Contingency Matrix: 
[[1124    0   38]
 [  30 1128    6]
 [  39    7 1103]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1124, 1128, 1103], Sum: 3355
All_Elements: [1124, 0, 38, 30, 1128, 6, 39, 7, 1103], Sum: 3475
Accuracy: 0.9654676258992806
Done!
