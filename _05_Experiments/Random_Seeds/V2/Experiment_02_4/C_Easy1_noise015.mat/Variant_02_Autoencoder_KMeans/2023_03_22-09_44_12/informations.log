Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_12
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211E80D1EF0>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DA0F0>
Epoch 1
-------------------------------
loss: 0.220267  [    0/ 3477]
loss: 0.243327  [  100/ 3477]
loss: 0.107401  [  200/ 3477]
loss: 0.097987  [  300/ 3477]
loss: 0.034125  [  400/ 3477]
loss: 0.090170  [  500/ 3477]
loss: 0.022137  [  600/ 3477]
loss: 0.024265  [  700/ 3477]
loss: 0.023768  [  800/ 3477]
loss: 0.029742  [  900/ 3477]
loss: 0.027256  [ 1000/ 3477]
loss: 0.004046  [ 1100/ 3477]
loss: 0.025001  [ 1200/ 3477]
loss: 0.014885  [ 1300/ 3477]
loss: 0.020172  [ 1400/ 3477]
loss: 0.021982  [ 1500/ 3477]
loss: 0.015310  [ 1600/ 3477]
loss: 0.014036  [ 1700/ 3477]
loss: 0.013509  [ 1800/ 3477]
loss: 0.017967  [ 1900/ 3477]
loss: 0.025028  [ 2000/ 3477]
loss: 0.029031  [ 2100/ 3477]
loss: 0.017006  [ 2200/ 3477]
loss: 0.012809  [ 2300/ 3477]
loss: 0.011163  [ 2400/ 3477]
loss: 0.098043  [ 2500/ 3477]
loss: 0.059729  [ 2600/ 3477]
loss: 0.013833  [ 2700/ 3477]
loss: 0.697390  [ 2800/ 3477]
loss: 0.018440  [ 2900/ 3477]
loss: 0.016101  [ 3000/ 3477]
loss: 0.014899  [ 3100/ 3477]
loss: 0.092651  [ 3200/ 3477]
loss: 0.017000  [ 3300/ 3477]
loss: 0.009460  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.017385  [    0/ 3477]
loss: 0.012077  [  100/ 3477]
loss: 0.038218  [  200/ 3477]
loss: 0.030229  [  300/ 3477]
loss: 0.017331  [  400/ 3477]
loss: 0.012770  [  500/ 3477]
loss: 0.015681  [  600/ 3477]
loss: 0.017812  [  700/ 3477]
loss: 0.022835  [  800/ 3477]
loss: 0.018132  [  900/ 3477]
loss: 0.011456  [ 1000/ 3477]
loss: 0.003635  [ 1100/ 3477]
loss: 0.025366  [ 1200/ 3477]
loss: 0.005207  [ 1300/ 3477]
loss: 0.015818  [ 1400/ 3477]
loss: 0.016188  [ 1500/ 3477]
loss: 0.014559  [ 1600/ 3477]
loss: 0.014898  [ 1700/ 3477]
loss: 0.011026  [ 1800/ 3477]
loss: 0.014938  [ 1900/ 3477]
loss: 0.023818  [ 2000/ 3477]
loss: 0.019453  [ 2100/ 3477]
loss: 0.019195  [ 2200/ 3477]
loss: 0.014073  [ 2300/ 3477]
loss: 0.010524  [ 2400/ 3477]
loss: 0.095391  [ 2500/ 3477]
loss: 0.067037  [ 2600/ 3477]
loss: 0.011938  [ 2700/ 3477]
loss: 0.445181  [ 2800/ 3477]
loss: 0.020253  [ 2900/ 3477]
loss: 0.017153  [ 3000/ 3477]
loss: 0.015709  [ 3100/ 3477]
loss: 0.077039  [ 3200/ 3477]
loss: 0.013522  [ 3300/ 3477]
loss: 0.008652  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.006291  [    0/ 3477]
loss: 0.012509  [  100/ 3477]
loss: 0.023845  [  200/ 3477]
loss: 0.030006  [  300/ 3477]
loss: 0.017084  [  400/ 3477]
loss: 0.012065  [  500/ 3477]
loss: 0.009816  [  600/ 3477]
loss: 0.017873  [  700/ 3477]
loss: 0.021306  [  800/ 3477]
loss: 0.017293  [  900/ 3477]
loss: 0.010683  [ 1000/ 3477]
loss: 0.002110  [ 1100/ 3477]
loss: 0.021533  [ 1200/ 3477]
loss: 0.007132  [ 1300/ 3477]
loss: 0.014376  [ 1400/ 3477]
loss: 0.010469  [ 1500/ 3477]
loss: 0.013519  [ 1600/ 3477]
loss: 0.012577  [ 1700/ 3477]
loss: 0.012181  [ 1800/ 3477]
loss: 0.011981  [ 1900/ 3477]
loss: 0.022963  [ 2000/ 3477]
loss: 0.012953  [ 2100/ 3477]
loss: 0.021125  [ 2200/ 3477]
loss: 0.013670  [ 2300/ 3477]
loss: 0.009308  [ 2400/ 3477]
loss: 0.090408  [ 2500/ 3477]
loss: 0.070851  [ 2600/ 3477]
loss: 0.011982  [ 2700/ 3477]
loss: 0.380435  [ 2800/ 3477]
loss: 0.020252  [ 2900/ 3477]
loss: 0.017235  [ 3000/ 3477]
loss: 0.015824  [ 3100/ 3477]
loss: 0.076804  [ 3200/ 3477]
loss: 0.011557  [ 3300/ 3477]
loss: 0.008186  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.004522  [    0/ 3477]
loss: 0.014768  [  100/ 3477]
loss: 0.017317  [  200/ 3477]
loss: 0.030532  [  300/ 3477]
loss: 0.016458  [  400/ 3477]
loss: 0.010991  [  500/ 3477]
loss: 0.009377  [  600/ 3477]
loss: 0.019222  [  700/ 3477]
loss: 0.019150  [  800/ 3477]
loss: 0.017148  [  900/ 3477]
loss: 0.012737  [ 1000/ 3477]
loss: 0.002005  [ 1100/ 3477]
loss: 0.020316  [ 1200/ 3477]
loss: 0.009057  [ 1300/ 3477]
loss: 0.013138  [ 1400/ 3477]
loss: 0.010250  [ 1500/ 3477]
loss: 0.014536  [ 1600/ 3477]
loss: 0.012150  [ 1700/ 3477]
loss: 0.012924  [ 1800/ 3477]
loss: 0.010580  [ 1900/ 3477]
loss: 0.022075  [ 2000/ 3477]
loss: 0.011828  [ 2100/ 3477]
loss: 0.021567  [ 2200/ 3477]
loss: 0.013447  [ 2300/ 3477]
loss: 0.008403  [ 2400/ 3477]
loss: 0.086683  [ 2500/ 3477]
loss: 0.072597  [ 2600/ 3477]
loss: 0.010632  [ 2700/ 3477]
loss: 0.313358  [ 2800/ 3477]
loss: 0.019904  [ 2900/ 3477]
loss: 0.017307  [ 3000/ 3477]
loss: 0.015818  [ 3100/ 3477]
loss: 0.077463  [ 3200/ 3477]
loss: 0.011435  [ 3300/ 3477]
loss: 0.008555  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.005519  [    0/ 3477]
loss: 0.016072  [  100/ 3477]
loss: 0.015191  [  200/ 3477]
loss: 0.031000  [  300/ 3477]
loss: 0.016150  [  400/ 3477]
loss: 0.010487  [  500/ 3477]
loss: 0.010156  [  600/ 3477]
loss: 0.019994  [  700/ 3477]
loss: 0.017848  [  800/ 3477]
loss: 0.016336  [  900/ 3477]
loss: 0.014385  [ 1000/ 3477]
loss: 0.001982  [ 1100/ 3477]
loss: 0.020222  [ 1200/ 3477]
loss: 0.009808  [ 1300/ 3477]
loss: 0.012401  [ 1400/ 3477]
loss: 0.010638  [ 1500/ 3477]
loss: 0.014599  [ 1600/ 3477]
loss: 0.012063  [ 1700/ 3477]
loss: 0.013018  [ 1800/ 3477]
loss: 0.009889  [ 1900/ 3477]
loss: 0.021508  [ 2000/ 3477]
loss: 0.011326  [ 2100/ 3477]
loss: 0.021591  [ 2200/ 3477]
loss: 0.013286  [ 2300/ 3477]
loss: 0.008041  [ 2400/ 3477]
loss: 0.084362  [ 2500/ 3477]
loss: 0.073818  [ 2600/ 3477]
loss: 0.009471  [ 2700/ 3477]
loss: 0.284146  [ 2800/ 3477]
loss: 0.019695  [ 2900/ 3477]
loss: 0.017338  [ 3000/ 3477]
loss: 0.015708  [ 3100/ 3477]
loss: 0.078338  [ 3200/ 3477]
loss: 0.011075  [ 3300/ 3477]
loss: 0.008176  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.006814  [    0/ 3477]
loss: 0.016591  [  100/ 3477]
loss: 0.015213  [  200/ 3477]
loss: 0.031566  [  300/ 3477]
loss: 0.015652  [  400/ 3477]
loss: 0.009653  [  500/ 3477]
loss: 0.010915  [  600/ 3477]
loss: 0.020316  [  700/ 3477]
loss: 0.017535  [  800/ 3477]
loss: 0.016415  [  900/ 3477]
loss: 0.015309  [ 1000/ 3477]
loss: 0.001970  [ 1100/ 3477]
loss: 0.019980  [ 1200/ 3477]
loss: 0.010222  [ 1300/ 3477]
loss: 0.012286  [ 1400/ 3477]
loss: 0.011040  [ 1500/ 3477]
loss: 0.014250  [ 1600/ 3477]
loss: 0.011978  [ 1700/ 3477]
loss: 0.013065  [ 1800/ 3477]
loss: 0.009298  [ 1900/ 3477]
loss: 0.021230  [ 2000/ 3477]
loss: 0.011116  [ 2100/ 3477]
loss: 0.021658  [ 2200/ 3477]
loss: 0.013160  [ 2300/ 3477]
loss: 0.007971  [ 2400/ 3477]
loss: 0.083694  [ 2500/ 3477]
loss: 0.074389  [ 2600/ 3477]
loss: 0.008573  [ 2700/ 3477]
loss: 0.277142  [ 2800/ 3477]
loss: 0.019617  [ 2900/ 3477]
loss: 0.017355  [ 3000/ 3477]
loss: 0.015683  [ 3100/ 3477]
loss: 0.079033  [ 3200/ 3477]
loss: 0.010827  [ 3300/ 3477]
loss: 0.008190  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.007573  [    0/ 3477]
loss: 0.016932  [  100/ 3477]
loss: 0.015449  [  200/ 3477]
loss: 0.032916  [  300/ 3477]
loss: 0.015317  [  400/ 3477]
loss: 0.009069  [  500/ 3477]
loss: 0.011159  [  600/ 3477]
loss: 0.020556  [  700/ 3477]
loss: 0.017595  [  800/ 3477]
loss: 0.016257  [  900/ 3477]
loss: 0.015877  [ 1000/ 3477]
loss: 0.001908  [ 1100/ 3477]
loss: 0.019973  [ 1200/ 3477]
loss: 0.010358  [ 1300/ 3477]
loss: 0.012293  [ 1400/ 3477]
loss: 0.011187  [ 1500/ 3477]
loss: 0.013395  [ 1600/ 3477]
loss: 0.011945  [ 1700/ 3477]
loss: 0.013048  [ 1800/ 3477]
loss: 0.009162  [ 1900/ 3477]
loss: 0.021156  [ 2000/ 3477]
loss: 0.011019  [ 2100/ 3477]
loss: 0.021517  [ 2200/ 3477]
loss: 0.013052  [ 2300/ 3477]
loss: 0.007942  [ 2400/ 3477]
loss: 0.083301  [ 2500/ 3477]
loss: 0.074156  [ 2600/ 3477]
loss: 0.008043  [ 2700/ 3477]
loss: 0.268418  [ 2800/ 3477]
loss: 0.019644  [ 2900/ 3477]
loss: 0.017363  [ 3000/ 3477]
loss: 0.015489  [ 3100/ 3477]
loss: 0.079543  [ 3200/ 3477]
loss: 0.010648  [ 3300/ 3477]
loss: 0.008128  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.008143  [    0/ 3477]
loss: 0.017026  [  100/ 3477]
loss: 0.015947  [  200/ 3477]
loss: 0.032838  [  300/ 3477]
loss: 0.014980  [  400/ 3477]
loss: 0.008289  [  500/ 3477]
loss: 0.011034  [  600/ 3477]
loss: 0.020276  [  700/ 3477]
loss: 0.017813  [  800/ 3477]
loss: 0.016237  [  900/ 3477]
loss: 0.016104  [ 1000/ 3477]
loss: 0.001889  [ 1100/ 3477]
loss: 0.019905  [ 1200/ 3477]
loss: 0.010425  [ 1300/ 3477]
loss: 0.012257  [ 1400/ 3477]
loss: 0.011134  [ 1500/ 3477]
loss: 0.013071  [ 1600/ 3477]
loss: 0.011940  [ 1700/ 3477]
loss: 0.013048  [ 1800/ 3477]
loss: 0.009047  [ 1900/ 3477]
loss: 0.021091  [ 2000/ 3477]
loss: 0.010823  [ 2100/ 3477]
loss: 0.021254  [ 2200/ 3477]
loss: 0.013051  [ 2300/ 3477]
loss: 0.007955  [ 2400/ 3477]
loss: 0.083282  [ 2500/ 3477]
loss: 0.073472  [ 2600/ 3477]
loss: 0.007608  [ 2700/ 3477]
loss: 0.260176  [ 2800/ 3477]
loss: 0.019668  [ 2900/ 3477]
loss: 0.017347  [ 3000/ 3477]
loss: 0.015439  [ 3100/ 3477]
loss: 0.079859  [ 3200/ 3477]
loss: 0.010611  [ 3300/ 3477]
loss: 0.008197  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [1.7444935 2.0131428]
[1 0 2 ... 1 1 1]
[1 2 0 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1185
Cluster 1 Occurrences: 1188; KMEANS: 1164
Cluster 2 Occurrences: 1157; KMEANS: 1128
Centroids: [[-0.9529456, 0.28419548], [1.1971602, 1.8493321], [-2.5835912, -0.75124747]]
Centroids: [[-2.5787828, -0.75028175], [1.2620716, 1.8962023], [-0.9387562, 0.29381815]]
Contingency Matrix: 
[[  13    1 1118]
 [  15 1163   10]
 [1157    0    0]]
[[13, -1, 1118], [-1, -1, -1], [1157, -1, 0]]
[[-1, -1, 1118], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1118    1   13]
 [  10 1163   15]
 [   0    0 1157]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1118, 1163, 1157], Sum: 3438
All_Elements: [1118, 1, 13, 10, 1163, 15, 0, 0, 1157], Sum: 3477
Accuracy: 0.9887834339948232
Done!
