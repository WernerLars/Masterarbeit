Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_50_08
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211E80DF128>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DA518>
Epoch 1
-------------------------------
loss: 0.134670  [    0/ 3386]
loss: 0.278497  [  100/ 3386]
loss: 0.219980  [  200/ 3386]
loss: 0.180653  [  300/ 3386]
loss: 0.254711  [  400/ 3386]
loss: 0.079654  [  500/ 3386]
loss: 0.180229  [  600/ 3386]
loss: 0.049956  [  700/ 3386]
loss: 0.111636  [  800/ 3386]
loss: 0.084294  [  900/ 3386]
loss: 0.246709  [ 1000/ 3386]
loss: 0.344899  [ 1100/ 3386]
loss: 0.229532  [ 1200/ 3386]
loss: 0.026214  [ 1300/ 3386]
loss: 0.048725  [ 1400/ 3386]
loss: 0.098526  [ 1500/ 3386]
loss: 0.034270  [ 1600/ 3386]
loss: 0.025159  [ 1700/ 3386]
loss: 0.255033  [ 1800/ 3386]
loss: 0.499077  [ 1900/ 3386]
loss: 0.029906  [ 2000/ 3386]
loss: 0.137339  [ 2100/ 3386]
loss: 0.581333  [ 2200/ 3386]
loss: 0.094157  [ 2300/ 3386]
loss: 0.133090  [ 2400/ 3386]
loss: 0.052829  [ 2500/ 3386]
loss: 0.178376  [ 2600/ 3386]
loss: 0.236580  [ 2700/ 3386]
loss: 0.084395  [ 2800/ 3386]
loss: 0.056217  [ 2900/ 3386]
loss: 0.015767  [ 3000/ 3386]
loss: 0.112114  [ 3100/ 3386]
loss: 0.091919  [ 3200/ 3386]
loss: 0.104695  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.053026  [    0/ 3386]
loss: 0.114590  [  100/ 3386]
loss: 0.085735  [  200/ 3386]
loss: 0.105552  [  300/ 3386]
loss: 0.198279  [  400/ 3386]
loss: 0.071220  [  500/ 3386]
loss: 0.166531  [  600/ 3386]
loss: 0.043055  [  700/ 3386]
loss: 0.117583  [  800/ 3386]
loss: 0.050182  [  900/ 3386]
loss: 0.136278  [ 1000/ 3386]
loss: 0.292028  [ 1100/ 3386]
loss: 0.159973  [ 1200/ 3386]
loss: 0.025737  [ 1300/ 3386]
loss: 0.052404  [ 1400/ 3386]
loss: 0.065809  [ 1500/ 3386]
loss: 0.034208  [ 1600/ 3386]
loss: 0.026284  [ 1700/ 3386]
loss: 0.253254  [ 1800/ 3386]
loss: 0.464317  [ 1900/ 3386]
loss: 0.024168  [ 2000/ 3386]
loss: 0.100539  [ 2100/ 3386]
loss: 0.682830  [ 2200/ 3386]
loss: 0.080640  [ 2300/ 3386]
loss: 0.099112  [ 2400/ 3386]
loss: 0.049540  [ 2500/ 3386]
loss: 0.159516  [ 2600/ 3386]
loss: 0.212093  [ 2700/ 3386]
loss: 0.077484  [ 2800/ 3386]
loss: 0.059644  [ 2900/ 3386]
loss: 0.015207  [ 3000/ 3386]
loss: 0.113050  [ 3100/ 3386]
loss: 0.080913  [ 3200/ 3386]
loss: 0.105007  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.051790  [    0/ 3386]
loss: 0.114861  [  100/ 3386]
loss: 0.082611  [  200/ 3386]
loss: 0.118148  [  300/ 3386]
loss: 0.189348  [  400/ 3386]
loss: 0.072574  [  500/ 3386]
loss: 0.162664  [  600/ 3386]
loss: 0.043717  [  700/ 3386]
loss: 0.119016  [  800/ 3386]
loss: 0.048971  [  900/ 3386]
loss: 0.129364  [ 1000/ 3386]
loss: 0.279059  [ 1100/ 3386]
loss: 0.151800  [ 1200/ 3386]
loss: 0.024883  [ 1300/ 3386]
loss: 0.052784  [ 1400/ 3386]
loss: 0.065430  [ 1500/ 3386]
loss: 0.035960  [ 1600/ 3386]
loss: 0.026935  [ 1700/ 3386]
loss: 0.220931  [ 1800/ 3386]
loss: 0.437905  [ 1900/ 3386]
loss: 0.019981  [ 2000/ 3386]
loss: 0.095959  [ 2100/ 3386]
loss: 0.640202  [ 2200/ 3386]
loss: 0.077766  [ 2300/ 3386]
loss: 0.089537  [ 2400/ 3386]
loss: 0.051689  [ 2500/ 3386]
loss: 0.143673  [ 2600/ 3386]
loss: 0.208568  [ 2700/ 3386]
loss: 0.070551  [ 2800/ 3386]
loss: 0.059089  [ 2900/ 3386]
loss: 0.015662  [ 3000/ 3386]
loss: 0.113993  [ 3100/ 3386]
loss: 0.072617  [ 3200/ 3386]
loss: 0.101952  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.051836  [    0/ 3386]
loss: 0.113439  [  100/ 3386]
loss: 0.079475  [  200/ 3386]
loss: 0.068001  [  300/ 3386]
loss: 0.195009  [  400/ 3386]
loss: 0.071322  [  500/ 3386]
loss: 0.159536  [  600/ 3386]
loss: 0.044121  [  700/ 3386]
loss: 0.118227  [  800/ 3386]
loss: 0.051656  [  900/ 3386]
loss: 0.126277  [ 1000/ 3386]
loss: 0.270679  [ 1100/ 3386]
loss: 0.153296  [ 1200/ 3386]
loss: 0.022740  [ 1300/ 3386]
loss: 0.054635  [ 1400/ 3386]
loss: 0.064377  [ 1500/ 3386]
loss: 0.036098  [ 1600/ 3386]
loss: 0.027229  [ 1700/ 3386]
loss: 0.181664  [ 1800/ 3386]
loss: 0.420757  [ 1900/ 3386]
loss: 0.020677  [ 2000/ 3386]
loss: 0.095484  [ 2100/ 3386]
loss: 0.690944  [ 2200/ 3386]
loss: 0.081152  [ 2300/ 3386]
loss: 0.082462  [ 2400/ 3386]
loss: 0.055093  [ 2500/ 3386]
loss: 0.125982  [ 2600/ 3386]
loss: 0.203759  [ 2700/ 3386]
loss: 0.068665  [ 2800/ 3386]
loss: 0.059987  [ 2900/ 3386]
loss: 0.015540  [ 3000/ 3386]
loss: 0.115978  [ 3100/ 3386]
loss: 0.066405  [ 3200/ 3386]
loss: 0.104472  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.050226  [    0/ 3386]
loss: 0.114405  [  100/ 3386]
loss: 0.078870  [  200/ 3386]
loss: 0.085009  [  300/ 3386]
loss: 0.197255  [  400/ 3386]
loss: 0.072296  [  500/ 3386]
loss: 0.158484  [  600/ 3386]
loss: 0.044405  [  700/ 3386]
loss: 0.116010  [  800/ 3386]
loss: 0.053976  [  900/ 3386]
loss: 0.125277  [ 1000/ 3386]
loss: 0.258556  [ 1100/ 3386]
loss: 0.157623  [ 1200/ 3386]
loss: 0.022071  [ 1300/ 3386]
loss: 0.053864  [ 1400/ 3386]
loss: 0.062893  [ 1500/ 3386]
loss: 0.035957  [ 1600/ 3386]
loss: 0.027786  [ 1700/ 3386]
loss: 0.165514  [ 1800/ 3386]
loss: 0.405627  [ 1900/ 3386]
loss: 0.022310  [ 2000/ 3386]
loss: 0.097847  [ 2100/ 3386]
loss: 0.655615  [ 2200/ 3386]
loss: 0.083722  [ 2300/ 3386]
loss: 0.076727  [ 2400/ 3386]
loss: 0.057277  [ 2500/ 3386]
loss: 0.110690  [ 2600/ 3386]
loss: 0.199765  [ 2700/ 3386]
loss: 0.072852  [ 2800/ 3386]
loss: 0.059929  [ 2900/ 3386]
loss: 0.015249  [ 3000/ 3386]
loss: 0.118643  [ 3100/ 3386]
loss: 0.059821  [ 3200/ 3386]
loss: 0.104502  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.049280  [    0/ 3386]
loss: 0.115493  [  100/ 3386]
loss: 0.087446  [  200/ 3386]
loss: 0.072286  [  300/ 3386]
loss: 0.201676  [  400/ 3386]
loss: 0.071194  [  500/ 3386]
loss: 0.157538  [  600/ 3386]
loss: 0.044885  [  700/ 3386]
loss: 0.115480  [  800/ 3386]
loss: 0.055675  [  900/ 3386]
loss: 0.122305  [ 1000/ 3386]
loss: 0.243888  [ 1100/ 3386]
loss: 0.159762  [ 1200/ 3386]
loss: 0.022512  [ 1300/ 3386]
loss: 0.053066  [ 1400/ 3386]
loss: 0.060357  [ 1500/ 3386]
loss: 0.035678  [ 1600/ 3386]
loss: 0.028516  [ 1700/ 3386]
loss: 0.156366  [ 1800/ 3386]
loss: 0.392467  [ 1900/ 3386]
loss: 0.024358  [ 2000/ 3386]
loss: 0.099745  [ 2100/ 3386]
loss: 0.753541  [ 2200/ 3386]
loss: 0.087359  [ 2300/ 3386]
loss: 0.069353  [ 2400/ 3386]
loss: 0.059545  [ 2500/ 3386]
loss: 0.096730  [ 2600/ 3386]
loss: 0.198040  [ 2700/ 3386]
loss: 0.077975  [ 2800/ 3386]
loss: 0.060026  [ 2900/ 3386]
loss: 0.015040  [ 3000/ 3386]
loss: 0.118135  [ 3100/ 3386]
loss: 0.057281  [ 3200/ 3386]
loss: 0.104342  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.049262  [    0/ 3386]
loss: 0.115601  [  100/ 3386]
loss: 0.089558  [  200/ 3386]
loss: 0.061755  [  300/ 3386]
loss: 0.204207  [  400/ 3386]
loss: 0.070657  [  500/ 3386]
loss: 0.156936  [  600/ 3386]
loss: 0.045015  [  700/ 3386]
loss: 0.114968  [  800/ 3386]
loss: 0.056172  [  900/ 3386]
loss: 0.121046  [ 1000/ 3386]
loss: 0.229670  [ 1100/ 3386]
loss: 0.159995  [ 1200/ 3386]
loss: 0.023131  [ 1300/ 3386]
loss: 0.051381  [ 1400/ 3386]
loss: 0.056156  [ 1500/ 3386]
loss: 0.035695  [ 1600/ 3386]
loss: 0.028183  [ 1700/ 3386]
loss: 0.162389  [ 1800/ 3386]
loss: 0.367032  [ 1900/ 3386]
loss: 0.028983  [ 2000/ 3386]
loss: 0.101452  [ 2100/ 3386]
loss: 0.623686  [ 2200/ 3386]
loss: 0.088802  [ 2300/ 3386]
loss: 0.066669  [ 2400/ 3386]
loss: 0.057369  [ 2500/ 3386]
loss: 0.084382  [ 2600/ 3386]
loss: 0.197090  [ 2700/ 3386]
loss: 0.081075  [ 2800/ 3386]
loss: 0.060616  [ 2900/ 3386]
loss: 0.014901  [ 3000/ 3386]
loss: 0.116816  [ 3100/ 3386]
loss: 0.056446  [ 3200/ 3386]
loss: 0.104014  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.049660  [    0/ 3386]
loss: 0.115623  [  100/ 3386]
loss: 0.089805  [  200/ 3386]
loss: 0.053700  [  300/ 3386]
loss: 0.203957  [  400/ 3386]
loss: 0.070034  [  500/ 3386]
loss: 0.156775  [  600/ 3386]
loss: 0.045231  [  700/ 3386]
loss: 0.114895  [  800/ 3386]
loss: 0.056162  [  900/ 3386]
loss: 0.119476  [ 1000/ 3386]
loss: 0.218151  [ 1100/ 3386]
loss: 0.159411  [ 1200/ 3386]
loss: 0.024185  [ 1300/ 3386]
loss: 0.050866  [ 1400/ 3386]
loss: 0.054377  [ 1500/ 3386]
loss: 0.034830  [ 1600/ 3386]
loss: 0.028545  [ 1700/ 3386]
loss: 0.162101  [ 1800/ 3386]
loss: 0.354477  [ 1900/ 3386]
loss: 0.031110  [ 2000/ 3386]
loss: 0.101820  [ 2100/ 3386]
loss: 0.573501  [ 2200/ 3386]
loss: 0.088930  [ 2300/ 3386]
loss: 0.063446  [ 2400/ 3386]
loss: 0.057545  [ 2500/ 3386]
loss: 0.076025  [ 2600/ 3386]
loss: 0.196700  [ 2700/ 3386]
loss: 0.081939  [ 2800/ 3386]
loss: 0.060197  [ 2900/ 3386]
loss: 0.014825  [ 3000/ 3386]
loss: 0.116182  [ 3100/ 3386]
loss: 0.057362  [ 3200/ 3386]
loss: 0.104053  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [-0.5642179   0.22026719]
[0 2 0 ... 2 0 1]
[1 1 1 ... 1 1 0]
Cluster 0 Occurrences: 1079; KMEANS: 546
Cluster 1 Occurrences: 1158; KMEANS: 2246
Cluster 2 Occurrences: 1149; KMEANS: 594
Centroids: [[-0.6456964, 0.027128667], [2.25854, 4.4336395], [-1.4919372, -0.13368689]]
Centroids: [[3.004254, 5.6236033], [-1.0797668, -0.05441695], [1.6654552, 3.4706283]]
Contingency Matrix: 
[[   0 1079    0]
 [ 546   24  588]
 [   0 1143    6]]
[[0, -1, 0], [546, -1, 588], [-1, -1, -1]]
[[0, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 2, 0: 0}
New Contingency Matrix: 
[[   0    0 1079]
 [ 546  588   24]
 [   0    6 1143]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [0, 588, 1143], Sum: 1731
All_Elements: [0, 0, 1079, 546, 588, 24, 0, 6, 1143], Sum: 3386
Accuracy: 0.5112226816302422
Done!
