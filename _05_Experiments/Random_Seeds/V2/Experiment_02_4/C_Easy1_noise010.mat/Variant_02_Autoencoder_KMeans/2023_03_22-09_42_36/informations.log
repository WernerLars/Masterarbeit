Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise010.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy1_noise010.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_42_36
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211E899BC88>
Sampling rate: 24000.0
Raw: [ 0.01223885  0.0148803   0.02156532 ... -0.02666729 -0.01215068
  0.01031338]
Times: [   1012    1333    1523 ... 1438660 1438690 1439287]
Cluster: [1 3 3 ... 3 1 3]
Number of different clusters:  3
Number of Spikes: 3522
First aligned Spike Frame: [-0.12579972 -0.1620207  -0.18573939 -0.18961714 -0.17315203 -0.1396561
 -0.09426159 -0.04496221  0.00734136  0.05943689  0.09448878  0.07601101
  0.00811529  0.09446653  0.55421698  1.03008495  0.90195967  0.23116337
 -0.37857386 -0.59030761 -0.52695739 -0.40967661 -0.32471334 -0.26000903
 -0.19839489 -0.13975054 -0.09692457 -0.08223204 -0.09147337 -0.10695261
 -0.11338461 -0.10883018 -0.09915766 -0.08637804 -0.06514771 -0.03537735
 -0.00695045  0.01307529  0.02539335  0.03607991  0.04814655  0.05257424
  0.04538721  0.03512231  0.03354714  0.04141478  0.0511991 ]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1134
Cluster 2, Occurrences: 1237
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DAEF0>
Epoch 1
-------------------------------
loss: 0.121479  [    0/ 3522]
loss: 0.216352  [  100/ 3522]
loss: 0.104828  [  200/ 3522]
loss: 0.051205  [  300/ 3522]
loss: 0.014731  [  400/ 3522]
loss: 0.009249  [  500/ 3522]
loss: 0.118382  [  600/ 3522]
loss: 0.043185  [  700/ 3522]
loss: 0.034552  [  800/ 3522]
loss: 0.010526  [  900/ 3522]
loss: 0.005268  [ 1000/ 3522]
loss: 0.007805  [ 1100/ 3522]
loss: 0.094653  [ 1200/ 3522]
loss: 0.016315  [ 1300/ 3522]
loss: 0.004839  [ 1400/ 3522]
loss: 0.010977  [ 1500/ 3522]
loss: 0.011772  [ 1600/ 3522]
loss: 0.009203  [ 1700/ 3522]
loss: 0.012150  [ 1800/ 3522]
loss: 0.015329  [ 1900/ 3522]
loss: 0.011205  [ 2000/ 3522]
loss: 0.015114  [ 2100/ 3522]
loss: 0.004736  [ 2200/ 3522]
loss: 0.006229  [ 2300/ 3522]
loss: 0.009600  [ 2400/ 3522]
loss: 0.005251  [ 2500/ 3522]
loss: 0.077366  [ 2600/ 3522]
loss: 0.011168  [ 2700/ 3522]
loss: 0.003164  [ 2800/ 3522]
loss: 0.004551  [ 2900/ 3522]
loss: 0.010903  [ 3000/ 3522]
loss: 0.010765  [ 3100/ 3522]
loss: 0.014474  [ 3200/ 3522]
loss: 0.010065  [ 3300/ 3522]
loss: 0.009909  [ 3400/ 3522]
loss: 0.009848  [ 3500/ 3522]
Epoch 2
-------------------------------
loss: 0.010614  [    0/ 3522]
loss: 0.008604  [  100/ 3522]
loss: 0.005355  [  200/ 3522]
loss: 0.050326  [  300/ 3522]
loss: 0.007249  [  400/ 3522]
loss: 0.008885  [  500/ 3522]
loss: 0.109459  [  600/ 3522]
loss: 0.015045  [  700/ 3522]
loss: 0.012830  [  800/ 3522]
loss: 0.007045  [  900/ 3522]
loss: 0.005353  [ 1000/ 3522]
loss: 0.003954  [ 1100/ 3522]
loss: 0.088842  [ 1200/ 3522]
loss: 0.015504  [ 1300/ 3522]
loss: 0.002786  [ 1400/ 3522]
loss: 0.002878  [ 1500/ 3522]
loss: 0.010022  [ 1600/ 3522]
loss: 0.006755  [ 1700/ 3522]
loss: 0.008263  [ 1800/ 3522]
loss: 0.015613  [ 1900/ 3522]
loss: 0.011283  [ 2000/ 3522]
loss: 0.013944  [ 2100/ 3522]
loss: 0.004755  [ 2200/ 3522]
loss: 0.004322  [ 2300/ 3522]
loss: 0.006735  [ 2400/ 3522]
loss: 0.004875  [ 2500/ 3522]
loss: 0.065627  [ 2600/ 3522]
loss: 0.008958  [ 2700/ 3522]
loss: 0.003172  [ 2800/ 3522]
loss: 0.004181  [ 2900/ 3522]
loss: 0.008314  [ 3000/ 3522]
loss: 0.008251  [ 3100/ 3522]
loss: 0.015559  [ 3200/ 3522]
loss: 0.009900  [ 3300/ 3522]
loss: 0.009392  [ 3400/ 3522]
loss: 0.006121  [ 3500/ 3522]
Epoch 3
-------------------------------
loss: 0.010220  [    0/ 3522]
loss: 0.007332  [  100/ 3522]
loss: 0.004490  [  200/ 3522]
loss: 0.041463  [  300/ 3522]
loss: 0.007862  [  400/ 3522]
loss: 0.009457  [  500/ 3522]
loss: 0.099700  [  600/ 3522]
loss: 0.015146  [  700/ 3522]
loss: 0.013373  [  800/ 3522]
loss: 0.005558  [  900/ 3522]
loss: 0.004315  [ 1000/ 3522]
loss: 0.003996  [ 1100/ 3522]
loss: 0.089685  [ 1200/ 3522]
loss: 0.016034  [ 1300/ 3522]
loss: 0.002630  [ 1400/ 3522]
loss: 0.003215  [ 1500/ 3522]
loss: 0.010802  [ 1600/ 3522]
loss: 0.005809  [ 1700/ 3522]
loss: 0.010186  [ 1800/ 3522]
loss: 0.013408  [ 1900/ 3522]
loss: 0.009617  [ 2000/ 3522]
loss: 0.009600  [ 2100/ 3522]
loss: 0.004398  [ 2200/ 3522]
loss: 0.003797  [ 2300/ 3522]
loss: 0.004963  [ 2400/ 3522]
loss: 0.004427  [ 2500/ 3522]
loss: 0.069949  [ 2600/ 3522]
loss: 0.005271  [ 2700/ 3522]
loss: 0.002951  [ 2800/ 3522]
loss: 0.005217  [ 2900/ 3522]
loss: 0.009525  [ 3000/ 3522]
loss: 0.007798  [ 3100/ 3522]
loss: 0.015076  [ 3200/ 3522]
loss: 0.009838  [ 3300/ 3522]
loss: 0.012940  [ 3400/ 3522]
loss: 0.005420  [ 3500/ 3522]
Epoch 4
-------------------------------
loss: 0.010056  [    0/ 3522]
loss: 0.006691  [  100/ 3522]
loss: 0.003326  [  200/ 3522]
loss: 0.028889  [  300/ 3522]
loss: 0.006031  [  400/ 3522]
loss: 0.010071  [  500/ 3522]
loss: 0.093121  [  600/ 3522]
loss: 0.012493  [  700/ 3522]
loss: 0.010416  [  800/ 3522]
loss: 0.004885  [  900/ 3522]
loss: 0.003775  [ 1000/ 3522]
loss: 0.003926  [ 1100/ 3522]
loss: 0.091414  [ 1200/ 3522]
loss: 0.014820  [ 1300/ 3522]
loss: 0.002473  [ 1400/ 3522]
loss: 0.005039  [ 1500/ 3522]
loss: 0.010546  [ 1600/ 3522]
loss: 0.005604  [ 1700/ 3522]
loss: 0.013022  [ 1800/ 3522]
loss: 0.012297  [ 1900/ 3522]
loss: 0.008715  [ 2000/ 3522]
loss: 0.007878  [ 2100/ 3522]
loss: 0.003667  [ 2200/ 3522]
loss: 0.004891  [ 2300/ 3522]
loss: 0.003494  [ 2400/ 3522]
loss: 0.004173  [ 2500/ 3522]
loss: 0.074051  [ 2600/ 3522]
loss: 0.005809  [ 2700/ 3522]
loss: 0.002736  [ 2800/ 3522]
loss: 0.004763  [ 2900/ 3522]
loss: 0.008105  [ 3000/ 3522]
loss: 0.007086  [ 3100/ 3522]
loss: 0.014324  [ 3200/ 3522]
loss: 0.008870  [ 3300/ 3522]
loss: 0.014587  [ 3400/ 3522]
loss: 0.005155  [ 3500/ 3522]
Epoch 5
-------------------------------
loss: 0.009622  [    0/ 3522]
loss: 0.006552  [  100/ 3522]
loss: 0.002952  [  200/ 3522]
loss: 0.021360  [  300/ 3522]
loss: 0.004990  [  400/ 3522]
loss: 0.009974  [  500/ 3522]
loss: 0.071498  [  600/ 3522]
loss: 0.011791  [  700/ 3522]
loss: 0.008646  [  800/ 3522]
loss: 0.004409  [  900/ 3522]
loss: 0.003513  [ 1000/ 3522]
loss: 0.003881  [ 1100/ 3522]
loss: 0.101673  [ 1200/ 3522]
loss: 0.013808  [ 1300/ 3522]
loss: 0.002327  [ 1400/ 3522]
loss: 0.005165  [ 1500/ 3522]
loss: 0.010083  [ 1600/ 3522]
loss: 0.005662  [ 1700/ 3522]
loss: 0.013205  [ 1800/ 3522]
loss: 0.011314  [ 1900/ 3522]
loss: 0.008906  [ 2000/ 3522]
loss: 0.007039  [ 2100/ 3522]
loss: 0.003522  [ 2200/ 3522]
loss: 0.004422  [ 2300/ 3522]
loss: 0.003448  [ 2400/ 3522]
loss: 0.003931  [ 2500/ 3522]
loss: 0.076334  [ 2600/ 3522]
loss: 0.006884  [ 2700/ 3522]
loss: 0.002629  [ 2800/ 3522]
loss: 0.004757  [ 2900/ 3522]
loss: 0.007891  [ 3000/ 3522]
loss: 0.005512  [ 3100/ 3522]
loss: 0.013786  [ 3200/ 3522]
loss: 0.007714  [ 3300/ 3522]
loss: 0.014998  [ 3400/ 3522]
loss: 0.005810  [ 3500/ 3522]
Epoch 6
-------------------------------
loss: 0.009087  [    0/ 3522]
loss: 0.006864  [  100/ 3522]
loss: 0.002798  [  200/ 3522]
loss: 0.014777  [  300/ 3522]
loss: 0.004587  [  400/ 3522]
loss: 0.009832  [  500/ 3522]
loss: 0.062919  [  600/ 3522]
loss: 0.011407  [  700/ 3522]
loss: 0.006721  [  800/ 3522]
loss: 0.004477  [  900/ 3522]
loss: 0.003466  [ 1000/ 3522]
loss: 0.003819  [ 1100/ 3522]
loss: 0.103432  [ 1200/ 3522]
loss: 0.012942  [ 1300/ 3522]
loss: 0.002319  [ 1400/ 3522]
loss: 0.004852  [ 1500/ 3522]
loss: 0.009806  [ 1600/ 3522]
loss: 0.006000  [ 1700/ 3522]
loss: 0.013569  [ 1800/ 3522]
loss: 0.010699  [ 1900/ 3522]
loss: 0.008998  [ 2000/ 3522]
loss: 0.006921  [ 2100/ 3522]
loss: 0.003499  [ 2200/ 3522]
loss: 0.004872  [ 2300/ 3522]
loss: 0.003826  [ 2400/ 3522]
loss: 0.003579  [ 2500/ 3522]
loss: 0.076984  [ 2600/ 3522]
loss: 0.007272  [ 2700/ 3522]
loss: 0.002630  [ 2800/ 3522]
loss: 0.004447  [ 2900/ 3522]
loss: 0.007672  [ 3000/ 3522]
loss: 0.005087  [ 3100/ 3522]
loss: 0.013168  [ 3200/ 3522]
loss: 0.007303  [ 3300/ 3522]
loss: 0.015451  [ 3400/ 3522]
loss: 0.006339  [ 3500/ 3522]
Epoch 7
-------------------------------
loss: 0.008765  [    0/ 3522]
loss: 0.007260  [  100/ 3522]
loss: 0.002831  [  200/ 3522]
loss: 0.012570  [  300/ 3522]
loss: 0.004398  [  400/ 3522]
loss: 0.009788  [  500/ 3522]
loss: 0.062687  [  600/ 3522]
loss: 0.011566  [  700/ 3522]
loss: 0.006293  [  800/ 3522]
loss: 0.004759  [  900/ 3522]
loss: 0.003348  [ 1000/ 3522]
loss: 0.003756  [ 1100/ 3522]
loss: 0.101643  [ 1200/ 3522]
loss: 0.012245  [ 1300/ 3522]
loss: 0.002408  [ 1400/ 3522]
loss: 0.004650  [ 1500/ 3522]
loss: 0.009691  [ 1600/ 3522]
loss: 0.005851  [ 1700/ 3522]
loss: 0.014072  [ 1800/ 3522]
loss: 0.010446  [ 1900/ 3522]
loss: 0.009182  [ 2000/ 3522]
loss: 0.006805  [ 2100/ 3522]
loss: 0.003450  [ 2200/ 3522]
loss: 0.004227  [ 2300/ 3522]
loss: 0.004127  [ 2400/ 3522]
loss: 0.003149  [ 2500/ 3522]
loss: 0.077227  [ 2600/ 3522]
loss: 0.007307  [ 2700/ 3522]
loss: 0.002570  [ 2800/ 3522]
loss: 0.004398  [ 2900/ 3522]
loss: 0.007656  [ 3000/ 3522]
loss: 0.005109  [ 3100/ 3522]
loss: 0.012341  [ 3200/ 3522]
loss: 0.006963  [ 3300/ 3522]
loss: 0.015515  [ 3400/ 3522]
loss: 0.006555  [ 3500/ 3522]
Epoch 8
-------------------------------
loss: 0.008326  [    0/ 3522]
loss: 0.007678  [  100/ 3522]
loss: 0.002810  [  200/ 3522]
loss: 0.011603  [  300/ 3522]
loss: 0.004308  [  400/ 3522]
loss: 0.009813  [  500/ 3522]
loss: 0.064181  [  600/ 3522]
loss: 0.011776  [  700/ 3522]
loss: 0.006287  [  800/ 3522]
loss: 0.005333  [  900/ 3522]
loss: 0.003410  [ 1000/ 3522]
loss: 0.003753  [ 1100/ 3522]
loss: 0.098212  [ 1200/ 3522]
loss: 0.011959  [ 1300/ 3522]
loss: 0.002507  [ 1400/ 3522]
loss: 0.004263  [ 1500/ 3522]
loss: 0.009508  [ 1600/ 3522]
loss: 0.005900  [ 1700/ 3522]
loss: 0.013690  [ 1800/ 3522]
loss: 0.010378  [ 1900/ 3522]
loss: 0.009475  [ 2000/ 3522]
loss: 0.006933  [ 2100/ 3522]
loss: 0.003448  [ 2200/ 3522]
loss: 0.004473  [ 2300/ 3522]
loss: 0.003864  [ 2400/ 3522]
loss: 0.002886  [ 2500/ 3522]
loss: 0.077146  [ 2600/ 3522]
loss: 0.007109  [ 2700/ 3522]
loss: 0.002568  [ 2800/ 3522]
loss: 0.004305  [ 2900/ 3522]
loss: 0.007853  [ 3000/ 3522]
loss: 0.004769  [ 3100/ 3522]
loss: 0.011050  [ 3200/ 3522]
loss: 0.006914  [ 3300/ 3522]
loss: 0.015476  [ 3400/ 3522]
loss: 0.006624  [ 3500/ 3522]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3522
First Spike after testing: [-1.0407838  0.5874376]
[0 2 2 ... 2 0 2]
[2 0 0 ... 0 2 0]
Cluster 0 Occurrences: 1151; KMEANS: 1248
Cluster 1 Occurrences: 1134; KMEANS: 1127
Cluster 2 Occurrences: 1237; KMEANS: 1147
Centroids: [[-1.2928071, 0.60571915], [2.1996033, 0.47891805], [-1.3866243, -1.2102778]]
Centroids: [[-1.3894067, -1.2064055], [2.2214177, 0.47724825], [-1.2890006, 0.61978865]]
Contingency Matrix: 
[[  10    0 1141]
 [   4 1124    6]
 [1234    3    0]]
[[-1, 0, 1141], [-1, 1124, 6], [-1, -1, -1]]
[[-1, -1, -1], [-1, 1124, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 0: 2, 1: 1}
New Contingency Matrix: 
[[1141    0   10]
 [   6 1124    4]
 [   0    3 1234]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1141, 1124, 1234], Sum: 3499
All_Elements: [1141, 0, 10, 6, 1124, 4, 0, 3, 1234], Sum: 3522
Accuracy: 0.9934696195343555
Done!
