Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_20
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211E899B588>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DA080>
Epoch 1
-------------------------------
loss: 0.151693  [    0/ 3474]
loss: 0.087339  [  100/ 3474]
loss: 0.124657  [  200/ 3474]
loss: 0.107422  [  300/ 3474]
loss: 0.273997  [  400/ 3474]
loss: 0.012104  [  500/ 3474]
loss: 0.046595  [  600/ 3474]
loss: 0.023512  [  700/ 3474]
loss: 0.057035  [  800/ 3474]
loss: 0.176514  [  900/ 3474]
loss: 0.035448  [ 1000/ 3474]
loss: 0.365530  [ 1100/ 3474]
loss: 0.061874  [ 1200/ 3474]
loss: 0.099243  [ 1300/ 3474]
loss: 0.032743  [ 1400/ 3474]
loss: 0.102098  [ 1500/ 3474]
loss: 0.018198  [ 1600/ 3474]
loss: 0.122832  [ 1700/ 3474]
loss: 0.022909  [ 1800/ 3474]
loss: 0.033678  [ 1900/ 3474]
loss: 0.018415  [ 2000/ 3474]
loss: 0.016873  [ 2100/ 3474]
loss: 0.023400  [ 2200/ 3474]
loss: 0.081582  [ 2300/ 3474]
loss: 0.015680  [ 2400/ 3474]
loss: 0.053337  [ 2500/ 3474]
loss: 0.028035  [ 2600/ 3474]
loss: 0.072258  [ 2700/ 3474]
loss: 0.040504  [ 2800/ 3474]
loss: 0.018051  [ 2900/ 3474]
loss: 0.306740  [ 3000/ 3474]
loss: 0.010210  [ 3100/ 3474]
loss: 0.012156  [ 3200/ 3474]
loss: 0.050540  [ 3300/ 3474]
loss: 0.081005  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.040443  [    0/ 3474]
loss: 0.009558  [  100/ 3474]
loss: 0.019427  [  200/ 3474]
loss: 0.057971  [  300/ 3474]
loss: 0.097386  [  400/ 3474]
loss: 0.007789  [  500/ 3474]
loss: 0.041129  [  600/ 3474]
loss: 0.010512  [  700/ 3474]
loss: 0.052345  [  800/ 3474]
loss: 0.176481  [  900/ 3474]
loss: 0.007310  [ 1000/ 3474]
loss: 0.236412  [ 1100/ 3474]
loss: 0.031988  [ 1200/ 3474]
loss: 0.037605  [ 1300/ 3474]
loss: 0.012706  [ 1400/ 3474]
loss: 0.085470  [ 1500/ 3474]
loss: 0.010689  [ 1600/ 3474]
loss: 0.079587  [ 1700/ 3474]
loss: 0.027375  [ 1800/ 3474]
loss: 0.031359  [ 1900/ 3474]
loss: 0.009476  [ 2000/ 3474]
loss: 0.017450  [ 2100/ 3474]
loss: 0.026056  [ 2200/ 3474]
loss: 0.068165  [ 2300/ 3474]
loss: 0.006346  [ 2400/ 3474]
loss: 0.053467  [ 2500/ 3474]
loss: 0.022830  [ 2600/ 3474]
loss: 0.040822  [ 2700/ 3474]
loss: 0.038645  [ 2800/ 3474]
loss: 0.017441  [ 2900/ 3474]
loss: 0.262704  [ 3000/ 3474]
loss: 0.010108  [ 3100/ 3474]
loss: 0.013411  [ 3200/ 3474]
loss: 0.049174  [ 3300/ 3474]
loss: 0.071708  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.031468  [    0/ 3474]
loss: 0.009051  [  100/ 3474]
loss: 0.018613  [  200/ 3474]
loss: 0.056845  [  300/ 3474]
loss: 0.063257  [  400/ 3474]
loss: 0.005607  [  500/ 3474]
loss: 0.038010  [  600/ 3474]
loss: 0.009268  [  700/ 3474]
loss: 0.050029  [  800/ 3474]
loss: 0.189621  [  900/ 3474]
loss: 0.007172  [ 1000/ 3474]
loss: 0.195809  [ 1100/ 3474]
loss: 0.025885  [ 1200/ 3474]
loss: 0.031108  [ 1300/ 3474]
loss: 0.013282  [ 1400/ 3474]
loss: 0.084272  [ 1500/ 3474]
loss: 0.010043  [ 1600/ 3474]
loss: 0.069558  [ 1700/ 3474]
loss: 0.027820  [ 1800/ 3474]
loss: 0.031095  [ 1900/ 3474]
loss: 0.008450  [ 2000/ 3474]
loss: 0.016885  [ 2100/ 3474]
loss: 0.025288  [ 2200/ 3474]
loss: 0.068984  [ 2300/ 3474]
loss: 0.006556  [ 2400/ 3474]
loss: 0.053464  [ 2500/ 3474]
loss: 0.021156  [ 2600/ 3474]
loss: 0.035101  [ 2700/ 3474]
loss: 0.035783  [ 2800/ 3474]
loss: 0.017445  [ 2900/ 3474]
loss: 0.244531  [ 3000/ 3474]
loss: 0.009995  [ 3100/ 3474]
loss: 0.014323  [ 3200/ 3474]
loss: 0.045923  [ 3300/ 3474]
loss: 0.070457  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.029953  [    0/ 3474]
loss: 0.009086  [  100/ 3474]
loss: 0.019836  [  200/ 3474]
loss: 0.057414  [  300/ 3474]
loss: 0.057491  [  400/ 3474]
loss: 0.005463  [  500/ 3474]
loss: 0.033916  [  600/ 3474]
loss: 0.008719  [  700/ 3474]
loss: 0.046588  [  800/ 3474]
loss: 0.189564  [  900/ 3474]
loss: 0.007317  [ 1000/ 3474]
loss: 0.179115  [ 1100/ 3474]
loss: 0.025372  [ 1200/ 3474]
loss: 0.034310  [ 1300/ 3474]
loss: 0.013767  [ 1400/ 3474]
loss: 0.084702  [ 1500/ 3474]
loss: 0.010044  [ 1600/ 3474]
loss: 0.064302  [ 1700/ 3474]
loss: 0.027136  [ 1800/ 3474]
loss: 0.030940  [ 1900/ 3474]
loss: 0.007845  [ 2000/ 3474]
loss: 0.016338  [ 2100/ 3474]
loss: 0.023173  [ 2200/ 3474]
loss: 0.070440  [ 2300/ 3474]
loss: 0.006874  [ 2400/ 3474]
loss: 0.050880  [ 2500/ 3474]
loss: 0.020125  [ 2600/ 3474]
loss: 0.032991  [ 2700/ 3474]
loss: 0.029353  [ 2800/ 3474]
loss: 0.017492  [ 2900/ 3474]
loss: 0.236789  [ 3000/ 3474]
loss: 0.010689  [ 3100/ 3474]
loss: 0.015905  [ 3200/ 3474]
loss: 0.042786  [ 3300/ 3474]
loss: 0.054366  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.028026  [    0/ 3474]
loss: 0.009104  [  100/ 3474]
loss: 0.020284  [  200/ 3474]
loss: 0.057748  [  300/ 3474]
loss: 0.056689  [  400/ 3474]
loss: 0.005512  [  500/ 3474]
loss: 0.026628  [  600/ 3474]
loss: 0.008440  [  700/ 3474]
loss: 0.044639  [  800/ 3474]
loss: 0.187679  [  900/ 3474]
loss: 0.007368  [ 1000/ 3474]
loss: 0.183027  [ 1100/ 3474]
loss: 0.025371  [ 1200/ 3474]
loss: 0.036348  [ 1300/ 3474]
loss: 0.013466  [ 1400/ 3474]
loss: 0.085983  [ 1500/ 3474]
loss: 0.010767  [ 1600/ 3474]
loss: 0.059379  [ 1700/ 3474]
loss: 0.027346  [ 1800/ 3474]
loss: 0.031191  [ 1900/ 3474]
loss: 0.007524  [ 2000/ 3474]
loss: 0.016147  [ 2100/ 3474]
loss: 0.022775  [ 2200/ 3474]
loss: 0.072056  [ 2300/ 3474]
loss: 0.007100  [ 2400/ 3474]
loss: 0.047380  [ 2500/ 3474]
loss: 0.019547  [ 2600/ 3474]
loss: 0.033256  [ 2700/ 3474]
loss: 0.025526  [ 2800/ 3474]
loss: 0.017714  [ 2900/ 3474]
loss: 0.231616  [ 3000/ 3474]
loss: 0.012798  [ 3100/ 3474]
loss: 0.016076  [ 3200/ 3474]
loss: 0.041434  [ 3300/ 3474]
loss: 0.041588  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.025554  [    0/ 3474]
loss: 0.008995  [  100/ 3474]
loss: 0.020878  [  200/ 3474]
loss: 0.058209  [  300/ 3474]
loss: 0.057146  [  400/ 3474]
loss: 0.005559  [  500/ 3474]
loss: 0.019330  [  600/ 3474]
loss: 0.008203  [  700/ 3474]
loss: 0.042765  [  800/ 3474]
loss: 0.186668  [  900/ 3474]
loss: 0.007628  [ 1000/ 3474]
loss: 0.193219  [ 1100/ 3474]
loss: 0.024952  [ 1200/ 3474]
loss: 0.036518  [ 1300/ 3474]
loss: 0.012365  [ 1400/ 3474]
loss: 0.085759  [ 1500/ 3474]
loss: 0.012169  [ 1600/ 3474]
loss: 0.056843  [ 1700/ 3474]
loss: 0.027259  [ 1800/ 3474]
loss: 0.031621  [ 1900/ 3474]
loss: 0.007459  [ 2000/ 3474]
loss: 0.016040  [ 2100/ 3474]
loss: 0.022694  [ 2200/ 3474]
loss: 0.073736  [ 2300/ 3474]
loss: 0.007385  [ 2400/ 3474]
loss: 0.042618  [ 2500/ 3474]
loss: 0.018958  [ 2600/ 3474]
loss: 0.031318  [ 2700/ 3474]
loss: 0.022463  [ 2800/ 3474]
loss: 0.017649  [ 2900/ 3474]
loss: 0.226885  [ 3000/ 3474]
loss: 0.013010  [ 3100/ 3474]
loss: 0.015827  [ 3200/ 3474]
loss: 0.043972  [ 3300/ 3474]
loss: 0.031883  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.024106  [    0/ 3474]
loss: 0.008998  [  100/ 3474]
loss: 0.020466  [  200/ 3474]
loss: 0.058912  [  300/ 3474]
loss: 0.060098  [  400/ 3474]
loss: 0.005665  [  500/ 3474]
loss: 0.014708  [  600/ 3474]
loss: 0.008036  [  700/ 3474]
loss: 0.040762  [  800/ 3474]
loss: 0.186458  [  900/ 3474]
loss: 0.008190  [ 1000/ 3474]
loss: 0.201305  [ 1100/ 3474]
loss: 0.023821  [ 1200/ 3474]
loss: 0.038566  [ 1300/ 3474]
loss: 0.013111  [ 1400/ 3474]
loss: 0.084973  [ 1500/ 3474]
loss: 0.013612  [ 1600/ 3474]
loss: 0.054226  [ 1700/ 3474]
loss: 0.027065  [ 1800/ 3474]
loss: 0.031746  [ 1900/ 3474]
loss: 0.007665  [ 2000/ 3474]
loss: 0.016004  [ 2100/ 3474]
loss: 0.023142  [ 2200/ 3474]
loss: 0.074361  [ 2300/ 3474]
loss: 0.007402  [ 2400/ 3474]
loss: 0.038216  [ 2500/ 3474]
loss: 0.018662  [ 2600/ 3474]
loss: 0.031048  [ 2700/ 3474]
loss: 0.020401  [ 2800/ 3474]
loss: 0.017517  [ 2900/ 3474]
loss: 0.222524  [ 3000/ 3474]
loss: 0.012970  [ 3100/ 3474]
loss: 0.015784  [ 3200/ 3474]
loss: 0.047539  [ 3300/ 3474]
loss: 0.025151  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.023917  [    0/ 3474]
loss: 0.009058  [  100/ 3474]
loss: 0.020174  [  200/ 3474]
loss: 0.058922  [  300/ 3474]
loss: 0.063025  [  400/ 3474]
loss: 0.005568  [  500/ 3474]
loss: 0.012216  [  600/ 3474]
loss: 0.007917  [  700/ 3474]
loss: 0.039171  [  800/ 3474]
loss: 0.187133  [  900/ 3474]
loss: 0.012008  [ 1000/ 3474]
loss: 0.223397  [ 1100/ 3474]
loss: 0.022430  [ 1200/ 3474]
loss: 0.041441  [ 1300/ 3474]
loss: 0.012485  [ 1400/ 3474]
loss: 0.083919  [ 1500/ 3474]
loss: 0.014385  [ 1600/ 3474]
loss: 0.047426  [ 1700/ 3474]
loss: 0.026457  [ 1800/ 3474]
loss: 0.032158  [ 1900/ 3474]
loss: 0.007781  [ 2000/ 3474]
loss: 0.015942  [ 2100/ 3474]
loss: 0.024241  [ 2200/ 3474]
loss: 0.075082  [ 2300/ 3474]
loss: 0.007557  [ 2400/ 3474]
loss: 0.033624  [ 2500/ 3474]
loss: 0.018154  [ 2600/ 3474]
loss: 0.028258  [ 2700/ 3474]
loss: 0.018692  [ 2800/ 3474]
loss: 0.017386  [ 2900/ 3474]
loss: 0.217327  [ 3000/ 3474]
loss: 0.013174  [ 3100/ 3474]
loss: 0.015899  [ 3200/ 3474]
loss: 0.050970  [ 3300/ 3474]
loss: 0.019596  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [-0.8489332   0.44658488]
[0 1 1 ... 1 1 2]
[2 0 0 ... 0 0 1]
Cluster 0 Occurrences: 1198; KMEANS: 1108
Cluster 1 Occurrences: 1128; KMEANS: 1176
Cluster 2 Occurrences: 1148; KMEANS: 1190
Centroids: [[-0.6682924, -0.05296017], [0.423516, 3.0416532], [-2.1794364, -0.25518864]]
Centroids: [[0.45579746, 3.0993865], [-2.1759827, -0.26550266], [-0.6478568, -0.039753787]]
Contingency Matrix: 
[[   0   24 1174]
 [1108   11    9]
 [   0 1141    7]]
[[-1, -1, -1], [1108, 11, -1], [0, 1141, -1]]
[[-1, -1, -1], [1108, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 1, 1: 0}
New Contingency Matrix: 
[[1174    0   24]
 [   9 1108   11]
 [   7    0 1141]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1174, 1108, 1141], Sum: 3423
All_Elements: [1174, 0, 24, 9, 1108, 11, 7, 0, 1141], Sum: 3474
Accuracy: 0.9853195164075993
Done!
