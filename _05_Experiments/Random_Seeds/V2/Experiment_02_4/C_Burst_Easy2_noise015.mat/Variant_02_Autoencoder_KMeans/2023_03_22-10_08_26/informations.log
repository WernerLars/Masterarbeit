Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_26
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000212158D64E0>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x0000021209B12588>
Epoch 1
-------------------------------
loss: 0.245013  [    0/ 3442]
loss: 0.123057  [  100/ 3442]
loss: 0.052890  [  200/ 3442]
loss: 0.032402  [  300/ 3442]
loss: 0.025725  [  400/ 3442]
loss: 0.027346  [  500/ 3442]
loss: 0.033680  [  600/ 3442]
loss: 0.016228  [  700/ 3442]
loss: 0.020206  [  800/ 3442]
loss: 0.045989  [  900/ 3442]
loss: 0.022619  [ 1000/ 3442]
loss: 0.003997  [ 1100/ 3442]
loss: 0.022678  [ 1200/ 3442]
loss: 0.012825  [ 1300/ 3442]
loss: 0.016463  [ 1400/ 3442]
loss: 0.021648  [ 1500/ 3442]
loss: 0.017501  [ 1600/ 3442]
loss: 0.012231  [ 1700/ 3442]
loss: 0.018431  [ 1800/ 3442]
loss: 0.023504  [ 1900/ 3442]
loss: 0.018707  [ 2000/ 3442]
loss: 0.014268  [ 2100/ 3442]
loss: 0.016753  [ 2200/ 3442]
loss: 0.011980  [ 2300/ 3442]
loss: 0.014835  [ 2400/ 3442]
loss: 0.036571  [ 2500/ 3442]
loss: 0.130228  [ 2600/ 3442]
loss: 0.028423  [ 2700/ 3442]
loss: 0.009913  [ 2800/ 3442]
loss: 0.006963  [ 2900/ 3442]
loss: 0.005684  [ 3000/ 3442]
loss: 0.017894  [ 3100/ 3442]
loss: 0.113278  [ 3200/ 3442]
loss: 0.014224  [ 3300/ 3442]
loss: 0.019072  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.012142  [    0/ 3442]
loss: 0.014391  [  100/ 3442]
loss: 0.029276  [  200/ 3442]
loss: 0.006790  [  300/ 3442]
loss: 0.013925  [  400/ 3442]
loss: 0.008402  [  500/ 3442]
loss: 0.027052  [  600/ 3442]
loss: 0.013236  [  700/ 3442]
loss: 0.015483  [  800/ 3442]
loss: 0.030112  [  900/ 3442]
loss: 0.011144  [ 1000/ 3442]
loss: 0.002920  [ 1100/ 3442]
loss: 0.015732  [ 1200/ 3442]
loss: 0.005565  [ 1300/ 3442]
loss: 0.015089  [ 1400/ 3442]
loss: 0.016764  [ 1500/ 3442]
loss: 0.017339  [ 1600/ 3442]
loss: 0.010438  [ 1700/ 3442]
loss: 0.018910  [ 1800/ 3442]
loss: 0.014511  [ 1900/ 3442]
loss: 0.016685  [ 2000/ 3442]
loss: 0.012843  [ 2100/ 3442]
loss: 0.012733  [ 2200/ 3442]
loss: 0.011039  [ 2300/ 3442]
loss: 0.012745  [ 2400/ 3442]
loss: 0.035169  [ 2500/ 3442]
loss: 0.128353  [ 2600/ 3442]
loss: 0.027536  [ 2700/ 3442]
loss: 0.009330  [ 2800/ 3442]
loss: 0.008389  [ 2900/ 3442]
loss: 0.005575  [ 3000/ 3442]
loss: 0.018324  [ 3100/ 3442]
loss: 0.112501  [ 3200/ 3442]
loss: 0.011335  [ 3300/ 3442]
loss: 0.027974  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.012148  [    0/ 3442]
loss: 0.015050  [  100/ 3442]
loss: 0.029316  [  200/ 3442]
loss: 0.006678  [  300/ 3442]
loss: 0.014969  [  400/ 3442]
loss: 0.008588  [  500/ 3442]
loss: 0.027053  [  600/ 3442]
loss: 0.013687  [  700/ 3442]
loss: 0.016234  [  800/ 3442]
loss: 0.028865  [  900/ 3442]
loss: 0.012101  [ 1000/ 3442]
loss: 0.003112  [ 1100/ 3442]
loss: 0.017230  [ 1200/ 3442]
loss: 0.004650  [ 1300/ 3442]
loss: 0.013828  [ 1400/ 3442]
loss: 0.017288  [ 1500/ 3442]
loss: 0.013678  [ 1600/ 3442]
loss: 0.010154  [ 1700/ 3442]
loss: 0.018750  [ 1800/ 3442]
loss: 0.014364  [ 1900/ 3442]
loss: 0.016459  [ 2000/ 3442]
loss: 0.012422  [ 2100/ 3442]
loss: 0.012188  [ 2200/ 3442]
loss: 0.010900  [ 2300/ 3442]
loss: 0.011955  [ 2400/ 3442]
loss: 0.034316  [ 2500/ 3442]
loss: 0.126974  [ 2600/ 3442]
loss: 0.028664  [ 2700/ 3442]
loss: 0.009520  [ 2800/ 3442]
loss: 0.008045  [ 2900/ 3442]
loss: 0.005459  [ 3000/ 3442]
loss: 0.018449  [ 3100/ 3442]
loss: 0.112130  [ 3200/ 3442]
loss: 0.010530  [ 3300/ 3442]
loss: 0.029996  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.014365  [    0/ 3442]
loss: 0.015089  [  100/ 3442]
loss: 0.030104  [  200/ 3442]
loss: 0.006723  [  300/ 3442]
loss: 0.013710  [  400/ 3442]
loss: 0.009298  [  500/ 3442]
loss: 0.027470  [  600/ 3442]
loss: 0.012910  [  700/ 3442]
loss: 0.015941  [  800/ 3442]
loss: 0.029203  [  900/ 3442]
loss: 0.010936  [ 1000/ 3442]
loss: 0.003177  [ 1100/ 3442]
loss: 0.017355  [ 1200/ 3442]
loss: 0.004489  [ 1300/ 3442]
loss: 0.013052  [ 1400/ 3442]
loss: 0.017494  [ 1500/ 3442]
loss: 0.012781  [ 1600/ 3442]
loss: 0.010181  [ 1700/ 3442]
loss: 0.018319  [ 1800/ 3442]
loss: 0.013959  [ 1900/ 3442]
loss: 0.016474  [ 2000/ 3442]
loss: 0.012013  [ 2100/ 3442]
loss: 0.011979  [ 2200/ 3442]
loss: 0.010805  [ 2300/ 3442]
loss: 0.011832  [ 2400/ 3442]
loss: 0.032326  [ 2500/ 3442]
loss: 0.125906  [ 2600/ 3442]
loss: 0.029704  [ 2700/ 3442]
loss: 0.009588  [ 2800/ 3442]
loss: 0.007763  [ 2900/ 3442]
loss: 0.005405  [ 3000/ 3442]
loss: 0.018664  [ 3100/ 3442]
loss: 0.112948  [ 3200/ 3442]
loss: 0.010123  [ 3300/ 3442]
loss: 0.032384  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.015525  [    0/ 3442]
loss: 0.015102  [  100/ 3442]
loss: 0.030602  [  200/ 3442]
loss: 0.006691  [  300/ 3442]
loss: 0.012849  [  400/ 3442]
loss: 0.009630  [  500/ 3442]
loss: 0.027584  [  600/ 3442]
loss: 0.012219  [  700/ 3442]
loss: 0.015667  [  800/ 3442]
loss: 0.029251  [  900/ 3442]
loss: 0.009884  [ 1000/ 3442]
loss: 0.003230  [ 1100/ 3442]
loss: 0.016780  [ 1200/ 3442]
loss: 0.004595  [ 1300/ 3442]
loss: 0.012434  [ 1400/ 3442]
loss: 0.017691  [ 1500/ 3442]
loss: 0.012069  [ 1600/ 3442]
loss: 0.010206  [ 1700/ 3442]
loss: 0.018212  [ 1800/ 3442]
loss: 0.013861  [ 1900/ 3442]
loss: 0.016332  [ 2000/ 3442]
loss: 0.011853  [ 2100/ 3442]
loss: 0.011730  [ 2200/ 3442]
loss: 0.010781  [ 2300/ 3442]
loss: 0.011892  [ 2400/ 3442]
loss: 0.032285  [ 2500/ 3442]
loss: 0.125746  [ 2600/ 3442]
loss: 0.030384  [ 2700/ 3442]
loss: 0.009616  [ 2800/ 3442]
loss: 0.007644  [ 2900/ 3442]
loss: 0.005453  [ 3000/ 3442]
loss: 0.018807  [ 3100/ 3442]
loss: 0.113122  [ 3200/ 3442]
loss: 0.010110  [ 3300/ 3442]
loss: 0.033278  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.016502  [    0/ 3442]
loss: 0.014992  [  100/ 3442]
loss: 0.030766  [  200/ 3442]
loss: 0.006630  [  300/ 3442]
loss: 0.012693  [  400/ 3442]
loss: 0.009768  [  500/ 3442]
loss: 0.027565  [  600/ 3442]
loss: 0.011643  [  700/ 3442]
loss: 0.015768  [  800/ 3442]
loss: 0.029110  [  900/ 3442]
loss: 0.008977  [ 1000/ 3442]
loss: 0.003156  [ 1100/ 3442]
loss: 0.016246  [ 1200/ 3442]
loss: 0.004706  [ 1300/ 3442]
loss: 0.012003  [ 1400/ 3442]
loss: 0.017839  [ 1500/ 3442]
loss: 0.011747  [ 1600/ 3442]
loss: 0.010244  [ 1700/ 3442]
loss: 0.017962  [ 1800/ 3442]
loss: 0.013775  [ 1900/ 3442]
loss: 0.016286  [ 2000/ 3442]
loss: 0.011826  [ 2100/ 3442]
loss: 0.011563  [ 2200/ 3442]
loss: 0.010667  [ 2300/ 3442]
loss: 0.012036  [ 2400/ 3442]
loss: 0.031907  [ 2500/ 3442]
loss: 0.125862  [ 2600/ 3442]
loss: 0.030789  [ 2700/ 3442]
loss: 0.009560  [ 2800/ 3442]
loss: 0.007598  [ 2900/ 3442]
loss: 0.005415  [ 3000/ 3442]
loss: 0.018946  [ 3100/ 3442]
loss: 0.113315  [ 3200/ 3442]
loss: 0.009718  [ 3300/ 3442]
loss: 0.035891  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.016602  [    0/ 3442]
loss: 0.014905  [  100/ 3442]
loss: 0.030841  [  200/ 3442]
loss: 0.006671  [  300/ 3442]
loss: 0.012441  [  400/ 3442]
loss: 0.009843  [  500/ 3442]
loss: 0.027550  [  600/ 3442]
loss: 0.011333  [  700/ 3442]
loss: 0.015678  [  800/ 3442]
loss: 0.028963  [  900/ 3442]
loss: 0.008344  [ 1000/ 3442]
loss: 0.003149  [ 1100/ 3442]
loss: 0.015883  [ 1200/ 3442]
loss: 0.004874  [ 1300/ 3442]
loss: 0.011635  [ 1400/ 3442]
loss: 0.018225  [ 1500/ 3442]
loss: 0.011629  [ 1600/ 3442]
loss: 0.010268  [ 1700/ 3442]
loss: 0.017845  [ 1800/ 3442]
loss: 0.013741  [ 1900/ 3442]
loss: 0.016210  [ 2000/ 3442]
loss: 0.011755  [ 2100/ 3442]
loss: 0.011611  [ 2200/ 3442]
loss: 0.010651  [ 2300/ 3442]
loss: 0.012123  [ 2400/ 3442]
loss: 0.031411  [ 2500/ 3442]
loss: 0.125548  [ 2600/ 3442]
loss: 0.030780  [ 2700/ 3442]
loss: 0.009471  [ 2800/ 3442]
loss: 0.007538  [ 2900/ 3442]
loss: 0.005456  [ 3000/ 3442]
loss: 0.019017  [ 3100/ 3442]
loss: 0.113054  [ 3200/ 3442]
loss: 0.009933  [ 3300/ 3442]
loss: 0.034629  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.017363  [    0/ 3442]
loss: 0.014752  [  100/ 3442]
loss: 0.030989  [  200/ 3442]
loss: 0.006671  [  300/ 3442]
loss: 0.012049  [  400/ 3442]
loss: 0.009942  [  500/ 3442]
loss: 0.027542  [  600/ 3442]
loss: 0.010925  [  700/ 3442]
loss: 0.015729  [  800/ 3442]
loss: 0.028952  [  900/ 3442]
loss: 0.007884  [ 1000/ 3442]
loss: 0.003215  [ 1100/ 3442]
loss: 0.015504  [ 1200/ 3442]
loss: 0.005014  [ 1300/ 3442]
loss: 0.011426  [ 1400/ 3442]
loss: 0.018451  [ 1500/ 3442]
loss: 0.011536  [ 1600/ 3442]
loss: 0.010184  [ 1700/ 3442]
loss: 0.017832  [ 1800/ 3442]
loss: 0.013688  [ 1900/ 3442]
loss: 0.015989  [ 2000/ 3442]
loss: 0.011716  [ 2100/ 3442]
loss: 0.011501  [ 2200/ 3442]
loss: 0.010593  [ 2300/ 3442]
loss: 0.012152  [ 2400/ 3442]
loss: 0.031041  [ 2500/ 3442]
loss: 0.124612  [ 2600/ 3442]
loss: 0.031223  [ 2700/ 3442]
loss: 0.009379  [ 2800/ 3442]
loss: 0.007489  [ 2900/ 3442]
loss: 0.005435  [ 3000/ 3442]
loss: 0.019050  [ 3100/ 3442]
loss: 0.112898  [ 3200/ 3442]
loss: 0.009809  [ 3300/ 3442]
loss: 0.035058  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [-0.20334154  1.3869706 ]
[1 0 0 ... 1 2 0]
[1 2 2 ... 1 0 1]
Cluster 0 Occurrences: 1159; KMEANS: 1077
Cluster 1 Occurrences: 1156; KMEANS: 1142
Cluster 2 Occurrences: 1127; KMEANS: 1223
Centroids: [[0.5036433, 1.9466549], [0.14004654, 1.205992], [-2.0557954, -0.6536877]]
Centroids: [[-2.1103897, -0.7165344], [0.065387174, 1.1344736], [0.51263505, 1.9539921]]
Contingency Matrix: 
[[   0   37 1122]
 [   1 1056   99]
 [1076   49    2]]
[[-1, -1, -1], [1, 1056, -1], [1076, 49, -1]]
[[-1, -1, -1], [-1, 1056, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 0, 1: 1}
New Contingency Matrix: 
[[1122   37    0]
 [  99 1056    1]
 [   2   49 1076]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1122, 1056, 1076], Sum: 3254
All_Elements: [1122, 37, 0, 99, 1056, 1, 2, 49, 1076], Sum: 3442
Accuracy: 0.9453805926786751
Done!
