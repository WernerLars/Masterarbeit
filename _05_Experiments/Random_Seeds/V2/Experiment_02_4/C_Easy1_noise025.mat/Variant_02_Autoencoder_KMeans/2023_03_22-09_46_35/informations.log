Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_35
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000211E899BBE0>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x00000211E96DAE80>
Epoch 1
-------------------------------
loss: 0.233884  [    0/ 3298]
loss: 0.342566  [  100/ 3298]
loss: 0.170570  [  200/ 3298]
loss: 0.068379  [  300/ 3298]
loss: 0.027666  [  400/ 3298]
loss: 0.032592  [  500/ 3298]
loss: 0.042067  [  600/ 3298]
loss: 0.101120  [  700/ 3298]
loss: 0.028389  [  800/ 3298]
loss: 0.112443  [  900/ 3298]
loss: 0.036207  [ 1000/ 3298]
loss: 0.096094  [ 1100/ 3298]
loss: 0.074034  [ 1200/ 3298]
loss: 0.041973  [ 1300/ 3298]
loss: 0.016355  [ 1400/ 3298]
loss: 0.046067  [ 1500/ 3298]
loss: 0.031122  [ 1600/ 3298]
loss: 0.157318  [ 1700/ 3298]
loss: 0.031950  [ 1800/ 3298]
loss: 0.028550  [ 1900/ 3298]
loss: 0.011197  [ 2000/ 3298]
loss: 0.009642  [ 2100/ 3298]
loss: 0.026092  [ 2200/ 3298]
loss: 0.024884  [ 2300/ 3298]
loss: 0.031366  [ 2400/ 3298]
loss: 0.032531  [ 2500/ 3298]
loss: 0.024581  [ 2600/ 3298]
loss: 0.038710  [ 2700/ 3298]
loss: 0.312713  [ 2800/ 3298]
loss: 0.035686  [ 2900/ 3298]
loss: 0.033986  [ 3000/ 3298]
loss: 0.020237  [ 3100/ 3298]
loss: 0.038446  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.050137  [    0/ 3298]
loss: 0.018136  [  100/ 3298]
loss: 0.104632  [  200/ 3298]
loss: 0.017046  [  300/ 3298]
loss: 0.017745  [  400/ 3298]
loss: 0.023864  [  500/ 3298]
loss: 0.033126  [  600/ 3298]
loss: 0.062320  [  700/ 3298]
loss: 0.023022  [  800/ 3298]
loss: 0.075532  [  900/ 3298]
loss: 0.037142  [ 1000/ 3298]
loss: 0.096182  [ 1100/ 3298]
loss: 0.073866  [ 1200/ 3298]
loss: 0.031545  [ 1300/ 3298]
loss: 0.009153  [ 1400/ 3298]
loss: 0.031403  [ 1500/ 3298]
loss: 0.026788  [ 1600/ 3298]
loss: 0.143521  [ 1700/ 3298]
loss: 0.022180  [ 1800/ 3298]
loss: 0.021785  [ 1900/ 3298]
loss: 0.009579  [ 2000/ 3298]
loss: 0.013477  [ 2100/ 3298]
loss: 0.021171  [ 2200/ 3298]
loss: 0.020332  [ 2300/ 3298]
loss: 0.024499  [ 2400/ 3298]
loss: 0.015297  [ 2500/ 3298]
loss: 0.026772  [ 2600/ 3298]
loss: 0.044486  [ 2700/ 3298]
loss: 0.376701  [ 2800/ 3298]
loss: 0.029837  [ 2900/ 3298]
loss: 0.034279  [ 3000/ 3298]
loss: 0.020341  [ 3100/ 3298]
loss: 0.044817  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.046587  [    0/ 3298]
loss: 0.018906  [  100/ 3298]
loss: 0.105644  [  200/ 3298]
loss: 0.017156  [  300/ 3298]
loss: 0.017754  [  400/ 3298]
loss: 0.021924  [  500/ 3298]
loss: 0.031413  [  600/ 3298]
loss: 0.062101  [  700/ 3298]
loss: 0.023145  [  800/ 3298]
loss: 0.073131  [  900/ 3298]
loss: 0.037436  [ 1000/ 3298]
loss: 0.093853  [ 1100/ 3298]
loss: 0.075994  [ 1200/ 3298]
loss: 0.028177  [ 1300/ 3298]
loss: 0.009003  [ 1400/ 3298]
loss: 0.026553  [ 1500/ 3298]
loss: 0.021393  [ 1600/ 3298]
loss: 0.126598  [ 1700/ 3298]
loss: 0.023575  [ 1800/ 3298]
loss: 0.021785  [ 1900/ 3298]
loss: 0.009738  [ 2000/ 3298]
loss: 0.014428  [ 2100/ 3298]
loss: 0.020008  [ 2200/ 3298]
loss: 0.019233  [ 2300/ 3298]
loss: 0.023624  [ 2400/ 3298]
loss: 0.016511  [ 2500/ 3298]
loss: 0.028581  [ 2600/ 3298]
loss: 0.044712  [ 2700/ 3298]
loss: 0.371668  [ 2800/ 3298]
loss: 0.029599  [ 2900/ 3298]
loss: 0.033690  [ 3000/ 3298]
loss: 0.019928  [ 3100/ 3298]
loss: 0.047166  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.042070  [    0/ 3298]
loss: 0.019168  [  100/ 3298]
loss: 0.106266  [  200/ 3298]
loss: 0.016307  [  300/ 3298]
loss: 0.015242  [  400/ 3298]
loss: 0.020594  [  500/ 3298]
loss: 0.029910  [  600/ 3298]
loss: 0.061535  [  700/ 3298]
loss: 0.023012  [  800/ 3298]
loss: 0.063978  [  900/ 3298]
loss: 0.037719  [ 1000/ 3298]
loss: 0.085197  [ 1100/ 3298]
loss: 0.077853  [ 1200/ 3298]
loss: 0.026788  [ 1300/ 3298]
loss: 0.009146  [ 1400/ 3298]
loss: 0.024972  [ 1500/ 3298]
loss: 0.018711  [ 1600/ 3298]
loss: 0.103142  [ 1700/ 3298]
loss: 0.025202  [ 1800/ 3298]
loss: 0.021643  [ 1900/ 3298]
loss: 0.009758  [ 2000/ 3298]
loss: 0.014585  [ 2100/ 3298]
loss: 0.019050  [ 2200/ 3298]
loss: 0.019108  [ 2300/ 3298]
loss: 0.023266  [ 2400/ 3298]
loss: 0.017972  [ 2500/ 3298]
loss: 0.032355  [ 2600/ 3298]
loss: 0.043955  [ 2700/ 3298]
loss: 0.357244  [ 2800/ 3298]
loss: 0.030268  [ 2900/ 3298]
loss: 0.033508  [ 3000/ 3298]
loss: 0.019990  [ 3100/ 3298]
loss: 0.048567  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.033799  [    0/ 3298]
loss: 0.019608  [  100/ 3298]
loss: 0.105713  [  200/ 3298]
loss: 0.014936  [  300/ 3298]
loss: 0.014216  [  400/ 3298]
loss: 0.019441  [  500/ 3298]
loss: 0.029274  [  600/ 3298]
loss: 0.060980  [  700/ 3298]
loss: 0.022778  [  800/ 3298]
loss: 0.050344  [  900/ 3298]
loss: 0.038342  [ 1000/ 3298]
loss: 0.062747  [ 1100/ 3298]
loss: 0.078199  [ 1200/ 3298]
loss: 0.025598  [ 1300/ 3298]
loss: 0.010824  [ 1400/ 3298]
loss: 0.024149  [ 1500/ 3298]
loss: 0.016449  [ 1600/ 3298]
loss: 0.074516  [ 1700/ 3298]
loss: 0.025908  [ 1800/ 3298]
loss: 0.021656  [ 1900/ 3298]
loss: 0.009831  [ 2000/ 3298]
loss: 0.014381  [ 2100/ 3298]
loss: 0.017967  [ 2200/ 3298]
loss: 0.019520  [ 2300/ 3298]
loss: 0.022434  [ 2400/ 3298]
loss: 0.019584  [ 2500/ 3298]
loss: 0.036289  [ 2600/ 3298]
loss: 0.043780  [ 2700/ 3298]
loss: 0.331228  [ 2800/ 3298]
loss: 0.031303  [ 2900/ 3298]
loss: 0.033339  [ 3000/ 3298]
loss: 0.019816  [ 3100/ 3298]
loss: 0.049168  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.022771  [    0/ 3298]
loss: 0.019868  [  100/ 3298]
loss: 0.105277  [  200/ 3298]
loss: 0.014786  [  300/ 3298]
loss: 0.009596  [  400/ 3298]
loss: 0.019930  [  500/ 3298]
loss: 0.029022  [  600/ 3298]
loss: 0.060496  [  700/ 3298]
loss: 0.022839  [  800/ 3298]
loss: 0.033284  [  900/ 3298]
loss: 0.038451  [ 1000/ 3298]
loss: 0.037759  [ 1100/ 3298]
loss: 0.075726  [ 1200/ 3298]
loss: 0.025290  [ 1300/ 3298]
loss: 0.013094  [ 1400/ 3298]
loss: 0.024092  [ 1500/ 3298]
loss: 0.015570  [ 1600/ 3298]
loss: 0.050907  [ 1700/ 3298]
loss: 0.025997  [ 1800/ 3298]
loss: 0.022230  [ 1900/ 3298]
loss: 0.009596  [ 2000/ 3298]
loss: 0.014392  [ 2100/ 3298]
loss: 0.017010  [ 2200/ 3298]
loss: 0.020026  [ 2300/ 3298]
loss: 0.021459  [ 2400/ 3298]
loss: 0.020868  [ 2500/ 3298]
loss: 0.037334  [ 2600/ 3298]
loss: 0.043456  [ 2700/ 3298]
loss: 0.249016  [ 2800/ 3298]
loss: 0.031508  [ 2900/ 3298]
loss: 0.033554  [ 3000/ 3298]
loss: 0.020585  [ 3100/ 3298]
loss: 0.049180  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.017257  [    0/ 3298]
loss: 0.020378  [  100/ 3298]
loss: 0.104293  [  200/ 3298]
loss: 0.016412  [  300/ 3298]
loss: 0.008618  [  400/ 3298]
loss: 0.021013  [  500/ 3298]
loss: 0.027997  [  600/ 3298]
loss: 0.060679  [  700/ 3298]
loss: 0.022736  [  800/ 3298]
loss: 0.021604  [  900/ 3298]
loss: 0.038542  [ 1000/ 3298]
loss: 0.015524  [ 1100/ 3298]
loss: 0.072406  [ 1200/ 3298]
loss: 0.025729  [ 1300/ 3298]
loss: 0.014619  [ 1400/ 3298]
loss: 0.023980  [ 1500/ 3298]
loss: 0.014625  [ 1600/ 3298]
loss: 0.032560  [ 1700/ 3298]
loss: 0.025812  [ 1800/ 3298]
loss: 0.022421  [ 1900/ 3298]
loss: 0.009507  [ 2000/ 3298]
loss: 0.014072  [ 2100/ 3298]
loss: 0.015913  [ 2200/ 3298]
loss: 0.020038  [ 2300/ 3298]
loss: 0.020134  [ 2400/ 3298]
loss: 0.020461  [ 2500/ 3298]
loss: 0.034200  [ 2600/ 3298]
loss: 0.043638  [ 2700/ 3298]
loss: 0.222445  [ 2800/ 3298]
loss: 0.031711  [ 2900/ 3298]
loss: 0.033370  [ 3000/ 3298]
loss: 0.020480  [ 3100/ 3298]
loss: 0.049106  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.017513  [    0/ 3298]
loss: 0.020670  [  100/ 3298]
loss: 0.103910  [  200/ 3298]
loss: 0.016921  [  300/ 3298]
loss: 0.010311  [  400/ 3298]
loss: 0.022476  [  500/ 3298]
loss: 0.027408  [  600/ 3298]
loss: 0.060468  [  700/ 3298]
loss: 0.022828  [  800/ 3298]
loss: 0.016901  [  900/ 3298]
loss: 0.038532  [ 1000/ 3298]
loss: 0.012880  [ 1100/ 3298]
loss: 0.070540  [ 1200/ 3298]
loss: 0.027053  [ 1300/ 3298]
loss: 0.014738  [ 1400/ 3298]
loss: 0.024353  [ 1500/ 3298]
loss: 0.014028  [ 1600/ 3298]
loss: 0.026932  [ 1700/ 3298]
loss: 0.025681  [ 1800/ 3298]
loss: 0.022557  [ 1900/ 3298]
loss: 0.009616  [ 2000/ 3298]
loss: 0.013685  [ 2100/ 3298]
loss: 0.015391  [ 2200/ 3298]
loss: 0.020090  [ 2300/ 3298]
loss: 0.019539  [ 2400/ 3298]
loss: 0.019759  [ 2500/ 3298]
loss: 0.032811  [ 2600/ 3298]
loss: 0.043550  [ 2700/ 3298]
loss: 0.202271  [ 2800/ 3298]
loss: 0.031371  [ 2900/ 3298]
loss: 0.032868  [ 3000/ 3298]
loss: 0.019724  [ 3100/ 3298]
loss: 0.048011  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [3.0325506 0.6796547]
[1 0 0 ... 0 1 2]
[0 2 2 ... 2 0 1]
Cluster 0 Occurrences: 1094; KMEANS: 1089
Cluster 1 Occurrences: 1089; KMEANS: 1112
Cluster 2 Occurrences: 1115; KMEANS: 1097
Centroids: [[-1.006393, 0.6147352], [2.2447872, 0.81357217], [-1.4940686, -0.72111267]]
Centroids: [[2.2602131, 0.80303735], [-1.5306337, -0.7259192], [-0.9859747, 0.6264122]]
Contingency Matrix: 
[[   0   19 1075]
 [1079    1    9]
 [  10 1092   13]]
[[0, -1, 1075], [1079, -1, 9], [-1, -1, -1]]
[[-1, -1, 1075], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 0, 0: 2}
New Contingency Matrix: 
[[1075    0   19]
 [   9 1079    1]
 [  13   10 1092]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1075, 1079, 1092], Sum: 3246
All_Elements: [1075, 0, 19, 9, 1079, 1, 13, 10, 1092], Sum: 3298
Accuracy: 0.9842328684050939
Done!
