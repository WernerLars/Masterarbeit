Experiment_path: Random_Seeds//V2/Experiment_02_4
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_4/C_Difficult1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_59_57
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000212039CBBE0>
Sampling rate: 24000.0
Raw: [ 0.04887081  0.02693095 -0.0154249  ... -0.09301659 -0.11629005
 -0.14613101]
Times: [    340     491     641 ... 1439047 1439065 1439816]
Cluster: [1 1 1 ... 3 2 2]
Number of different clusters:  3
Number of Spikes: 3472
First aligned Spike Frame: [ 0.12751554  0.12305882  0.10482977  0.09479529  0.10214978  0.11675932
  0.11777927  0.09307299  0.04670706 -0.00574343 -0.06143573 -0.14637617
 -0.20942665 -0.00208103  0.52241508  0.81651544  0.46446121 -0.19226425
 -0.60927882 -0.6713583  -0.57871227 -0.49011309 -0.4269388  -0.3668903
 -0.30523219 -0.24747124 -0.19738203 -0.15189972 -0.10449507 -0.05533325
 -0.01452429  0.01008816  0.02570853  0.04365027  0.06334113  0.07980397
  0.08484457  0.07688513  0.06142919  0.04320028  0.02240626  0.00477291
 -0.00393242 -0.00135684  0.00575182  0.0026944  -0.01541647]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1172
Cluster 2, Occurrences: 1141
<torch.utils.data.dataloader.DataLoader object at 0x0000021209B12C18>
Epoch 1
-------------------------------
loss: 0.110177  [    0/ 3472]
loss: 0.091721  [  100/ 3472]
loss: 0.019312  [  200/ 3472]
loss: 0.025527  [  300/ 3472]
loss: 0.041757  [  400/ 3472]
loss: 0.016103  [  500/ 3472]
loss: 0.022575  [  600/ 3472]
loss: 0.015952  [  700/ 3472]
loss: 0.035319  [  800/ 3472]
loss: 0.033613  [  900/ 3472]
loss: 0.009061  [ 1000/ 3472]
loss: 0.025954  [ 1100/ 3472]
loss: 0.024739  [ 1200/ 3472]
loss: 0.010555  [ 1300/ 3472]
loss: 0.003885  [ 1400/ 3472]
loss: 0.043678  [ 1500/ 3472]
loss: 0.020512  [ 1600/ 3472]
loss: 0.012518  [ 1700/ 3472]
loss: 0.032689  [ 1800/ 3472]
loss: 0.137813  [ 1900/ 3472]
loss: 0.050656  [ 2000/ 3472]
loss: 0.013745  [ 2100/ 3472]
loss: 0.011182  [ 2200/ 3472]
loss: 0.005131  [ 2300/ 3472]
loss: 0.008094  [ 2400/ 3472]
loss: 0.016592  [ 2500/ 3472]
loss: 0.014212  [ 2600/ 3472]
loss: 0.011085  [ 2700/ 3472]
loss: 0.025653  [ 2800/ 3472]
loss: 0.032481  [ 2900/ 3472]
loss: 0.010164  [ 3000/ 3472]
loss: 0.020400  [ 3100/ 3472]
loss: 0.014802  [ 3200/ 3472]
loss: 0.009927  [ 3300/ 3472]
loss: 0.008901  [ 3400/ 3472]
Epoch 2
-------------------------------
loss: 0.009071  [    0/ 3472]
loss: 0.008189  [  100/ 3472]
loss: 0.009992  [  200/ 3472]
loss: 0.017660  [  300/ 3472]
loss: 0.005365  [  400/ 3472]
loss: 0.015697  [  500/ 3472]
loss: 0.012308  [  600/ 3472]
loss: 0.005588  [  700/ 3472]
loss: 0.036496  [  800/ 3472]
loss: 0.028969  [  900/ 3472]
loss: 0.009821  [ 1000/ 3472]
loss: 0.027009  [ 1100/ 3472]
loss: 0.025057  [ 1200/ 3472]
loss: 0.005806  [ 1300/ 3472]
loss: 0.002948  [ 1400/ 3472]
loss: 0.036222  [ 1500/ 3472]
loss: 0.019230  [ 1600/ 3472]
loss: 0.011584  [ 1700/ 3472]
loss: 0.029516  [ 1800/ 3472]
loss: 0.134389  [ 1900/ 3472]
loss: 0.050001  [ 2000/ 3472]
loss: 0.013961  [ 2100/ 3472]
loss: 0.011097  [ 2200/ 3472]
loss: 0.005137  [ 2300/ 3472]
loss: 0.009856  [ 2400/ 3472]
loss: 0.014032  [ 2500/ 3472]
loss: 0.013817  [ 2600/ 3472]
loss: 0.011124  [ 2700/ 3472]
loss: 0.025366  [ 2800/ 3472]
loss: 0.032088  [ 2900/ 3472]
loss: 0.009348  [ 3000/ 3472]
loss: 0.020543  [ 3100/ 3472]
loss: 0.013977  [ 3200/ 3472]
loss: 0.009227  [ 3300/ 3472]
loss: 0.009030  [ 3400/ 3472]
Epoch 3
-------------------------------
loss: 0.008924  [    0/ 3472]
loss: 0.007475  [  100/ 3472]
loss: 0.010127  [  200/ 3472]
loss: 0.017867  [  300/ 3472]
loss: 0.005305  [  400/ 3472]
loss: 0.015421  [  500/ 3472]
loss: 0.012296  [  600/ 3472]
loss: 0.005551  [  700/ 3472]
loss: 0.036509  [  800/ 3472]
loss: 0.029005  [  900/ 3472]
loss: 0.009834  [ 1000/ 3472]
loss: 0.026833  [ 1100/ 3472]
loss: 0.024772  [ 1200/ 3472]
loss: 0.005888  [ 1300/ 3472]
loss: 0.002985  [ 1400/ 3472]
loss: 0.032819  [ 1500/ 3472]
loss: 0.019172  [ 1600/ 3472]
loss: 0.011236  [ 1700/ 3472]
loss: 0.028488  [ 1800/ 3472]
loss: 0.134574  [ 1900/ 3472]
loss: 0.050209  [ 2000/ 3472]
loss: 0.013624  [ 2100/ 3472]
loss: 0.011075  [ 2200/ 3472]
loss: 0.005027  [ 2300/ 3472]
loss: 0.009850  [ 2400/ 3472]
loss: 0.013904  [ 2500/ 3472]
loss: 0.013641  [ 2600/ 3472]
loss: 0.011053  [ 2700/ 3472]
loss: 0.024954  [ 2800/ 3472]
loss: 0.032108  [ 2900/ 3472]
loss: 0.009229  [ 3000/ 3472]
loss: 0.020595  [ 3100/ 3472]
loss: 0.013683  [ 3200/ 3472]
loss: 0.008841  [ 3300/ 3472]
loss: 0.008759  [ 3400/ 3472]
Epoch 4
-------------------------------
loss: 0.008724  [    0/ 3472]
loss: 0.007204  [  100/ 3472]
loss: 0.010186  [  200/ 3472]
loss: 0.017904  [  300/ 3472]
loss: 0.005233  [  400/ 3472]
loss: 0.015396  [  500/ 3472]
loss: 0.012334  [  600/ 3472]
loss: 0.005344  [  700/ 3472]
loss: 0.036194  [  800/ 3472]
loss: 0.029654  [  900/ 3472]
loss: 0.009765  [ 1000/ 3472]
loss: 0.026832  [ 1100/ 3472]
loss: 0.024592  [ 1200/ 3472]
loss: 0.005958  [ 1300/ 3472]
loss: 0.002978  [ 1400/ 3472]
loss: 0.028853  [ 1500/ 3472]
loss: 0.019030  [ 1600/ 3472]
loss: 0.011084  [ 1700/ 3472]
loss: 0.027524  [ 1800/ 3472]
loss: 0.134542  [ 1900/ 3472]
loss: 0.050150  [ 2000/ 3472]
loss: 0.013333  [ 2100/ 3472]
loss: 0.011136  [ 2200/ 3472]
loss: 0.004912  [ 2300/ 3472]
loss: 0.009624  [ 2400/ 3472]
loss: 0.013665  [ 2500/ 3472]
loss: 0.013126  [ 2600/ 3472]
loss: 0.011063  [ 2700/ 3472]
loss: 0.024444  [ 2800/ 3472]
loss: 0.032315  [ 2900/ 3472]
loss: 0.009346  [ 3000/ 3472]
loss: 0.020570  [ 3100/ 3472]
loss: 0.013491  [ 3200/ 3472]
loss: 0.008694  [ 3300/ 3472]
loss: 0.008377  [ 3400/ 3472]
Epoch 5
-------------------------------
loss: 0.008567  [    0/ 3472]
loss: 0.006938  [  100/ 3472]
loss: 0.010211  [  200/ 3472]
loss: 0.017938  [  300/ 3472]
loss: 0.005051  [  400/ 3472]
loss: 0.015387  [  500/ 3472]
loss: 0.012287  [  600/ 3472]
loss: 0.005130  [  700/ 3472]
loss: 0.035948  [  800/ 3472]
loss: 0.029760  [  900/ 3472]
loss: 0.009785  [ 1000/ 3472]
loss: 0.026927  [ 1100/ 3472]
loss: 0.024506  [ 1200/ 3472]
loss: 0.005996  [ 1300/ 3472]
loss: 0.002949  [ 1400/ 3472]
loss: 0.024848  [ 1500/ 3472]
loss: 0.018667  [ 1600/ 3472]
loss: 0.010927  [ 1700/ 3472]
loss: 0.026589  [ 1800/ 3472]
loss: 0.134887  [ 1900/ 3472]
loss: 0.049909  [ 2000/ 3472]
loss: 0.013030  [ 2100/ 3472]
loss: 0.011236  [ 2200/ 3472]
loss: 0.004836  [ 2300/ 3472]
loss: 0.009367  [ 2400/ 3472]
loss: 0.013447  [ 2500/ 3472]
loss: 0.012317  [ 2600/ 3472]
loss: 0.010906  [ 2700/ 3472]
loss: 0.023955  [ 2800/ 3472]
loss: 0.032641  [ 2900/ 3472]
loss: 0.009398  [ 3000/ 3472]
loss: 0.019719  [ 3100/ 3472]
loss: 0.013374  [ 3200/ 3472]
loss: 0.008790  [ 3300/ 3472]
loss: 0.008070  [ 3400/ 3472]
Epoch 6
-------------------------------
loss: 0.008297  [    0/ 3472]
loss: 0.006722  [  100/ 3472]
loss: 0.010194  [  200/ 3472]
loss: 0.017978  [  300/ 3472]
loss: 0.004704  [  400/ 3472]
loss: 0.015388  [  500/ 3472]
loss: 0.012248  [  600/ 3472]
loss: 0.004865  [  700/ 3472]
loss: 0.035867  [  800/ 3472]
loss: 0.029893  [  900/ 3472]
loss: 0.009951  [ 1000/ 3472]
loss: 0.027020  [ 1100/ 3472]
loss: 0.024587  [ 1200/ 3472]
loss: 0.006066  [ 1300/ 3472]
loss: 0.002940  [ 1400/ 3472]
loss: 0.020250  [ 1500/ 3472]
loss: 0.018314  [ 1600/ 3472]
loss: 0.010692  [ 1700/ 3472]
loss: 0.025665  [ 1800/ 3472]
loss: 0.135578  [ 1900/ 3472]
loss: 0.049243  [ 2000/ 3472]
loss: 0.012741  [ 2100/ 3472]
loss: 0.011331  [ 2200/ 3472]
loss: 0.004762  [ 2300/ 3472]
loss: 0.008965  [ 2400/ 3472]
loss: 0.013109  [ 2500/ 3472]
loss: 0.011306  [ 2600/ 3472]
loss: 0.010995  [ 2700/ 3472]
loss: 0.023412  [ 2800/ 3472]
loss: 0.032948  [ 2900/ 3472]
loss: 0.009398  [ 3000/ 3472]
loss: 0.018411  [ 3100/ 3472]
loss: 0.013299  [ 3200/ 3472]
loss: 0.009042  [ 3300/ 3472]
loss: 0.007657  [ 3400/ 3472]
Epoch 7
-------------------------------
loss: 0.008006  [    0/ 3472]
loss: 0.006464  [  100/ 3472]
loss: 0.010197  [  200/ 3472]
loss: 0.018012  [  300/ 3472]
loss: 0.004474  [  400/ 3472]
loss: 0.015458  [  500/ 3472]
loss: 0.012182  [  600/ 3472]
loss: 0.004553  [  700/ 3472]
loss: 0.035863  [  800/ 3472]
loss: 0.030117  [  900/ 3472]
loss: 0.010221  [ 1000/ 3472]
loss: 0.027221  [ 1100/ 3472]
loss: 0.024706  [ 1200/ 3472]
loss: 0.006087  [ 1300/ 3472]
loss: 0.002841  [ 1400/ 3472]
loss: 0.016882  [ 1500/ 3472]
loss: 0.017739  [ 1600/ 3472]
loss: 0.010479  [ 1700/ 3472]
loss: 0.024669  [ 1800/ 3472]
loss: 0.136519  [ 1900/ 3472]
loss: 0.048612  [ 2000/ 3472]
loss: 0.012445  [ 2100/ 3472]
loss: 0.011382  [ 2200/ 3472]
loss: 0.004688  [ 2300/ 3472]
loss: 0.008490  [ 2400/ 3472]
loss: 0.012936  [ 2500/ 3472]
loss: 0.011005  [ 2600/ 3472]
loss: 0.010618  [ 2700/ 3472]
loss: 0.022710  [ 2800/ 3472]
loss: 0.033146  [ 2900/ 3472]
loss: 0.009416  [ 3000/ 3472]
loss: 0.017529  [ 3100/ 3472]
loss: 0.013292  [ 3200/ 3472]
loss: 0.008950  [ 3300/ 3472]
loss: 0.007377  [ 3400/ 3472]
Epoch 8
-------------------------------
loss: 0.007954  [    0/ 3472]
loss: 0.006375  [  100/ 3472]
loss: 0.010131  [  200/ 3472]
loss: 0.018124  [  300/ 3472]
loss: 0.004405  [  400/ 3472]
loss: 0.015499  [  500/ 3472]
loss: 0.012177  [  600/ 3472]
loss: 0.004324  [  700/ 3472]
loss: 0.035765  [  800/ 3472]
loss: 0.030438  [  900/ 3472]
loss: 0.010263  [ 1000/ 3472]
loss: 0.027286  [ 1100/ 3472]
loss: 0.024674  [ 1200/ 3472]
loss: 0.006200  [ 1300/ 3472]
loss: 0.002787  [ 1400/ 3472]
loss: 0.014948  [ 1500/ 3472]
loss: 0.017442  [ 1600/ 3472]
loss: 0.010299  [ 1700/ 3472]
loss: 0.024142  [ 1800/ 3472]
loss: 0.137018  [ 1900/ 3472]
loss: 0.048330  [ 2000/ 3472]
loss: 0.012225  [ 2100/ 3472]
loss: 0.011456  [ 2200/ 3472]
loss: 0.004643  [ 2300/ 3472]
loss: 0.008036  [ 2400/ 3472]
loss: 0.012837  [ 2500/ 3472]
loss: 0.010547  [ 2600/ 3472]
loss: 0.010355  [ 2700/ 3472]
loss: 0.022548  [ 2800/ 3472]
loss: 0.033167  [ 2900/ 3472]
loss: 0.009387  [ 3000/ 3472]
loss: 0.016379  [ 3100/ 3472]
loss: 0.013285  [ 3200/ 3472]
loss: 0.009047  [ 3300/ 3472]
loss: 0.007157  [ 3400/ 3472]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3472
First Spike after testing: [-0.36041486  0.27242562]
[0 0 0 ... 2 1 1]
[2 2 2 ... 1 1 2]
Cluster 0 Occurrences: 1159; KMEANS: 532
Cluster 1 Occurrences: 1172; KMEANS: 1172
Cluster 2 Occurrences: 1141; KMEANS: 1768
Centroids: [[-0.5683539, 0.2480182], [-0.7271094, 0.45558807], [-1.034476, -0.028258862]]
Centroids: [[-0.41745275, 0.8040697], [-1.0742115, -0.09812454], [-0.6844858, 0.2694547]]
Contingency Matrix: 
[[183 136 840]
 [340 168 664]
 [  9 868 264]]
[[183, -1, 840], [340, -1, 664], [-1, -1, -1]]
[[-1, -1, -1], [340, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 2, 1: 0}
New Contingency Matrix: 
[[840 183 136]
 [664 340 168]
 [264   9 868]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [840, 340, 868], Sum: 2048
All_Elements: [840, 183, 136, 664, 340, 168, 264, 9, 868], Sum: 3472
Accuracy: 0.5898617511520737
Done!
