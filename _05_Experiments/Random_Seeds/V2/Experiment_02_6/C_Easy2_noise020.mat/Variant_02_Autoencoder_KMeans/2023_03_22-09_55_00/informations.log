Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_55_00
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000020687B1CC50>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x0000020629A670F0>
Epoch 1
-------------------------------
loss: 0.133752  [    0/ 3526]
loss: 0.247321  [  100/ 3526]
loss: 0.152257  [  200/ 3526]
loss: 0.106828  [  300/ 3526]
loss: 0.118780  [  400/ 3526]
loss: 0.101494  [  500/ 3526]
loss: 0.050766  [  600/ 3526]
loss: 0.043062  [  700/ 3526]
loss: 0.065749  [  800/ 3526]
loss: 0.022471  [  900/ 3526]
loss: 0.054722  [ 1000/ 3526]
loss: 0.032891  [ 1100/ 3526]
loss: 0.072592  [ 1200/ 3526]
loss: 0.096449  [ 1300/ 3526]
loss: 0.037445  [ 1400/ 3526]
loss: 0.054987  [ 1500/ 3526]
loss: 0.052668  [ 1600/ 3526]
loss: 0.038041  [ 1700/ 3526]
loss: 0.044118  [ 1800/ 3526]
loss: 0.029553  [ 1900/ 3526]
loss: 0.020435  [ 2000/ 3526]
loss: 0.190386  [ 2100/ 3526]
loss: 0.039968  [ 2200/ 3526]
loss: 0.017271  [ 2300/ 3526]
loss: 0.017762  [ 2400/ 3526]
loss: 0.014872  [ 2500/ 3526]
loss: 0.046512  [ 2600/ 3526]
loss: 0.033959  [ 2700/ 3526]
loss: 0.016820  [ 2800/ 3526]
loss: 0.020906  [ 2900/ 3526]
loss: 0.111471  [ 3000/ 3526]
loss: 0.166075  [ 3100/ 3526]
loss: 0.043045  [ 3200/ 3526]
loss: 0.037730  [ 3300/ 3526]
loss: 0.041000  [ 3400/ 3526]
loss: 0.038464  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.024783  [    0/ 3526]
loss: 0.029049  [  100/ 3526]
loss: 0.039527  [  200/ 3526]
loss: 0.010956  [  300/ 3526]
loss: 0.014252  [  400/ 3526]
loss: 0.048541  [  500/ 3526]
loss: 0.019071  [  600/ 3526]
loss: 0.015405  [  700/ 3526]
loss: 0.024736  [  800/ 3526]
loss: 0.013866  [  900/ 3526]
loss: 0.033239  [ 1000/ 3526]
loss: 0.025051  [ 1100/ 3526]
loss: 0.046648  [ 1200/ 3526]
loss: 0.027163  [ 1300/ 3526]
loss: 0.030517  [ 1400/ 3526]
loss: 0.014177  [ 1500/ 3526]
loss: 0.045303  [ 1600/ 3526]
loss: 0.022907  [ 1700/ 3526]
loss: 0.019864  [ 1800/ 3526]
loss: 0.019899  [ 1900/ 3526]
loss: 0.020403  [ 2000/ 3526]
loss: 0.091683  [ 2100/ 3526]
loss: 0.028079  [ 2200/ 3526]
loss: 0.017264  [ 2300/ 3526]
loss: 0.010486  [ 2400/ 3526]
loss: 0.014717  [ 2500/ 3526]
loss: 0.029395  [ 2600/ 3526]
loss: 0.018681  [ 2700/ 3526]
loss: 0.008498  [ 2800/ 3526]
loss: 0.012100  [ 2900/ 3526]
loss: 0.076738  [ 3000/ 3526]
loss: 0.151753  [ 3100/ 3526]
loss: 0.028887  [ 3200/ 3526]
loss: 0.029297  [ 3300/ 3526]
loss: 0.037268  [ 3400/ 3526]
loss: 0.037819  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.019396  [    0/ 3526]
loss: 0.036103  [  100/ 3526]
loss: 0.043640  [  200/ 3526]
loss: 0.007649  [  300/ 3526]
loss: 0.014997  [  400/ 3526]
loss: 0.036646  [  500/ 3526]
loss: 0.012739  [  600/ 3526]
loss: 0.016297  [  700/ 3526]
loss: 0.024401  [  800/ 3526]
loss: 0.012932  [  900/ 3526]
loss: 0.025092  [ 1000/ 3526]
loss: 0.020885  [ 1100/ 3526]
loss: 0.036568  [ 1200/ 3526]
loss: 0.023905  [ 1300/ 3526]
loss: 0.029026  [ 1400/ 3526]
loss: 0.014253  [ 1500/ 3526]
loss: 0.044875  [ 1600/ 3526]
loss: 0.020486  [ 1700/ 3526]
loss: 0.019435  [ 1800/ 3526]
loss: 0.013271  [ 1900/ 3526]
loss: 0.018009  [ 2000/ 3526]
loss: 0.111280  [ 2100/ 3526]
loss: 0.024703  [ 2200/ 3526]
loss: 0.016927  [ 2300/ 3526]
loss: 0.009499  [ 2400/ 3526]
loss: 0.015343  [ 2500/ 3526]
loss: 0.023428  [ 2600/ 3526]
loss: 0.015859  [ 2700/ 3526]
loss: 0.006958  [ 2800/ 3526]
loss: 0.009636  [ 2900/ 3526]
loss: 0.070176  [ 3000/ 3526]
loss: 0.152706  [ 3100/ 3526]
loss: 0.027220  [ 3200/ 3526]
loss: 0.027546  [ 3300/ 3526]
loss: 0.036809  [ 3400/ 3526]
loss: 0.038478  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.017917  [    0/ 3526]
loss: 0.041268  [  100/ 3526]
loss: 0.048456  [  200/ 3526]
loss: 0.011439  [  300/ 3526]
loss: 0.015652  [  400/ 3526]
loss: 0.035228  [  500/ 3526]
loss: 0.011045  [  600/ 3526]
loss: 0.016342  [  700/ 3526]
loss: 0.026474  [  800/ 3526]
loss: 0.012960  [  900/ 3526]
loss: 0.022428  [ 1000/ 3526]
loss: 0.019404  [ 1100/ 3526]
loss: 0.033410  [ 1200/ 3526]
loss: 0.021798  [ 1300/ 3526]
loss: 0.028548  [ 1400/ 3526]
loss: 0.015402  [ 1500/ 3526]
loss: 0.043894  [ 1600/ 3526]
loss: 0.019551  [ 1700/ 3526]
loss: 0.019641  [ 1800/ 3526]
loss: 0.012326  [ 1900/ 3526]
loss: 0.016763  [ 2000/ 3526]
loss: 0.107659  [ 2100/ 3526]
loss: 0.024010  [ 2200/ 3526]
loss: 0.016893  [ 2300/ 3526]
loss: 0.009081  [ 2400/ 3526]
loss: 0.015827  [ 2500/ 3526]
loss: 0.022401  [ 2600/ 3526]
loss: 0.015846  [ 2700/ 3526]
loss: 0.006559  [ 2800/ 3526]
loss: 0.009231  [ 2900/ 3526]
loss: 0.068163  [ 3000/ 3526]
loss: 0.156090  [ 3100/ 3526]
loss: 0.027899  [ 3200/ 3526]
loss: 0.027073  [ 3300/ 3526]
loss: 0.039720  [ 3400/ 3526]
loss: 0.039364  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.018180  [    0/ 3526]
loss: 0.041199  [  100/ 3526]
loss: 0.049157  [  200/ 3526]
loss: 0.006325  [  300/ 3526]
loss: 0.016278  [  400/ 3526]
loss: 0.034424  [  500/ 3526]
loss: 0.010771  [  600/ 3526]
loss: 0.016154  [  700/ 3526]
loss: 0.027974  [  800/ 3526]
loss: 0.013015  [  900/ 3526]
loss: 0.020395  [ 1000/ 3526]
loss: 0.018915  [ 1100/ 3526]
loss: 0.031848  [ 1200/ 3526]
loss: 0.020409  [ 1300/ 3526]
loss: 0.028310  [ 1400/ 3526]
loss: 0.015953  [ 1500/ 3526]
loss: 0.043363  [ 1600/ 3526]
loss: 0.018244  [ 1700/ 3526]
loss: 0.019628  [ 1800/ 3526]
loss: 0.012419  [ 1900/ 3526]
loss: 0.015996  [ 2000/ 3526]
loss: 0.107377  [ 2100/ 3526]
loss: 0.023128  [ 2200/ 3526]
loss: 0.016808  [ 2300/ 3526]
loss: 0.008829  [ 2400/ 3526]
loss: 0.016049  [ 2500/ 3526]
loss: 0.021201  [ 2600/ 3526]
loss: 0.015719  [ 2700/ 3526]
loss: 0.006825  [ 2800/ 3526]
loss: 0.008941  [ 2900/ 3526]
loss: 0.066493  [ 3000/ 3526]
loss: 0.155217  [ 3100/ 3526]
loss: 0.027204  [ 3200/ 3526]
loss: 0.026350  [ 3300/ 3526]
loss: 0.040702  [ 3400/ 3526]
loss: 0.039052  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.017285  [    0/ 3526]
loss: 0.043126  [  100/ 3526]
loss: 0.049463  [  200/ 3526]
loss: 0.006309  [  300/ 3526]
loss: 0.016670  [  400/ 3526]
loss: 0.033645  [  500/ 3526]
loss: 0.010651  [  600/ 3526]
loss: 0.016057  [  700/ 3526]
loss: 0.028803  [  800/ 3526]
loss: 0.012967  [  900/ 3526]
loss: 0.019244  [ 1000/ 3526]
loss: 0.018667  [ 1100/ 3526]
loss: 0.031347  [ 1200/ 3526]
loss: 0.019892  [ 1300/ 3526]
loss: 0.028111  [ 1400/ 3526]
loss: 0.015995  [ 1500/ 3526]
loss: 0.043212  [ 1600/ 3526]
loss: 0.017708  [ 1700/ 3526]
loss: 0.019476  [ 1800/ 3526]
loss: 0.012670  [ 1900/ 3526]
loss: 0.015930  [ 2000/ 3526]
loss: 0.107759  [ 2100/ 3526]
loss: 0.022706  [ 2200/ 3526]
loss: 0.016761  [ 2300/ 3526]
loss: 0.008704  [ 2400/ 3526]
loss: 0.016141  [ 2500/ 3526]
loss: 0.019734  [ 2600/ 3526]
loss: 0.015552  [ 2700/ 3526]
loss: 0.007047  [ 2800/ 3526]
loss: 0.008924  [ 2900/ 3526]
loss: 0.065936  [ 3000/ 3526]
loss: 0.154727  [ 3100/ 3526]
loss: 0.026869  [ 3200/ 3526]
loss: 0.025550  [ 3300/ 3526]
loss: 0.041138  [ 3400/ 3526]
loss: 0.039011  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.016764  [    0/ 3526]
loss: 0.043577  [  100/ 3526]
loss: 0.049636  [  200/ 3526]
loss: 0.006049  [  300/ 3526]
loss: 0.016847  [  400/ 3526]
loss: 0.033492  [  500/ 3526]
loss: 0.010395  [  600/ 3526]
loss: 0.016128  [  700/ 3526]
loss: 0.029304  [  800/ 3526]
loss: 0.012910  [  900/ 3526]
loss: 0.018952  [ 1000/ 3526]
loss: 0.018230  [ 1100/ 3526]
loss: 0.031047  [ 1200/ 3526]
loss: 0.019774  [ 1300/ 3526]
loss: 0.027909  [ 1400/ 3526]
loss: 0.016075  [ 1500/ 3526]
loss: 0.043342  [ 1600/ 3526]
loss: 0.017556  [ 1700/ 3526]
loss: 0.019240  [ 1800/ 3526]
loss: 0.012793  [ 1900/ 3526]
loss: 0.015888  [ 2000/ 3526]
loss: 0.112483  [ 2100/ 3526]
loss: 0.022686  [ 2200/ 3526]
loss: 0.016754  [ 2300/ 3526]
loss: 0.008607  [ 2400/ 3526]
loss: 0.016144  [ 2500/ 3526]
loss: 0.019285  [ 2600/ 3526]
loss: 0.015370  [ 2700/ 3526]
loss: 0.007189  [ 2800/ 3526]
loss: 0.008892  [ 2900/ 3526]
loss: 0.065278  [ 3000/ 3526]
loss: 0.154544  [ 3100/ 3526]
loss: 0.026630  [ 3200/ 3526]
loss: 0.025164  [ 3300/ 3526]
loss: 0.041323  [ 3400/ 3526]
loss: 0.039054  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.016580  [    0/ 3526]
loss: 0.044165  [  100/ 3526]
loss: 0.049746  [  200/ 3526]
loss: 0.009781  [  300/ 3526]
loss: 0.016844  [  400/ 3526]
loss: 0.033851  [  500/ 3526]
loss: 0.010386  [  600/ 3526]
loss: 0.016180  [  700/ 3526]
loss: 0.029480  [  800/ 3526]
loss: 0.012883  [  900/ 3526]
loss: 0.019546  [ 1000/ 3526]
loss: 0.018159  [ 1100/ 3526]
loss: 0.030953  [ 1200/ 3526]
loss: 0.020431  [ 1300/ 3526]
loss: 0.027859  [ 1400/ 3526]
loss: 0.016212  [ 1500/ 3526]
loss: 0.043145  [ 1600/ 3526]
loss: 0.017726  [ 1700/ 3526]
loss: 0.018970  [ 1800/ 3526]
loss: 0.013017  [ 1900/ 3526]
loss: 0.015965  [ 2000/ 3526]
loss: 0.111222  [ 2100/ 3526]
loss: 0.022735  [ 2200/ 3526]
loss: 0.016870  [ 2300/ 3526]
loss: 0.008574  [ 2400/ 3526]
loss: 0.016100  [ 2500/ 3526]
loss: 0.019487  [ 2600/ 3526]
loss: 0.015228  [ 2700/ 3526]
loss: 0.006982  [ 2800/ 3526]
loss: 0.008909  [ 2900/ 3526]
loss: 0.064691  [ 3000/ 3526]
loss: 0.154649  [ 3100/ 3526]
loss: 0.026833  [ 3200/ 3526]
loss: 0.025643  [ 3300/ 3526]
loss: 0.041608  [ 3400/ 3526]
loss: 0.039586  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [-1.9785517 -6.783411 ]
[2 0 2 ... 0 2 2]
[2 1 2 ... 1 2 2]
Cluster 0 Occurrences: 1186; KMEANS: 1391
Cluster 1 Occurrences: 1188; KMEANS: 1163
Cluster 2 Occurrences: 1152; KMEANS: 972
Centroids: [[1.6684916, -6.6227455], [0.8070544, -4.933027], [-2.4490137, -5.756647]]
Centroids: [[1.0077802, -4.0467253], [1.0401042, -8.4421425], [-2.5669913, -5.0405984]]
Contingency Matrix: 
[[519 667   0]
 [869 319   0]
 [  3 177 972]]
[[519, 667, -1], [869, 319, -1], [-1, -1, -1]]
[[-1, 667, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 2, 1: 0, 0: 1}
New Contingency Matrix: 
[[667 519   0]
 [319 869   0]
 [177   3 972]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [667, 869, 972], Sum: 2508
All_Elements: [667, 519, 0, 319, 869, 0, 177, 3, 972], Sum: 3526
Accuracy: 0.711287577992059
Done!
