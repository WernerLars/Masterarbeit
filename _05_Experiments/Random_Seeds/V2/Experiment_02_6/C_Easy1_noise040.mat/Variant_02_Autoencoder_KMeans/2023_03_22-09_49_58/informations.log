Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_49_58
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000020628482390>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000020628D95080>
Epoch 1
-------------------------------
loss: 0.097228  [    0/ 3386]
loss: 0.365084  [  100/ 3386]
loss: 0.171644  [  200/ 3386]
loss: 0.175477  [  300/ 3386]
loss: 0.256657  [  400/ 3386]
loss: 0.080360  [  500/ 3386]
loss: 0.190817  [  600/ 3386]
loss: 0.121637  [  700/ 3386]
loss: 0.188698  [  800/ 3386]
loss: 0.061041  [  900/ 3386]
loss: 0.235064  [ 1000/ 3386]
loss: 0.252690  [ 1100/ 3386]
loss: 0.192728  [ 1200/ 3386]
loss: 0.026661  [ 1300/ 3386]
loss: 0.070125  [ 1400/ 3386]
loss: 0.039207  [ 1500/ 3386]
loss: 0.049553  [ 1600/ 3386]
loss: 0.061702  [ 1700/ 3386]
loss: 0.284245  [ 1800/ 3386]
loss: 0.239906  [ 1900/ 3386]
loss: 0.100494  [ 2000/ 3386]
loss: 0.199486  [ 2100/ 3386]
loss: 0.093278  [ 2200/ 3386]
loss: 0.209937  [ 2300/ 3386]
loss: 0.121599  [ 2400/ 3386]
loss: 0.055869  [ 2500/ 3386]
loss: 0.141224  [ 2600/ 3386]
loss: 0.237690  [ 2700/ 3386]
loss: 0.144517  [ 2800/ 3386]
loss: 0.196496  [ 2900/ 3386]
loss: 0.018164  [ 3000/ 3386]
loss: 0.118530  [ 3100/ 3386]
loss: 0.063892  [ 3200/ 3386]
loss: 0.142696  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.055612  [    0/ 3386]
loss: 0.110151  [  100/ 3386]
loss: 0.079577  [  200/ 3386]
loss: 0.065554  [  300/ 3386]
loss: 0.298588  [  400/ 3386]
loss: 0.074173  [  500/ 3386]
loss: 0.167609  [  600/ 3386]
loss: 0.054108  [  700/ 3386]
loss: 0.119955  [  800/ 3386]
loss: 0.056963  [  900/ 3386]
loss: 0.125800  [ 1000/ 3386]
loss: 0.271750  [ 1100/ 3386]
loss: 0.203817  [ 1200/ 3386]
loss: 0.022872  [ 1300/ 3386]
loss: 0.062780  [ 1400/ 3386]
loss: 0.052684  [ 1500/ 3386]
loss: 0.037586  [ 1600/ 3386]
loss: 0.020161  [ 1700/ 3386]
loss: 0.287144  [ 1800/ 3386]
loss: 0.287723  [ 1900/ 3386]
loss: 0.033889  [ 2000/ 3386]
loss: 0.140813  [ 2100/ 3386]
loss: 0.365691  [ 2200/ 3386]
loss: 0.116546  [ 2300/ 3386]
loss: 0.084419  [ 2400/ 3386]
loss: 0.054880  [ 2500/ 3386]
loss: 0.159677  [ 2600/ 3386]
loss: 0.204981  [ 2700/ 3386]
loss: 0.083303  [ 2800/ 3386]
loss: 0.068304  [ 2900/ 3386]
loss: 0.018168  [ 3000/ 3386]
loss: 0.121797  [ 3100/ 3386]
loss: 0.059708  [ 3200/ 3386]
loss: 0.102928  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.052492  [    0/ 3386]
loss: 0.113362  [  100/ 3386]
loss: 0.075726  [  200/ 3386]
loss: 0.036873  [  300/ 3386]
loss: 0.231200  [  400/ 3386]
loss: 0.074405  [  500/ 3386]
loss: 0.161968  [  600/ 3386]
loss: 0.046553  [  700/ 3386]
loss: 0.116830  [  800/ 3386]
loss: 0.046716  [  900/ 3386]
loss: 0.122564  [ 1000/ 3386]
loss: 0.283968  [ 1100/ 3386]
loss: 0.162455  [ 1200/ 3386]
loss: 0.025106  [ 1300/ 3386]
loss: 0.048790  [ 1400/ 3386]
loss: 0.066778  [ 1500/ 3386]
loss: 0.038319  [ 1600/ 3386]
loss: 0.023313  [ 1700/ 3386]
loss: 0.184118  [ 1800/ 3386]
loss: 0.357429  [ 1900/ 3386]
loss: 0.019852  [ 2000/ 3386]
loss: 0.108413  [ 2100/ 3386]
loss: 0.449978  [ 2200/ 3386]
loss: 0.091495  [ 2300/ 3386]
loss: 0.067336  [ 2400/ 3386]
loss: 0.053673  [ 2500/ 3386]
loss: 0.142262  [ 2600/ 3386]
loss: 0.202252  [ 2700/ 3386]
loss: 0.080738  [ 2800/ 3386]
loss: 0.054466  [ 2900/ 3386]
loss: 0.017401  [ 3000/ 3386]
loss: 0.122883  [ 3100/ 3386]
loss: 0.060415  [ 3200/ 3386]
loss: 0.099046  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.055506  [    0/ 3386]
loss: 0.114463  [  100/ 3386]
loss: 0.073804  [  200/ 3386]
loss: 0.030029  [  300/ 3386]
loss: 0.215896  [  400/ 3386]
loss: 0.073872  [  500/ 3386]
loss: 0.159540  [  600/ 3386]
loss: 0.046912  [  700/ 3386]
loss: 0.115036  [  800/ 3386]
loss: 0.046815  [  900/ 3386]
loss: 0.122207  [ 1000/ 3386]
loss: 0.264729  [ 1100/ 3386]
loss: 0.155966  [ 1200/ 3386]
loss: 0.025787  [ 1300/ 3386]
loss: 0.048021  [ 1400/ 3386]
loss: 0.065365  [ 1500/ 3386]
loss: 0.039109  [ 1600/ 3386]
loss: 0.024866  [ 1700/ 3386]
loss: 0.145812  [ 1800/ 3386]
loss: 0.360532  [ 1900/ 3386]
loss: 0.025773  [ 2000/ 3386]
loss: 0.098223  [ 2100/ 3386]
loss: 0.455081  [ 2200/ 3386]
loss: 0.088232  [ 2300/ 3386]
loss: 0.063793  [ 2400/ 3386]
loss: 0.057968  [ 2500/ 3386]
loss: 0.116441  [ 2600/ 3386]
loss: 0.199579  [ 2700/ 3386]
loss: 0.083926  [ 2800/ 3386]
loss: 0.054369  [ 2900/ 3386]
loss: 0.016655  [ 3000/ 3386]
loss: 0.125899  [ 3100/ 3386]
loss: 0.054282  [ 3200/ 3386]
loss: 0.098621  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.055800  [    0/ 3386]
loss: 0.115813  [  100/ 3386]
loss: 0.078552  [  200/ 3386]
loss: 0.022751  [  300/ 3386]
loss: 0.218854  [  400/ 3386]
loss: 0.072756  [  500/ 3386]
loss: 0.157020  [  600/ 3386]
loss: 0.049262  [  700/ 3386]
loss: 0.114608  [  800/ 3386]
loss: 0.048254  [  900/ 3386]
loss: 0.122884  [ 1000/ 3386]
loss: 0.242016  [ 1100/ 3386]
loss: 0.155240  [ 1200/ 3386]
loss: 0.025793  [ 1300/ 3386]
loss: 0.047826  [ 1400/ 3386]
loss: 0.060747  [ 1500/ 3386]
loss: 0.040912  [ 1600/ 3386]
loss: 0.025815  [ 1700/ 3386]
loss: 0.140762  [ 1800/ 3386]
loss: 0.341514  [ 1900/ 3386]
loss: 0.031996  [ 2000/ 3386]
loss: 0.096311  [ 2100/ 3386]
loss: 0.445552  [ 2200/ 3386]
loss: 0.084701  [ 2300/ 3386]
loss: 0.063508  [ 2400/ 3386]
loss: 0.059378  [ 2500/ 3386]
loss: 0.097936  [ 2600/ 3386]
loss: 0.196712  [ 2700/ 3386]
loss: 0.086163  [ 2800/ 3386]
loss: 0.056613  [ 2900/ 3386]
loss: 0.015871  [ 3000/ 3386]
loss: 0.126655  [ 3100/ 3386]
loss: 0.049552  [ 3200/ 3386]
loss: 0.098578  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.057031  [    0/ 3386]
loss: 0.116649  [  100/ 3386]
loss: 0.086261  [  200/ 3386]
loss: 0.019601  [  300/ 3386]
loss: 0.220580  [  400/ 3386]
loss: 0.072081  [  500/ 3386]
loss: 0.154666  [  600/ 3386]
loss: 0.050863  [  700/ 3386]
loss: 0.115829  [  800/ 3386]
loss: 0.049158  [  900/ 3386]
loss: 0.125742  [ 1000/ 3386]
loss: 0.225744  [ 1100/ 3386]
loss: 0.154884  [ 1200/ 3386]
loss: 0.025907  [ 1300/ 3386]
loss: 0.047875  [ 1400/ 3386]
loss: 0.056027  [ 1500/ 3386]
loss: 0.040256  [ 1600/ 3386]
loss: 0.025924  [ 1700/ 3386]
loss: 0.147528  [ 1800/ 3386]
loss: 0.326015  [ 1900/ 3386]
loss: 0.037747  [ 2000/ 3386]
loss: 0.095362  [ 2100/ 3386]
loss: 0.428111  [ 2200/ 3386]
loss: 0.083847  [ 2300/ 3386]
loss: 0.062080  [ 2400/ 3386]
loss: 0.060386  [ 2500/ 3386]
loss: 0.086651  [ 2600/ 3386]
loss: 0.195720  [ 2700/ 3386]
loss: 0.085027  [ 2800/ 3386]
loss: 0.058861  [ 2900/ 3386]
loss: 0.015545  [ 3000/ 3386]
loss: 0.124363  [ 3100/ 3386]
loss: 0.047167  [ 3200/ 3386]
loss: 0.099134  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.057586  [    0/ 3386]
loss: 0.116392  [  100/ 3386]
loss: 0.089334  [  200/ 3386]
loss: 0.019663  [  300/ 3386]
loss: 0.219301  [  400/ 3386]
loss: 0.071135  [  500/ 3386]
loss: 0.153451  [  600/ 3386]
loss: 0.051233  [  700/ 3386]
loss: 0.118655  [  800/ 3386]
loss: 0.049397  [  900/ 3386]
loss: 0.124759  [ 1000/ 3386]
loss: 0.215296  [ 1100/ 3386]
loss: 0.153835  [ 1200/ 3386]
loss: 0.026765  [ 1300/ 3386]
loss: 0.048268  [ 1400/ 3386]
loss: 0.052693  [ 1500/ 3386]
loss: 0.039272  [ 1600/ 3386]
loss: 0.026071  [ 1700/ 3386]
loss: 0.151434  [ 1800/ 3386]
loss: 0.315669  [ 1900/ 3386]
loss: 0.040842  [ 2000/ 3386]
loss: 0.093145  [ 2100/ 3386]
loss: 0.400562  [ 2200/ 3386]
loss: 0.084601  [ 2300/ 3386]
loss: 0.060525  [ 2400/ 3386]
loss: 0.061318  [ 2500/ 3386]
loss: 0.078108  [ 2600/ 3386]
loss: 0.196112  [ 2700/ 3386]
loss: 0.084481  [ 2800/ 3386]
loss: 0.060539  [ 2900/ 3386]
loss: 0.015586  [ 3000/ 3386]
loss: 0.121218  [ 3100/ 3386]
loss: 0.046575  [ 3200/ 3386]
loss: 0.099334  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.059352  [    0/ 3386]
loss: 0.115431  [  100/ 3386]
loss: 0.089685  [  200/ 3386]
loss: 0.021869  [  300/ 3386]
loss: 0.216620  [  400/ 3386]
loss: 0.070518  [  500/ 3386]
loss: 0.153381  [  600/ 3386]
loss: 0.051538  [  700/ 3386]
loss: 0.120541  [  800/ 3386]
loss: 0.049054  [  900/ 3386]
loss: 0.124724  [ 1000/ 3386]
loss: 0.206527  [ 1100/ 3386]
loss: 0.150876  [ 1200/ 3386]
loss: 0.027366  [ 1300/ 3386]
loss: 0.048741  [ 1400/ 3386]
loss: 0.051194  [ 1500/ 3386]
loss: 0.037252  [ 1600/ 3386]
loss: 0.026862  [ 1700/ 3386]
loss: 0.155475  [ 1800/ 3386]
loss: 0.305856  [ 1900/ 3386]
loss: 0.041607  [ 2000/ 3386]
loss: 0.091570  [ 2100/ 3386]
loss: 0.398770  [ 2200/ 3386]
loss: 0.084797  [ 2300/ 3386]
loss: 0.059365  [ 2400/ 3386]
loss: 0.061814  [ 2500/ 3386]
loss: 0.074130  [ 2600/ 3386]
loss: 0.197260  [ 2700/ 3386]
loss: 0.083738  [ 2800/ 3386]
loss: 0.059843  [ 2900/ 3386]
loss: 0.015521  [ 3000/ 3386]
loss: 0.119186  [ 3100/ 3386]
loss: 0.046875  [ 3200/ 3386]
loss: 0.099779  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [ 0.75888944 -0.971661  ]
[0 2 0 ... 2 0 1]
[1 1 1 ... 1 1 0]
Cluster 0 Occurrences: 1079; KMEANS: 574
Cluster 1 Occurrences: 1158; KMEANS: 2196
Cluster 2 Occurrences: 1149; KMEANS: 616
Centroids: [[0.8119536, -1.354247], [-1.2363229, 5.312108], [-0.21512263, -2.1248045]]
Centroids: [[-1.7232251, 6.8214684], [0.3039941, -1.8154988], [-0.781136, 3.766397]]
Contingency Matrix: 
[[   0 1079    0]
 [ 573    9  576]
 [   1 1108   40]]
[[0, -1, 0], [573, -1, 576], [-1, -1, -1]]
[[0, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 2, 0: 0}
New Contingency Matrix: 
[[   0    0 1079]
 [ 573  576    9]
 [   1   40 1108]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [0, 576, 1108], Sum: 1684
All_Elements: [0, 0, 1079, 573, 576, 9, 1, 40, 1108], Sum: 3386
Accuracy: 0.49734199645599525
Done!
