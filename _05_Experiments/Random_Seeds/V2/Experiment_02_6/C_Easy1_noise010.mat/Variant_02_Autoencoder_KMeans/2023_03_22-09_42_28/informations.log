Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise010.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy1_noise010.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_42_28
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000020629A67E10>
Sampling rate: 24000.0
Raw: [ 0.01223885  0.0148803   0.02156532 ... -0.02666729 -0.01215068
  0.01031338]
Times: [   1012    1333    1523 ... 1438660 1438690 1439287]
Cluster: [1 3 3 ... 3 1 3]
Number of different clusters:  3
Number of Spikes: 3522
First aligned Spike Frame: [-0.12579972 -0.1620207  -0.18573939 -0.18961714 -0.17315203 -0.1396561
 -0.09426159 -0.04496221  0.00734136  0.05943689  0.09448878  0.07601101
  0.00811529  0.09446653  0.55421698  1.03008495  0.90195967  0.23116337
 -0.37857386 -0.59030761 -0.52695739 -0.40967661 -0.32471334 -0.26000903
 -0.19839489 -0.13975054 -0.09692457 -0.08223204 -0.09147337 -0.10695261
 -0.11338461 -0.10883018 -0.09915766 -0.08637804 -0.06514771 -0.03537735
 -0.00695045  0.01307529  0.02539335  0.03607991  0.04814655  0.05257424
  0.04538721  0.03512231  0.03354714  0.04141478  0.0511991 ]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1134
Cluster 2, Occurrences: 1237
<torch.utils.data.dataloader.DataLoader object at 0x0000020628D95160>
Epoch 1
-------------------------------
loss: 0.108451  [    0/ 3522]
loss: 0.290064  [  100/ 3522]
loss: 0.078977  [  200/ 3522]
loss: 0.071301  [  300/ 3522]
loss: 0.016147  [  400/ 3522]
loss: 0.009269  [  500/ 3522]
loss: 0.116903  [  600/ 3522]
loss: 0.017952  [  700/ 3522]
loss: 0.015213  [  800/ 3522]
loss: 0.006066  [  900/ 3522]
loss: 0.005003  [ 1000/ 3522]
loss: 0.004419  [ 1100/ 3522]
loss: 0.092870  [ 1200/ 3522]
loss: 0.015848  [ 1300/ 3522]
loss: 0.002563  [ 1400/ 3522]
loss: 0.003537  [ 1500/ 3522]
loss: 0.009610  [ 1600/ 3522]
loss: 0.008097  [ 1700/ 3522]
loss: 0.008962  [ 1800/ 3522]
loss: 0.015190  [ 1900/ 3522]
loss: 0.012445  [ 2000/ 3522]
loss: 0.012797  [ 2100/ 3522]
loss: 0.005282  [ 2200/ 3522]
loss: 0.005841  [ 2300/ 3522]
loss: 0.006780  [ 2400/ 3522]
loss: 0.004179  [ 2500/ 3522]
loss: 0.069705  [ 2600/ 3522]
loss: 0.009729  [ 2700/ 3522]
loss: 0.002722  [ 2800/ 3522]
loss: 0.004347  [ 2900/ 3522]
loss: 0.008805  [ 3000/ 3522]
loss: 0.006076  [ 3100/ 3522]
loss: 0.014699  [ 3200/ 3522]
loss: 0.009161  [ 3300/ 3522]
loss: 0.009607  [ 3400/ 3522]
loss: 0.006386  [ 3500/ 3522]
Epoch 2
-------------------------------
loss: 0.011303  [    0/ 3522]
loss: 0.007972  [  100/ 3522]
loss: 0.004066  [  200/ 3522]
loss: 0.039462  [  300/ 3522]
loss: 0.007218  [  400/ 3522]
loss: 0.009681  [  500/ 3522]
loss: 0.117118  [  600/ 3522]
loss: 0.015160  [  700/ 3522]
loss: 0.013752  [  800/ 3522]
loss: 0.006080  [  900/ 3522]
loss: 0.004027  [ 1000/ 3522]
loss: 0.004460  [ 1100/ 3522]
loss: 0.093261  [ 1200/ 3522]
loss: 0.015155  [ 1300/ 3522]
loss: 0.002407  [ 1400/ 3522]
loss: 0.003741  [ 1500/ 3522]
loss: 0.009969  [ 1600/ 3522]
loss: 0.008589  [ 1700/ 3522]
loss: 0.010386  [ 1800/ 3522]
loss: 0.012016  [ 1900/ 3522]
loss: 0.011180  [ 2000/ 3522]
loss: 0.008675  [ 2100/ 3522]
loss: 0.004444  [ 2200/ 3522]
loss: 0.005705  [ 2300/ 3522]
loss: 0.005749  [ 2400/ 3522]
loss: 0.004049  [ 2500/ 3522]
loss: 0.073134  [ 2600/ 3522]
loss: 0.007874  [ 2700/ 3522]
loss: 0.002624  [ 2800/ 3522]
loss: 0.004352  [ 2900/ 3522]
loss: 0.009336  [ 3000/ 3522]
loss: 0.005083  [ 3100/ 3522]
loss: 0.013725  [ 3200/ 3522]
loss: 0.009399  [ 3300/ 3522]
loss: 0.009746  [ 3400/ 3522]
loss: 0.006036  [ 3500/ 3522]
Epoch 3
-------------------------------
loss: 0.010737  [    0/ 3522]
loss: 0.007587  [  100/ 3522]
loss: 0.003056  [  200/ 3522]
loss: 0.036039  [  300/ 3522]
loss: 0.005509  [  400/ 3522]
loss: 0.009663  [  500/ 3522]
loss: 0.099172  [  600/ 3522]
loss: 0.014642  [  700/ 3522]
loss: 0.012346  [  800/ 3522]
loss: 0.005868  [  900/ 3522]
loss: 0.003577  [ 1000/ 3522]
loss: 0.004191  [ 1100/ 3522]
loss: 0.092982  [ 1200/ 3522]
loss: 0.013052  [ 1300/ 3522]
loss: 0.002396  [ 1400/ 3522]
loss: 0.003656  [ 1500/ 3522]
loss: 0.009710  [ 1600/ 3522]
loss: 0.007754  [ 1700/ 3522]
loss: 0.012683  [ 1800/ 3522]
loss: 0.011620  [ 1900/ 3522]
loss: 0.010775  [ 2000/ 3522]
loss: 0.007643  [ 2100/ 3522]
loss: 0.004239  [ 2200/ 3522]
loss: 0.005765  [ 2300/ 3522]
loss: 0.003562  [ 2400/ 3522]
loss: 0.003950  [ 2500/ 3522]
loss: 0.074283  [ 2600/ 3522]
loss: 0.007337  [ 2700/ 3522]
loss: 0.002698  [ 2800/ 3522]
loss: 0.004465  [ 2900/ 3522]
loss: 0.009685  [ 3000/ 3522]
loss: 0.004778  [ 3100/ 3522]
loss: 0.013217  [ 3200/ 3522]
loss: 0.008139  [ 3300/ 3522]
loss: 0.009805  [ 3400/ 3522]
loss: 0.006175  [ 3500/ 3522]
Epoch 4
-------------------------------
loss: 0.010147  [    0/ 3522]
loss: 0.007339  [  100/ 3522]
loss: 0.002734  [  200/ 3522]
loss: 0.033097  [  300/ 3522]
loss: 0.005048  [  400/ 3522]
loss: 0.009551  [  500/ 3522]
loss: 0.078618  [  600/ 3522]
loss: 0.014012  [  700/ 3522]
loss: 0.011213  [  800/ 3522]
loss: 0.005407  [  900/ 3522]
loss: 0.003085  [ 1000/ 3522]
loss: 0.004172  [ 1100/ 3522]
loss: 0.093580  [ 1200/ 3522]
loss: 0.012229  [ 1300/ 3522]
loss: 0.002343  [ 1400/ 3522]
loss: 0.003592  [ 1500/ 3522]
loss: 0.010112  [ 1600/ 3522]
loss: 0.007810  [ 1700/ 3522]
loss: 0.014163  [ 1800/ 3522]
loss: 0.010994  [ 1900/ 3522]
loss: 0.010656  [ 2000/ 3522]
loss: 0.006948  [ 2100/ 3522]
loss: 0.003674  [ 2200/ 3522]
loss: 0.005452  [ 2300/ 3522]
loss: 0.003103  [ 2400/ 3522]
loss: 0.003789  [ 2500/ 3522]
loss: 0.074213  [ 2600/ 3522]
loss: 0.008136  [ 2700/ 3522]
loss: 0.002653  [ 2800/ 3522]
loss: 0.004400  [ 2900/ 3522]
loss: 0.009509  [ 3000/ 3522]
loss: 0.004308  [ 3100/ 3522]
loss: 0.012534  [ 3200/ 3522]
loss: 0.007583  [ 3300/ 3522]
loss: 0.009783  [ 3400/ 3522]
loss: 0.006532  [ 3500/ 3522]
Epoch 5
-------------------------------
loss: 0.009640  [    0/ 3522]
loss: 0.007569  [  100/ 3522]
loss: 0.003092  [  200/ 3522]
loss: 0.030047  [  300/ 3522]
loss: 0.004791  [  400/ 3522]
loss: 0.009471  [  500/ 3522]
loss: 0.067005  [  600/ 3522]
loss: 0.013286  [  700/ 3522]
loss: 0.010555  [  800/ 3522]
loss: 0.005252  [  900/ 3522]
loss: 0.002871  [ 1000/ 3522]
loss: 0.004035  [ 1100/ 3522]
loss: 0.092809  [ 1200/ 3522]
loss: 0.011727  [ 1300/ 3522]
loss: 0.002126  [ 1400/ 3522]
loss: 0.003696  [ 1500/ 3522]
loss: 0.009964  [ 1600/ 3522]
loss: 0.007008  [ 1700/ 3522]
loss: 0.012549  [ 1800/ 3522]
loss: 0.011080  [ 1900/ 3522]
loss: 0.010542  [ 2000/ 3522]
loss: 0.006766  [ 2100/ 3522]
loss: 0.003186  [ 2200/ 3522]
loss: 0.005216  [ 2300/ 3522]
loss: 0.003236  [ 2400/ 3522]
loss: 0.003691  [ 2500/ 3522]
loss: 0.074566  [ 2600/ 3522]
loss: 0.008369  [ 2700/ 3522]
loss: 0.002611  [ 2800/ 3522]
loss: 0.004432  [ 2900/ 3522]
loss: 0.009391  [ 3000/ 3522]
loss: 0.003988  [ 3100/ 3522]
loss: 0.011728  [ 3200/ 3522]
loss: 0.007622  [ 3300/ 3522]
loss: 0.009909  [ 3400/ 3522]
loss: 0.006711  [ 3500/ 3522]
Epoch 6
-------------------------------
loss: 0.009282  [    0/ 3522]
loss: 0.007705  [  100/ 3522]
loss: 0.003419  [  200/ 3522]
loss: 0.026611  [  300/ 3522]
loss: 0.004764  [  400/ 3522]
loss: 0.009302  [  500/ 3522]
loss: 0.060750  [  600/ 3522]
loss: 0.012857  [  700/ 3522]
loss: 0.009697  [  800/ 3522]
loss: 0.005169  [  900/ 3522]
loss: 0.002751  [ 1000/ 3522]
loss: 0.003845  [ 1100/ 3522]
loss: 0.092219  [ 1200/ 3522]
loss: 0.011765  [ 1300/ 3522]
loss: 0.002230  [ 1400/ 3522]
loss: 0.003601  [ 1500/ 3522]
loss: 0.009981  [ 1600/ 3522]
loss: 0.007065  [ 1700/ 3522]
loss: 0.013290  [ 1800/ 3522]
loss: 0.010956  [ 1900/ 3522]
loss: 0.010212  [ 2000/ 3522]
loss: 0.006672  [ 2100/ 3522]
loss: 0.002871  [ 2200/ 3522]
loss: 0.004944  [ 2300/ 3522]
loss: 0.003089  [ 2400/ 3522]
loss: 0.003568  [ 2500/ 3522]
loss: 0.073833  [ 2600/ 3522]
loss: 0.008784  [ 2700/ 3522]
loss: 0.002588  [ 2800/ 3522]
loss: 0.004484  [ 2900/ 3522]
loss: 0.009257  [ 3000/ 3522]
loss: 0.003353  [ 3100/ 3522]
loss: 0.011312  [ 3200/ 3522]
loss: 0.007325  [ 3300/ 3522]
loss: 0.010041  [ 3400/ 3522]
loss: 0.006887  [ 3500/ 3522]
Epoch 7
-------------------------------
loss: 0.009090  [    0/ 3522]
loss: 0.007861  [  100/ 3522]
loss: 0.003529  [  200/ 3522]
loss: 0.023557  [  300/ 3522]
loss: 0.004757  [  400/ 3522]
loss: 0.009156  [  500/ 3522]
loss: 0.060684  [  600/ 3522]
loss: 0.012567  [  700/ 3522]
loss: 0.009255  [  800/ 3522]
loss: 0.005152  [  900/ 3522]
loss: 0.002643  [ 1000/ 3522]
loss: 0.003764  [ 1100/ 3522]
loss: 0.091592  [ 1200/ 3522]
loss: 0.011658  [ 1300/ 3522]
loss: 0.002370  [ 1400/ 3522]
loss: 0.003561  [ 1500/ 3522]
loss: 0.010011  [ 1600/ 3522]
loss: 0.007103  [ 1700/ 3522]
loss: 0.013945  [ 1800/ 3522]
loss: 0.010806  [ 1900/ 3522]
loss: 0.009955  [ 2000/ 3522]
loss: 0.006555  [ 2100/ 3522]
loss: 0.002692  [ 2200/ 3522]
loss: 0.004807  [ 2300/ 3522]
loss: 0.003226  [ 2400/ 3522]
loss: 0.003438  [ 2500/ 3522]
loss: 0.073624  [ 2600/ 3522]
loss: 0.009045  [ 2700/ 3522]
loss: 0.002586  [ 2800/ 3522]
loss: 0.004421  [ 2900/ 3522]
loss: 0.008847  [ 3000/ 3522]
loss: 0.003304  [ 3100/ 3522]
loss: 0.010909  [ 3200/ 3522]
loss: 0.006968  [ 3300/ 3522]
loss: 0.010197  [ 3400/ 3522]
loss: 0.007025  [ 3500/ 3522]
Epoch 8
-------------------------------
loss: 0.008886  [    0/ 3522]
loss: 0.007854  [  100/ 3522]
loss: 0.003531  [  200/ 3522]
loss: 0.020726  [  300/ 3522]
loss: 0.004840  [  400/ 3522]
loss: 0.009066  [  500/ 3522]
loss: 0.059831  [  600/ 3522]
loss: 0.012416  [  700/ 3522]
loss: 0.009007  [  800/ 3522]
loss: 0.005098  [  900/ 3522]
loss: 0.002628  [ 1000/ 3522]
loss: 0.003691  [ 1100/ 3522]
loss: 0.090609  [ 1200/ 3522]
loss: 0.011556  [ 1300/ 3522]
loss: 0.002480  [ 1400/ 3522]
loss: 0.003507  [ 1500/ 3522]
loss: 0.010168  [ 1600/ 3522]
loss: 0.006884  [ 1700/ 3522]
loss: 0.013391  [ 1800/ 3522]
loss: 0.010665  [ 1900/ 3522]
loss: 0.009822  [ 2000/ 3522]
loss: 0.006448  [ 2100/ 3522]
loss: 0.002601  [ 2200/ 3522]
loss: 0.004749  [ 2300/ 3522]
loss: 0.003096  [ 2400/ 3522]
loss: 0.003308  [ 2500/ 3522]
loss: 0.073378  [ 2600/ 3522]
loss: 0.008580  [ 2700/ 3522]
loss: 0.002572  [ 2800/ 3522]
loss: 0.004548  [ 2900/ 3522]
loss: 0.008301  [ 3000/ 3522]
loss: 0.002975  [ 3100/ 3522]
loss: 0.010585  [ 3200/ 3522]
loss: 0.006759  [ 3300/ 3522]
loss: 0.010344  [ 3400/ 3522]
loss: 0.007126  [ 3500/ 3522]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3522
First Spike after testing: [ 1.1272132 -1.9219052]
[0 2 2 ... 2 0 2]
[0 1 1 ... 1 0 1]
Cluster 0 Occurrences: 1151; KMEANS: 1144
Cluster 1 Occurrences: 1134; KMEANS: 1258
Cluster 2 Occurrences: 1237; KMEANS: 1120
Centroids: [[1.1755009, -2.1672752], [0.40330702, 1.4653283], [-1.2282724, -3.525286]]
Centroids: [[1.1998072, -2.1438854], [-1.2161323, -3.529653], [0.40026212, 1.5172122]]
Contingency Matrix: 
[[1135   13    3]
 [   9   10 1115]
 [   0 1235    2]]
[[1135, -1, 3], [9, -1, 1115], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, 1115], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 0, 1: 2}
New Contingency Matrix: 
[[1135    3   13]
 [   9 1115   10]
 [   0    2 1235]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [1135, 1115, 1235], Sum: 3485
All_Elements: [1135, 3, 13, 9, 1115, 10, 0, 2, 1235], Sum: 3522
Accuracy: 0.9894946053378763
Done!
