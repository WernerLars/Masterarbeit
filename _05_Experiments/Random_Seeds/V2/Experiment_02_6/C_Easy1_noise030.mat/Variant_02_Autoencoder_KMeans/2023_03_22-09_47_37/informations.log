Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000020628C1BB38>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000020628D95128>
Epoch 1
-------------------------------
loss: 0.337416  [    0/ 3475]
loss: 0.161569  [  100/ 3475]
loss: 0.324114  [  200/ 3475]
loss: 0.266864  [  300/ 3475]
loss: 0.186381  [  400/ 3475]
loss: 0.054626  [  500/ 3475]
loss: 0.085959  [  600/ 3475]
loss: 0.148973  [  700/ 3475]
loss: 0.127807  [  800/ 3475]
loss: 0.200923  [  900/ 3475]
loss: 0.051180  [ 1000/ 3475]
loss: 0.098899  [ 1100/ 3475]
loss: 0.095299  [ 1200/ 3475]
loss: 0.040143  [ 1300/ 3475]
loss: 0.078600  [ 1400/ 3475]
loss: 0.077028  [ 1500/ 3475]
loss: 0.032924  [ 1600/ 3475]
loss: 0.015953  [ 1700/ 3475]
loss: 0.154725  [ 1800/ 3475]
loss: 0.038510  [ 1900/ 3475]
loss: 0.033032  [ 2000/ 3475]
loss: 0.025673  [ 2100/ 3475]
loss: 0.141304  [ 2200/ 3475]
loss: 0.094673  [ 2300/ 3475]
loss: 0.017287  [ 2400/ 3475]
loss: 0.107634  [ 2500/ 3475]
loss: 0.073367  [ 2600/ 3475]
loss: 0.064757  [ 2700/ 3475]
loss: 0.035859  [ 2800/ 3475]
loss: 0.100719  [ 2900/ 3475]
loss: 0.054188  [ 3000/ 3475]
loss: 0.050072  [ 3100/ 3475]
loss: 0.039485  [ 3200/ 3475]
loss: 0.030376  [ 3300/ 3475]
loss: 0.042339  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.057241  [    0/ 3475]
loss: 0.023511  [  100/ 3475]
loss: 0.057147  [  200/ 3475]
loss: 0.218879  [  300/ 3475]
loss: 0.095735  [  400/ 3475]
loss: 0.032977  [  500/ 3475]
loss: 0.058289  [  600/ 3475]
loss: 0.023885  [  700/ 3475]
loss: 0.096107  [  800/ 3475]
loss: 0.101519  [  900/ 3475]
loss: 0.014882  [ 1000/ 3475]
loss: 0.057331  [ 1100/ 3475]
loss: 0.028297  [ 1200/ 3475]
loss: 0.033970  [ 1300/ 3475]
loss: 0.058152  [ 1400/ 3475]
loss: 0.030033  [ 1500/ 3475]
loss: 0.034261  [ 1600/ 3475]
loss: 0.017768  [ 1700/ 3475]
loss: 0.149640  [ 1800/ 3475]
loss: 0.038889  [ 1900/ 3475]
loss: 0.017602  [ 2000/ 3475]
loss: 0.027293  [ 2100/ 3475]
loss: 0.089599  [ 2200/ 3475]
loss: 0.028910  [ 2300/ 3475]
loss: 0.015793  [ 2400/ 3475]
loss: 0.102988  [ 2500/ 3475]
loss: 0.062898  [ 2600/ 3475]
loss: 0.069261  [ 2700/ 3475]
loss: 0.035515  [ 2800/ 3475]
loss: 0.039978  [ 2900/ 3475]
loss: 0.053245  [ 3000/ 3475]
loss: 0.049948  [ 3100/ 3475]
loss: 0.042802  [ 3200/ 3475]
loss: 0.023160  [ 3300/ 3475]
loss: 0.018032  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.013253  [    0/ 3475]
loss: 0.027814  [  100/ 3475]
loss: 0.053362  [  200/ 3475]
loss: 0.227874  [  300/ 3475]
loss: 0.074920  [  400/ 3475]
loss: 0.029442  [  500/ 3475]
loss: 0.026564  [  600/ 3475]
loss: 0.026216  [  700/ 3475]
loss: 0.100689  [  800/ 3475]
loss: 0.082992  [  900/ 3475]
loss: 0.027542  [ 1000/ 3475]
loss: 0.030985  [ 1100/ 3475]
loss: 0.023742  [ 1200/ 3475]
loss: 0.042885  [ 1300/ 3475]
loss: 0.054648  [ 1400/ 3475]
loss: 0.034568  [ 1500/ 3475]
loss: 0.037131  [ 1600/ 3475]
loss: 0.015207  [ 1700/ 3475]
loss: 0.142309  [ 1800/ 3475]
loss: 0.039230  [ 1900/ 3475]
loss: 0.015909  [ 2000/ 3475]
loss: 0.027431  [ 2100/ 3475]
loss: 0.083081  [ 2200/ 3475]
loss: 0.015273  [ 2300/ 3475]
loss: 0.015413  [ 2400/ 3475]
loss: 0.096832  [ 2500/ 3475]
loss: 0.050086  [ 2600/ 3475]
loss: 0.075284  [ 2700/ 3475]
loss: 0.035939  [ 2800/ 3475]
loss: 0.031526  [ 2900/ 3475]
loss: 0.053156  [ 3000/ 3475]
loss: 0.054713  [ 3100/ 3475]
loss: 0.046645  [ 3200/ 3475]
loss: 0.018403  [ 3300/ 3475]
loss: 0.020314  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.018173  [    0/ 3475]
loss: 0.031628  [  100/ 3475]
loss: 0.052762  [  200/ 3475]
loss: 0.233235  [  300/ 3475]
loss: 0.068903  [  400/ 3475]
loss: 0.029079  [  500/ 3475]
loss: 0.024280  [  600/ 3475]
loss: 0.041625  [  700/ 3475]
loss: 0.102972  [  800/ 3475]
loss: 0.081199  [  900/ 3475]
loss: 0.032580  [ 1000/ 3475]
loss: 0.024631  [ 1100/ 3475]
loss: 0.023609  [ 1200/ 3475]
loss: 0.045826  [ 1300/ 3475]
loss: 0.048866  [ 1400/ 3475]
loss: 0.037463  [ 1500/ 3475]
loss: 0.040887  [ 1600/ 3475]
loss: 0.013554  [ 1700/ 3475]
loss: 0.125926  [ 1800/ 3475]
loss: 0.038797  [ 1900/ 3475]
loss: 0.015596  [ 2000/ 3475]
loss: 0.028006  [ 2100/ 3475]
loss: 0.083082  [ 2200/ 3475]
loss: 0.012886  [ 2300/ 3475]
loss: 0.015846  [ 2400/ 3475]
loss: 0.093855  [ 2500/ 3475]
loss: 0.044122  [ 2600/ 3475]
loss: 0.090415  [ 2700/ 3475]
loss: 0.036917  [ 2800/ 3475]
loss: 0.031639  [ 2900/ 3475]
loss: 0.053243  [ 3000/ 3475]
loss: 0.056429  [ 3100/ 3475]
loss: 0.048389  [ 3200/ 3475]
loss: 0.015482  [ 3300/ 3475]
loss: 0.022246  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.021382  [    0/ 3475]
loss: 0.034008  [  100/ 3475]
loss: 0.052412  [  200/ 3475]
loss: 0.235802  [  300/ 3475]
loss: 0.064948  [  400/ 3475]
loss: 0.028958  [  500/ 3475]
loss: 0.023274  [  600/ 3475]
loss: 0.060385  [  700/ 3475]
loss: 0.103647  [  800/ 3475]
loss: 0.077719  [  900/ 3475]
loss: 0.033383  [ 1000/ 3475]
loss: 0.022249  [ 1100/ 3475]
loss: 0.023513  [ 1200/ 3475]
loss: 0.046203  [ 1300/ 3475]
loss: 0.043499  [ 1400/ 3475]
loss: 0.039401  [ 1500/ 3475]
loss: 0.043116  [ 1600/ 3475]
loss: 0.012274  [ 1700/ 3475]
loss: 0.109152  [ 1800/ 3475]
loss: 0.038523  [ 1900/ 3475]
loss: 0.015141  [ 2000/ 3475]
loss: 0.030006  [ 2100/ 3475]
loss: 0.083225  [ 2200/ 3475]
loss: 0.012213  [ 2300/ 3475]
loss: 0.016018  [ 2400/ 3475]
loss: 0.090456  [ 2500/ 3475]
loss: 0.041703  [ 2600/ 3475]
loss: 0.102632  [ 2700/ 3475]
loss: 0.037032  [ 2800/ 3475]
loss: 0.031513  [ 2900/ 3475]
loss: 0.053440  [ 3000/ 3475]
loss: 0.056319  [ 3100/ 3475]
loss: 0.049342  [ 3200/ 3475]
loss: 0.014867  [ 3300/ 3475]
loss: 0.023137  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.022952  [    0/ 3475]
loss: 0.035573  [  100/ 3475]
loss: 0.052267  [  200/ 3475]
loss: 0.234240  [  300/ 3475]
loss: 0.062057  [  400/ 3475]
loss: 0.028109  [  500/ 3475]
loss: 0.022848  [  600/ 3475]
loss: 0.070859  [  700/ 3475]
loss: 0.104203  [  800/ 3475]
loss: 0.075947  [  900/ 3475]
loss: 0.032588  [ 1000/ 3475]
loss: 0.021289  [ 1100/ 3475]
loss: 0.023819  [ 1200/ 3475]
loss: 0.045827  [ 1300/ 3475]
loss: 0.040654  [ 1400/ 3475]
loss: 0.040053  [ 1500/ 3475]
loss: 0.043980  [ 1600/ 3475]
loss: 0.011887  [ 1700/ 3475]
loss: 0.098414  [ 1800/ 3475]
loss: 0.038289  [ 1900/ 3475]
loss: 0.014876  [ 2000/ 3475]
loss: 0.032245  [ 2100/ 3475]
loss: 0.083122  [ 2200/ 3475]
loss: 0.011619  [ 2300/ 3475]
loss: 0.015753  [ 2400/ 3475]
loss: 0.086593  [ 2500/ 3475]
loss: 0.040089  [ 2600/ 3475]
loss: 0.110614  [ 2700/ 3475]
loss: 0.036747  [ 2800/ 3475]
loss: 0.030698  [ 2900/ 3475]
loss: 0.053322  [ 3000/ 3475]
loss: 0.056158  [ 3100/ 3475]
loss: 0.048458  [ 3200/ 3475]
loss: 0.014856  [ 3300/ 3475]
loss: 0.023197  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.023562  [    0/ 3475]
loss: 0.036215  [  100/ 3475]
loss: 0.052777  [  200/ 3475]
loss: 0.230247  [  300/ 3475]
loss: 0.057470  [  400/ 3475]
loss: 0.027421  [  500/ 3475]
loss: 0.023883  [  600/ 3475]
loss: 0.077854  [  700/ 3475]
loss: 0.104776  [  800/ 3475]
loss: 0.075172  [  900/ 3475]
loss: 0.032756  [ 1000/ 3475]
loss: 0.020361  [ 1100/ 3475]
loss: 0.023568  [ 1200/ 3475]
loss: 0.045876  [ 1300/ 3475]
loss: 0.038513  [ 1400/ 3475]
loss: 0.039849  [ 1500/ 3475]
loss: 0.044071  [ 1600/ 3475]
loss: 0.011712  [ 1700/ 3475]
loss: 0.093370  [ 1800/ 3475]
loss: 0.038450  [ 1900/ 3475]
loss: 0.014825  [ 2000/ 3475]
loss: 0.034061  [ 2100/ 3475]
loss: 0.081794  [ 2200/ 3475]
loss: 0.011476  [ 2300/ 3475]
loss: 0.016014  [ 2400/ 3475]
loss: 0.084412  [ 2500/ 3475]
loss: 0.039403  [ 2600/ 3475]
loss: 0.113999  [ 2700/ 3475]
loss: 0.036777  [ 2800/ 3475]
loss: 0.029411  [ 2900/ 3475]
loss: 0.052964  [ 3000/ 3475]
loss: 0.056506  [ 3100/ 3475]
loss: 0.046579  [ 3200/ 3475]
loss: 0.015730  [ 3300/ 3475]
loss: 0.023130  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.023416  [    0/ 3475]
loss: 0.036371  [  100/ 3475]
loss: 0.052420  [  200/ 3475]
loss: 0.228161  [  300/ 3475]
loss: 0.051276  [  400/ 3475]
loss: 0.027152  [  500/ 3475]
loss: 0.024575  [  600/ 3475]
loss: 0.078561  [  700/ 3475]
loss: 0.105034  [  800/ 3475]
loss: 0.074797  [  900/ 3475]
loss: 0.032505  [ 1000/ 3475]
loss: 0.020091  [ 1100/ 3475]
loss: 0.023024  [ 1200/ 3475]
loss: 0.045992  [ 1300/ 3475]
loss: 0.037252  [ 1400/ 3475]
loss: 0.039393  [ 1500/ 3475]
loss: 0.044170  [ 1600/ 3475]
loss: 0.011681  [ 1700/ 3475]
loss: 0.090907  [ 1800/ 3475]
loss: 0.038588  [ 1900/ 3475]
loss: 0.014872  [ 2000/ 3475]
loss: 0.034923  [ 2100/ 3475]
loss: 0.080547  [ 2200/ 3475]
loss: 0.011769  [ 2300/ 3475]
loss: 0.016403  [ 2400/ 3475]
loss: 0.082061  [ 2500/ 3475]
loss: 0.038651  [ 2600/ 3475]
loss: 0.113973  [ 2700/ 3475]
loss: 0.036864  [ 2800/ 3475]
loss: 0.028512  [ 2900/ 3475]
loss: 0.053014  [ 3000/ 3475]
loss: 0.056270  [ 3100/ 3475]
loss: 0.044992  [ 3200/ 3475]
loss: 0.017698  [ 3300/ 3475]
loss: 0.022940  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [ 0.23915882 -2.7949367 ]
[2 1 2 ... 1 0 1]
[2 1 2 ... 1 0 1]
Cluster 0 Occurrences: 1162; KMEANS: 1191
Cluster 1 Occurrences: 1164; KMEANS: 1136
Cluster 2 Occurrences: 1149; KMEANS: 1148
Centroids: [[0.22382973, -0.8168849], [1.0020261, 3.574095], [-0.9781406, -2.6221695]]
Centroids: [[0.2355577, -0.75474906], [1.0289387, 3.6636584], [-0.9990058, -2.6697357]]
Contingency Matrix: 
[[1115    0   47]
 [  27 1132    5]
 [  49    4 1096]]
[[1115, -1, 47], [-1, -1, -1], [49, -1, 1096]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 1096]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 0, 2: 2}
New Contingency Matrix: 
[[1115    0   47]
 [  27 1132    5]
 [  49    4 1096]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1115, 1132, 1096], Sum: 3343
All_Elements: [1115, 0, 47, 27, 1132, 5, 49, 4, 1096], Sum: 3475
Accuracy: 0.9620143884892086
Done!
