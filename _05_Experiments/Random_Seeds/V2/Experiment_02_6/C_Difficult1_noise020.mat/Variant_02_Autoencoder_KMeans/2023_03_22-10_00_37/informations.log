Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Difficult1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_00_37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002069410D160>
Sampling rate: 24000.0
Raw: [0.07805807 0.0565915  0.02594138 ... 0.2738422  0.26036418 0.26764671]
Times: [    368     630     648 ... 1439484 1439672 1439764]
Cluster: [2 2 1 ... 1 1 3]
Number of different clusters:  3
Number of Spikes: 3414
First aligned Spike Frame: [-0.38605838 -0.38126768 -0.38226316 -0.39416749 -0.41080739 -0.41171959
 -0.38717544 -0.35786686 -0.38107535 -0.49467824 -0.61843181 -0.58172559
 -0.24572387  0.35672948  0.89024247  1.0117557   0.76459666  0.43756704
  0.20139815  0.0394919  -0.09249478 -0.19279146 -0.23446076 -0.21807174
 -0.17478611 -0.12910555 -0.08593802 -0.03628316  0.029471    0.09510752
  0.13901987  0.15819091  0.1762069   0.21560464  0.27527193  0.33233202
  0.36740003  0.38309659  0.39850514  0.42031497  0.43304033  0.41207346
  0.35038997  0.26929981  0.20084763  0.15457014  0.10809812]
Cluster 0, Occurrences: 1136
Cluster 1, Occurrences: 1099
Cluster 2, Occurrences: 1179
<torch.utils.data.dataloader.DataLoader object at 0x00000206899FF588>
Epoch 1
-------------------------------
loss: 0.159912  [    0/ 3414]
loss: 0.157559  [  100/ 3414]
loss: 0.118317  [  200/ 3414]
loss: 0.135230  [  300/ 3414]
loss: 0.054435  [  400/ 3414]
loss: 0.071199  [  500/ 3414]
loss: 0.039991  [  600/ 3414]
loss: 0.059129  [  700/ 3414]
loss: 0.085505  [  800/ 3414]
loss: 0.079369  [  900/ 3414]
loss: 0.030913  [ 1000/ 3414]
loss: 0.022037  [ 1100/ 3414]
loss: 0.063142  [ 1200/ 3414]
loss: 0.033296  [ 1300/ 3414]
loss: 0.023748  [ 1400/ 3414]
loss: 0.031447  [ 1500/ 3414]
loss: 0.032077  [ 1600/ 3414]
loss: 0.042294  [ 1700/ 3414]
loss: 0.032265  [ 1800/ 3414]
loss: 0.044149  [ 1900/ 3414]
loss: 0.041487  [ 2000/ 3414]
loss: 0.062776  [ 2100/ 3414]
loss: 0.067041  [ 2200/ 3414]
loss: 0.065683  [ 2300/ 3414]
loss: 0.028404  [ 2400/ 3414]
loss: 0.179644  [ 2500/ 3414]
loss: 0.027561  [ 2600/ 3414]
loss: 0.018572  [ 2700/ 3414]
loss: 0.056031  [ 2800/ 3414]
loss: 0.037756  [ 2900/ 3414]
loss: 0.073343  [ 3000/ 3414]
loss: 0.044722  [ 3100/ 3414]
loss: 0.042055  [ 3200/ 3414]
loss: 0.013311  [ 3300/ 3414]
loss: 0.035083  [ 3400/ 3414]
Epoch 2
-------------------------------
loss: 0.081325  [    0/ 3414]
loss: 0.031083  [  100/ 3414]
loss: 0.011086  [  200/ 3414]
loss: 0.043911  [  300/ 3414]
loss: 0.047293  [  400/ 3414]
loss: 0.058131  [  500/ 3414]
loss: 0.036322  [  600/ 3414]
loss: 0.053474  [  700/ 3414]
loss: 0.070841  [  800/ 3414]
loss: 0.037068  [  900/ 3414]
loss: 0.017313  [ 1000/ 3414]
loss: 0.019914  [ 1100/ 3414]
loss: 0.051130  [ 1200/ 3414]
loss: 0.032412  [ 1300/ 3414]
loss: 0.020372  [ 1400/ 3414]
loss: 0.031622  [ 1500/ 3414]
loss: 0.031192  [ 1600/ 3414]
loss: 0.032802  [ 1700/ 3414]
loss: 0.029937  [ 1800/ 3414]
loss: 0.043501  [ 1900/ 3414]
loss: 0.040053  [ 2000/ 3414]
loss: 0.056425  [ 2100/ 3414]
loss: 0.050977  [ 2200/ 3414]
loss: 0.049608  [ 2300/ 3414]
loss: 0.018268  [ 2400/ 3414]
loss: 0.168000  [ 2500/ 3414]
loss: 0.025413  [ 2600/ 3414]
loss: 0.015906  [ 2700/ 3414]
loss: 0.048871  [ 2800/ 3414]
loss: 0.036503  [ 2900/ 3414]
loss: 0.062802  [ 3000/ 3414]
loss: 0.043660  [ 3100/ 3414]
loss: 0.042467  [ 3200/ 3414]
loss: 0.011711  [ 3300/ 3414]
loss: 0.034949  [ 3400/ 3414]
Epoch 3
-------------------------------
loss: 0.072737  [    0/ 3414]
loss: 0.031190  [  100/ 3414]
loss: 0.009282  [  200/ 3414]
loss: 0.046608  [  300/ 3414]
loss: 0.050066  [  400/ 3414]
loss: 0.048689  [  500/ 3414]
loss: 0.037171  [  600/ 3414]
loss: 0.031672  [  700/ 3414]
loss: 0.051553  [  800/ 3414]
loss: 0.028625  [  900/ 3414]
loss: 0.015311  [ 1000/ 3414]
loss: 0.021507  [ 1100/ 3414]
loss: 0.007744  [ 1200/ 3414]
loss: 0.032324  [ 1300/ 3414]
loss: 0.019016  [ 1400/ 3414]
loss: 0.027461  [ 1500/ 3414]
loss: 0.012025  [ 1600/ 3414]
loss: 0.011513  [ 1700/ 3414]
loss: 0.028887  [ 1800/ 3414]
loss: 0.019476  [ 1900/ 3414]
loss: 0.034442  [ 2000/ 3414]
loss: 0.029946  [ 2100/ 3414]
loss: 0.039217  [ 2200/ 3414]
loss: 0.034169  [ 2300/ 3414]
loss: 0.007445  [ 2400/ 3414]
loss: 0.161758  [ 2500/ 3414]
loss: 0.023156  [ 2600/ 3414]
loss: 0.012842  [ 2700/ 3414]
loss: 0.010735  [ 2800/ 3414]
loss: 0.036743  [ 2900/ 3414]
loss: 0.042017  [ 3000/ 3414]
loss: 0.031950  [ 3100/ 3414]
loss: 0.020796  [ 3200/ 3414]
loss: 0.011016  [ 3300/ 3414]
loss: 0.025838  [ 3400/ 3414]
Epoch 4
-------------------------------
loss: 0.047295  [    0/ 3414]
loss: 0.030449  [  100/ 3414]
loss: 0.008007  [  200/ 3414]
loss: 0.047008  [  300/ 3414]
loss: 0.015727  [  400/ 3414]
loss: 0.043754  [  500/ 3414]
loss: 0.031145  [  600/ 3414]
loss: 0.033691  [  700/ 3414]
loss: 0.051902  [  800/ 3414]
loss: 0.022973  [  900/ 3414]
loss: 0.014975  [ 1000/ 3414]
loss: 0.021966  [ 1100/ 3414]
loss: 0.006645  [ 1200/ 3414]
loss: 0.032213  [ 1300/ 3414]
loss: 0.018922  [ 1400/ 3414]
loss: 0.027215  [ 1500/ 3414]
loss: 0.011138  [ 1600/ 3414]
loss: 0.010565  [ 1700/ 3414]
loss: 0.028895  [ 1800/ 3414]
loss: 0.019596  [ 1900/ 3414]
loss: 0.033086  [ 2000/ 3414]
loss: 0.031174  [ 2100/ 3414]
loss: 0.038203  [ 2200/ 3414]
loss: 0.034853  [ 2300/ 3414]
loss: 0.007379  [ 2400/ 3414]
loss: 0.158976  [ 2500/ 3414]
loss: 0.022586  [ 2600/ 3414]
loss: 0.012633  [ 2700/ 3414]
loss: 0.011503  [ 2800/ 3414]
loss: 0.037934  [ 2900/ 3414]
loss: 0.043706  [ 3000/ 3414]
loss: 0.033156  [ 3100/ 3414]
loss: 0.020745  [ 3200/ 3414]
loss: 0.011164  [ 3300/ 3414]
loss: 0.025640  [ 3400/ 3414]
Epoch 5
-------------------------------
loss: 0.045017  [    0/ 3414]
loss: 0.030250  [  100/ 3414]
loss: 0.008018  [  200/ 3414]
loss: 0.047208  [  300/ 3414]
loss: 0.014943  [  400/ 3414]
loss: 0.043467  [  500/ 3414]
loss: 0.030172  [  600/ 3414]
loss: 0.035136  [  700/ 3414]
loss: 0.051722  [  800/ 3414]
loss: 0.022025  [  900/ 3414]
loss: 0.014936  [ 1000/ 3414]
loss: 0.022193  [ 1100/ 3414]
loss: 0.006659  [ 1200/ 3414]
loss: 0.032303  [ 1300/ 3414]
loss: 0.018970  [ 1400/ 3414]
loss: 0.027175  [ 1500/ 3414]
loss: 0.010943  [ 1600/ 3414]
loss: 0.010289  [ 1700/ 3414]
loss: 0.028892  [ 1800/ 3414]
loss: 0.019877  [ 1900/ 3414]
loss: 0.032660  [ 2000/ 3414]
loss: 0.031740  [ 2100/ 3414]
loss: 0.037764  [ 2200/ 3414]
loss: 0.034771  [ 2300/ 3414]
loss: 0.007615  [ 2400/ 3414]
loss: 0.158897  [ 2500/ 3414]
loss: 0.022852  [ 2600/ 3414]
loss: 0.012703  [ 2700/ 3414]
loss: 0.011663  [ 2800/ 3414]
loss: 0.038423  [ 2900/ 3414]
loss: 0.043988  [ 3000/ 3414]
loss: 0.033411  [ 3100/ 3414]
loss: 0.020661  [ 3200/ 3414]
loss: 0.011236  [ 3300/ 3414]
loss: 0.025476  [ 3400/ 3414]
Epoch 6
-------------------------------
loss: 0.044499  [    0/ 3414]
loss: 0.030269  [  100/ 3414]
loss: 0.008108  [  200/ 3414]
loss: 0.047463  [  300/ 3414]
loss: 0.014655  [  400/ 3414]
loss: 0.043296  [  500/ 3414]
loss: 0.029731  [  600/ 3414]
loss: 0.035830  [  700/ 3414]
loss: 0.051668  [  800/ 3414]
loss: 0.021786  [  900/ 3414]
loss: 0.014905  [ 1000/ 3414]
loss: 0.022313  [ 1100/ 3414]
loss: 0.007475  [ 1200/ 3414]
loss: 0.032258  [ 1300/ 3414]
loss: 0.018966  [ 1400/ 3414]
loss: 0.027115  [ 1500/ 3414]
loss: 0.010910  [ 1600/ 3414]
loss: 0.010234  [ 1700/ 3414]
loss: 0.028892  [ 1800/ 3414]
loss: 0.020022  [ 1900/ 3414]
loss: 0.032470  [ 2000/ 3414]
loss: 0.031869  [ 2100/ 3414]
loss: 0.037806  [ 2200/ 3414]
loss: 0.034596  [ 2300/ 3414]
loss: 0.007588  [ 2400/ 3414]
loss: 0.159070  [ 2500/ 3414]
loss: 0.022678  [ 2600/ 3414]
loss: 0.012684  [ 2700/ 3414]
loss: 0.011772  [ 2800/ 3414]
loss: 0.038847  [ 2900/ 3414]
loss: 0.044159  [ 3000/ 3414]
loss: 0.033615  [ 3100/ 3414]
loss: 0.020639  [ 3200/ 3414]
loss: 0.011225  [ 3300/ 3414]
loss: 0.025338  [ 3400/ 3414]
Epoch 7
-------------------------------
loss: 0.043845  [    0/ 3414]
loss: 0.030268  [  100/ 3414]
loss: 0.008107  [  200/ 3414]
loss: 0.047566  [  300/ 3414]
loss: 0.014499  [  400/ 3414]
loss: 0.043265  [  500/ 3414]
loss: 0.029585  [  600/ 3414]
loss: 0.036280  [  700/ 3414]
loss: 0.051578  [  800/ 3414]
loss: 0.021685  [  900/ 3414]
loss: 0.014876  [ 1000/ 3414]
loss: 0.022350  [ 1100/ 3414]
loss: 0.006764  [ 1200/ 3414]
loss: 0.032277  [ 1300/ 3414]
loss: 0.018951  [ 1400/ 3414]
loss: 0.027090  [ 1500/ 3414]
loss: 0.010896  [ 1600/ 3414]
loss: 0.010267  [ 1700/ 3414]
loss: 0.028858  [ 1800/ 3414]
loss: 0.020045  [ 1900/ 3414]
loss: 0.032388  [ 2000/ 3414]
loss: 0.031948  [ 2100/ 3414]
loss: 0.037960  [ 2200/ 3414]
loss: 0.034408  [ 2300/ 3414]
loss: 0.007524  [ 2400/ 3414]
loss: 0.159281  [ 2500/ 3414]
loss: 0.022612  [ 2600/ 3414]
loss: 0.012704  [ 2700/ 3414]
loss: 0.011804  [ 2800/ 3414]
loss: 0.039134  [ 2900/ 3414]
loss: 0.044164  [ 3000/ 3414]
loss: 0.033719  [ 3100/ 3414]
loss: 0.020643  [ 3200/ 3414]
loss: 0.011212  [ 3300/ 3414]
loss: 0.025246  [ 3400/ 3414]
Epoch 8
-------------------------------
loss: 0.043686  [    0/ 3414]
loss: 0.030266  [  100/ 3414]
loss: 0.008117  [  200/ 3414]
loss: 0.047648  [  300/ 3414]
loss: 0.014392  [  400/ 3414]
loss: 0.043207  [  500/ 3414]
loss: 0.029483  [  600/ 3414]
loss: 0.036419  [  700/ 3414]
loss: 0.051563  [  800/ 3414]
loss: 0.021620  [  900/ 3414]
loss: 0.014846  [ 1000/ 3414]
loss: 0.022364  [ 1100/ 3414]
loss: 0.006804  [ 1200/ 3414]
loss: 0.032244  [ 1300/ 3414]
loss: 0.018936  [ 1400/ 3414]
loss: 0.027039  [ 1500/ 3414]
loss: 0.010845  [ 1600/ 3414]
loss: 0.010305  [ 1700/ 3414]
loss: 0.028828  [ 1800/ 3414]
loss: 0.020073  [ 1900/ 3414]
loss: 0.032354  [ 2000/ 3414]
loss: 0.031902  [ 2100/ 3414]
loss: 0.037679  [ 2200/ 3414]
loss: 0.034521  [ 2300/ 3414]
loss: 0.007552  [ 2400/ 3414]
loss: 0.159648  [ 2500/ 3414]
loss: 0.022705  [ 2600/ 3414]
loss: 0.012668  [ 2700/ 3414]
loss: 0.011793  [ 2800/ 3414]
loss: 0.039197  [ 2900/ 3414]
loss: 0.044111  [ 3000/ 3414]
loss: 0.033681  [ 3100/ 3414]
loss: 0.020605  [ 3200/ 3414]
loss: 0.011185  [ 3300/ 3414]
loss: 0.025224  [ 3400/ 3414]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3414
First Spike after testing: [0.1308418  0.00216633]
[1 1 0 ... 0 0 2]
[1 2 0 ... 2 2 0]
Cluster 0 Occurrences: 1136; KMEANS: 802
Cluster 1 Occurrences: 1099; KMEANS: 1124
Cluster 2 Occurrences: 1179; KMEANS: 1488
Centroids: [[-2.4602327, -7.792514], [-2.3409793, -8.132132], [-4.5967402, -13.404099]]
Centroids: [[-5.79443, -16.08474], [-1.0479211, -4.879452], [-3.3347588, -10.220751]]
Contingency Matrix: 
[[ 89 542 505]
 [ 98 504 497]
 [615  78 486]]
[[-1, 542, 505], [-1, 504, 497], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, 497], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 0: 1, 1: 2}
New Contingency Matrix: 
[[542 505  89]
 [504 497  98]
 [ 78 486 615]]
New Clustered Label Sequence: [1, 2, 0]
Diagonal_Elements: [542, 497, 615], Sum: 1654
All_Elements: [542, 505, 89, 504, 497, 98, 78, 486, 615], Sum: 3414
Accuracy: 0.4844756883421207
Done!
