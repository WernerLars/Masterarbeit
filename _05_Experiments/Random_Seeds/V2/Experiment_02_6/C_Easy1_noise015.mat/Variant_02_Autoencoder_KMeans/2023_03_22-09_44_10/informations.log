Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_10
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000020628D3BEB8>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x0000020628D95080>
Epoch 1
-------------------------------
loss: 0.164306  [    0/ 3477]
loss: 0.337314  [  100/ 3477]
loss: 0.099092  [  200/ 3477]
loss: 0.098940  [  300/ 3477]
loss: 0.032643  [  400/ 3477]
loss: 0.090660  [  500/ 3477]
loss: 0.025756  [  600/ 3477]
loss: 0.024244  [  700/ 3477]
loss: 0.022954  [  800/ 3477]
loss: 0.020812  [  900/ 3477]
loss: 0.023397  [ 1000/ 3477]
loss: 0.003532  [ 1100/ 3477]
loss: 0.023791  [ 1200/ 3477]
loss: 0.017187  [ 1300/ 3477]
loss: 0.016229  [ 1400/ 3477]
loss: 0.023917  [ 1500/ 3477]
loss: 0.012525  [ 1600/ 3477]
loss: 0.013094  [ 1700/ 3477]
loss: 0.011723  [ 1800/ 3477]
loss: 0.015562  [ 1900/ 3477]
loss: 0.022172  [ 2000/ 3477]
loss: 0.027102  [ 2100/ 3477]
loss: 0.021196  [ 2200/ 3477]
loss: 0.012340  [ 2300/ 3477]
loss: 0.011614  [ 2400/ 3477]
loss: 0.097713  [ 2500/ 3477]
loss: 0.069364  [ 2600/ 3477]
loss: 0.013352  [ 2700/ 3477]
loss: 0.264156  [ 2800/ 3477]
loss: 0.019098  [ 2900/ 3477]
loss: 0.018000  [ 3000/ 3477]
loss: 0.015830  [ 3100/ 3477]
loss: 0.103421  [ 3200/ 3477]
loss: 0.011736  [ 3300/ 3477]
loss: 0.006225  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.011897  [    0/ 3477]
loss: 0.031132  [  100/ 3477]
loss: 0.026417  [  200/ 3477]
loss: 0.031029  [  300/ 3477]
loss: 0.017446  [  400/ 3477]
loss: 0.005981  [  500/ 3477]
loss: 0.017607  [  600/ 3477]
loss: 0.023186  [  700/ 3477]
loss: 0.016488  [  800/ 3477]
loss: 0.021227  [  900/ 3477]
loss: 0.010507  [ 1000/ 3477]
loss: 0.003506  [ 1100/ 3477]
loss: 0.024173  [ 1200/ 3477]
loss: 0.017448  [ 1300/ 3477]
loss: 0.016634  [ 1400/ 3477]
loss: 0.023610  [ 1500/ 3477]
loss: 0.011383  [ 1600/ 3477]
loss: 0.013160  [ 1700/ 3477]
loss: 0.005932  [ 1800/ 3477]
loss: 0.015351  [ 1900/ 3477]
loss: 0.021681  [ 2000/ 3477]
loss: 0.023364  [ 2100/ 3477]
loss: 0.020836  [ 2200/ 3477]
loss: 0.012436  [ 2300/ 3477]
loss: 0.012375  [ 2400/ 3477]
loss: 0.091872  [ 2500/ 3477]
loss: 0.069251  [ 2600/ 3477]
loss: 0.010433  [ 2700/ 3477]
loss: 0.090878  [ 2800/ 3477]
loss: 0.018847  [ 2900/ 3477]
loss: 0.017136  [ 3000/ 3477]
loss: 0.015593  [ 3100/ 3477]
loss: 0.103884  [ 3200/ 3477]
loss: 0.008134  [ 3300/ 3477]
loss: 0.005969  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.003271  [    0/ 3477]
loss: 0.029725  [  100/ 3477]
loss: 0.016349  [  200/ 3477]
loss: 0.026441  [  300/ 3477]
loss: 0.016551  [  400/ 3477]
loss: 0.007959  [  500/ 3477]
loss: 0.010303  [  600/ 3477]
loss: 0.021510  [  700/ 3477]
loss: 0.021123  [  800/ 3477]
loss: 0.018937  [  900/ 3477]
loss: 0.005364  [ 1000/ 3477]
loss: 0.003333  [ 1100/ 3477]
loss: 0.022784  [ 1200/ 3477]
loss: 0.017205  [ 1300/ 3477]
loss: 0.016208  [ 1400/ 3477]
loss: 0.017790  [ 1500/ 3477]
loss: 0.011158  [ 1600/ 3477]
loss: 0.010173  [ 1700/ 3477]
loss: 0.006639  [ 1800/ 3477]
loss: 0.013515  [ 1900/ 3477]
loss: 0.020115  [ 2000/ 3477]
loss: 0.008913  [ 2100/ 3477]
loss: 0.021477  [ 2200/ 3477]
loss: 0.012392  [ 2300/ 3477]
loss: 0.010705  [ 2400/ 3477]
loss: 0.083731  [ 2500/ 3477]
loss: 0.067490  [ 2600/ 3477]
loss: 0.006958  [ 2700/ 3477]
loss: 0.062289  [ 2800/ 3477]
loss: 0.019287  [ 2900/ 3477]
loss: 0.014956  [ 3000/ 3477]
loss: 0.015511  [ 3100/ 3477]
loss: 0.104678  [ 3200/ 3477]
loss: 0.006123  [ 3300/ 3477]
loss: 0.005680  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.006026  [    0/ 3477]
loss: 0.026871  [  100/ 3477]
loss: 0.012504  [  200/ 3477]
loss: 0.026525  [  300/ 3477]
loss: 0.016612  [  400/ 3477]
loss: 0.010911  [  500/ 3477]
loss: 0.007443  [  600/ 3477]
loss: 0.020056  [  700/ 3477]
loss: 0.021928  [  800/ 3477]
loss: 0.016712  [  900/ 3477]
loss: 0.008820  [ 1000/ 3477]
loss: 0.002955  [ 1100/ 3477]
loss: 0.019656  [ 1200/ 3477]
loss: 0.016507  [ 1300/ 3477]
loss: 0.015505  [ 1400/ 3477]
loss: 0.013597  [ 1500/ 3477]
loss: 0.011725  [ 1600/ 3477]
loss: 0.008776  [ 1700/ 3477]
loss: 0.009184  [ 1800/ 3477]
loss: 0.010976  [ 1900/ 3477]
loss: 0.019112  [ 2000/ 3477]
loss: 0.005107  [ 2100/ 3477]
loss: 0.021782  [ 2200/ 3477]
loss: 0.012705  [ 2300/ 3477]
loss: 0.009059  [ 2400/ 3477]
loss: 0.076674  [ 2500/ 3477]
loss: 0.069810  [ 2600/ 3477]
loss: 0.005778  [ 2700/ 3477]
loss: 0.049785  [ 2800/ 3477]
loss: 0.019852  [ 2900/ 3477]
loss: 0.014563  [ 3000/ 3477]
loss: 0.015372  [ 3100/ 3477]
loss: 0.101274  [ 3200/ 3477]
loss: 0.006458  [ 3300/ 3477]
loss: 0.005950  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.008121  [    0/ 3477]
loss: 0.024052  [  100/ 3477]
loss: 0.012871  [  200/ 3477]
loss: 0.028287  [  300/ 3477]
loss: 0.015794  [  400/ 3477]
loss: 0.010759  [  500/ 3477]
loss: 0.007220  [  600/ 3477]
loss: 0.019587  [  700/ 3477]
loss: 0.021452  [  800/ 3477]
loss: 0.015236  [  900/ 3477]
loss: 0.010611  [ 1000/ 3477]
loss: 0.002458  [ 1100/ 3477]
loss: 0.018773  [ 1200/ 3477]
loss: 0.015169  [ 1300/ 3477]
loss: 0.014815  [ 1400/ 3477]
loss: 0.012519  [ 1500/ 3477]
loss: 0.012316  [ 1600/ 3477]
loss: 0.008791  [ 1700/ 3477]
loss: 0.010186  [ 1800/ 3477]
loss: 0.009645  [ 1900/ 3477]
loss: 0.018879  [ 2000/ 3477]
loss: 0.004939  [ 2100/ 3477]
loss: 0.021538  [ 2200/ 3477]
loss: 0.013478  [ 2300/ 3477]
loss: 0.008120  [ 2400/ 3477]
loss: 0.073178  [ 2500/ 3477]
loss: 0.071812  [ 2600/ 3477]
loss: 0.005403  [ 2700/ 3477]
loss: 0.040432  [ 2800/ 3477]
loss: 0.020099  [ 2900/ 3477]
loss: 0.014677  [ 3000/ 3477]
loss: 0.015123  [ 3100/ 3477]
loss: 0.095619  [ 3200/ 3477]
loss: 0.006624  [ 3300/ 3477]
loss: 0.006362  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.008928  [    0/ 3477]
loss: 0.021884  [  100/ 3477]
loss: 0.013604  [  200/ 3477]
loss: 0.030387  [  300/ 3477]
loss: 0.015644  [  400/ 3477]
loss: 0.010058  [  500/ 3477]
loss: 0.007312  [  600/ 3477]
loss: 0.019482  [  700/ 3477]
loss: 0.020999  [  800/ 3477]
loss: 0.015030  [  900/ 3477]
loss: 0.011382  [ 1000/ 3477]
loss: 0.002309  [ 1100/ 3477]
loss: 0.018280  [ 1200/ 3477]
loss: 0.013618  [ 1300/ 3477]
loss: 0.014367  [ 1400/ 3477]
loss: 0.013072  [ 1500/ 3477]
loss: 0.012820  [ 1600/ 3477]
loss: 0.008565  [ 1700/ 3477]
loss: 0.010694  [ 1800/ 3477]
loss: 0.008951  [ 1900/ 3477]
loss: 0.018700  [ 2000/ 3477]
loss: 0.004967  [ 2100/ 3477]
loss: 0.022246  [ 2200/ 3477]
loss: 0.012923  [ 2300/ 3477]
loss: 0.007968  [ 2400/ 3477]
loss: 0.073835  [ 2500/ 3477]
loss: 0.073452  [ 2600/ 3477]
loss: 0.005217  [ 2700/ 3477]
loss: 0.033446  [ 2800/ 3477]
loss: 0.020294  [ 2900/ 3477]
loss: 0.014761  [ 3000/ 3477]
loss: 0.014986  [ 3100/ 3477]
loss: 0.090586  [ 3200/ 3477]
loss: 0.007457  [ 3300/ 3477]
loss: 0.006650  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.009471  [    0/ 3477]
loss: 0.020611  [  100/ 3477]
loss: 0.014128  [  200/ 3477]
loss: 0.028936  [  300/ 3477]
loss: 0.015580  [  400/ 3477]
loss: 0.008721  [  500/ 3477]
loss: 0.007737  [  600/ 3477]
loss: 0.019404  [  700/ 3477]
loss: 0.020773  [  800/ 3477]
loss: 0.015261  [  900/ 3477]
loss: 0.012266  [ 1000/ 3477]
loss: 0.001956  [ 1100/ 3477]
loss: 0.017962  [ 1200/ 3477]
loss: 0.012372  [ 1300/ 3477]
loss: 0.014387  [ 1400/ 3477]
loss: 0.013933  [ 1500/ 3477]
loss: 0.011885  [ 1600/ 3477]
loss: 0.008680  [ 1700/ 3477]
loss: 0.010894  [ 1800/ 3477]
loss: 0.008411  [ 1900/ 3477]
loss: 0.018563  [ 2000/ 3477]
loss: 0.005066  [ 2100/ 3477]
loss: 0.022529  [ 2200/ 3477]
loss: 0.012899  [ 2300/ 3477]
loss: 0.007843  [ 2400/ 3477]
loss: 0.075428  [ 2500/ 3477]
loss: 0.072382  [ 2600/ 3477]
loss: 0.004894  [ 2700/ 3477]
loss: 0.028643  [ 2800/ 3477]
loss: 0.020477  [ 2900/ 3477]
loss: 0.014787  [ 3000/ 3477]
loss: 0.014818  [ 3100/ 3477]
loss: 0.086960  [ 3200/ 3477]
loss: 0.007013  [ 3300/ 3477]
loss: 0.006685  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.009552  [    0/ 3477]
loss: 0.019658  [  100/ 3477]
loss: 0.015563  [  200/ 3477]
loss: 0.028901  [  300/ 3477]
loss: 0.016076  [  400/ 3477]
loss: 0.008836  [  500/ 3477]
loss: 0.008233  [  600/ 3477]
loss: 0.019295  [  700/ 3477]
loss: 0.020898  [  800/ 3477]
loss: 0.015231  [  900/ 3477]
loss: 0.013111  [ 1000/ 3477]
loss: 0.001836  [ 1100/ 3477]
loss: 0.018106  [ 1200/ 3477]
loss: 0.011341  [ 1300/ 3477]
loss: 0.014510  [ 1400/ 3477]
loss: 0.013732  [ 1500/ 3477]
loss: 0.011546  [ 1600/ 3477]
loss: 0.008900  [ 1700/ 3477]
loss: 0.011026  [ 1800/ 3477]
loss: 0.008252  [ 1900/ 3477]
loss: 0.018538  [ 2000/ 3477]
loss: 0.005341  [ 2100/ 3477]
loss: 0.022555  [ 2200/ 3477]
loss: 0.012820  [ 2300/ 3477]
loss: 0.007814  [ 2400/ 3477]
loss: 0.078139  [ 2500/ 3477]
loss: 0.073360  [ 2600/ 3477]
loss: 0.004779  [ 2700/ 3477]
loss: 0.028036  [ 2800/ 3477]
loss: 0.020510  [ 2900/ 3477]
loss: 0.014745  [ 3000/ 3477]
loss: 0.014707  [ 3100/ 3477]
loss: 0.083554  [ 3200/ 3477]
loss: 0.007023  [ 3300/ 3477]
loss: 0.006752  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [0.10377514 2.4839554 ]
[1 0 2 ... 1 1 1]
[1 0 2 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1124
Cluster 1 Occurrences: 1188; KMEANS: 1177
Cluster 2 Occurrences: 1157; KMEANS: 1176
Centroids: [[0.9991081, -1.4732084], [0.13233472, 1.9989406], [-0.88026094, -2.7494662]]
Centroids: [[1.0109712, -1.458109], [0.14309238, 2.0254784], [-0.88011, -2.7373607]]
Contingency Matrix: 
[[1119    1   12]
 [   5 1175    8]
 [   0    1 1156]]
[[1119, -1, 12], [-1, -1, -1], [0, -1, 1156]]
[[1119, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 2, 0: 0}
New Contingency Matrix: 
[[1119    1   12]
 [   5 1175    8]
 [   0    1 1156]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1119, 1175, 1156], Sum: 3450
All_Elements: [1119, 1, 12, 5, 1175, 8, 0, 1, 1156], Sum: 3477
Accuracy: 0.9922346850733391
Done!
