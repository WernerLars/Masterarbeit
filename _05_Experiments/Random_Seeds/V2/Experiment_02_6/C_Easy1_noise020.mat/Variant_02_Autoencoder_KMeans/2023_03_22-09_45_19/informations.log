Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_19
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000206284822B0>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x0000020628D95128>
Epoch 1
-------------------------------
loss: 0.132293  [    0/ 3474]
loss: 0.106074  [  100/ 3474]
loss: 0.109527  [  200/ 3474]
loss: 0.116706  [  300/ 3474]
loss: 0.270525  [  400/ 3474]
loss: 0.009535  [  500/ 3474]
loss: 0.051619  [  600/ 3474]
loss: 0.022595  [  700/ 3474]
loss: 0.053050  [  800/ 3474]
loss: 0.219357  [  900/ 3474]
loss: 0.043123  [ 1000/ 3474]
loss: 0.210745  [ 1100/ 3474]
loss: 0.057537  [ 1200/ 3474]
loss: 0.097266  [ 1300/ 3474]
loss: 0.043339  [ 1400/ 3474]
loss: 0.090713  [ 1500/ 3474]
loss: 0.016022  [ 1600/ 3474]
loss: 0.122905  [ 1700/ 3474]
loss: 0.024609  [ 1800/ 3474]
loss: 0.026815  [ 1900/ 3474]
loss: 0.018947  [ 2000/ 3474]
loss: 0.017587  [ 2100/ 3474]
loss: 0.026143  [ 2200/ 3474]
loss: 0.085310  [ 2300/ 3474]
loss: 0.034169  [ 2400/ 3474]
loss: 0.020739  [ 2500/ 3474]
loss: 0.025084  [ 2600/ 3474]
loss: 0.101585  [ 2700/ 3474]
loss: 0.030430  [ 2800/ 3474]
loss: 0.017755  [ 2900/ 3474]
loss: 0.191575  [ 3000/ 3474]
loss: 0.013706  [ 3100/ 3474]
loss: 0.032931  [ 3200/ 3474]
loss: 0.035046  [ 3300/ 3474]
loss: 0.049930  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.056121  [    0/ 3474]
loss: 0.010298  [  100/ 3474]
loss: 0.022823  [  200/ 3474]
loss: 0.058825  [  300/ 3474]
loss: 0.064825  [  400/ 3474]
loss: 0.004568  [  500/ 3474]
loss: 0.032195  [  600/ 3474]
loss: 0.022453  [  700/ 3474]
loss: 0.017687  [  800/ 3474]
loss: 0.197940  [  900/ 3474]
loss: 0.031689  [ 1000/ 3474]
loss: 0.060532  [ 1100/ 3474]
loss: 0.036421  [ 1200/ 3474]
loss: 0.091334  [ 1300/ 3474]
loss: 0.040257  [ 1400/ 3474]
loss: 0.067463  [ 1500/ 3474]
loss: 0.016860  [ 1600/ 3474]
loss: 0.067695  [ 1700/ 3474]
loss: 0.019351  [ 1800/ 3474]
loss: 0.023911  [ 1900/ 3474]
loss: 0.009823  [ 2000/ 3474]
loss: 0.015715  [ 2100/ 3474]
loss: 0.027207  [ 2200/ 3474]
loss: 0.079877  [ 2300/ 3474]
loss: 0.021608  [ 2400/ 3474]
loss: 0.012040  [ 2500/ 3474]
loss: 0.018286  [ 2600/ 3474]
loss: 0.065022  [ 2700/ 3474]
loss: 0.029039  [ 2800/ 3474]
loss: 0.015467  [ 2900/ 3474]
loss: 0.101301  [ 3000/ 3474]
loss: 0.011380  [ 3100/ 3474]
loss: 0.017540  [ 3200/ 3474]
loss: 0.040978  [ 3300/ 3474]
loss: 0.045019  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.033338  [    0/ 3474]
loss: 0.009873  [  100/ 3474]
loss: 0.021411  [  200/ 3474]
loss: 0.059290  [  300/ 3474]
loss: 0.058119  [  400/ 3474]
loss: 0.004839  [  500/ 3474]
loss: 0.023366  [  600/ 3474]
loss: 0.015577  [  700/ 3474]
loss: 0.018672  [  800/ 3474]
loss: 0.209135  [  900/ 3474]
loss: 0.015217  [ 1000/ 3474]
loss: 0.072003  [ 1100/ 3474]
loss: 0.022329  [ 1200/ 3474]
loss: 0.092977  [ 1300/ 3474]
loss: 0.039536  [ 1400/ 3474]
loss: 0.068524  [ 1500/ 3474]
loss: 0.018359  [ 1600/ 3474]
loss: 0.035528  [ 1700/ 3474]
loss: 0.021478  [ 1800/ 3474]
loss: 0.024728  [ 1900/ 3474]
loss: 0.009593  [ 2000/ 3474]
loss: 0.015079  [ 2100/ 3474]
loss: 0.026149  [ 2200/ 3474]
loss: 0.078601  [ 2300/ 3474]
loss: 0.009713  [ 2400/ 3474]
loss: 0.010774  [ 2500/ 3474]
loss: 0.017314  [ 2600/ 3474]
loss: 0.045070  [ 2700/ 3474]
loss: 0.027814  [ 2800/ 3474]
loss: 0.015836  [ 2900/ 3474]
loss: 0.137894  [ 3000/ 3474]
loss: 0.011245  [ 3100/ 3474]
loss: 0.014227  [ 3200/ 3474]
loss: 0.044008  [ 3300/ 3474]
loss: 0.043382  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.028724  [    0/ 3474]
loss: 0.009674  [  100/ 3474]
loss: 0.020373  [  200/ 3474]
loss: 0.059926  [  300/ 3474]
loss: 0.064257  [  400/ 3474]
loss: 0.004993  [  500/ 3474]
loss: 0.022888  [  600/ 3474]
loss: 0.011808  [  700/ 3474]
loss: 0.019540  [  800/ 3474]
loss: 0.204239  [  900/ 3474]
loss: 0.009753  [ 1000/ 3474]
loss: 0.079519  [ 1100/ 3474]
loss: 0.022720  [ 1200/ 3474]
loss: 0.091544  [ 1300/ 3474]
loss: 0.037163  [ 1400/ 3474]
loss: 0.075881  [ 1500/ 3474]
loss: 0.017447  [ 1600/ 3474]
loss: 0.036609  [ 1700/ 3474]
loss: 0.021791  [ 1800/ 3474]
loss: 0.025588  [ 1900/ 3474]
loss: 0.009567  [ 2000/ 3474]
loss: 0.014713  [ 2100/ 3474]
loss: 0.026105  [ 2200/ 3474]
loss: 0.077718  [ 2300/ 3474]
loss: 0.006565  [ 2400/ 3474]
loss: 0.011492  [ 2500/ 3474]
loss: 0.018655  [ 2600/ 3474]
loss: 0.039255  [ 2700/ 3474]
loss: 0.026173  [ 2800/ 3474]
loss: 0.016195  [ 2900/ 3474]
loss: 0.171932  [ 3000/ 3474]
loss: 0.011733  [ 3100/ 3474]
loss: 0.013367  [ 3200/ 3474]
loss: 0.043763  [ 3300/ 3474]
loss: 0.041701  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.030659  [    0/ 3474]
loss: 0.009229  [  100/ 3474]
loss: 0.021187  [  200/ 3474]
loss: 0.059786  [  300/ 3474]
loss: 0.073937  [  400/ 3474]
loss: 0.005027  [  500/ 3474]
loss: 0.022336  [  600/ 3474]
loss: 0.010823  [  700/ 3474]
loss: 0.019380  [  800/ 3474]
loss: 0.198972  [  900/ 3474]
loss: 0.007906  [ 1000/ 3474]
loss: 0.085334  [ 1100/ 3474]
loss: 0.023666  [ 1200/ 3474]
loss: 0.082915  [ 1300/ 3474]
loss: 0.035467  [ 1400/ 3474]
loss: 0.078054  [ 1500/ 3474]
loss: 0.017241  [ 1600/ 3474]
loss: 0.035055  [ 1700/ 3474]
loss: 0.021440  [ 1800/ 3474]
loss: 0.025375  [ 1900/ 3474]
loss: 0.009293  [ 2000/ 3474]
loss: 0.014519  [ 2100/ 3474]
loss: 0.026442  [ 2200/ 3474]
loss: 0.076781  [ 2300/ 3474]
loss: 0.005530  [ 2400/ 3474]
loss: 0.011475  [ 2500/ 3474]
loss: 0.019611  [ 2600/ 3474]
loss: 0.042551  [ 2700/ 3474]
loss: 0.025677  [ 2800/ 3474]
loss: 0.016237  [ 2900/ 3474]
loss: 0.190251  [ 3000/ 3474]
loss: 0.011902  [ 3100/ 3474]
loss: 0.013613  [ 3200/ 3474]
loss: 0.043966  [ 3300/ 3474]
loss: 0.037438  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.033287  [    0/ 3474]
loss: 0.008909  [  100/ 3474]
loss: 0.021036  [  200/ 3474]
loss: 0.060495  [  300/ 3474]
loss: 0.079418  [  400/ 3474]
loss: 0.005242  [  500/ 3474]
loss: 0.022656  [  600/ 3474]
loss: 0.010706  [  700/ 3474]
loss: 0.019138  [  800/ 3474]
loss: 0.195460  [  900/ 3474]
loss: 0.007054  [ 1000/ 3474]
loss: 0.090840  [ 1100/ 3474]
loss: 0.024252  [ 1200/ 3474]
loss: 0.072529  [ 1300/ 3474]
loss: 0.032183  [ 1400/ 3474]
loss: 0.078543  [ 1500/ 3474]
loss: 0.016320  [ 1600/ 3474]
loss: 0.037104  [ 1700/ 3474]
loss: 0.021195  [ 1800/ 3474]
loss: 0.025039  [ 1900/ 3474]
loss: 0.009366  [ 2000/ 3474]
loss: 0.014407  [ 2100/ 3474]
loss: 0.026557  [ 2200/ 3474]
loss: 0.075979  [ 2300/ 3474]
loss: 0.005030  [ 2400/ 3474]
loss: 0.011733  [ 2500/ 3474]
loss: 0.020481  [ 2600/ 3474]
loss: 0.045721  [ 2700/ 3474]
loss: 0.024828  [ 2800/ 3474]
loss: 0.016160  [ 2900/ 3474]
loss: 0.199956  [ 3000/ 3474]
loss: 0.011434  [ 3100/ 3474]
loss: 0.012893  [ 3200/ 3474]
loss: 0.044353  [ 3300/ 3474]
loss: 0.025560  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.035231  [    0/ 3474]
loss: 0.008723  [  100/ 3474]
loss: 0.021299  [  200/ 3474]
loss: 0.059878  [  300/ 3474]
loss: 0.085394  [  400/ 3474]
loss: 0.005358  [  500/ 3474]
loss: 0.022162  [  600/ 3474]
loss: 0.010893  [  700/ 3474]
loss: 0.019151  [  800/ 3474]
loss: 0.193356  [  900/ 3474]
loss: 0.006135  [ 1000/ 3474]
loss: 0.100751  [ 1100/ 3474]
loss: 0.024248  [ 1200/ 3474]
loss: 0.059992  [ 1300/ 3474]
loss: 0.025904  [ 1400/ 3474]
loss: 0.078087  [ 1500/ 3474]
loss: 0.016020  [ 1600/ 3474]
loss: 0.038299  [ 1700/ 3474]
loss: 0.020814  [ 1800/ 3474]
loss: 0.024598  [ 1900/ 3474]
loss: 0.009408  [ 2000/ 3474]
loss: 0.014018  [ 2100/ 3474]
loss: 0.026913  [ 2200/ 3474]
loss: 0.075747  [ 2300/ 3474]
loss: 0.004661  [ 2400/ 3474]
loss: 0.011667  [ 2500/ 3474]
loss: 0.021048  [ 2600/ 3474]
loss: 0.048000  [ 2700/ 3474]
loss: 0.024301  [ 2800/ 3474]
loss: 0.016050  [ 2900/ 3474]
loss: 0.207422  [ 3000/ 3474]
loss: 0.011471  [ 3100/ 3474]
loss: 0.013437  [ 3200/ 3474]
loss: 0.044376  [ 3300/ 3474]
loss: 0.019729  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.035585  [    0/ 3474]
loss: 0.008380  [  100/ 3474]
loss: 0.021827  [  200/ 3474]
loss: 0.060011  [  300/ 3474]
loss: 0.086490  [  400/ 3474]
loss: 0.005371  [  500/ 3474]
loss: 0.021919  [  600/ 3474]
loss: 0.010928  [  700/ 3474]
loss: 0.019932  [  800/ 3474]
loss: 0.191166  [  900/ 3474]
loss: 0.005240  [ 1000/ 3474]
loss: 0.136924  [ 1100/ 3474]
loss: 0.024531  [ 1200/ 3474]
loss: 0.047282  [ 1300/ 3474]
loss: 0.021017  [ 1400/ 3474]
loss: 0.077862  [ 1500/ 3474]
loss: 0.015060  [ 1600/ 3474]
loss: 0.035980  [ 1700/ 3474]
loss: 0.021594  [ 1800/ 3474]
loss: 0.024590  [ 1900/ 3474]
loss: 0.009015  [ 2000/ 3474]
loss: 0.013930  [ 2100/ 3474]
loss: 0.026994  [ 2200/ 3474]
loss: 0.075575  [ 2300/ 3474]
loss: 0.004528  [ 2400/ 3474]
loss: 0.011572  [ 2500/ 3474]
loss: 0.021171  [ 2600/ 3474]
loss: 0.048480  [ 2700/ 3474]
loss: 0.022833  [ 2800/ 3474]
loss: 0.015910  [ 2900/ 3474]
loss: 0.212663  [ 3000/ 3474]
loss: 0.010715  [ 3100/ 3474]
loss: 0.013697  [ 3200/ 3474]
loss: 0.044970  [ 3300/ 3474]
loss: 0.015995  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [ 0.46127594 -0.57098746]
[0 1 1 ... 1 1 2]
[2 1 1 ... 1 1 0]
Cluster 0 Occurrences: 1198; KMEANS: 1170
Cluster 1 Occurrences: 1128; KMEANS: 1119
Cluster 2 Occurrences: 1148; KMEANS: 1185
Centroids: [[0.6420977, -1.2300047], [-0.23696502, 2.2999697], [-2.030268, -1.8393927]]
Centroids: [[-2.0234618, -1.8456489], [-0.23445742, 2.3215692], [0.67594707, -1.206101]]
Contingency Matrix: 
[[  21    0 1177]
 [   4 1118    6]
 [1145    1    2]]
[[-1, -1, -1], [4, 1118, -1], [1145, 1, -1]]
[[-1, -1, -1], [-1, 1118, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 0, 1: 1}
New Contingency Matrix: 
[[1177    0   21]
 [   6 1118    4]
 [   2    1 1145]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1177, 1118, 1145], Sum: 3440
All_Elements: [1177, 0, 21, 6, 1118, 4, 2, 1, 1145], Sum: 3474
Accuracy: 0.9902130109383995
Done!
