Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_06_30
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000206A019E8D0>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x00000206899FF240>
Epoch 1
-------------------------------
loss: 0.310934  [    0/ 3493]
loss: 0.186197  [  100/ 3493]
loss: 0.084341  [  200/ 3493]
loss: 0.138022  [  300/ 3493]
loss: 0.054482  [  400/ 3493]
loss: 0.034128  [  500/ 3493]
loss: 0.104072  [  600/ 3493]
loss: 0.208457  [  700/ 3493]
loss: 0.075325  [  800/ 3493]
loss: 0.025143  [  900/ 3493]
loss: 0.053238  [ 1000/ 3493]
loss: 0.091108  [ 1100/ 3493]
loss: 0.034782  [ 1200/ 3493]
loss: 0.013153  [ 1300/ 3493]
loss: 0.034238  [ 1400/ 3493]
loss: 0.038795  [ 1500/ 3493]
loss: 0.032919  [ 1600/ 3493]
loss: 0.038391  [ 1700/ 3493]
loss: 0.072028  [ 1800/ 3493]
loss: 0.031625  [ 1900/ 3493]
loss: 0.007189  [ 2000/ 3493]
loss: 0.049276  [ 2100/ 3493]
loss: 0.014289  [ 2200/ 3493]
loss: 0.027696  [ 2300/ 3493]
loss: 0.014478  [ 2400/ 3493]
loss: 0.050883  [ 2500/ 3493]
loss: 0.029253  [ 2600/ 3493]
loss: 0.043740  [ 2700/ 3493]
loss: 0.027091  [ 2800/ 3493]
loss: 0.022983  [ 2900/ 3493]
loss: 0.041359  [ 3000/ 3493]
loss: 0.021652  [ 3100/ 3493]
loss: 0.029139  [ 3200/ 3493]
loss: 0.017444  [ 3300/ 3493]
loss: 0.010366  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.082483  [    0/ 3493]
loss: 0.013860  [  100/ 3493]
loss: 0.039448  [  200/ 3493]
loss: 0.041045  [  300/ 3493]
loss: 0.061158  [  400/ 3493]
loss: 0.024217  [  500/ 3493]
loss: 0.056985  [  600/ 3493]
loss: 0.214463  [  700/ 3493]
loss: 0.058486  [  800/ 3493]
loss: 0.022276  [  900/ 3493]
loss: 0.041478  [ 1000/ 3493]
loss: 0.069656  [ 1100/ 3493]
loss: 0.026444  [ 1200/ 3493]
loss: 0.013027  [ 1300/ 3493]
loss: 0.033147  [ 1400/ 3493]
loss: 0.030720  [ 1500/ 3493]
loss: 0.028027  [ 1600/ 3493]
loss: 0.020028  [ 1700/ 3493]
loss: 0.079759  [ 1800/ 3493]
loss: 0.020743  [ 1900/ 3493]
loss: 0.007026  [ 2000/ 3493]
loss: 0.030984  [ 2100/ 3493]
loss: 0.012629  [ 2200/ 3493]
loss: 0.027349  [ 2300/ 3493]
loss: 0.013881  [ 2400/ 3493]
loss: 0.057173  [ 2500/ 3493]
loss: 0.017392  [ 2600/ 3493]
loss: 0.025396  [ 2700/ 3493]
loss: 0.017058  [ 2800/ 3493]
loss: 0.019282  [ 2900/ 3493]
loss: 0.042649  [ 3000/ 3493]
loss: 0.021869  [ 3100/ 3493]
loss: 0.035438  [ 3200/ 3493]
loss: 0.016496  [ 3300/ 3493]
loss: 0.008440  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.063880  [    0/ 3493]
loss: 0.014139  [  100/ 3493]
loss: 0.037557  [  200/ 3493]
loss: 0.034919  [  300/ 3493]
loss: 0.061918  [  400/ 3493]
loss: 0.028367  [  500/ 3493]
loss: 0.051827  [  600/ 3493]
loss: 0.221202  [  700/ 3493]
loss: 0.042786  [  800/ 3493]
loss: 0.023890  [  900/ 3493]
loss: 0.042463  [ 1000/ 3493]
loss: 0.054012  [ 1100/ 3493]
loss: 0.025249  [ 1200/ 3493]
loss: 0.013187  [ 1300/ 3493]
loss: 0.033957  [ 1400/ 3493]
loss: 0.029713  [ 1500/ 3493]
loss: 0.027247  [ 1600/ 3493]
loss: 0.017895  [ 1700/ 3493]
loss: 0.093375  [ 1800/ 3493]
loss: 0.013539  [ 1900/ 3493]
loss: 0.009020  [ 2000/ 3493]
loss: 0.019338  [ 2100/ 3493]
loss: 0.012264  [ 2200/ 3493]
loss: 0.026968  [ 2300/ 3493]
loss: 0.014690  [ 2400/ 3493]
loss: 0.061768  [ 2500/ 3493]
loss: 0.009459  [ 2600/ 3493]
loss: 0.011768  [ 2700/ 3493]
loss: 0.015791  [ 2800/ 3493]
loss: 0.014942  [ 2900/ 3493]
loss: 0.042275  [ 3000/ 3493]
loss: 0.021346  [ 3100/ 3493]
loss: 0.040418  [ 3200/ 3493]
loss: 0.018421  [ 3300/ 3493]
loss: 0.007043  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.053237  [    0/ 3493]
loss: 0.018139  [  100/ 3493]
loss: 0.036909  [  200/ 3493]
loss: 0.026828  [  300/ 3493]
loss: 0.060209  [  400/ 3493]
loss: 0.030345  [  500/ 3493]
loss: 0.048142  [  600/ 3493]
loss: 0.221935  [  700/ 3493]
loss: 0.027017  [  800/ 3493]
loss: 0.023584  [  900/ 3493]
loss: 0.043336  [ 1000/ 3493]
loss: 0.037561  [ 1100/ 3493]
loss: 0.021825  [ 1200/ 3493]
loss: 0.013681  [ 1300/ 3493]
loss: 0.033342  [ 1400/ 3493]
loss: 0.032778  [ 1500/ 3493]
loss: 0.028166  [ 1600/ 3493]
loss: 0.025115  [ 1700/ 3493]
loss: 0.093304  [ 1800/ 3493]
loss: 0.010100  [ 1900/ 3493]
loss: 0.012276  [ 2000/ 3493]
loss: 0.013676  [ 2100/ 3493]
loss: 0.011587  [ 2200/ 3493]
loss: 0.026940  [ 2300/ 3493]
loss: 0.015797  [ 2400/ 3493]
loss: 0.064703  [ 2500/ 3493]
loss: 0.005099  [ 2600/ 3493]
loss: 0.009174  [ 2700/ 3493]
loss: 0.022323  [ 2800/ 3493]
loss: 0.011505  [ 2900/ 3493]
loss: 0.042624  [ 3000/ 3493]
loss: 0.019673  [ 3100/ 3493]
loss: 0.039928  [ 3200/ 3493]
loss: 0.021176  [ 3300/ 3493]
loss: 0.007162  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.052244  [    0/ 3493]
loss: 0.023485  [  100/ 3493]
loss: 0.036920  [  200/ 3493]
loss: 0.019233  [  300/ 3493]
loss: 0.057969  [  400/ 3493]
loss: 0.030586  [  500/ 3493]
loss: 0.048348  [  600/ 3493]
loss: 0.222149  [  700/ 3493]
loss: 0.017315  [  800/ 3493]
loss: 0.021545  [  900/ 3493]
loss: 0.043050  [ 1000/ 3493]
loss: 0.030640  [ 1100/ 3493]
loss: 0.018714  [ 1200/ 3493]
loss: 0.014034  [ 1300/ 3493]
loss: 0.032997  [ 1400/ 3493]
loss: 0.039961  [ 1500/ 3493]
loss: 0.028572  [ 1600/ 3493]
loss: 0.034769  [ 1700/ 3493]
loss: 0.083742  [ 1800/ 3493]
loss: 0.011446  [ 1900/ 3493]
loss: 0.014745  [ 2000/ 3493]
loss: 0.012681  [ 2100/ 3493]
loss: 0.011127  [ 2200/ 3493]
loss: 0.027050  [ 2300/ 3493]
loss: 0.016745  [ 2400/ 3493]
loss: 0.065449  [ 2500/ 3493]
loss: 0.003989  [ 2600/ 3493]
loss: 0.012921  [ 2700/ 3493]
loss: 0.029985  [ 2800/ 3493]
loss: 0.010038  [ 2900/ 3493]
loss: 0.043531  [ 3000/ 3493]
loss: 0.018219  [ 3100/ 3493]
loss: 0.036603  [ 3200/ 3493]
loss: 0.023826  [ 3300/ 3493]
loss: 0.008070  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.055544  [    0/ 3493]
loss: 0.026944  [  100/ 3493]
loss: 0.036650  [  200/ 3493]
loss: 0.014903  [  300/ 3493]
loss: 0.055644  [  400/ 3493]
loss: 0.029580  [  500/ 3493]
loss: 0.049912  [  600/ 3493]
loss: 0.217798  [  700/ 3493]
loss: 0.012789  [  800/ 3493]
loss: 0.019431  [  900/ 3493]
loss: 0.042693  [ 1000/ 3493]
loss: 0.028711  [ 1100/ 3493]
loss: 0.016972  [ 1200/ 3493]
loss: 0.013988  [ 1300/ 3493]
loss: 0.033384  [ 1400/ 3493]
loss: 0.044834  [ 1500/ 3493]
loss: 0.029685  [ 1600/ 3493]
loss: 0.042487  [ 1700/ 3493]
loss: 0.073602  [ 1800/ 3493]
loss: 0.014015  [ 1900/ 3493]
loss: 0.016437  [ 2000/ 3493]
loss: 0.014850  [ 2100/ 3493]
loss: 0.011132  [ 2200/ 3493]
loss: 0.027238  [ 2300/ 3493]
loss: 0.017391  [ 2400/ 3493]
loss: 0.065037  [ 2500/ 3493]
loss: 0.004677  [ 2600/ 3493]
loss: 0.017570  [ 2700/ 3493]
loss: 0.036549  [ 2800/ 3493]
loss: 0.009385  [ 2900/ 3493]
loss: 0.044502  [ 3000/ 3493]
loss: 0.016708  [ 3100/ 3493]
loss: 0.034056  [ 3200/ 3493]
loss: 0.025095  [ 3300/ 3493]
loss: 0.008793  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.060501  [    0/ 3493]
loss: 0.028915  [  100/ 3493]
loss: 0.037044  [  200/ 3493]
loss: 0.013774  [  300/ 3493]
loss: 0.054571  [  400/ 3493]
loss: 0.029464  [  500/ 3493]
loss: 0.051217  [  600/ 3493]
loss: 0.219161  [  700/ 3493]
loss: 0.011903  [  800/ 3493]
loss: 0.018168  [  900/ 3493]
loss: 0.042765  [ 1000/ 3493]
loss: 0.027500  [ 1100/ 3493]
loss: 0.015517  [ 1200/ 3493]
loss: 0.014463  [ 1300/ 3493]
loss: 0.032804  [ 1400/ 3493]
loss: 0.052460  [ 1500/ 3493]
loss: 0.029271  [ 1600/ 3493]
loss: 0.047191  [ 1700/ 3493]
loss: 0.065685  [ 1800/ 3493]
loss: 0.016556  [ 1900/ 3493]
loss: 0.016964  [ 2000/ 3493]
loss: 0.014003  [ 2100/ 3493]
loss: 0.011268  [ 2200/ 3493]
loss: 0.027317  [ 2300/ 3493]
loss: 0.017668  [ 2400/ 3493]
loss: 0.063727  [ 2500/ 3493]
loss: 0.004985  [ 2600/ 3493]
loss: 0.020885  [ 2700/ 3493]
loss: 0.039417  [ 2800/ 3493]
loss: 0.009635  [ 2900/ 3493]
loss: 0.046320  [ 3000/ 3493]
loss: 0.015803  [ 3100/ 3493]
loss: 0.031662  [ 3200/ 3493]
loss: 0.026672  [ 3300/ 3493]
loss: 0.009286  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.060501  [    0/ 3493]
loss: 0.029862  [  100/ 3493]
loss: 0.036235  [  200/ 3493]
loss: 0.011889  [  300/ 3493]
loss: 0.053758  [  400/ 3493]
loss: 0.027696  [  500/ 3493]
loss: 0.052740  [  600/ 3493]
loss: 0.219483  [  700/ 3493]
loss: 0.011042  [  800/ 3493]
loss: 0.017024  [  900/ 3493]
loss: 0.042255  [ 1000/ 3493]
loss: 0.024877  [ 1100/ 3493]
loss: 0.014937  [ 1200/ 3493]
loss: 0.014052  [ 1300/ 3493]
loss: 0.033698  [ 1400/ 3493]
loss: 0.052578  [ 1500/ 3493]
loss: 0.030237  [ 1600/ 3493]
loss: 0.050077  [ 1700/ 3493]
loss: 0.061253  [ 1800/ 3493]
loss: 0.020643  [ 1900/ 3493]
loss: 0.017326  [ 2000/ 3493]
loss: 0.012014  [ 2100/ 3493]
loss: 0.011493  [ 2200/ 3493]
loss: 0.027065  [ 2300/ 3493]
loss: 0.017899  [ 2400/ 3493]
loss: 0.063012  [ 2500/ 3493]
loss: 0.005700  [ 2600/ 3493]
loss: 0.023289  [ 2700/ 3493]
loss: 0.043026  [ 2800/ 3493]
loss: 0.009787  [ 2900/ 3493]
loss: 0.045404  [ 3000/ 3493]
loss: 0.015426  [ 3100/ 3493]
loss: 0.029737  [ 3200/ 3493]
loss: 0.027447  [ 3300/ 3493]
loss: 0.009474  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [ 0.10293706 -5.673748  ]
[1 0 0 ... 1 0 2]
[1 2 2 ... 0 2 2]
Cluster 0 Occurrences: 1151; KMEANS: 1107
Cluster 1 Occurrences: 1195; KMEANS: 1159
Cluster 2 Occurrences: 1147; KMEANS: 1227
Centroids: [[1.5669171, -2.4809353], [-0.78357005, -3.5676851], [1.1765757, -3.519196]]
Centroids: [[-0.8706954, -3.4189007], [1.5192436, -4.138403], [1.1570857, -2.0980647]]
Contingency Matrix: 
[[   1  353  797]
 [1094   91   10]
 [  12  715  420]]
[[-1, 353, 797], [-1, -1, -1], [-1, 715, 420]]
[[-1, -1, -1], [-1, -1, -1], [-1, 715, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 0: 2, 2: 1}
New Contingency Matrix: 
[[ 797    1  353]
 [  10 1094   91]
 [ 420   12  715]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [797, 1094, 715], Sum: 2606
All_Elements: [797, 1, 353, 10, 1094, 91, 420, 12, 715], Sum: 3493
Accuracy: 0.7460635556827941
Done!
