Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_29
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000020628D3BDD8>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x0000020628D950F0>
Epoch 1
-------------------------------
loss: 0.188617  [    0/ 3298]
loss: 0.496125  [  100/ 3298]
loss: 0.309640  [  200/ 3298]
loss: 0.082139  [  300/ 3298]
loss: 0.055551  [  400/ 3298]
loss: 0.049501  [  500/ 3298]
loss: 0.039900  [  600/ 3298]
loss: 0.096669  [  700/ 3298]
loss: 0.032571  [  800/ 3298]
loss: 0.093277  [  900/ 3298]
loss: 0.038254  [ 1000/ 3298]
loss: 0.065380  [ 1100/ 3298]
loss: 0.074432  [ 1200/ 3298]
loss: 0.045451  [ 1300/ 3298]
loss: 0.013400  [ 1400/ 3298]
loss: 0.041773  [ 1500/ 3298]
loss: 0.017268  [ 1600/ 3298]
loss: 0.106254  [ 1700/ 3298]
loss: 0.046839  [ 1800/ 3298]
loss: 0.025340  [ 1900/ 3298]
loss: 0.010717  [ 2000/ 3298]
loss: 0.011347  [ 2100/ 3298]
loss: 0.013423  [ 2200/ 3298]
loss: 0.022694  [ 2300/ 3298]
loss: 0.021678  [ 2400/ 3298]
loss: 0.093288  [ 2500/ 3298]
loss: 0.030281  [ 2600/ 3298]
loss: 0.020064  [ 2700/ 3298]
loss: 0.194457  [ 2800/ 3298]
loss: 0.041611  [ 2900/ 3298]
loss: 0.035746  [ 3000/ 3298]
loss: 0.023389  [ 3100/ 3298]
loss: 0.068146  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.056005  [    0/ 3298]
loss: 0.022050  [  100/ 3298]
loss: 0.101987  [  200/ 3298]
loss: 0.012102  [  300/ 3298]
loss: 0.007900  [  400/ 3298]
loss: 0.021129  [  500/ 3298]
loss: 0.037971  [  600/ 3298]
loss: 0.066875  [  700/ 3298]
loss: 0.025058  [  800/ 3298]
loss: 0.059193  [  900/ 3298]
loss: 0.035759  [ 1000/ 3298]
loss: 0.095331  [ 1100/ 3298]
loss: 0.073060  [ 1200/ 3298]
loss: 0.039231  [ 1300/ 3298]
loss: 0.013906  [ 1400/ 3298]
loss: 0.026261  [ 1500/ 3298]
loss: 0.011644  [ 1600/ 3298]
loss: 0.102097  [ 1700/ 3298]
loss: 0.047433  [ 1800/ 3298]
loss: 0.031464  [ 1900/ 3298]
loss: 0.009165  [ 2000/ 3298]
loss: 0.014459  [ 2100/ 3298]
loss: 0.013757  [ 2200/ 3298]
loss: 0.020307  [ 2300/ 3298]
loss: 0.016096  [ 2400/ 3298]
loss: 0.075491  [ 2500/ 3298]
loss: 0.027911  [ 2600/ 3298]
loss: 0.015744  [ 2700/ 3298]
loss: 0.121944  [ 2800/ 3298]
loss: 0.028625  [ 2900/ 3298]
loss: 0.038808  [ 3000/ 3298]
loss: 0.023496  [ 3100/ 3298]
loss: 0.106302  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.044933  [    0/ 3298]
loss: 0.025538  [  100/ 3298]
loss: 0.100895  [  200/ 3298]
loss: 0.012927  [  300/ 3298]
loss: 0.006979  [  400/ 3298]
loss: 0.022455  [  500/ 3298]
loss: 0.031279  [  600/ 3298]
loss: 0.061562  [  700/ 3298]
loss: 0.023604  [  800/ 3298]
loss: 0.045885  [  900/ 3298]
loss: 0.037349  [ 1000/ 3298]
loss: 0.050618  [ 1100/ 3298]
loss: 0.071413  [ 1200/ 3298]
loss: 0.025597  [ 1300/ 3298]
loss: 0.015121  [ 1400/ 3298]
loss: 0.015470  [ 1500/ 3298]
loss: 0.009696  [ 1600/ 3298]
loss: 0.047403  [ 1700/ 3298]
loss: 0.042281  [ 1800/ 3298]
loss: 0.026908  [ 1900/ 3298]
loss: 0.008900  [ 2000/ 3298]
loss: 0.013792  [ 2100/ 3298]
loss: 0.018171  [ 2200/ 3298]
loss: 0.021664  [ 2300/ 3298]
loss: 0.017487  [ 2400/ 3298]
loss: 0.047151  [ 2500/ 3298]
loss: 0.028196  [ 2600/ 3298]
loss: 0.019748  [ 2700/ 3298]
loss: 0.152498  [ 2800/ 3298]
loss: 0.023842  [ 2900/ 3298]
loss: 0.036866  [ 3000/ 3298]
loss: 0.019614  [ 3100/ 3298]
loss: 0.095223  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.039218  [    0/ 3298]
loss: 0.027924  [  100/ 3298]
loss: 0.100232  [  200/ 3298]
loss: 0.013535  [  300/ 3298]
loss: 0.006914  [  400/ 3298]
loss: 0.023974  [  500/ 3298]
loss: 0.026020  [  600/ 3298]
loss: 0.062239  [  700/ 3298]
loss: 0.023220  [  800/ 3298]
loss: 0.024519  [  900/ 3298]
loss: 0.038265  [ 1000/ 3298]
loss: 0.045536  [ 1100/ 3298]
loss: 0.071864  [ 1200/ 3298]
loss: 0.022467  [ 1300/ 3298]
loss: 0.014576  [ 1400/ 3298]
loss: 0.017190  [ 1500/ 3298]
loss: 0.010712  [ 1600/ 3298]
loss: 0.035529  [ 1700/ 3298]
loss: 0.038425  [ 1800/ 3298]
loss: 0.022258  [ 1900/ 3298]
loss: 0.009717  [ 2000/ 3298]
loss: 0.015684  [ 2100/ 3298]
loss: 0.016219  [ 2200/ 3298]
loss: 0.020683  [ 2300/ 3298]
loss: 0.016703  [ 2400/ 3298]
loss: 0.031266  [ 2500/ 3298]
loss: 0.028867  [ 2600/ 3298]
loss: 0.030691  [ 2700/ 3298]
loss: 0.187953  [ 2800/ 3298]
loss: 0.023726  [ 2900/ 3298]
loss: 0.033370  [ 3000/ 3298]
loss: 0.018118  [ 3100/ 3298]
loss: 0.076504  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.037678  [    0/ 3298]
loss: 0.029368  [  100/ 3298]
loss: 0.101563  [  200/ 3298]
loss: 0.013836  [  300/ 3298]
loss: 0.006728  [  400/ 3298]
loss: 0.024547  [  500/ 3298]
loss: 0.022520  [  600/ 3298]
loss: 0.060954  [  700/ 3298]
loss: 0.023373  [  800/ 3298]
loss: 0.018738  [  900/ 3298]
loss: 0.039576  [ 1000/ 3298]
loss: 0.047025  [ 1100/ 3298]
loss: 0.073036  [ 1200/ 3298]
loss: 0.021331  [ 1300/ 3298]
loss: 0.014124  [ 1400/ 3298]
loss: 0.019832  [ 1500/ 3298]
loss: 0.011291  [ 1600/ 3298]
loss: 0.038322  [ 1700/ 3298]
loss: 0.034521  [ 1800/ 3298]
loss: 0.020630  [ 1900/ 3298]
loss: 0.010205  [ 2000/ 3298]
loss: 0.015865  [ 2100/ 3298]
loss: 0.013330  [ 2200/ 3298]
loss: 0.020065  [ 2300/ 3298]
loss: 0.016081  [ 2400/ 3298]
loss: 0.023613  [ 2500/ 3298]
loss: 0.030476  [ 2600/ 3298]
loss: 0.035664  [ 2700/ 3298]
loss: 0.213772  [ 2800/ 3298]
loss: 0.025301  [ 2900/ 3298]
loss: 0.031536  [ 3000/ 3298]
loss: 0.017222  [ 3100/ 3298]
loss: 0.068468  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.036450  [    0/ 3298]
loss: 0.030333  [  100/ 3298]
loss: 0.102101  [  200/ 3298]
loss: 0.014402  [  300/ 3298]
loss: 0.007287  [  400/ 3298]
loss: 0.025043  [  500/ 3298]
loss: 0.021402  [  600/ 3298]
loss: 0.059517  [  700/ 3298]
loss: 0.023418  [  800/ 3298]
loss: 0.016194  [  900/ 3298]
loss: 0.039631  [ 1000/ 3298]
loss: 0.044921  [ 1100/ 3298]
loss: 0.072971  [ 1200/ 3298]
loss: 0.023427  [ 1300/ 3298]
loss: 0.014077  [ 1400/ 3298]
loss: 0.021374  [ 1500/ 3298]
loss: 0.011888  [ 1600/ 3298]
loss: 0.038172  [ 1700/ 3298]
loss: 0.033165  [ 1800/ 3298]
loss: 0.020387  [ 1900/ 3298]
loss: 0.010524  [ 2000/ 3298]
loss: 0.014991  [ 2100/ 3298]
loss: 0.011863  [ 2200/ 3298]
loss: 0.019721  [ 2300/ 3298]
loss: 0.015550  [ 2400/ 3298]
loss: 0.021145  [ 2500/ 3298]
loss: 0.030963  [ 2600/ 3298]
loss: 0.037935  [ 2700/ 3298]
loss: 0.223265  [ 2800/ 3298]
loss: 0.026207  [ 2900/ 3298]
loss: 0.031164  [ 3000/ 3298]
loss: 0.016631  [ 3100/ 3298]
loss: 0.063526  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.035895  [    0/ 3298]
loss: 0.031236  [  100/ 3298]
loss: 0.102344  [  200/ 3298]
loss: 0.014513  [  300/ 3298]
loss: 0.008069  [  400/ 3298]
loss: 0.025408  [  500/ 3298]
loss: 0.020926  [  600/ 3298]
loss: 0.058196  [  700/ 3298]
loss: 0.023403  [  800/ 3298]
loss: 0.015380  [  900/ 3298]
loss: 0.039781  [ 1000/ 3298]
loss: 0.043971  [ 1100/ 3298]
loss: 0.072861  [ 1200/ 3298]
loss: 0.024988  [ 1300/ 3298]
loss: 0.014022  [ 1400/ 3298]
loss: 0.021932  [ 1500/ 3298]
loss: 0.012289  [ 1600/ 3298]
loss: 0.036482  [ 1700/ 3298]
loss: 0.032213  [ 1800/ 3298]
loss: 0.020060  [ 1900/ 3298]
loss: 0.010767  [ 2000/ 3298]
loss: 0.014539  [ 2100/ 3298]
loss: 0.011018  [ 2200/ 3298]
loss: 0.019657  [ 2300/ 3298]
loss: 0.015216  [ 2400/ 3298]
loss: 0.019815  [ 2500/ 3298]
loss: 0.031048  [ 2600/ 3298]
loss: 0.039623  [ 2700/ 3298]
loss: 0.228395  [ 2800/ 3298]
loss: 0.026763  [ 2900/ 3298]
loss: 0.031086  [ 3000/ 3298]
loss: 0.016461  [ 3100/ 3298]
loss: 0.059789  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.035198  [    0/ 3298]
loss: 0.031522  [  100/ 3298]
loss: 0.102622  [  200/ 3298]
loss: 0.014410  [  300/ 3298]
loss: 0.008685  [  400/ 3298]
loss: 0.025522  [  500/ 3298]
loss: 0.020709  [  600/ 3298]
loss: 0.057879  [  700/ 3298]
loss: 0.023505  [  800/ 3298]
loss: 0.014430  [  900/ 3298]
loss: 0.039297  [ 1000/ 3298]
loss: 0.041943  [ 1100/ 3298]
loss: 0.072459  [ 1200/ 3298]
loss: 0.025971  [ 1300/ 3298]
loss: 0.013979  [ 1400/ 3298]
loss: 0.022090  [ 1500/ 3298]
loss: 0.012563  [ 1600/ 3298]
loss: 0.035004  [ 1700/ 3298]
loss: 0.031326  [ 1800/ 3298]
loss: 0.020193  [ 1900/ 3298]
loss: 0.010822  [ 2000/ 3298]
loss: 0.014799  [ 2100/ 3298]
loss: 0.010942  [ 2200/ 3298]
loss: 0.019608  [ 2300/ 3298]
loss: 0.014917  [ 2400/ 3298]
loss: 0.019242  [ 2500/ 3298]
loss: 0.031280  [ 2600/ 3298]
loss: 0.040362  [ 2700/ 3298]
loss: 0.223705  [ 2800/ 3298]
loss: 0.026956  [ 2900/ 3298]
loss: 0.030765  [ 3000/ 3298]
loss: 0.015857  [ 3100/ 3298]
loss: 0.058327  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [0.36745295 1.8422916 ]
[1 0 0 ... 0 1 2]
[1 2 2 ... 2 1 0]
Cluster 0 Occurrences: 1094; KMEANS: 1080
Cluster 1 Occurrences: 1089; KMEANS: 1086
Cluster 2 Occurrences: 1115; KMEANS: 1132
Centroids: [[1.2809726, -1.0894253], [0.003317399, 2.405801], [-0.17236358, -2.1944032]]
Centroids: [[-0.18207826, -2.2685103], [0.0016320344, 2.4176152], [1.2435366, -1.0549577]]
Contingency Matrix: 
[[  16    1 1077]
 [   1 1082    6]
 [1063    3   49]]
[[16, -1, 1077], [-1, -1, -1], [1063, -1, 49]]
[[-1, -1, -1], [-1, -1, -1], [1063, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 2, 2: 0}
New Contingency Matrix: 
[[1077    1   16]
 [   6 1082    1]
 [  49    3 1063]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1077, 1082, 1063], Sum: 3222
All_Elements: [1077, 1, 16, 6, 1082, 1, 49, 3, 1063], Sum: 3298
Accuracy: 0.9769557307459066
Done!
