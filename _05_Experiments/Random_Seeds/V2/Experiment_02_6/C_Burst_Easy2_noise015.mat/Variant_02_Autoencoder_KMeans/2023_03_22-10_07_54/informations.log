Experiment_path: Random_Seeds//V2/Experiment_02_6
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_6/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_07_54
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000206A3853320>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x00000206899FF550>
Epoch 1
-------------------------------
loss: 0.186567  [    0/ 3442]
loss: 0.145935  [  100/ 3442]
loss: 0.097599  [  200/ 3442]
loss: 0.076690  [  300/ 3442]
loss: 0.098834  [  400/ 3442]
loss: 0.029212  [  500/ 3442]
loss: 0.028391  [  600/ 3442]
loss: 0.019768  [  700/ 3442]
loss: 0.023384  [  800/ 3442]
loss: 0.054404  [  900/ 3442]
loss: 0.035396  [ 1000/ 3442]
loss: 0.004892  [ 1100/ 3442]
loss: 0.029837  [ 1200/ 3442]
loss: 0.013947  [ 1300/ 3442]
loss: 0.025341  [ 1400/ 3442]
loss: 0.020751  [ 1500/ 3442]
loss: 0.012857  [ 1600/ 3442]
loss: 0.028038  [ 1700/ 3442]
loss: 0.043123  [ 1800/ 3442]
loss: 0.040739  [ 1900/ 3442]
loss: 0.030858  [ 2000/ 3442]
loss: 0.037127  [ 2100/ 3442]
loss: 0.018700  [ 2200/ 3442]
loss: 0.011649  [ 2300/ 3442]
loss: 0.016946  [ 2400/ 3442]
loss: 0.036282  [ 2500/ 3442]
loss: 0.137798  [ 2600/ 3442]
loss: 0.030833  [ 2700/ 3442]
loss: 0.019488  [ 2800/ 3442]
loss: 0.004297  [ 2900/ 3442]
loss: 0.014436  [ 3000/ 3442]
loss: 0.025901  [ 3100/ 3442]
loss: 0.056339  [ 3200/ 3442]
loss: 0.009606  [ 3300/ 3442]
loss: 0.014756  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.036207  [    0/ 3442]
loss: 0.011411  [  100/ 3442]
loss: 0.038219  [  200/ 3442]
loss: 0.007361  [  300/ 3442]
loss: 0.016379  [  400/ 3442]
loss: 0.024238  [  500/ 3442]
loss: 0.031434  [  600/ 3442]
loss: 0.014862  [  700/ 3442]
loss: 0.016720  [  800/ 3442]
loss: 0.028608  [  900/ 3442]
loss: 0.020544  [ 1000/ 3442]
loss: 0.004028  [ 1100/ 3442]
loss: 0.024681  [ 1200/ 3442]
loss: 0.012872  [ 1300/ 3442]
loss: 0.019844  [ 1400/ 3442]
loss: 0.019304  [ 1500/ 3442]
loss: 0.014332  [ 1600/ 3442]
loss: 0.011962  [ 1700/ 3442]
loss: 0.014144  [ 1800/ 3442]
loss: 0.028804  [ 1900/ 3442]
loss: 0.019227  [ 2000/ 3442]
loss: 0.014052  [ 2100/ 3442]
loss: 0.018167  [ 2200/ 3442]
loss: 0.010466  [ 2300/ 3442]
loss: 0.020394  [ 2400/ 3442]
loss: 0.027838  [ 2500/ 3442]
loss: 0.135611  [ 2600/ 3442]
loss: 0.032932  [ 2700/ 3442]
loss: 0.012730  [ 2800/ 3442]
loss: 0.004439  [ 2900/ 3442]
loss: 0.008690  [ 3000/ 3442]
loss: 0.022551  [ 3100/ 3442]
loss: 0.115454  [ 3200/ 3442]
loss: 0.013169  [ 3300/ 3442]
loss: 0.010177  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.031140  [    0/ 3442]
loss: 0.010480  [  100/ 3442]
loss: 0.034621  [  200/ 3442]
loss: 0.007216  [  300/ 3442]
loss: 0.008710  [  400/ 3442]
loss: 0.014896  [  500/ 3442]
loss: 0.029136  [  600/ 3442]
loss: 0.014565  [  700/ 3442]
loss: 0.014598  [  800/ 3442]
loss: 0.026575  [  900/ 3442]
loss: 0.019195  [ 1000/ 3442]
loss: 0.003651  [ 1100/ 3442]
loss: 0.020697  [ 1200/ 3442]
loss: 0.011158  [ 1300/ 3442]
loss: 0.014076  [ 1400/ 3442]
loss: 0.019261  [ 1500/ 3442]
loss: 0.017461  [ 1600/ 3442]
loss: 0.011176  [ 1700/ 3442]
loss: 0.013666  [ 1800/ 3442]
loss: 0.024428  [ 1900/ 3442]
loss: 0.016579  [ 2000/ 3442]
loss: 0.012034  [ 2100/ 3442]
loss: 0.017325  [ 2200/ 3442]
loss: 0.010765  [ 2300/ 3442]
loss: 0.020863  [ 2400/ 3442]
loss: 0.029879  [ 2500/ 3442]
loss: 0.130229  [ 2600/ 3442]
loss: 0.032955  [ 2700/ 3442]
loss: 0.011271  [ 2800/ 3442]
loss: 0.005728  [ 2900/ 3442]
loss: 0.006527  [ 3000/ 3442]
loss: 0.021251  [ 3100/ 3442]
loss: 0.114656  [ 3200/ 3442]
loss: 0.015043  [ 3300/ 3442]
loss: 0.010831  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.022997  [    0/ 3442]
loss: 0.010646  [  100/ 3442]
loss: 0.033252  [  200/ 3442]
loss: 0.007974  [  300/ 3442]
loss: 0.011162  [  400/ 3442]
loss: 0.011952  [  500/ 3442]
loss: 0.027306  [  600/ 3442]
loss: 0.013994  [  700/ 3442]
loss: 0.015259  [  800/ 3442]
loss: 0.026625  [  900/ 3442]
loss: 0.010822  [ 1000/ 3442]
loss: 0.003848  [ 1100/ 3442]
loss: 0.017615  [ 1200/ 3442]
loss: 0.011129  [ 1300/ 3442]
loss: 0.014775  [ 1400/ 3442]
loss: 0.019479  [ 1500/ 3442]
loss: 0.017842  [ 1600/ 3442]
loss: 0.010966  [ 1700/ 3442]
loss: 0.015979  [ 1800/ 3442]
loss: 0.019259  [ 1900/ 3442]
loss: 0.015950  [ 2000/ 3442]
loss: 0.011669  [ 2100/ 3442]
loss: 0.015492  [ 2200/ 3442]
loss: 0.010784  [ 2300/ 3442]
loss: 0.020522  [ 2400/ 3442]
loss: 0.030618  [ 2500/ 3442]
loss: 0.127096  [ 2600/ 3442]
loss: 0.032395  [ 2700/ 3442]
loss: 0.010392  [ 2800/ 3442]
loss: 0.007723  [ 2900/ 3442]
loss: 0.006025  [ 3000/ 3442]
loss: 0.020819  [ 3100/ 3442]
loss: 0.115022  [ 3200/ 3442]
loss: 0.016583  [ 3300/ 3442]
loss: 0.013848  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.018584  [    0/ 3442]
loss: 0.011558  [  100/ 3442]
loss: 0.032158  [  200/ 3442]
loss: 0.010391  [  300/ 3442]
loss: 0.013610  [  400/ 3442]
loss: 0.010550  [  500/ 3442]
loss: 0.025611  [  600/ 3442]
loss: 0.012832  [  700/ 3442]
loss: 0.015957  [  800/ 3442]
loss: 0.026223  [  900/ 3442]
loss: 0.012253  [ 1000/ 3442]
loss: 0.005050  [ 1100/ 3442]
loss: 0.018041  [ 1200/ 3442]
loss: 0.011242  [ 1300/ 3442]
loss: 0.015588  [ 1400/ 3442]
loss: 0.017948  [ 1500/ 3442]
loss: 0.018147  [ 1600/ 3442]
loss: 0.010700  [ 1700/ 3442]
loss: 0.017738  [ 1800/ 3442]
loss: 0.014903  [ 1900/ 3442]
loss: 0.015259  [ 2000/ 3442]
loss: 0.011188  [ 2100/ 3442]
loss: 0.011946  [ 2200/ 3442]
loss: 0.010319  [ 2300/ 3442]
loss: 0.017728  [ 2400/ 3442]
loss: 0.030592  [ 2500/ 3442]
loss: 0.127771  [ 2600/ 3442]
loss: 0.032156  [ 2700/ 3442]
loss: 0.009548  [ 2800/ 3442]
loss: 0.008283  [ 2900/ 3442]
loss: 0.005967  [ 3000/ 3442]
loss: 0.020316  [ 3100/ 3442]
loss: 0.115671  [ 3200/ 3442]
loss: 0.017080  [ 3300/ 3442]
loss: 0.017637  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.017020  [    0/ 3442]
loss: 0.014555  [  100/ 3442]
loss: 0.030959  [  200/ 3442]
loss: 0.008398  [  300/ 3442]
loss: 0.013772  [  400/ 3442]
loss: 0.010146  [  500/ 3442]
loss: 0.024795  [  600/ 3442]
loss: 0.013257  [  700/ 3442]
loss: 0.015899  [  800/ 3442]
loss: 0.027436  [  900/ 3442]
loss: 0.014942  [ 1000/ 3442]
loss: 0.003482  [ 1100/ 3442]
loss: 0.018339  [ 1200/ 3442]
loss: 0.010559  [ 1300/ 3442]
loss: 0.015691  [ 1400/ 3442]
loss: 0.016541  [ 1500/ 3442]
loss: 0.017982  [ 1600/ 3442]
loss: 0.010440  [ 1700/ 3442]
loss: 0.018477  [ 1800/ 3442]
loss: 0.013034  [ 1900/ 3442]
loss: 0.015163  [ 2000/ 3442]
loss: 0.010982  [ 2100/ 3442]
loss: 0.009851  [ 2200/ 3442]
loss: 0.009940  [ 2300/ 3442]
loss: 0.017095  [ 2400/ 3442]
loss: 0.030045  [ 2500/ 3442]
loss: 0.127899  [ 2600/ 3442]
loss: 0.032972  [ 2700/ 3442]
loss: 0.009398  [ 2800/ 3442]
loss: 0.007974  [ 2900/ 3442]
loss: 0.005818  [ 3000/ 3442]
loss: 0.020303  [ 3100/ 3442]
loss: 0.115770  [ 3200/ 3442]
loss: 0.016552  [ 3300/ 3442]
loss: 0.018771  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.017183  [    0/ 3442]
loss: 0.014757  [  100/ 3442]
loss: 0.030657  [  200/ 3442]
loss: 0.008421  [  300/ 3442]
loss: 0.012991  [  400/ 3442]
loss: 0.010205  [  500/ 3442]
loss: 0.024980  [  600/ 3442]
loss: 0.013356  [  700/ 3442]
loss: 0.015689  [  800/ 3442]
loss: 0.027853  [  900/ 3442]
loss: 0.016543  [ 1000/ 3442]
loss: 0.003324  [ 1100/ 3442]
loss: 0.019179  [ 1200/ 3442]
loss: 0.009925  [ 1300/ 3442]
loss: 0.015318  [ 1400/ 3442]
loss: 0.016490  [ 1500/ 3442]
loss: 0.017759  [ 1600/ 3442]
loss: 0.010333  [ 1700/ 3442]
loss: 0.018361  [ 1800/ 3442]
loss: 0.012832  [ 1900/ 3442]
loss: 0.015129  [ 2000/ 3442]
loss: 0.011010  [ 2100/ 3442]
loss: 0.009667  [ 2200/ 3442]
loss: 0.009891  [ 2300/ 3442]
loss: 0.016989  [ 2400/ 3442]
loss: 0.030003  [ 2500/ 3442]
loss: 0.127006  [ 2600/ 3442]
loss: 0.033200  [ 2700/ 3442]
loss: 0.009436  [ 2800/ 3442]
loss: 0.007836  [ 2900/ 3442]
loss: 0.005645  [ 3000/ 3442]
loss: 0.020363  [ 3100/ 3442]
loss: 0.116323  [ 3200/ 3442]
loss: 0.016605  [ 3300/ 3442]
loss: 0.019964  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.017711  [    0/ 3442]
loss: 0.015233  [  100/ 3442]
loss: 0.030425  [  200/ 3442]
loss: 0.008283  [  300/ 3442]
loss: 0.012550  [  400/ 3442]
loss: 0.010252  [  500/ 3442]
loss: 0.024968  [  600/ 3442]
loss: 0.013462  [  700/ 3442]
loss: 0.015583  [  800/ 3442]
loss: 0.028172  [  900/ 3442]
loss: 0.017716  [ 1000/ 3442]
loss: 0.003036  [ 1100/ 3442]
loss: 0.019630  [ 1200/ 3442]
loss: 0.009195  [ 1300/ 3442]
loss: 0.015082  [ 1400/ 3442]
loss: 0.016258  [ 1500/ 3442]
loss: 0.017661  [ 1600/ 3442]
loss: 0.010289  [ 1700/ 3442]
loss: 0.018097  [ 1800/ 3442]
loss: 0.012879  [ 1900/ 3442]
loss: 0.014954  [ 2000/ 3442]
loss: 0.011103  [ 2100/ 3442]
loss: 0.009462  [ 2200/ 3442]
loss: 0.009779  [ 2300/ 3442]
loss: 0.016952  [ 2400/ 3442]
loss: 0.029590  [ 2500/ 3442]
loss: 0.126647  [ 2600/ 3442]
loss: 0.033782  [ 2700/ 3442]
loss: 0.009460  [ 2800/ 3442]
loss: 0.007497  [ 2900/ 3442]
loss: 0.005629  [ 3000/ 3442]
loss: 0.020493  [ 3100/ 3442]
loss: 0.116166  [ 3200/ 3442]
loss: 0.016840  [ 3300/ 3442]
loss: 0.019231  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [-3.7094505   0.91353655]
[1 0 0 ... 1 2 0]
[0 2 2 ... 2 1 0]
Cluster 0 Occurrences: 1159; KMEANS: 1137
Cluster 1 Occurrences: 1156; KMEANS: 1112
Cluster 2 Occurrences: 1127; KMEANS: 1193
Centroids: [[-4.9577956, 2.6437016], [-2.3978968, 1.0127697], [-2.2836611, -2.5540571]]
Centroids: [[-2.1964772, 0.9772219], [-2.2412212, -2.5952048], [-5.114927, 2.6246066]]
Contingency Matrix: 
[[  87    0 1072]
 [1044    2  110]
 [   6 1110   11]]
[[87, -1, 1072], [1044, -1, 110], [-1, -1, -1]]
[[-1, -1, -1], [1044, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 2, 1: 0}
New Contingency Matrix: 
[[1072   87    0]
 [ 110 1044    2]
 [  11    6 1110]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1072, 1044, 1110], Sum: 3226
All_Elements: [1072, 87, 0, 110, 1044, 2, 11, 6, 1110], Sum: 3442
Accuracy: 0.937245787332946
Done!
