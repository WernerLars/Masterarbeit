Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_49_58
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028B8542CDA0>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000028B85474F98>
Epoch 1
-------------------------------
loss: 0.095288  [    0/ 3386]
loss: 0.330598  [  100/ 3386]
loss: 0.258090  [  200/ 3386]
loss: 0.155870  [  300/ 3386]
loss: 0.253503  [  400/ 3386]
loss: 0.077686  [  500/ 3386]
loss: 0.197210  [  600/ 3386]
loss: 0.084632  [  700/ 3386]
loss: 0.161785  [  800/ 3386]
loss: 0.074290  [  900/ 3386]
loss: 0.158121  [ 1000/ 3386]
loss: 0.238113  [ 1100/ 3386]
loss: 0.248454  [ 1200/ 3386]
loss: 0.027994  [ 1300/ 3386]
loss: 0.051339  [ 1400/ 3386]
loss: 0.070847  [ 1500/ 3386]
loss: 0.032933  [ 1600/ 3386]
loss: 0.043692  [ 1700/ 3386]
loss: 0.245098  [ 1800/ 3386]
loss: 0.256482  [ 1900/ 3386]
loss: 0.044242  [ 2000/ 3386]
loss: 0.199029  [ 2100/ 3386]
loss: 0.275086  [ 2200/ 3386]
loss: 0.170898  [ 2300/ 3386]
loss: 0.131395  [ 2400/ 3386]
loss: 0.051520  [ 2500/ 3386]
loss: 0.152924  [ 2600/ 3386]
loss: 0.229794  [ 2700/ 3386]
loss: 0.102275  [ 2800/ 3386]
loss: 0.088516  [ 2900/ 3386]
loss: 0.019609  [ 3000/ 3386]
loss: 0.108920  [ 3100/ 3386]
loss: 0.066280  [ 3200/ 3386]
loss: 0.127342  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.053168  [    0/ 3386]
loss: 0.110804  [  100/ 3386]
loss: 0.074449  [  200/ 3386]
loss: 0.031566  [  300/ 3386]
loss: 0.228835  [  400/ 3386]
loss: 0.069306  [  500/ 3386]
loss: 0.161879  [  600/ 3386]
loss: 0.044911  [  700/ 3386]
loss: 0.129648  [  800/ 3386]
loss: 0.049464  [  900/ 3386]
loss: 0.133664  [ 1000/ 3386]
loss: 0.284405  [ 1100/ 3386]
loss: 0.167676  [ 1200/ 3386]
loss: 0.022498  [ 1300/ 3386]
loss: 0.046025  [ 1400/ 3386]
loss: 0.071004  [ 1500/ 3386]
loss: 0.036219  [ 1600/ 3386]
loss: 0.025375  [ 1700/ 3386]
loss: 0.217746  [ 1800/ 3386]
loss: 0.358106  [ 1900/ 3386]
loss: 0.024374  [ 2000/ 3386]
loss: 0.114143  [ 2100/ 3386]
loss: 0.459847  [ 2200/ 3386]
loss: 0.096377  [ 2300/ 3386]
loss: 0.075456  [ 2400/ 3386]
loss: 0.049032  [ 2500/ 3386]
loss: 0.147609  [ 2600/ 3386]
loss: 0.206288  [ 2700/ 3386]
loss: 0.089386  [ 2800/ 3386]
loss: 0.058616  [ 2900/ 3386]
loss: 0.017653  [ 3000/ 3386]
loss: 0.127210  [ 3100/ 3386]
loss: 0.062863  [ 3200/ 3386]
loss: 0.107924  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.051769  [    0/ 3386]
loss: 0.115573  [  100/ 3386]
loss: 0.068518  [  200/ 3386]
loss: 0.025911  [  300/ 3386]
loss: 0.216287  [  400/ 3386]
loss: 0.072492  [  500/ 3386]
loss: 0.158788  [  600/ 3386]
loss: 0.046290  [  700/ 3386]
loss: 0.121692  [  800/ 3386]
loss: 0.050339  [  900/ 3386]
loss: 0.132246  [ 1000/ 3386]
loss: 0.262668  [ 1100/ 3386]
loss: 0.160947  [ 1200/ 3386]
loss: 0.023865  [ 1300/ 3386]
loss: 0.049891  [ 1400/ 3386]
loss: 0.066518  [ 1500/ 3386]
loss: 0.038140  [ 1600/ 3386]
loss: 0.025742  [ 1700/ 3386]
loss: 0.168988  [ 1800/ 3386]
loss: 0.360773  [ 1900/ 3386]
loss: 0.027488  [ 2000/ 3386]
loss: 0.104625  [ 2100/ 3386]
loss: 0.457586  [ 2200/ 3386]
loss: 0.091739  [ 2300/ 3386]
loss: 0.065366  [ 2400/ 3386]
loss: 0.051332  [ 2500/ 3386]
loss: 0.118775  [ 2600/ 3386]
loss: 0.202357  [ 2700/ 3386]
loss: 0.087481  [ 2800/ 3386]
loss: 0.054389  [ 2900/ 3386]
loss: 0.016552  [ 3000/ 3386]
loss: 0.126648  [ 3100/ 3386]
loss: 0.060856  [ 3200/ 3386]
loss: 0.103557  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.055320  [    0/ 3386]
loss: 0.118439  [  100/ 3386]
loss: 0.068285  [  200/ 3386]
loss: 0.021809  [  300/ 3386]
loss: 0.218142  [  400/ 3386]
loss: 0.072088  [  500/ 3386]
loss: 0.157853  [  600/ 3386]
loss: 0.047876  [  700/ 3386]
loss: 0.119541  [  800/ 3386]
loss: 0.052586  [  900/ 3386]
loss: 0.133997  [ 1000/ 3386]
loss: 0.241972  [ 1100/ 3386]
loss: 0.158603  [ 1200/ 3386]
loss: 0.024749  [ 1300/ 3386]
loss: 0.050802  [ 1400/ 3386]
loss: 0.063086  [ 1500/ 3386]
loss: 0.038008  [ 1600/ 3386]
loss: 0.025552  [ 1700/ 3386]
loss: 0.148126  [ 1800/ 3386]
loss: 0.333833  [ 1900/ 3386]
loss: 0.030111  [ 2000/ 3386]
loss: 0.102590  [ 2100/ 3386]
loss: 0.427783  [ 2200/ 3386]
loss: 0.091490  [ 2300/ 3386]
loss: 0.064935  [ 2400/ 3386]
loss: 0.054391  [ 2500/ 3386]
loss: 0.096847  [ 2600/ 3386]
loss: 0.199389  [ 2700/ 3386]
loss: 0.089434  [ 2800/ 3386]
loss: 0.055633  [ 2900/ 3386]
loss: 0.015539  [ 3000/ 3386]
loss: 0.126576  [ 3100/ 3386]
loss: 0.058744  [ 3200/ 3386]
loss: 0.101489  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.056323  [    0/ 3386]
loss: 0.117770  [  100/ 3386]
loss: 0.074364  [  200/ 3386]
loss: 0.021391  [  300/ 3386]
loss: 0.217776  [  400/ 3386]
loss: 0.071865  [  500/ 3386]
loss: 0.155268  [  600/ 3386]
loss: 0.049942  [  700/ 3386]
loss: 0.118898  [  800/ 3386]
loss: 0.054064  [  900/ 3386]
loss: 0.132876  [ 1000/ 3386]
loss: 0.228539  [ 1100/ 3386]
loss: 0.158581  [ 1200/ 3386]
loss: 0.025738  [ 1300/ 3386]
loss: 0.049707  [ 1400/ 3386]
loss: 0.058371  [ 1500/ 3386]
loss: 0.038908  [ 1600/ 3386]
loss: 0.025393  [ 1700/ 3386]
loss: 0.149736  [ 1800/ 3386]
loss: 0.325019  [ 1900/ 3386]
loss: 0.034691  [ 2000/ 3386]
loss: 0.104205  [ 2100/ 3386]
loss: 0.391407  [ 2200/ 3386]
loss: 0.095377  [ 2300/ 3386]
loss: 0.064409  [ 2400/ 3386]
loss: 0.058487  [ 2500/ 3386]
loss: 0.084751  [ 2600/ 3386]
loss: 0.197684  [ 2700/ 3386]
loss: 0.090873  [ 2800/ 3386]
loss: 0.056478  [ 2900/ 3386]
loss: 0.015119  [ 3000/ 3386]
loss: 0.127628  [ 3100/ 3386]
loss: 0.058264  [ 3200/ 3386]
loss: 0.102972  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.056206  [    0/ 3386]
loss: 0.117030  [  100/ 3386]
loss: 0.082773  [  200/ 3386]
loss: 0.023308  [  300/ 3386]
loss: 0.220850  [  400/ 3386]
loss: 0.071625  [  500/ 3386]
loss: 0.152812  [  600/ 3386]
loss: 0.050855  [  700/ 3386]
loss: 0.120268  [  800/ 3386]
loss: 0.054412  [  900/ 3386]
loss: 0.129007  [ 1000/ 3386]
loss: 0.217950  [ 1100/ 3386]
loss: 0.156600  [ 1200/ 3386]
loss: 0.026393  [ 1300/ 3386]
loss: 0.049050  [ 1400/ 3386]
loss: 0.055639  [ 1500/ 3386]
loss: 0.038277  [ 1600/ 3386]
loss: 0.026348  [ 1700/ 3386]
loss: 0.154155  [ 1800/ 3386]
loss: 0.316795  [ 1900/ 3386]
loss: 0.035767  [ 2000/ 3386]
loss: 0.103633  [ 2100/ 3386]
loss: 0.343673  [ 2200/ 3386]
loss: 0.097372  [ 2300/ 3386]
loss: 0.062338  [ 2400/ 3386]
loss: 0.060320  [ 2500/ 3386]
loss: 0.076512  [ 2600/ 3386]
loss: 0.196146  [ 2700/ 3386]
loss: 0.090651  [ 2800/ 3386]
loss: 0.056912  [ 2900/ 3386]
loss: 0.014993  [ 3000/ 3386]
loss: 0.124417  [ 3100/ 3386]
loss: 0.059365  [ 3200/ 3386]
loss: 0.104652  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.056204  [    0/ 3386]
loss: 0.116522  [  100/ 3386]
loss: 0.087890  [  200/ 3386]
loss: 0.027165  [  300/ 3386]
loss: 0.217438  [  400/ 3386]
loss: 0.070798  [  500/ 3386]
loss: 0.151186  [  600/ 3386]
loss: 0.051460  [  700/ 3386]
loss: 0.121669  [  800/ 3386]
loss: 0.053846  [  900/ 3386]
loss: 0.125520  [ 1000/ 3386]
loss: 0.207461  [ 1100/ 3386]
loss: 0.153482  [ 1200/ 3386]
loss: 0.026932  [ 1300/ 3386]
loss: 0.048828  [ 1400/ 3386]
loss: 0.053476  [ 1500/ 3386]
loss: 0.038277  [ 1600/ 3386]
loss: 0.026895  [ 1700/ 3386]
loss: 0.159202  [ 1800/ 3386]
loss: 0.298852  [ 1900/ 3386]
loss: 0.035469  [ 2000/ 3386]
loss: 0.101111  [ 2100/ 3386]
loss: 0.291462  [ 2200/ 3386]
loss: 0.096238  [ 2300/ 3386]
loss: 0.060901  [ 2400/ 3386]
loss: 0.063137  [ 2500/ 3386]
loss: 0.067922  [ 2600/ 3386]
loss: 0.196382  [ 2700/ 3386]
loss: 0.086729  [ 2800/ 3386]
loss: 0.057114  [ 2900/ 3386]
loss: 0.014985  [ 3000/ 3386]
loss: 0.122231  [ 3100/ 3386]
loss: 0.058431  [ 3200/ 3386]
loss: 0.105219  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.056430  [    0/ 3386]
loss: 0.116496  [  100/ 3386]
loss: 0.090894  [  200/ 3386]
loss: 0.032838  [  300/ 3386]
loss: 0.212604  [  400/ 3386]
loss: 0.069821  [  500/ 3386]
loss: 0.150942  [  600/ 3386]
loss: 0.052046  [  700/ 3386]
loss: 0.123060  [  800/ 3386]
loss: 0.053452  [  900/ 3386]
loss: 0.121955  [ 1000/ 3386]
loss: 0.197008  [ 1100/ 3386]
loss: 0.149783  [ 1200/ 3386]
loss: 0.027542  [ 1300/ 3386]
loss: 0.048668  [ 1400/ 3386]
loss: 0.051909  [ 1500/ 3386]
loss: 0.036626  [ 1600/ 3386]
loss: 0.027079  [ 1700/ 3386]
loss: 0.162185  [ 1800/ 3386]
loss: 0.291268  [ 1900/ 3386]
loss: 0.036242  [ 2000/ 3386]
loss: 0.098736  [ 2100/ 3386]
loss: 0.309424  [ 2200/ 3386]
loss: 0.097252  [ 2300/ 3386]
loss: 0.057275  [ 2400/ 3386]
loss: 0.061430  [ 2500/ 3386]
loss: 0.066086  [ 2600/ 3386]
loss: 0.197984  [ 2700/ 3386]
loss: 0.085128  [ 2800/ 3386]
loss: 0.056847  [ 2900/ 3386]
loss: 0.015224  [ 3000/ 3386]
loss: 0.120205  [ 3100/ 3386]
loss: 0.058270  [ 3200/ 3386]
loss: 0.104421  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [0.7628081  0.50817424]
[0 2 0 ... 2 0 1]
[1 1 1 ... 1 1 2]
Cluster 0 Occurrences: 1079; KMEANS: 631
Cluster 1 Occurrences: 1158; KMEANS: 2197
Cluster 2 Occurrences: 1149; KMEANS: 558
Centroids: [[1.0071343, 0.7101106], [-3.7049086, -2.531799], [0.5252634, 1.5838752]]
Centroids: [[-2.6557813, -1.7278246], [0.80677605, 1.1861432], [-4.8328805, -3.3359156]]
Contingency Matrix: 
[[   0 1079    0]
 [ 590   11  557]
 [  41 1107    1]]
[[0, -1, 0], [590, -1, 557], [-1, -1, -1]]
[[-1, -1, 0], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 0, 0: 2}
New Contingency Matrix: 
[[   0    0 1079]
 [ 557  590   11]
 [   1   41 1107]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [0, 590, 1107], Sum: 1697
All_Elements: [0, 0, 1079, 557, 590, 11, 1, 41, 1107], Sum: 3386
Accuracy: 0.5011813349084465
Done!
