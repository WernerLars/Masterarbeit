Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_06_25
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028C0D2DB8D0>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x0000028C072218D0>
Epoch 1
-------------------------------
loss: 0.304425  [    0/ 3493]
loss: 0.163087  [  100/ 3493]
loss: 0.055119  [  200/ 3493]
loss: 0.062996  [  300/ 3493]
loss: 0.061245  [  400/ 3493]
loss: 0.022884  [  500/ 3493]
loss: 0.066987  [  600/ 3493]
loss: 0.173464  [  700/ 3493]
loss: 0.060829  [  800/ 3493]
loss: 0.024232  [  900/ 3493]
loss: 0.036341  [ 1000/ 3493]
loss: 0.073404  [ 1100/ 3493]
loss: 0.023873  [ 1200/ 3493]
loss: 0.015345  [ 1300/ 3493]
loss: 0.021821  [ 1400/ 3493]
loss: 0.019275  [ 1500/ 3493]
loss: 0.026059  [ 1600/ 3493]
loss: 0.043212  [ 1700/ 3493]
loss: 0.099806  [ 1800/ 3493]
loss: 0.008908  [ 1900/ 3493]
loss: 0.012126  [ 2000/ 3493]
loss: 0.013083  [ 2100/ 3493]
loss: 0.010321  [ 2200/ 3493]
loss: 0.029599  [ 2300/ 3493]
loss: 0.015834  [ 2400/ 3493]
loss: 0.063845  [ 2500/ 3493]
loss: 0.009310  [ 2600/ 3493]
loss: 0.020459  [ 2700/ 3493]
loss: 0.035473  [ 2800/ 3493]
loss: 0.010746  [ 2900/ 3493]
loss: 0.033669  [ 3000/ 3493]
loss: 0.019006  [ 3100/ 3493]
loss: 0.036162  [ 3200/ 3493]
loss: 0.023006  [ 3300/ 3493]
loss: 0.009789  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.069035  [    0/ 3493]
loss: 0.028556  [  100/ 3493]
loss: 0.039557  [  200/ 3493]
loss: 0.016141  [  300/ 3493]
loss: 0.061596  [  400/ 3493]
loss: 0.028709  [  500/ 3493]
loss: 0.050591  [  600/ 3493]
loss: 0.215052  [  700/ 3493]
loss: 0.015621  [  800/ 3493]
loss: 0.018252  [  900/ 3493]
loss: 0.043732  [ 1000/ 3493]
loss: 0.018045  [ 1100/ 3493]
loss: 0.016321  [ 1200/ 3493]
loss: 0.014534  [ 1300/ 3493]
loss: 0.030368  [ 1400/ 3493]
loss: 0.046843  [ 1500/ 3493]
loss: 0.027939  [ 1600/ 3493]
loss: 0.049743  [ 1700/ 3493]
loss: 0.069811  [ 1800/ 3493]
loss: 0.013635  [ 1900/ 3493]
loss: 0.016545  [ 2000/ 3493]
loss: 0.015448  [ 2100/ 3493]
loss: 0.011413  [ 2200/ 3493]
loss: 0.027912  [ 2300/ 3493]
loss: 0.016320  [ 2400/ 3493]
loss: 0.065446  [ 2500/ 3493]
loss: 0.005895  [ 2600/ 3493]
loss: 0.027037  [ 2700/ 3493]
loss: 0.043608  [ 2800/ 3493]
loss: 0.008654  [ 2900/ 3493]
loss: 0.043918  [ 3000/ 3493]
loss: 0.014684  [ 3100/ 3493]
loss: 0.029165  [ 3200/ 3493]
loss: 0.027047  [ 3300/ 3493]
loss: 0.012273  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.069180  [    0/ 3493]
loss: 0.032514  [  100/ 3493]
loss: 0.039077  [  200/ 3493]
loss: 0.012971  [  300/ 3493]
loss: 0.058893  [  400/ 3493]
loss: 0.024724  [  500/ 3493]
loss: 0.053648  [  600/ 3493]
loss: 0.220982  [  700/ 3493]
loss: 0.013690  [  800/ 3493]
loss: 0.015840  [  900/ 3493]
loss: 0.043912  [ 1000/ 3493]
loss: 0.019162  [ 1100/ 3493]
loss: 0.014390  [ 1200/ 3493]
loss: 0.014554  [ 1300/ 3493]
loss: 0.030173  [ 1400/ 3493]
loss: 0.056214  [ 1500/ 3493]
loss: 0.028557  [ 1600/ 3493]
loss: 0.053799  [ 1700/ 3493]
loss: 0.056458  [ 1800/ 3493]
loss: 0.017461  [ 1900/ 3493]
loss: 0.017016  [ 2000/ 3493]
loss: 0.023655  [ 2100/ 3493]
loss: 0.012510  [ 2200/ 3493]
loss: 0.027821  [ 2300/ 3493]
loss: 0.016373  [ 2400/ 3493]
loss: 0.066338  [ 2500/ 3493]
loss: 0.007787  [ 2600/ 3493]
loss: 0.029276  [ 2700/ 3493]
loss: 0.046352  [ 2800/ 3493]
loss: 0.008669  [ 2900/ 3493]
loss: 0.045808  [ 3000/ 3493]
loss: 0.014051  [ 3100/ 3493]
loss: 0.024467  [ 3200/ 3493]
loss: 0.027597  [ 3300/ 3493]
loss: 0.012019  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.073590  [    0/ 3493]
loss: 0.031463  [  100/ 3493]
loss: 0.038449  [  200/ 3493]
loss: 0.013186  [  300/ 3493]
loss: 0.057743  [  400/ 3493]
loss: 0.019733  [  500/ 3493]
loss: 0.054271  [  600/ 3493]
loss: 0.220127  [  700/ 3493]
loss: 0.014547  [  800/ 3493]
loss: 0.014225  [  900/ 3493]
loss: 0.045106  [ 1000/ 3493]
loss: 0.032268  [ 1100/ 3493]
loss: 0.014193  [ 1200/ 3493]
loss: 0.014575  [ 1300/ 3493]
loss: 0.028490  [ 1400/ 3493]
loss: 0.057966  [ 1500/ 3493]
loss: 0.030025  [ 1600/ 3493]
loss: 0.049285  [ 1700/ 3493]
loss: 0.050365  [ 1800/ 3493]
loss: 0.018016  [ 1900/ 3493]
loss: 0.015417  [ 2000/ 3493]
loss: 0.029902  [ 2100/ 3493]
loss: 0.015005  [ 2200/ 3493]
loss: 0.028013  [ 2300/ 3493]
loss: 0.015158  [ 2400/ 3493]
loss: 0.077594  [ 2500/ 3493]
loss: 0.016545  [ 2600/ 3493]
loss: 0.024157  [ 2700/ 3493]
loss: 0.044163  [ 2800/ 3493]
loss: 0.008900  [ 2900/ 3493]
loss: 0.050004  [ 3000/ 3493]
loss: 0.013333  [ 3100/ 3493]
loss: 0.015673  [ 3200/ 3493]
loss: 0.025859  [ 3300/ 3493]
loss: 0.009599  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.090277  [    0/ 3493]
loss: 0.023417  [  100/ 3493]
loss: 0.034691  [  200/ 3493]
loss: 0.014599  [  300/ 3493]
loss: 0.055863  [  400/ 3493]
loss: 0.015746  [  500/ 3493]
loss: 0.054976  [  600/ 3493]
loss: 0.218549  [  700/ 3493]
loss: 0.011995  [  800/ 3493]
loss: 0.012381  [  900/ 3493]
loss: 0.046101  [ 1000/ 3493]
loss: 0.056043  [ 1100/ 3493]
loss: 0.015351  [ 1200/ 3493]
loss: 0.012090  [ 1300/ 3493]
loss: 0.030811  [ 1400/ 3493]
loss: 0.067456  [ 1500/ 3493]
loss: 0.031326  [ 1600/ 3493]
loss: 0.028936  [ 1700/ 3493]
loss: 0.026674  [ 1800/ 3493]
loss: 0.018206  [ 1900/ 3493]
loss: 0.010664  [ 2000/ 3493]
loss: 0.029764  [ 2100/ 3493]
loss: 0.016727  [ 2200/ 3493]
loss: 0.026067  [ 2300/ 3493]
loss: 0.011801  [ 2400/ 3493]
loss: 0.081893  [ 2500/ 3493]
loss: 0.013922  [ 2600/ 3493]
loss: 0.013735  [ 2700/ 3493]
loss: 0.024757  [ 2800/ 3493]
loss: 0.009198  [ 2900/ 3493]
loss: 0.047250  [ 3000/ 3493]
loss: 0.013092  [ 3100/ 3493]
loss: 0.015426  [ 3200/ 3493]
loss: 0.023175  [ 3300/ 3493]
loss: 0.009352  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.081046  [    0/ 3493]
loss: 0.015031  [  100/ 3493]
loss: 0.025911  [  200/ 3493]
loss: 0.016367  [  300/ 3493]
loss: 0.056466  [  400/ 3493]
loss: 0.018835  [  500/ 3493]
loss: 0.054355  [  600/ 3493]
loss: 0.194145  [  700/ 3493]
loss: 0.009348  [  800/ 3493]
loss: 0.012780  [  900/ 3493]
loss: 0.041377  [ 1000/ 3493]
loss: 0.053636  [ 1100/ 3493]
loss: 0.017052  [ 1200/ 3493]
loss: 0.013597  [ 1300/ 3493]
loss: 0.031140  [ 1400/ 3493]
loss: 0.065989  [ 1500/ 3493]
loss: 0.030095  [ 1600/ 3493]
loss: 0.022719  [ 1700/ 3493]
loss: 0.027017  [ 1800/ 3493]
loss: 0.017579  [ 1900/ 3493]
loss: 0.010475  [ 2000/ 3493]
loss: 0.026032  [ 2100/ 3493]
loss: 0.016163  [ 2200/ 3493]
loss: 0.024038  [ 2300/ 3493]
loss: 0.013685  [ 2400/ 3493]
loss: 0.088460  [ 2500/ 3493]
loss: 0.011380  [ 2600/ 3493]
loss: 0.013530  [ 2700/ 3493]
loss: 0.020768  [ 2800/ 3493]
loss: 0.009001  [ 2900/ 3493]
loss: 0.044142  [ 3000/ 3493]
loss: 0.013241  [ 3100/ 3493]
loss: 0.016046  [ 3200/ 3493]
loss: 0.023180  [ 3300/ 3493]
loss: 0.010406  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.071453  [    0/ 3493]
loss: 0.016078  [  100/ 3493]
loss: 0.023620  [  200/ 3493]
loss: 0.015701  [  300/ 3493]
loss: 0.056109  [  400/ 3493]
loss: 0.018195  [  500/ 3493]
loss: 0.053730  [  600/ 3493]
loss: 0.181341  [  700/ 3493]
loss: 0.009542  [  800/ 3493]
loss: 0.012450  [  900/ 3493]
loss: 0.037259  [ 1000/ 3493]
loss: 0.048412  [ 1100/ 3493]
loss: 0.017545  [ 1200/ 3493]
loss: 0.013660  [ 1300/ 3493]
loss: 0.030510  [ 1400/ 3493]
loss: 0.062951  [ 1500/ 3493]
loss: 0.029660  [ 1600/ 3493]
loss: 0.020886  [ 1700/ 3493]
loss: 0.030919  [ 1800/ 3493]
loss: 0.017125  [ 1900/ 3493]
loss: 0.011790  [ 2000/ 3493]
loss: 0.023508  [ 2100/ 3493]
loss: 0.015907  [ 2200/ 3493]
loss: 0.023295  [ 2300/ 3493]
loss: 0.014469  [ 2400/ 3493]
loss: 0.086909  [ 2500/ 3493]
loss: 0.010426  [ 2600/ 3493]
loss: 0.013525  [ 2700/ 3493]
loss: 0.020264  [ 2800/ 3493]
loss: 0.008810  [ 2900/ 3493]
loss: 0.041920  [ 3000/ 3493]
loss: 0.013416  [ 3100/ 3493]
loss: 0.016606  [ 3200/ 3493]
loss: 0.023293  [ 3300/ 3493]
loss: 0.010883  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.065946  [    0/ 3493]
loss: 0.016743  [  100/ 3493]
loss: 0.022787  [  200/ 3493]
loss: 0.015590  [  300/ 3493]
loss: 0.056109  [  400/ 3493]
loss: 0.016933  [  500/ 3493]
loss: 0.053116  [  600/ 3493]
loss: 0.174206  [  700/ 3493]
loss: 0.010120  [  800/ 3493]
loss: 0.012356  [  900/ 3493]
loss: 0.035603  [ 1000/ 3493]
loss: 0.044719  [ 1100/ 3493]
loss: 0.017716  [ 1200/ 3493]
loss: 0.013736  [ 1300/ 3493]
loss: 0.029777  [ 1400/ 3493]
loss: 0.061548  [ 1500/ 3493]
loss: 0.029519  [ 1600/ 3493]
loss: 0.019941  [ 1700/ 3493]
loss: 0.032668  [ 1800/ 3493]
loss: 0.016906  [ 1900/ 3493]
loss: 0.011342  [ 2000/ 3493]
loss: 0.022104  [ 2100/ 3493]
loss: 0.015237  [ 2200/ 3493]
loss: 0.022971  [ 2300/ 3493]
loss: 0.014704  [ 2400/ 3493]
loss: 0.086685  [ 2500/ 3493]
loss: 0.009937  [ 2600/ 3493]
loss: 0.013448  [ 2700/ 3493]
loss: 0.020303  [ 2800/ 3493]
loss: 0.008596  [ 2900/ 3493]
loss: 0.040338  [ 3000/ 3493]
loss: 0.013547  [ 3100/ 3493]
loss: 0.016175  [ 3200/ 3493]
loss: 0.023365  [ 3300/ 3493]
loss: 0.010995  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [1.2531943 2.4407897]
[1 0 0 ... 1 0 2]
[2 1 1 ... 0 1 1]
Cluster 0 Occurrences: 1151; KMEANS: 591
Cluster 1 Occurrences: 1195; KMEANS: 2302
Cluster 2 Occurrences: 1147; KMEANS: 600
Centroids: [[-0.682839, -0.24815166], [1.2571359, 1.8818406], [-0.57097274, 0.19624886]]
Centroids: [[0.8749635, 1.6101283], [-0.63128597, -0.031088596], [1.6625682, 2.1804245]]
Contingency Matrix: 
[[   2 1149    0]
 [ 585   11  599]
 [   4 1142    1]]
[[-1, -1, -1], [585, -1, 599], [4, -1, 1]]
[[-1, -1, -1], [-1, -1, -1], [4, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 1, 1: 2, 2: 0}
New Contingency Matrix: 
[[1149    0    2]
 [  11  599  585]
 [1142    1    4]]
New Clustered Label Sequence: [1, 2, 0]
Diagonal_Elements: [1149, 599, 4], Sum: 1752
All_Elements: [1149, 0, 2, 11, 599, 585, 1142, 1, 4], Sum: 3493
Accuracy: 0.5015745777268823
Done!
