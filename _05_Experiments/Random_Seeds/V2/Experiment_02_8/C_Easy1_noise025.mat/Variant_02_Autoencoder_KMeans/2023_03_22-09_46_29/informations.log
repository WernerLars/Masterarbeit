Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_29
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028B8AA1FE48>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x0000028B85474198>
Epoch 1
-------------------------------
loss: 0.187809  [    0/ 3298]
loss: 0.439004  [  100/ 3298]
loss: 0.263712  [  200/ 3298]
loss: 0.042009  [  300/ 3298]
loss: 0.030570  [  400/ 3298]
loss: 0.022242  [  500/ 3298]
loss: 0.042901  [  600/ 3298]
loss: 0.063311  [  700/ 3298]
loss: 0.027727  [  800/ 3298]
loss: 0.048522  [  900/ 3298]
loss: 0.037935  [ 1000/ 3298]
loss: 0.046783  [ 1100/ 3298]
loss: 0.074863  [ 1200/ 3298]
loss: 0.050811  [ 1300/ 3298]
loss: 0.011846  [ 1400/ 3298]
loss: 0.046874  [ 1500/ 3298]
loss: 0.010485  [ 1600/ 3298]
loss: 0.033333  [ 1700/ 3298]
loss: 0.049655  [ 1800/ 3298]
loss: 0.024643  [ 1900/ 3298]
loss: 0.010540  [ 2000/ 3298]
loss: 0.012650  [ 2100/ 3298]
loss: 0.016996  [ 2200/ 3298]
loss: 0.026985  [ 2300/ 3298]
loss: 0.021313  [ 2400/ 3298]
loss: 0.093131  [ 2500/ 3298]
loss: 0.037817  [ 2600/ 3298]
loss: 0.027943  [ 2700/ 3298]
loss: 0.094069  [ 2800/ 3298]
loss: 0.042558  [ 2900/ 3298]
loss: 0.032161  [ 3000/ 3298]
loss: 0.016354  [ 3100/ 3298]
loss: 0.076717  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.039193  [    0/ 3298]
loss: 0.024473  [  100/ 3298]
loss: 0.103198  [  200/ 3298]
loss: 0.011271  [  300/ 3298]
loss: 0.005296  [  400/ 3298]
loss: 0.017966  [  500/ 3298]
loss: 0.028638  [  600/ 3298]
loss: 0.062086  [  700/ 3298]
loss: 0.024758  [  800/ 3298]
loss: 0.022571  [  900/ 3298]
loss: 0.035775  [ 1000/ 3298]
loss: 0.038561  [ 1100/ 3298]
loss: 0.074570  [ 1200/ 3298]
loss: 0.048096  [ 1300/ 3298]
loss: 0.011493  [ 1400/ 3298]
loss: 0.047811  [ 1500/ 3298]
loss: 0.010525  [ 1600/ 3298]
loss: 0.048860  [ 1700/ 3298]
loss: 0.037125  [ 1800/ 3298]
loss: 0.026438  [ 1900/ 3298]
loss: 0.009948  [ 2000/ 3298]
loss: 0.011763  [ 2100/ 3298]
loss: 0.014240  [ 2200/ 3298]
loss: 0.018538  [ 2300/ 3298]
loss: 0.018295  [ 2400/ 3298]
loss: 0.036730  [ 2500/ 3298]
loss: 0.034679  [ 2600/ 3298]
loss: 0.029037  [ 2700/ 3298]
loss: 0.133334  [ 2800/ 3298]
loss: 0.042902  [ 2900/ 3298]
loss: 0.029515  [ 3000/ 3298]
loss: 0.004947  [ 3100/ 3298]
loss: 0.077477  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.056031  [    0/ 3298]
loss: 0.021831  [  100/ 3298]
loss: 0.109621  [  200/ 3298]
loss: 0.012366  [  300/ 3298]
loss: 0.005048  [  400/ 3298]
loss: 0.018896  [  500/ 3298]
loss: 0.016983  [  600/ 3298]
loss: 0.062758  [  700/ 3298]
loss: 0.024040  [  800/ 3298]
loss: 0.012768  [  900/ 3298]
loss: 0.045152  [ 1000/ 3298]
loss: 0.049917  [ 1100/ 3298]
loss: 0.076397  [ 1200/ 3298]
loss: 0.039412  [ 1300/ 3298]
loss: 0.013090  [ 1400/ 3298]
loss: 0.048194  [ 1500/ 3298]
loss: 0.012643  [ 1600/ 3298]
loss: 0.058338  [ 1700/ 3298]
loss: 0.023363  [ 1800/ 3298]
loss: 0.024662  [ 1900/ 3298]
loss: 0.011131  [ 2000/ 3298]
loss: 0.014512  [ 2100/ 3298]
loss: 0.009875  [ 2200/ 3298]
loss: 0.015856  [ 2300/ 3298]
loss: 0.014393  [ 2400/ 3298]
loss: 0.016259  [ 2500/ 3298]
loss: 0.033188  [ 2600/ 3298]
loss: 0.036874  [ 2700/ 3298]
loss: 0.169017  [ 2800/ 3298]
loss: 0.034874  [ 2900/ 3298]
loss: 0.031328  [ 3000/ 3298]
loss: 0.011296  [ 3100/ 3298]
loss: 0.069345  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.050531  [    0/ 3298]
loss: 0.021109  [  100/ 3298]
loss: 0.109251  [  200/ 3298]
loss: 0.012249  [  300/ 3298]
loss: 0.005535  [  400/ 3298]
loss: 0.021222  [  500/ 3298]
loss: 0.017663  [  600/ 3298]
loss: 0.058175  [  700/ 3298]
loss: 0.024381  [  800/ 3298]
loss: 0.013201  [  900/ 3298]
loss: 0.052514  [ 1000/ 3298]
loss: 0.049404  [ 1100/ 3298]
loss: 0.074438  [ 1200/ 3298]
loss: 0.024532  [ 1300/ 3298]
loss: 0.013209  [ 1400/ 3298]
loss: 0.039421  [ 1500/ 3298]
loss: 0.013182  [ 1600/ 3298]
loss: 0.052570  [ 1700/ 3298]
loss: 0.021550  [ 1800/ 3298]
loss: 0.021071  [ 1900/ 3298]
loss: 0.011737  [ 2000/ 3298]
loss: 0.014944  [ 2100/ 3298]
loss: 0.008616  [ 2200/ 3298]
loss: 0.016082  [ 2300/ 3298]
loss: 0.013824  [ 2400/ 3298]
loss: 0.022598  [ 2500/ 3298]
loss: 0.033183  [ 2600/ 3298]
loss: 0.037088  [ 2700/ 3298]
loss: 0.171315  [ 2800/ 3298]
loss: 0.030353  [ 2900/ 3298]
loss: 0.031364  [ 3000/ 3298]
loss: 0.021934  [ 3100/ 3298]
loss: 0.066547  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.042276  [    0/ 3298]
loss: 0.020978  [  100/ 3298]
loss: 0.107709  [  200/ 3298]
loss: 0.012589  [  300/ 3298]
loss: 0.006399  [  400/ 3298]
loss: 0.023005  [  500/ 3298]
loss: 0.019646  [  600/ 3298]
loss: 0.056404  [  700/ 3298]
loss: 0.025114  [  800/ 3298]
loss: 0.015909  [  900/ 3298]
loss: 0.052844  [ 1000/ 3298]
loss: 0.043598  [ 1100/ 3298]
loss: 0.074129  [ 1200/ 3298]
loss: 0.020906  [ 1300/ 3298]
loss: 0.012838  [ 1400/ 3298]
loss: 0.034695  [ 1500/ 3298]
loss: 0.012136  [ 1600/ 3298]
loss: 0.050040  [ 1700/ 3298]
loss: 0.023293  [ 1800/ 3298]
loss: 0.019929  [ 1900/ 3298]
loss: 0.010874  [ 2000/ 3298]
loss: 0.016379  [ 2100/ 3298]
loss: 0.009798  [ 2200/ 3298]
loss: 0.016574  [ 2300/ 3298]
loss: 0.014283  [ 2400/ 3298]
loss: 0.027583  [ 2500/ 3298]
loss: 0.032525  [ 2600/ 3298]
loss: 0.036587  [ 2700/ 3298]
loss: 0.174931  [ 2800/ 3298]
loss: 0.028693  [ 2900/ 3298]
loss: 0.030996  [ 3000/ 3298]
loss: 0.023458  [ 3100/ 3298]
loss: 0.064918  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.037243  [    0/ 3298]
loss: 0.020816  [  100/ 3298]
loss: 0.107019  [  200/ 3298]
loss: 0.012926  [  300/ 3298]
loss: 0.006910  [  400/ 3298]
loss: 0.023378  [  500/ 3298]
loss: 0.020320  [  600/ 3298]
loss: 0.054997  [  700/ 3298]
loss: 0.025164  [  800/ 3298]
loss: 0.015080  [  900/ 3298]
loss: 0.050891  [ 1000/ 3298]
loss: 0.040083  [ 1100/ 3298]
loss: 0.074015  [ 1200/ 3298]
loss: 0.020654  [ 1300/ 3298]
loss: 0.012804  [ 1400/ 3298]
loss: 0.032003  [ 1500/ 3298]
loss: 0.012098  [ 1600/ 3298]
loss: 0.043319  [ 1700/ 3298]
loss: 0.024795  [ 1800/ 3298]
loss: 0.019591  [ 1900/ 3298]
loss: 0.009850  [ 2000/ 3298]
loss: 0.016787  [ 2100/ 3298]
loss: 0.009638  [ 2200/ 3298]
loss: 0.016907  [ 2300/ 3298]
loss: 0.014274  [ 2400/ 3298]
loss: 0.028288  [ 2500/ 3298]
loss: 0.031499  [ 2600/ 3298]
loss: 0.037119  [ 2700/ 3298]
loss: 0.181821  [ 2800/ 3298]
loss: 0.027930  [ 2900/ 3298]
loss: 0.030254  [ 3000/ 3298]
loss: 0.021847  [ 3100/ 3298]
loss: 0.063432  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.034597  [    0/ 3298]
loss: 0.020014  [  100/ 3298]
loss: 0.106228  [  200/ 3298]
loss: 0.013260  [  300/ 3298]
loss: 0.006899  [  400/ 3298]
loss: 0.023194  [  500/ 3298]
loss: 0.020278  [  600/ 3298]
loss: 0.053524  [  700/ 3298]
loss: 0.025146  [  800/ 3298]
loss: 0.015404  [  900/ 3298]
loss: 0.047813  [ 1000/ 3298]
loss: 0.037392  [ 1100/ 3298]
loss: 0.073987  [ 1200/ 3298]
loss: 0.021070  [ 1300/ 3298]
loss: 0.012820  [ 1400/ 3298]
loss: 0.030875  [ 1500/ 3298]
loss: 0.012130  [ 1600/ 3298]
loss: 0.040334  [ 1700/ 3298]
loss: 0.026252  [ 1800/ 3298]
loss: 0.018937  [ 1900/ 3298]
loss: 0.009350  [ 2000/ 3298]
loss: 0.015730  [ 2100/ 3298]
loss: 0.009325  [ 2200/ 3298]
loss: 0.016976  [ 2300/ 3298]
loss: 0.014212  [ 2400/ 3298]
loss: 0.026779  [ 2500/ 3298]
loss: 0.031675  [ 2600/ 3298]
loss: 0.037934  [ 2700/ 3298]
loss: 0.190366  [ 2800/ 3298]
loss: 0.027360  [ 2900/ 3298]
loss: 0.030045  [ 3000/ 3298]
loss: 0.021152  [ 3100/ 3298]
loss: 0.061800  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.033328  [    0/ 3298]
loss: 0.019394  [  100/ 3298]
loss: 0.105715  [  200/ 3298]
loss: 0.013281  [  300/ 3298]
loss: 0.007033  [  400/ 3298]
loss: 0.023300  [  500/ 3298]
loss: 0.019486  [  600/ 3298]
loss: 0.052722  [  700/ 3298]
loss: 0.025034  [  800/ 3298]
loss: 0.014675  [  900/ 3298]
loss: 0.041684  [ 1000/ 3298]
loss: 0.035889  [ 1100/ 3298]
loss: 0.073693  [ 1200/ 3298]
loss: 0.021683  [ 1300/ 3298]
loss: 0.012841  [ 1400/ 3298]
loss: 0.031400  [ 1500/ 3298]
loss: 0.011569  [ 1600/ 3298]
loss: 0.037333  [ 1700/ 3298]
loss: 0.027200  [ 1800/ 3298]
loss: 0.018401  [ 1900/ 3298]
loss: 0.009021  [ 2000/ 3298]
loss: 0.015544  [ 2100/ 3298]
loss: 0.008981  [ 2200/ 3298]
loss: 0.017189  [ 2300/ 3298]
loss: 0.014046  [ 2400/ 3298]
loss: 0.024364  [ 2500/ 3298]
loss: 0.031562  [ 2600/ 3298]
loss: 0.038632  [ 2700/ 3298]
loss: 0.202629  [ 2800/ 3298]
loss: 0.027220  [ 2900/ 3298]
loss: 0.029964  [ 3000/ 3298]
loss: 0.021097  [ 3100/ 3298]
loss: 0.060277  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [0.04944253 1.2940803 ]
[1 0 0 ... 0 1 2]
[1 2 2 ... 2 1 0]
Cluster 0 Occurrences: 1094; KMEANS: 1132
Cluster 1 Occurrences: 1089; KMEANS: 1090
Cluster 2 Occurrences: 1115; KMEANS: 1076
Centroids: [[1.9153439, -0.83805656], [-0.1312573, 1.7111748], [-1.6336485, -0.85941625]]
Centroids: [[-1.6250209, -0.8613447], [-0.12858361, 1.7120341], [1.9615322, -0.8389301]]
Contingency Matrix: 
[[  17    6 1071]
 [   6 1079    4]
 [1109    5    1]]
[[-1, 6, 1071], [-1, 1079, 4], [-1, -1, -1]]
[[-1, -1, 1071], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 1: 1, 0: 2}
New Contingency Matrix: 
[[1071    6   17]
 [   4 1079    6]
 [   1    5 1109]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1071, 1079, 1109], Sum: 3259
All_Elements: [1071, 6, 17, 4, 1079, 6, 1, 5, 1109], Sum: 3298
Accuracy: 0.9881746513038205
Done!
