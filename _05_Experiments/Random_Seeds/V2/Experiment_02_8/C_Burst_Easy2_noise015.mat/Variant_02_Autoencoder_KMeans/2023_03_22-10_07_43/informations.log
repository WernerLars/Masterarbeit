Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_07_43
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028C035CDC88>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x0000028C072218D0>
Epoch 1
-------------------------------
loss: 0.206994  [    0/ 3442]
loss: 0.156720  [  100/ 3442]
loss: 0.053898  [  200/ 3442]
loss: 0.027238  [  300/ 3442]
loss: 0.039754  [  400/ 3442]
loss: 0.026839  [  500/ 3442]
loss: 0.031005  [  600/ 3442]
loss: 0.015974  [  700/ 3442]
loss: 0.030395  [  800/ 3442]
loss: 0.047742  [  900/ 3442]
loss: 0.029653  [ 1000/ 3442]
loss: 0.004378  [ 1100/ 3442]
loss: 0.026474  [ 1200/ 3442]
loss: 0.011806  [ 1300/ 3442]
loss: 0.018446  [ 1400/ 3442]
loss: 0.020562  [ 1500/ 3442]
loss: 0.016400  [ 1600/ 3442]
loss: 0.019657  [ 1700/ 3442]
loss: 0.024131  [ 1800/ 3442]
loss: 0.042784  [ 1900/ 3442]
loss: 0.013475  [ 2000/ 3442]
loss: 0.028454  [ 2100/ 3442]
loss: 0.020214  [ 2200/ 3442]
loss: 0.010546  [ 2300/ 3442]
loss: 0.014199  [ 2400/ 3442]
loss: 0.042411  [ 2500/ 3442]
loss: 0.129004  [ 2600/ 3442]
loss: 0.058184  [ 2700/ 3442]
loss: 0.011605  [ 2800/ 3442]
loss: 0.019750  [ 2900/ 3442]
loss: 0.003248  [ 3000/ 3442]
loss: 0.019675  [ 3100/ 3442]
loss: 0.058323  [ 3200/ 3442]
loss: 0.013818  [ 3300/ 3442]
loss: 0.022628  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.021875  [    0/ 3442]
loss: 0.012708  [  100/ 3442]
loss: 0.030414  [  200/ 3442]
loss: 0.006880  [  300/ 3442]
loss: 0.019495  [  400/ 3442]
loss: 0.009410  [  500/ 3442]
loss: 0.028600  [  600/ 3442]
loss: 0.013121  [  700/ 3442]
loss: 0.018251  [  800/ 3442]
loss: 0.036121  [  900/ 3442]
loss: 0.008887  [ 1000/ 3442]
loss: 0.003634  [ 1100/ 3442]
loss: 0.016439  [ 1200/ 3442]
loss: 0.009850  [ 1300/ 3442]
loss: 0.009599  [ 1400/ 3442]
loss: 0.020245  [ 1500/ 3442]
loss: 0.016799  [ 1600/ 3442]
loss: 0.012065  [ 1700/ 3442]
loss: 0.019068  [ 1800/ 3442]
loss: 0.022151  [ 1900/ 3442]
loss: 0.016271  [ 2000/ 3442]
loss: 0.018663  [ 2100/ 3442]
loss: 0.016521  [ 2200/ 3442]
loss: 0.011508  [ 2300/ 3442]
loss: 0.014019  [ 2400/ 3442]
loss: 0.035461  [ 2500/ 3442]
loss: 0.119932  [ 2600/ 3442]
loss: 0.037309  [ 2700/ 3442]
loss: 0.009823  [ 2800/ 3442]
loss: 0.009060  [ 2900/ 3442]
loss: 0.004212  [ 3000/ 3442]
loss: 0.018913  [ 3100/ 3442]
loss: 0.101824  [ 3200/ 3442]
loss: 0.016232  [ 3300/ 3442]
loss: 0.019954  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.013545  [    0/ 3442]
loss: 0.012319  [  100/ 3442]
loss: 0.029080  [  200/ 3442]
loss: 0.006843  [  300/ 3442]
loss: 0.011234  [  400/ 3442]
loss: 0.009749  [  500/ 3442]
loss: 0.027651  [  600/ 3442]
loss: 0.011633  [  700/ 3442]
loss: 0.015071  [  800/ 3442]
loss: 0.031800  [  900/ 3442]
loss: 0.008050  [ 1000/ 3442]
loss: 0.003269  [ 1100/ 3442]
loss: 0.013576  [ 1200/ 3442]
loss: 0.009289  [ 1300/ 3442]
loss: 0.010469  [ 1400/ 3442]
loss: 0.018853  [ 1500/ 3442]
loss: 0.017460  [ 1600/ 3442]
loss: 0.009865  [ 1700/ 3442]
loss: 0.017947  [ 1800/ 3442]
loss: 0.015594  [ 1900/ 3442]
loss: 0.016267  [ 2000/ 3442]
loss: 0.014003  [ 2100/ 3442]
loss: 0.014291  [ 2200/ 3442]
loss: 0.011139  [ 2300/ 3442]
loss: 0.014983  [ 2400/ 3442]
loss: 0.032727  [ 2500/ 3442]
loss: 0.121828  [ 2600/ 3442]
loss: 0.033618  [ 2700/ 3442]
loss: 0.009111  [ 2800/ 3442]
loss: 0.007533  [ 2900/ 3442]
loss: 0.004744  [ 3000/ 3442]
loss: 0.018974  [ 3100/ 3442]
loss: 0.115161  [ 3200/ 3442]
loss: 0.017470  [ 3300/ 3442]
loss: 0.021798  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.013373  [    0/ 3442]
loss: 0.012909  [  100/ 3442]
loss: 0.029445  [  200/ 3442]
loss: 0.007009  [  300/ 3442]
loss: 0.010610  [  400/ 3442]
loss: 0.009920  [  500/ 3442]
loss: 0.027030  [  600/ 3442]
loss: 0.012074  [  700/ 3442]
loss: 0.015505  [  800/ 3442]
loss: 0.030588  [  900/ 3442]
loss: 0.010333  [ 1000/ 3442]
loss: 0.002993  [ 1100/ 3442]
loss: 0.014369  [ 1200/ 3442]
loss: 0.008846  [ 1300/ 3442]
loss: 0.010926  [ 1400/ 3442]
loss: 0.018312  [ 1500/ 3442]
loss: 0.017627  [ 1600/ 3442]
loss: 0.009698  [ 1700/ 3442]
loss: 0.018218  [ 1800/ 3442]
loss: 0.013807  [ 1900/ 3442]
loss: 0.016010  [ 2000/ 3442]
loss: 0.012716  [ 2100/ 3442]
loss: 0.012975  [ 2200/ 3442]
loss: 0.010988  [ 2300/ 3442]
loss: 0.015213  [ 2400/ 3442]
loss: 0.031801  [ 2500/ 3442]
loss: 0.123020  [ 2600/ 3442]
loss: 0.033648  [ 2700/ 3442]
loss: 0.008975  [ 2800/ 3442]
loss: 0.007410  [ 2900/ 3442]
loss: 0.005063  [ 3000/ 3442]
loss: 0.019177  [ 3100/ 3442]
loss: 0.118555  [ 3200/ 3442]
loss: 0.018356  [ 3300/ 3442]
loss: 0.023935  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.013715  [    0/ 3442]
loss: 0.013482  [  100/ 3442]
loss: 0.029703  [  200/ 3442]
loss: 0.007153  [  300/ 3442]
loss: 0.011201  [  400/ 3442]
loss: 0.009991  [  500/ 3442]
loss: 0.026739  [  600/ 3442]
loss: 0.012739  [  700/ 3442]
loss: 0.015971  [  800/ 3442]
loss: 0.030129  [  900/ 3442]
loss: 0.012427  [ 1000/ 3442]
loss: 0.002901  [ 1100/ 3442]
loss: 0.015319  [ 1200/ 3442]
loss: 0.008333  [ 1300/ 3442]
loss: 0.010858  [ 1400/ 3442]
loss: 0.018031  [ 1500/ 3442]
loss: 0.017622  [ 1600/ 3442]
loss: 0.009760  [ 1700/ 3442]
loss: 0.018363  [ 1800/ 3442]
loss: 0.013175  [ 1900/ 3442]
loss: 0.016024  [ 2000/ 3442]
loss: 0.011704  [ 2100/ 3442]
loss: 0.012040  [ 2200/ 3442]
loss: 0.010976  [ 2300/ 3442]
loss: 0.015103  [ 2400/ 3442]
loss: 0.030818  [ 2500/ 3442]
loss: 0.123106  [ 2600/ 3442]
loss: 0.034389  [ 2700/ 3442]
loss: 0.008727  [ 2800/ 3442]
loss: 0.007359  [ 2900/ 3442]
loss: 0.005139  [ 3000/ 3442]
loss: 0.019364  [ 3100/ 3442]
loss: 0.119601  [ 3200/ 3442]
loss: 0.018541  [ 3300/ 3442]
loss: 0.025283  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.013590  [    0/ 3442]
loss: 0.014084  [  100/ 3442]
loss: 0.029828  [  200/ 3442]
loss: 0.007308  [  300/ 3442]
loss: 0.011353  [  400/ 3442]
loss: 0.010020  [  500/ 3442]
loss: 0.026667  [  600/ 3442]
loss: 0.013235  [  700/ 3442]
loss: 0.016357  [  800/ 3442]
loss: 0.029702  [  900/ 3442]
loss: 0.014335  [ 1000/ 3442]
loss: 0.002833  [ 1100/ 3442]
loss: 0.016637  [ 1200/ 3442]
loss: 0.007851  [ 1300/ 3442]
loss: 0.010673  [ 1400/ 3442]
loss: 0.017480  [ 1500/ 3442]
loss: 0.017262  [ 1600/ 3442]
loss: 0.009861  [ 1700/ 3442]
loss: 0.018329  [ 1800/ 3442]
loss: 0.013208  [ 1900/ 3442]
loss: 0.015962  [ 2000/ 3442]
loss: 0.011228  [ 2100/ 3442]
loss: 0.011504  [ 2200/ 3442]
loss: 0.010616  [ 2300/ 3442]
loss: 0.014800  [ 2400/ 3442]
loss: 0.030415  [ 2500/ 3442]
loss: 0.122955  [ 2600/ 3442]
loss: 0.035005  [ 2700/ 3442]
loss: 0.008623  [ 2800/ 3442]
loss: 0.007434  [ 2900/ 3442]
loss: 0.005116  [ 3000/ 3442]
loss: 0.019544  [ 3100/ 3442]
loss: 0.119372  [ 3200/ 3442]
loss: 0.017469  [ 3300/ 3442]
loss: 0.027399  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.014111  [    0/ 3442]
loss: 0.014635  [  100/ 3442]
loss: 0.029796  [  200/ 3442]
loss: 0.007579  [  300/ 3442]
loss: 0.011559  [  400/ 3442]
loss: 0.010208  [  500/ 3442]
loss: 0.026653  [  600/ 3442]
loss: 0.013482  [  700/ 3442]
loss: 0.016533  [  800/ 3442]
loss: 0.029639  [  900/ 3442]
loss: 0.016257  [ 1000/ 3442]
loss: 0.002812  [ 1100/ 3442]
loss: 0.017914  [ 1200/ 3442]
loss: 0.007073  [ 1300/ 3442]
loss: 0.010191  [ 1400/ 3442]
loss: 0.017153  [ 1500/ 3442]
loss: 0.016443  [ 1600/ 3442]
loss: 0.010015  [ 1700/ 3442]
loss: 0.018398  [ 1800/ 3442]
loss: 0.014159  [ 1900/ 3442]
loss: 0.015985  [ 2000/ 3442]
loss: 0.010971  [ 2100/ 3442]
loss: 0.011315  [ 2200/ 3442]
loss: 0.010415  [ 2300/ 3442]
loss: 0.014245  [ 2400/ 3442]
loss: 0.030342  [ 2500/ 3442]
loss: 0.122562  [ 2600/ 3442]
loss: 0.035664  [ 2700/ 3442]
loss: 0.008578  [ 2800/ 3442]
loss: 0.007570  [ 2900/ 3442]
loss: 0.005039  [ 3000/ 3442]
loss: 0.019795  [ 3100/ 3442]
loss: 0.118322  [ 3200/ 3442]
loss: 0.015555  [ 3300/ 3442]
loss: 0.029933  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.014810  [    0/ 3442]
loss: 0.015089  [  100/ 3442]
loss: 0.029853  [  200/ 3442]
loss: 0.007546  [  300/ 3442]
loss: 0.011444  [  400/ 3442]
loss: 0.010147  [  500/ 3442]
loss: 0.026676  [  600/ 3442]
loss: 0.013577  [  700/ 3442]
loss: 0.016749  [  800/ 3442]
loss: 0.029764  [  900/ 3442]
loss: 0.016732  [ 1000/ 3442]
loss: 0.002879  [ 1100/ 3442]
loss: 0.018878  [ 1200/ 3442]
loss: 0.006126  [ 1300/ 3442]
loss: 0.009646  [ 1400/ 3442]
loss: 0.017101  [ 1500/ 3442]
loss: 0.014932  [ 1600/ 3442]
loss: 0.010238  [ 1700/ 3442]
loss: 0.018187  [ 1800/ 3442]
loss: 0.014923  [ 1900/ 3442]
loss: 0.015854  [ 2000/ 3442]
loss: 0.010840  [ 2100/ 3442]
loss: 0.011243  [ 2200/ 3442]
loss: 0.010278  [ 2300/ 3442]
loss: 0.013082  [ 2400/ 3442]
loss: 0.030605  [ 2500/ 3442]
loss: 0.122316  [ 2600/ 3442]
loss: 0.036580  [ 2700/ 3442]
loss: 0.008536  [ 2800/ 3442]
loss: 0.007711  [ 2900/ 3442]
loss: 0.004949  [ 3000/ 3442]
loss: 0.020092  [ 3100/ 3442]
loss: 0.117156  [ 3200/ 3442]
loss: 0.013668  [ 3300/ 3442]
loss: 0.032781  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [-1.239709   1.0424143]
[1 0 0 ... 1 2 0]
[0 2 0 ... 0 1 0]
Cluster 0 Occurrences: 1159; KMEANS: 1140
Cluster 1 Occurrences: 1156; KMEANS: 1129
Cluster 2 Occurrences: 1127; KMEANS: 1173
Centroids: [[0.058311675, 2.281597], [-0.42690563, 1.2486767], [-0.86032987, -0.6276136]]
Centroids: [[-0.4879184, 1.2260648], [-0.86490124, -0.63777405], [0.11695572, 2.304223]]
Contingency Matrix: 
[[  75    1 1083]
 [1057   11   88]
 [   8 1117    2]]
[[75, -1, 1083], [1057, -1, 88], [-1, -1, -1]]
[[-1, -1, -1], [1057, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 2, 1: 0}
New Contingency Matrix: 
[[1083   75    1]
 [  88 1057   11]
 [   2    8 1117]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1083, 1057, 1117], Sum: 3257
All_Elements: [1083, 75, 1, 88, 1057, 11, 2, 8, 1117], Sum: 3442
Accuracy: 0.9462521789657176
Done!
