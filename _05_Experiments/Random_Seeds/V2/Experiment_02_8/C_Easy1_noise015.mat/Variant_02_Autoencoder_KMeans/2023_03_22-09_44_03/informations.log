Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_03
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028B8542C978>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x0000028B85474198>
Epoch 1
-------------------------------
loss: 0.179919  [    0/ 3477]
loss: 0.303331  [  100/ 3477]
loss: 0.083665  [  200/ 3477]
loss: 0.103515  [  300/ 3477]
loss: 0.035061  [  400/ 3477]
loss: 0.075617  [  500/ 3477]
loss: 0.020694  [  600/ 3477]
loss: 0.023695  [  700/ 3477]
loss: 0.022366  [  800/ 3477]
loss: 0.023032  [  900/ 3477]
loss: 0.024466  [ 1000/ 3477]
loss: 0.003153  [ 1100/ 3477]
loss: 0.024636  [ 1200/ 3477]
loss: 0.019656  [ 1300/ 3477]
loss: 0.016125  [ 1400/ 3477]
loss: 0.023474  [ 1500/ 3477]
loss: 0.013710  [ 1600/ 3477]
loss: 0.011612  [ 1700/ 3477]
loss: 0.007594  [ 1800/ 3477]
loss: 0.015593  [ 1900/ 3477]
loss: 0.023520  [ 2000/ 3477]
loss: 0.023412  [ 2100/ 3477]
loss: 0.021567  [ 2200/ 3477]
loss: 0.012655  [ 2300/ 3477]
loss: 0.012493  [ 2400/ 3477]
loss: 0.104218  [ 2500/ 3477]
loss: 0.065256  [ 2600/ 3477]
loss: 0.008179  [ 2700/ 3477]
loss: 0.044765  [ 2800/ 3477]
loss: 0.021511  [ 2900/ 3477]
loss: 0.016445  [ 3000/ 3477]
loss: 0.012541  [ 3100/ 3477]
loss: 0.105472  [ 3200/ 3477]
loss: 0.008533  [ 3300/ 3477]
loss: 0.006556  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.012640  [    0/ 3477]
loss: 0.026823  [  100/ 3477]
loss: 0.028575  [  200/ 3477]
loss: 0.025485  [  300/ 3477]
loss: 0.016519  [  400/ 3477]
loss: 0.009832  [  500/ 3477]
loss: 0.014404  [  600/ 3477]
loss: 0.026534  [  700/ 3477]
loss: 0.020025  [  800/ 3477]
loss: 0.020514  [  900/ 3477]
loss: 0.019223  [ 1000/ 3477]
loss: 0.003047  [ 1100/ 3477]
loss: 0.025540  [ 1200/ 3477]
loss: 0.017660  [ 1300/ 3477]
loss: 0.015770  [ 1400/ 3477]
loss: 0.022308  [ 1500/ 3477]
loss: 0.012488  [ 1600/ 3477]
loss: 0.010537  [ 1700/ 3477]
loss: 0.006802  [ 1800/ 3477]
loss: 0.015635  [ 1900/ 3477]
loss: 0.023232  [ 2000/ 3477]
loss: 0.021200  [ 2100/ 3477]
loss: 0.021538  [ 2200/ 3477]
loss: 0.012510  [ 2300/ 3477]
loss: 0.012309  [ 2400/ 3477]
loss: 0.100927  [ 2500/ 3477]
loss: 0.063148  [ 2600/ 3477]
loss: 0.005643  [ 2700/ 3477]
loss: 0.062061  [ 2800/ 3477]
loss: 0.021680  [ 2900/ 3477]
loss: 0.013203  [ 3000/ 3477]
loss: 0.012023  [ 3100/ 3477]
loss: 0.099104  [ 3200/ 3477]
loss: 0.008927  [ 3300/ 3477]
loss: 0.007297  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.005418  [    0/ 3477]
loss: 0.018436  [  100/ 3477]
loss: 0.019276  [  200/ 3477]
loss: 0.025165  [  300/ 3477]
loss: 0.009787  [  400/ 3477]
loss: 0.012735  [  500/ 3477]
loss: 0.011767  [  600/ 3477]
loss: 0.026119  [  700/ 3477]
loss: 0.018718  [  800/ 3477]
loss: 0.018051  [  900/ 3477]
loss: 0.012839  [ 1000/ 3477]
loss: 0.005231  [ 1100/ 3477]
loss: 0.024224  [ 1200/ 3477]
loss: 0.016337  [ 1300/ 3477]
loss: 0.014879  [ 1400/ 3477]
loss: 0.020202  [ 1500/ 3477]
loss: 0.012180  [ 1600/ 3477]
loss: 0.009228  [ 1700/ 3477]
loss: 0.006829  [ 1800/ 3477]
loss: 0.013749  [ 1900/ 3477]
loss: 0.023501  [ 2000/ 3477]
loss: 0.016968  [ 2100/ 3477]
loss: 0.023177  [ 2200/ 3477]
loss: 0.013497  [ 2300/ 3477]
loss: 0.010398  [ 2400/ 3477]
loss: 0.090527  [ 2500/ 3477]
loss: 0.072011  [ 2600/ 3477]
loss: 0.005077  [ 2700/ 3477]
loss: 0.103713  [ 2800/ 3477]
loss: 0.021398  [ 2900/ 3477]
loss: 0.011772  [ 3000/ 3477]
loss: 0.012601  [ 3100/ 3477]
loss: 0.089111  [ 3200/ 3477]
loss: 0.010463  [ 3300/ 3477]
loss: 0.008527  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.003834  [    0/ 3477]
loss: 0.013573  [  100/ 3477]
loss: 0.015738  [  200/ 3477]
loss: 0.024323  [  300/ 3477]
loss: 0.011458  [  400/ 3477]
loss: 0.010451  [  500/ 3477]
loss: 0.010095  [  600/ 3477]
loss: 0.024895  [  700/ 3477]
loss: 0.019595  [  800/ 3477]
loss: 0.015474  [  900/ 3477]
loss: 0.010668  [ 1000/ 3477]
loss: 0.005762  [ 1100/ 3477]
loss: 0.022831  [ 1200/ 3477]
loss: 0.012961  [ 1300/ 3477]
loss: 0.013570  [ 1400/ 3477]
loss: 0.020342  [ 1500/ 3477]
loss: 0.012406  [ 1600/ 3477]
loss: 0.008800  [ 1700/ 3477]
loss: 0.007278  [ 1800/ 3477]
loss: 0.011838  [ 1900/ 3477]
loss: 0.023004  [ 2000/ 3477]
loss: 0.013841  [ 2100/ 3477]
loss: 0.023227  [ 2200/ 3477]
loss: 0.014548  [ 2300/ 3477]
loss: 0.009500  [ 2400/ 3477]
loss: 0.086978  [ 2500/ 3477]
loss: 0.075788  [ 2600/ 3477]
loss: 0.005155  [ 2700/ 3477]
loss: 0.097288  [ 2800/ 3477]
loss: 0.021076  [ 2900/ 3477]
loss: 0.011354  [ 3000/ 3477]
loss: 0.013184  [ 3100/ 3477]
loss: 0.085034  [ 3200/ 3477]
loss: 0.010168  [ 3300/ 3477]
loss: 0.008696  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.003693  [    0/ 3477]
loss: 0.011954  [  100/ 3477]
loss: 0.013860  [  200/ 3477]
loss: 0.023314  [  300/ 3477]
loss: 0.014040  [  400/ 3477]
loss: 0.010116  [  500/ 3477]
loss: 0.008812  [  600/ 3477]
loss: 0.023165  [  700/ 3477]
loss: 0.020075  [  800/ 3477]
loss: 0.013929  [  900/ 3477]
loss: 0.009829  [ 1000/ 3477]
loss: 0.004197  [ 1100/ 3477]
loss: 0.021447  [ 1200/ 3477]
loss: 0.010655  [ 1300/ 3477]
loss: 0.012743  [ 1400/ 3477]
loss: 0.020409  [ 1500/ 3477]
loss: 0.011530  [ 1600/ 3477]
loss: 0.008429  [ 1700/ 3477]
loss: 0.007716  [ 1800/ 3477]
loss: 0.010391  [ 1900/ 3477]
loss: 0.022304  [ 2000/ 3477]
loss: 0.011556  [ 2100/ 3477]
loss: 0.022951  [ 2200/ 3477]
loss: 0.015032  [ 2300/ 3477]
loss: 0.008907  [ 2400/ 3477]
loss: 0.085922  [ 2500/ 3477]
loss: 0.072523  [ 2600/ 3477]
loss: 0.005556  [ 2700/ 3477]
loss: 0.079425  [ 2800/ 3477]
loss: 0.020418  [ 2900/ 3477]
loss: 0.011460  [ 3000/ 3477]
loss: 0.013788  [ 3100/ 3477]
loss: 0.084409  [ 3200/ 3477]
loss: 0.008842  [ 3300/ 3477]
loss: 0.008998  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.003824  [    0/ 3477]
loss: 0.013786  [  100/ 3477]
loss: 0.013200  [  200/ 3477]
loss: 0.023027  [  300/ 3477]
loss: 0.011531  [  400/ 3477]
loss: 0.010616  [  500/ 3477]
loss: 0.007732  [  600/ 3477]
loss: 0.021987  [  700/ 3477]
loss: 0.020415  [  800/ 3477]
loss: 0.013100  [  900/ 3477]
loss: 0.009351  [ 1000/ 3477]
loss: 0.004520  [ 1100/ 3477]
loss: 0.020132  [ 1200/ 3477]
loss: 0.009600  [ 1300/ 3477]
loss: 0.012143  [ 1400/ 3477]
loss: 0.020252  [ 1500/ 3477]
loss: 0.010478  [ 1600/ 3477]
loss: 0.008176  [ 1700/ 3477]
loss: 0.008154  [ 1800/ 3477]
loss: 0.009805  [ 1900/ 3477]
loss: 0.021848  [ 2000/ 3477]
loss: 0.009162  [ 2100/ 3477]
loss: 0.022903  [ 2200/ 3477]
loss: 0.015244  [ 2300/ 3477]
loss: 0.008756  [ 2400/ 3477]
loss: 0.084927  [ 2500/ 3477]
loss: 0.073211  [ 2600/ 3477]
loss: 0.005674  [ 2700/ 3477]
loss: 0.066527  [ 2800/ 3477]
loss: 0.019720  [ 2900/ 3477]
loss: 0.011709  [ 3000/ 3477]
loss: 0.014196  [ 3100/ 3477]
loss: 0.084326  [ 3200/ 3477]
loss: 0.008079  [ 3300/ 3477]
loss: 0.008522  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.004163  [    0/ 3477]
loss: 0.014541  [  100/ 3477]
loss: 0.012670  [  200/ 3477]
loss: 0.022857  [  300/ 3477]
loss: 0.012382  [  400/ 3477]
loss: 0.011241  [  500/ 3477]
loss: 0.006907  [  600/ 3477]
loss: 0.021235  [  700/ 3477]
loss: 0.020692  [  800/ 3477]
loss: 0.012390  [  900/ 3477]
loss: 0.009341  [ 1000/ 3477]
loss: 0.004844  [ 1100/ 3477]
loss: 0.019030  [ 1200/ 3477]
loss: 0.009601  [ 1300/ 3477]
loss: 0.011705  [ 1400/ 3477]
loss: 0.019884  [ 1500/ 3477]
loss: 0.010293  [ 1600/ 3477]
loss: 0.008088  [ 1700/ 3477]
loss: 0.008744  [ 1800/ 3477]
loss: 0.009375  [ 1900/ 3477]
loss: 0.021381  [ 2000/ 3477]
loss: 0.007365  [ 2100/ 3477]
loss: 0.022849  [ 2200/ 3477]
loss: 0.015608  [ 2300/ 3477]
loss: 0.008854  [ 2400/ 3477]
loss: 0.084552  [ 2500/ 3477]
loss: 0.073426  [ 2600/ 3477]
loss: 0.005633  [ 2700/ 3477]
loss: 0.059869  [ 2800/ 3477]
loss: 0.019363  [ 2900/ 3477]
loss: 0.011859  [ 3000/ 3477]
loss: 0.014402  [ 3100/ 3477]
loss: 0.083841  [ 3200/ 3477]
loss: 0.007730  [ 3300/ 3477]
loss: 0.008161  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.004870  [    0/ 3477]
loss: 0.014841  [  100/ 3477]
loss: 0.012661  [  200/ 3477]
loss: 0.022802  [  300/ 3477]
loss: 0.012386  [  400/ 3477]
loss: 0.011693  [  500/ 3477]
loss: 0.006611  [  600/ 3477]
loss: 0.020689  [  700/ 3477]
loss: 0.020943  [  800/ 3477]
loss: 0.012238  [  900/ 3477]
loss: 0.009441  [ 1000/ 3477]
loss: 0.005202  [ 1100/ 3477]
loss: 0.018146  [ 1200/ 3477]
loss: 0.009628  [ 1300/ 3477]
loss: 0.011369  [ 1400/ 3477]
loss: 0.019982  [ 1500/ 3477]
loss: 0.010337  [ 1600/ 3477]
loss: 0.008141  [ 1700/ 3477]
loss: 0.009334  [ 1800/ 3477]
loss: 0.008926  [ 1900/ 3477]
loss: 0.020696  [ 2000/ 3477]
loss: 0.006117  [ 2100/ 3477]
loss: 0.022316  [ 2200/ 3477]
loss: 0.015695  [ 2300/ 3477]
loss: 0.009193  [ 2400/ 3477]
loss: 0.084414  [ 2500/ 3477]
loss: 0.073380  [ 2600/ 3477]
loss: 0.005643  [ 2700/ 3477]
loss: 0.061889  [ 2800/ 3477]
loss: 0.019177  [ 2900/ 3477]
loss: 0.012112  [ 3000/ 3477]
loss: 0.014724  [ 3100/ 3477]
loss: 0.083394  [ 3200/ 3477]
loss: 0.007433  [ 3300/ 3477]
loss: 0.007922  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [-0.31218174  2.2897825 ]
[1 0 2 ... 1 1 1]
[1 2 0 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1176
Cluster 1 Occurrences: 1188; KMEANS: 1173
Cluster 2 Occurrences: 1157; KMEANS: 1128
Centroids: [[2.1144927, -0.60655403], [-0.33855438, 1.7405652], [-0.8213435, -2.0066984]]
Centroids: [[-0.8330317, -1.987401], [-0.32445177, 1.7680914], [2.128844, -0.60050124]]
Contingency Matrix: 
[[   8    1 1123]
 [  12 1172    4]
 [1156    0    1]]
[[8, -1, 1123], [-1, -1, -1], [1156, -1, 1]]
[[-1, -1, 1123], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1123    1    8]
 [   4 1172   12]
 [   1    0 1156]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1123, 1172, 1156], Sum: 3451
All_Elements: [1123, 1, 8, 4, 1172, 12, 1, 0, 1156], Sum: 3477
Accuracy: 0.9925222893298821
Done!
