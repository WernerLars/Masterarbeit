Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028B8542C358>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000028B85474F98>
Epoch 1
-------------------------------
loss: 0.298027  [    0/ 3475]
loss: 0.125895  [  100/ 3475]
loss: 0.377625  [  200/ 3475]
loss: 0.240295  [  300/ 3475]
loss: 0.137143  [  400/ 3475]
loss: 0.050713  [  500/ 3475]
loss: 0.068550  [  600/ 3475]
loss: 0.089962  [  700/ 3475]
loss: 0.113404  [  800/ 3475]
loss: 0.057985  [  900/ 3475]
loss: 0.045140  [ 1000/ 3475]
loss: 0.091249  [ 1100/ 3475]
loss: 0.028025  [ 1200/ 3475]
loss: 0.031079  [ 1300/ 3475]
loss: 0.050834  [ 1400/ 3475]
loss: 0.050220  [ 1500/ 3475]
loss: 0.040046  [ 1600/ 3475]
loss: 0.025375  [ 1700/ 3475]
loss: 0.073353  [ 1800/ 3475]
loss: 0.036119  [ 1900/ 3475]
loss: 0.023195  [ 2000/ 3475]
loss: 0.052079  [ 2100/ 3475]
loss: 0.104658  [ 2200/ 3475]
loss: 0.068853  [ 2300/ 3475]
loss: 0.017481  [ 2400/ 3475]
loss: 0.081956  [ 2500/ 3475]
loss: 0.040542  [ 2600/ 3475]
loss: 0.072561  [ 2700/ 3475]
loss: 0.102949  [ 2800/ 3475]
loss: 0.057220  [ 2900/ 3475]
loss: 0.051131  [ 3000/ 3475]
loss: 0.106847  [ 3100/ 3475]
loss: 0.118277  [ 3200/ 3475]
loss: 0.057789  [ 3300/ 3475]
loss: 0.064686  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.087937  [    0/ 3475]
loss: 0.054639  [  100/ 3475]
loss: 0.055396  [  200/ 3475]
loss: 0.230732  [  300/ 3475]
loss: 0.055605  [  400/ 3475]
loss: 0.028070  [  500/ 3475]
loss: 0.019985  [  600/ 3475]
loss: 0.082515  [  700/ 3475]
loss: 0.114158  [  800/ 3475]
loss: 0.050897  [  900/ 3475]
loss: 0.051223  [ 1000/ 3475]
loss: 0.037545  [ 1100/ 3475]
loss: 0.025424  [ 1200/ 3475]
loss: 0.040579  [ 1300/ 3475]
loss: 0.047213  [ 1400/ 3475]
loss: 0.044394  [ 1500/ 3475]
loss: 0.052635  [ 1600/ 3475]
loss: 0.016572  [ 1700/ 3475]
loss: 0.075932  [ 1800/ 3475]
loss: 0.034887  [ 1900/ 3475]
loss: 0.019538  [ 2000/ 3475]
loss: 0.056423  [ 2100/ 3475]
loss: 0.081148  [ 2200/ 3475]
loss: 0.024765  [ 2300/ 3475]
loss: 0.017282  [ 2400/ 3475]
loss: 0.076380  [ 2500/ 3475]
loss: 0.034362  [ 2600/ 3475]
loss: 0.068651  [ 2700/ 3475]
loss: 0.098261  [ 2800/ 3475]
loss: 0.023214  [ 2900/ 3475]
loss: 0.051426  [ 3000/ 3475]
loss: 0.073483  [ 3100/ 3475]
loss: 0.072589  [ 3200/ 3475]
loss: 0.041699  [ 3300/ 3475]
loss: 0.027657  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.034948  [    0/ 3475]
loss: 0.037039  [  100/ 3475]
loss: 0.052450  [  200/ 3475]
loss: 0.219167  [  300/ 3475]
loss: 0.021765  [  400/ 3475]
loss: 0.028790  [  500/ 3475]
loss: 0.019444  [  600/ 3475]
loss: 0.090057  [  700/ 3475]
loss: 0.106108  [  800/ 3475]
loss: 0.051464  [  900/ 3475]
loss: 0.043515  [ 1000/ 3475]
loss: 0.023413  [ 1100/ 3475]
loss: 0.023385  [ 1200/ 3475]
loss: 0.038838  [ 1300/ 3475]
loss: 0.046626  [ 1400/ 3475]
loss: 0.038193  [ 1500/ 3475]
loss: 0.057913  [ 1600/ 3475]
loss: 0.015082  [ 1700/ 3475]
loss: 0.083511  [ 1800/ 3475]
loss: 0.035248  [ 1900/ 3475]
loss: 0.017475  [ 2000/ 3475]
loss: 0.058316  [ 2100/ 3475]
loss: 0.073931  [ 2200/ 3475]
loss: 0.020837  [ 2300/ 3475]
loss: 0.018476  [ 2400/ 3475]
loss: 0.076782  [ 2500/ 3475]
loss: 0.030882  [ 2600/ 3475]
loss: 0.086590  [ 2700/ 3475]
loss: 0.075974  [ 2800/ 3475]
loss: 0.023973  [ 2900/ 3475]
loss: 0.051235  [ 3000/ 3475]
loss: 0.063313  [ 3100/ 3475]
loss: 0.057420  [ 3200/ 3475]
loss: 0.028797  [ 3300/ 3475]
loss: 0.023646  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.028658  [    0/ 3475]
loss: 0.030800  [  100/ 3475]
loss: 0.052621  [  200/ 3475]
loss: 0.218252  [  300/ 3475]
loss: 0.018178  [  400/ 3475]
loss: 0.028457  [  500/ 3475]
loss: 0.016745  [  600/ 3475]
loss: 0.094250  [  700/ 3475]
loss: 0.105259  [  800/ 3475]
loss: 0.046093  [  900/ 3475]
loss: 0.040854  [ 1000/ 3475]
loss: 0.022231  [ 1100/ 3475]
loss: 0.022352  [ 1200/ 3475]
loss: 0.041100  [ 1300/ 3475]
loss: 0.045375  [ 1400/ 3475]
loss: 0.038452  [ 1500/ 3475]
loss: 0.056005  [ 1600/ 3475]
loss: 0.014055  [ 1700/ 3475]
loss: 0.093065  [ 1800/ 3475]
loss: 0.034993  [ 1900/ 3475]
loss: 0.016473  [ 2000/ 3475]
loss: 0.059538  [ 2100/ 3475]
loss: 0.073318  [ 2200/ 3475]
loss: 0.021743  [ 2300/ 3475]
loss: 0.017623  [ 2400/ 3475]
loss: 0.078604  [ 2500/ 3475]
loss: 0.029170  [ 2600/ 3475]
loss: 0.110353  [ 2700/ 3475]
loss: 0.049879  [ 2800/ 3475]
loss: 0.026172  [ 2900/ 3475]
loss: 0.050490  [ 3000/ 3475]
loss: 0.061950  [ 3100/ 3475]
loss: 0.051396  [ 3200/ 3475]
loss: 0.013205  [ 3300/ 3475]
loss: 0.024076  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.026873  [    0/ 3475]
loss: 0.027271  [  100/ 3475]
loss: 0.052388  [  200/ 3475]
loss: 0.218561  [  300/ 3475]
loss: 0.018447  [  400/ 3475]
loss: 0.027472  [  500/ 3475]
loss: 0.018324  [  600/ 3475]
loss: 0.087738  [  700/ 3475]
loss: 0.105498  [  800/ 3475]
loss: 0.043618  [  900/ 3475]
loss: 0.039918  [ 1000/ 3475]
loss: 0.021610  [ 1100/ 3475]
loss: 0.021757  [ 1200/ 3475]
loss: 0.041339  [ 1300/ 3475]
loss: 0.045630  [ 1400/ 3475]
loss: 0.039160  [ 1500/ 3475]
loss: 0.050399  [ 1600/ 3475]
loss: 0.012536  [ 1700/ 3475]
loss: 0.095925  [ 1800/ 3475]
loss: 0.035898  [ 1900/ 3475]
loss: 0.016216  [ 2000/ 3475]
loss: 0.054780  [ 2100/ 3475]
loss: 0.076877  [ 2200/ 3475]
loss: 0.024466  [ 2300/ 3475]
loss: 0.016371  [ 2400/ 3475]
loss: 0.080280  [ 2500/ 3475]
loss: 0.028874  [ 2600/ 3475]
loss: 0.126687  [ 2700/ 3475]
loss: 0.033702  [ 2800/ 3475]
loss: 0.029077  [ 2900/ 3475]
loss: 0.050665  [ 3000/ 3475]
loss: 0.062694  [ 3100/ 3475]
loss: 0.051335  [ 3200/ 3475]
loss: 0.011229  [ 3300/ 3475]
loss: 0.025242  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.029294  [    0/ 3475]
loss: 0.025591  [  100/ 3475]
loss: 0.052878  [  200/ 3475]
loss: 0.218212  [  300/ 3475]
loss: 0.019157  [  400/ 3475]
loss: 0.027802  [  500/ 3475]
loss: 0.018729  [  600/ 3475]
loss: 0.079244  [  700/ 3475]
loss: 0.105495  [  800/ 3475]
loss: 0.042571  [  900/ 3475]
loss: 0.038709  [ 1000/ 3475]
loss: 0.021249  [ 1100/ 3475]
loss: 0.021592  [ 1200/ 3475]
loss: 0.039885  [ 1300/ 3475]
loss: 0.045337  [ 1400/ 3475]
loss: 0.039468  [ 1500/ 3475]
loss: 0.047334  [ 1600/ 3475]
loss: 0.012002  [ 1700/ 3475]
loss: 0.094876  [ 1800/ 3475]
loss: 0.036164  [ 1900/ 3475]
loss: 0.016406  [ 2000/ 3475]
loss: 0.048586  [ 2100/ 3475]
loss: 0.077493  [ 2200/ 3475]
loss: 0.025206  [ 2300/ 3475]
loss: 0.016449  [ 2400/ 3475]
loss: 0.080985  [ 2500/ 3475]
loss: 0.029183  [ 2600/ 3475]
loss: 0.126945  [ 2700/ 3475]
loss: 0.029574  [ 2800/ 3475]
loss: 0.031239  [ 2900/ 3475]
loss: 0.050493  [ 3000/ 3475]
loss: 0.063816  [ 3100/ 3475]
loss: 0.049590  [ 3200/ 3475]
loss: 0.011401  [ 3300/ 3475]
loss: 0.026038  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.031070  [    0/ 3475]
loss: 0.024407  [  100/ 3475]
loss: 0.052713  [  200/ 3475]
loss: 0.217440  [  300/ 3475]
loss: 0.019443  [  400/ 3475]
loss: 0.028649  [  500/ 3475]
loss: 0.018934  [  600/ 3475]
loss: 0.072162  [  700/ 3475]
loss: 0.106300  [  800/ 3475]
loss: 0.041905  [  900/ 3475]
loss: 0.038369  [ 1000/ 3475]
loss: 0.020386  [ 1100/ 3475]
loss: 0.021490  [ 1200/ 3475]
loss: 0.037502  [ 1300/ 3475]
loss: 0.045180  [ 1400/ 3475]
loss: 0.039583  [ 1500/ 3475]
loss: 0.044085  [ 1600/ 3475]
loss: 0.012037  [ 1700/ 3475]
loss: 0.090301  [ 1800/ 3475]
loss: 0.036331  [ 1900/ 3475]
loss: 0.016476  [ 2000/ 3475]
loss: 0.042414  [ 2100/ 3475]
loss: 0.077334  [ 2200/ 3475]
loss: 0.025971  [ 2300/ 3475]
loss: 0.016487  [ 2400/ 3475]
loss: 0.082051  [ 2500/ 3475]
loss: 0.029331  [ 2600/ 3475]
loss: 0.123646  [ 2700/ 3475]
loss: 0.029856  [ 2800/ 3475]
loss: 0.033092  [ 2900/ 3475]
loss: 0.050073  [ 3000/ 3475]
loss: 0.063346  [ 3100/ 3475]
loss: 0.049519  [ 3200/ 3475]
loss: 0.013203  [ 3300/ 3475]
loss: 0.026207  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.032352  [    0/ 3475]
loss: 0.023823  [  100/ 3475]
loss: 0.052661  [  200/ 3475]
loss: 0.216139  [  300/ 3475]
loss: 0.019482  [  400/ 3475]
loss: 0.028499  [  500/ 3475]
loss: 0.019437  [  600/ 3475]
loss: 0.067124  [  700/ 3475]
loss: 0.106191  [  800/ 3475]
loss: 0.042012  [  900/ 3475]
loss: 0.037570  [ 1000/ 3475]
loss: 0.019715  [ 1100/ 3475]
loss: 0.021506  [ 1200/ 3475]
loss: 0.036422  [ 1300/ 3475]
loss: 0.044104  [ 1400/ 3475]
loss: 0.039619  [ 1500/ 3475]
loss: 0.042221  [ 1600/ 3475]
loss: 0.012025  [ 1700/ 3475]
loss: 0.084596  [ 1800/ 3475]
loss: 0.036206  [ 1900/ 3475]
loss: 0.016699  [ 2000/ 3475]
loss: 0.038483  [ 2100/ 3475]
loss: 0.077291  [ 2200/ 3475]
loss: 0.025596  [ 2300/ 3475]
loss: 0.016729  [ 2400/ 3475]
loss: 0.082827  [ 2500/ 3475]
loss: 0.028120  [ 2600/ 3475]
loss: 0.118956  [ 2700/ 3475]
loss: 0.030484  [ 2800/ 3475]
loss: 0.035026  [ 2900/ 3475]
loss: 0.049849  [ 3000/ 3475]
loss: 0.063204  [ 3100/ 3475]
loss: 0.048513  [ 3200/ 3475]
loss: 0.015022  [ 3300/ 3475]
loss: 0.026639  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [-0.7901316   0.37903443]
[2 1 2 ... 1 0 1]
[0 1 0 ... 2 0 1]
Cluster 0 Occurrences: 1162; KMEANS: 2319
Cluster 1 Occurrences: 1164; KMEANS: 591
Cluster 2 Occurrences: 1149; KMEANS: 565
Centroids: [[0.052147847, -0.6164408], [3.0263093, 4.8103776], [-1.4555475, 0.07873148]]
Centroids: [[-0.70185494, -0.2788135], [2.3500881, 3.6868312], [3.804417, 6.0904236]]
Contingency Matrix: 
[[1162    0    0]
 [  16  583  565]
 [1141    8    0]]
[[-1, -1, -1], [-1, 583, 565], [-1, 8, 0]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 0]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 1: 1, 2: 2}
New Contingency Matrix: 
[[1162    0    0]
 [  16  583  565]
 [1141    8    0]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1162, 583, 0], Sum: 1745
All_Elements: [1162, 0, 0, 16, 583, 565, 1141, 8, 0], Sum: 3475
Accuracy: 0.5021582733812949
Done!
