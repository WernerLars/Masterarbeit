Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_55_12
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028BCC58C048>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x0000028B85474A90>
Epoch 1
-------------------------------
loss: 0.172326  [    0/ 3526]
loss: 0.242694  [  100/ 3526]
loss: 0.136465  [  200/ 3526]
loss: 0.084730  [  300/ 3526]
loss: 0.020527  [  400/ 3526]
loss: 0.061014  [  500/ 3526]
loss: 0.037876  [  600/ 3526]
loss: 0.022346  [  700/ 3526]
loss: 0.059501  [  800/ 3526]
loss: 0.013405  [  900/ 3526]
loss: 0.037032  [ 1000/ 3526]
loss: 0.028942  [ 1100/ 3526]
loss: 0.045515  [ 1200/ 3526]
loss: 0.039226  [ 1300/ 3526]
loss: 0.030134  [ 1400/ 3526]
loss: 0.037149  [ 1500/ 3526]
loss: 0.037121  [ 1600/ 3526]
loss: 0.020338  [ 1700/ 3526]
loss: 0.026873  [ 1800/ 3526]
loss: 0.029190  [ 1900/ 3526]
loss: 0.020001  [ 2000/ 3526]
loss: 0.179908  [ 2100/ 3526]
loss: 0.036542  [ 2200/ 3526]
loss: 0.016165  [ 2300/ 3526]
loss: 0.010118  [ 2400/ 3526]
loss: 0.014931  [ 2500/ 3526]
loss: 0.035876  [ 2600/ 3526]
loss: 0.030606  [ 2700/ 3526]
loss: 0.017136  [ 2800/ 3526]
loss: 0.016735  [ 2900/ 3526]
loss: 0.036639  [ 3000/ 3526]
loss: 0.180623  [ 3100/ 3526]
loss: 0.032230  [ 3200/ 3526]
loss: 0.021821  [ 3300/ 3526]
loss: 0.036046  [ 3400/ 3526]
loss: 0.048129  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.010333  [    0/ 3526]
loss: 0.038849  [  100/ 3526]
loss: 0.034955  [  200/ 3526]
loss: 0.008335  [  300/ 3526]
loss: 0.015701  [  400/ 3526]
loss: 0.054114  [  500/ 3526]
loss: 0.016909  [  600/ 3526]
loss: 0.021259  [  700/ 3526]
loss: 0.036918  [  800/ 3526]
loss: 0.011112  [  900/ 3526]
loss: 0.025952  [ 1000/ 3526]
loss: 0.016686  [ 1100/ 3526]
loss: 0.036768  [ 1200/ 3526]
loss: 0.028407  [ 1300/ 3526]
loss: 0.031006  [ 1400/ 3526]
loss: 0.026615  [ 1500/ 3526]
loss: 0.036365  [ 1600/ 3526]
loss: 0.014475  [ 1700/ 3526]
loss: 0.024857  [ 1800/ 3526]
loss: 0.020240  [ 1900/ 3526]
loss: 0.017637  [ 2000/ 3526]
loss: 0.167710  [ 2100/ 3526]
loss: 0.031416  [ 2200/ 3526]
loss: 0.016284  [ 2300/ 3526]
loss: 0.010919  [ 2400/ 3526]
loss: 0.014683  [ 2500/ 3526]
loss: 0.022979  [ 2600/ 3526]
loss: 0.022824  [ 2700/ 3526]
loss: 0.016480  [ 2800/ 3526]
loss: 0.016051  [ 2900/ 3526]
loss: 0.034699  [ 3000/ 3526]
loss: 0.176281  [ 3100/ 3526]
loss: 0.032603  [ 3200/ 3526]
loss: 0.021032  [ 3300/ 3526]
loss: 0.032862  [ 3400/ 3526]
loss: 0.040598  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.011113  [    0/ 3526]
loss: 0.041860  [  100/ 3526]
loss: 0.039825  [  200/ 3526]
loss: 0.010514  [  300/ 3526]
loss: 0.010748  [  400/ 3526]
loss: 0.046879  [  500/ 3526]
loss: 0.013420  [  600/ 3526]
loss: 0.020344  [  700/ 3526]
loss: 0.031688  [  800/ 3526]
loss: 0.011645  [  900/ 3526]
loss: 0.019848  [ 1000/ 3526]
loss: 0.014719  [ 1100/ 3526]
loss: 0.029885  [ 1200/ 3526]
loss: 0.023183  [ 1300/ 3526]
loss: 0.031913  [ 1400/ 3526]
loss: 0.022302  [ 1500/ 3526]
loss: 0.040955  [ 1600/ 3526]
loss: 0.015093  [ 1700/ 3526]
loss: 0.025817  [ 1800/ 3526]
loss: 0.017853  [ 1900/ 3526]
loss: 0.016522  [ 2000/ 3526]
loss: 0.209656  [ 2100/ 3526]
loss: 0.031593  [ 2200/ 3526]
loss: 0.016536  [ 2300/ 3526]
loss: 0.012693  [ 2400/ 3526]
loss: 0.014485  [ 2500/ 3526]
loss: 0.019981  [ 2600/ 3526]
loss: 0.015816  [ 2700/ 3526]
loss: 0.015043  [ 2800/ 3526]
loss: 0.012654  [ 2900/ 3526]
loss: 0.038881  [ 3000/ 3526]
loss: 0.165699  [ 3100/ 3526]
loss: 0.030156  [ 3200/ 3526]
loss: 0.023087  [ 3300/ 3526]
loss: 0.026115  [ 3400/ 3526]
loss: 0.033074  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.011192  [    0/ 3526]
loss: 0.039069  [  100/ 3526]
loss: 0.039660  [  200/ 3526]
loss: 0.010821  [  300/ 3526]
loss: 0.008222  [  400/ 3526]
loss: 0.041387  [  500/ 3526]
loss: 0.010995  [  600/ 3526]
loss: 0.017996  [  700/ 3526]
loss: 0.022770  [  800/ 3526]
loss: 0.012028  [  900/ 3526]
loss: 0.016753  [ 1000/ 3526]
loss: 0.014258  [ 1100/ 3526]
loss: 0.025352  [ 1200/ 3526]
loss: 0.021204  [ 1300/ 3526]
loss: 0.032623  [ 1400/ 3526]
loss: 0.019470  [ 1500/ 3526]
loss: 0.047564  [ 1600/ 3526]
loss: 0.018064  [ 1700/ 3526]
loss: 0.024934  [ 1800/ 3526]
loss: 0.017149  [ 1900/ 3526]
loss: 0.016606  [ 2000/ 3526]
loss: 0.239360  [ 2100/ 3526]
loss: 0.031386  [ 2200/ 3526]
loss: 0.017003  [ 2300/ 3526]
loss: 0.014064  [ 2400/ 3526]
loss: 0.014076  [ 2500/ 3526]
loss: 0.018037  [ 2600/ 3526]
loss: 0.014202  [ 2700/ 3526]
loss: 0.012534  [ 2800/ 3526]
loss: 0.010545  [ 2900/ 3526]
loss: 0.045351  [ 3000/ 3526]
loss: 0.157562  [ 3100/ 3526]
loss: 0.028831  [ 3200/ 3526]
loss: 0.023743  [ 3300/ 3526]
loss: 0.021641  [ 3400/ 3526]
loss: 0.029743  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.011487  [    0/ 3526]
loss: 0.036779  [  100/ 3526]
loss: 0.038702  [  200/ 3526]
loss: 0.009938  [  300/ 3526]
loss: 0.008069  [  400/ 3526]
loss: 0.038878  [  500/ 3526]
loss: 0.010171  [  600/ 3526]
loss: 0.016199  [  700/ 3526]
loss: 0.021132  [  800/ 3526]
loss: 0.012078  [  900/ 3526]
loss: 0.016507  [ 1000/ 3526]
loss: 0.014059  [ 1100/ 3526]
loss: 0.023794  [ 1200/ 3526]
loss: 0.021522  [ 1300/ 3526]
loss: 0.032855  [ 1400/ 3526]
loss: 0.019627  [ 1500/ 3526]
loss: 0.052587  [ 1600/ 3526]
loss: 0.021169  [ 1700/ 3526]
loss: 0.024514  [ 1800/ 3526]
loss: 0.016841  [ 1900/ 3526]
loss: 0.016870  [ 2000/ 3526]
loss: 0.256174  [ 2100/ 3526]
loss: 0.031263  [ 2200/ 3526]
loss: 0.017011  [ 2300/ 3526]
loss: 0.014434  [ 2400/ 3526]
loss: 0.013776  [ 2500/ 3526]
loss: 0.017063  [ 2600/ 3526]
loss: 0.014000  [ 2700/ 3526]
loss: 0.011636  [ 2800/ 3526]
loss: 0.009791  [ 2900/ 3526]
loss: 0.048617  [ 3000/ 3526]
loss: 0.153713  [ 3100/ 3526]
loss: 0.028015  [ 3200/ 3526]
loss: 0.024250  [ 3300/ 3526]
loss: 0.018801  [ 3400/ 3526]
loss: 0.029253  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.011792  [    0/ 3526]
loss: 0.035752  [  100/ 3526]
loss: 0.037603  [  200/ 3526]
loss: 0.009752  [  300/ 3526]
loss: 0.008589  [  400/ 3526]
loss: 0.038057  [  500/ 3526]
loss: 0.009712  [  600/ 3526]
loss: 0.015037  [  700/ 3526]
loss: 0.021361  [  800/ 3526]
loss: 0.012075  [  900/ 3526]
loss: 0.016460  [ 1000/ 3526]
loss: 0.013959  [ 1100/ 3526]
loss: 0.022672  [ 1200/ 3526]
loss: 0.023131  [ 1300/ 3526]
loss: 0.033192  [ 1400/ 3526]
loss: 0.020339  [ 1500/ 3526]
loss: 0.055335  [ 1600/ 3526]
loss: 0.023235  [ 1700/ 3526]
loss: 0.024481  [ 1800/ 3526]
loss: 0.016502  [ 1900/ 3526]
loss: 0.017206  [ 2000/ 3526]
loss: 0.253659  [ 2100/ 3526]
loss: 0.031590  [ 2200/ 3526]
loss: 0.017024  [ 2300/ 3526]
loss: 0.014551  [ 2400/ 3526]
loss: 0.013670  [ 2500/ 3526]
loss: 0.016714  [ 2600/ 3526]
loss: 0.013929  [ 2700/ 3526]
loss: 0.011289  [ 2800/ 3526]
loss: 0.009442  [ 2900/ 3526]
loss: 0.050544  [ 3000/ 3526]
loss: 0.152693  [ 3100/ 3526]
loss: 0.027191  [ 3200/ 3526]
loss: 0.024447  [ 3300/ 3526]
loss: 0.017287  [ 3400/ 3526]
loss: 0.029358  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.012043  [    0/ 3526]
loss: 0.035402  [  100/ 3526]
loss: 0.036831  [  200/ 3526]
loss: 0.009886  [  300/ 3526]
loss: 0.009074  [  400/ 3526]
loss: 0.038012  [  500/ 3526]
loss: 0.009471  [  600/ 3526]
loss: 0.014388  [  700/ 3526]
loss: 0.021865  [  800/ 3526]
loss: 0.011932  [  900/ 3526]
loss: 0.016569  [ 1000/ 3526]
loss: 0.013902  [ 1100/ 3526]
loss: 0.022094  [ 1200/ 3526]
loss: 0.023665  [ 1300/ 3526]
loss: 0.033395  [ 1400/ 3526]
loss: 0.020591  [ 1500/ 3526]
loss: 0.056139  [ 1600/ 3526]
loss: 0.025236  [ 1700/ 3526]
loss: 0.024566  [ 1800/ 3526]
loss: 0.016109  [ 1900/ 3526]
loss: 0.017457  [ 2000/ 3526]
loss: 0.239676  [ 2100/ 3526]
loss: 0.032400  [ 2200/ 3526]
loss: 0.017127  [ 2300/ 3526]
loss: 0.014583  [ 2400/ 3526]
loss: 0.013636  [ 2500/ 3526]
loss: 0.016767  [ 2600/ 3526]
loss: 0.013971  [ 2700/ 3526]
loss: 0.011187  [ 2800/ 3526]
loss: 0.009222  [ 2900/ 3526]
loss: 0.051667  [ 3000/ 3526]
loss: 0.151512  [ 3100/ 3526]
loss: 0.026895  [ 3200/ 3526]
loss: 0.024553  [ 3300/ 3526]
loss: 0.016085  [ 3400/ 3526]
loss: 0.029528  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.012269  [    0/ 3526]
loss: 0.033364  [  100/ 3526]
loss: 0.036361  [  200/ 3526]
loss: 0.010046  [  300/ 3526]
loss: 0.009506  [  400/ 3526]
loss: 0.038082  [  500/ 3526]
loss: 0.009300  [  600/ 3526]
loss: 0.014058  [  700/ 3526]
loss: 0.022145  [  800/ 3526]
loss: 0.011768  [  900/ 3526]
loss: 0.016815  [ 1000/ 3526]
loss: 0.013800  [ 1100/ 3526]
loss: 0.021660  [ 1200/ 3526]
loss: 0.023297  [ 1300/ 3526]
loss: 0.033988  [ 1400/ 3526]
loss: 0.020616  [ 1500/ 3526]
loss: 0.055012  [ 1600/ 3526]
loss: 0.025925  [ 1700/ 3526]
loss: 0.024707  [ 1800/ 3526]
loss: 0.015679  [ 1900/ 3526]
loss: 0.017529  [ 2000/ 3526]
loss: 0.239677  [ 2100/ 3526]
loss: 0.032391  [ 2200/ 3526]
loss: 0.017136  [ 2300/ 3526]
loss: 0.014514  [ 2400/ 3526]
loss: 0.013617  [ 2500/ 3526]
loss: 0.016634  [ 2600/ 3526]
loss: 0.013983  [ 2700/ 3526]
loss: 0.011091  [ 2800/ 3526]
loss: 0.009190  [ 2900/ 3526]
loss: 0.052154  [ 3000/ 3526]
loss: 0.150695  [ 3100/ 3526]
loss: 0.026951  [ 3200/ 3526]
loss: 0.024494  [ 3300/ 3526]
loss: 0.015236  [ 3400/ 3526]
loss: 0.029783  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [ 1.0766193  -0.29454547]
[2 0 2 ... 0 2 2]
[1 2 1 ... 2 1 1]
Cluster 0 Occurrences: 1186; KMEANS: 1193
Cluster 1 Occurrences: 1188; KMEANS: 1151
Cluster 2 Occurrences: 1152; KMEANS: 1182
Centroids: [[0.39234504, 1.5812004], [-0.40622544, 0.97920614], [0.7475322, -0.6325703]]
Centroids: [[-0.46192947, 0.95153064], [0.75026834, -0.6426064], [0.44958165, 1.6195798]]
Contingency Matrix: 
[[ 122    0 1064]
 [1069    5  114]
 [   2 1146    4]]
[[122, -1, 1064], [1069, -1, 114], [-1, -1, -1]]
[[-1, -1, 1064], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 0, 0: 2}
New Contingency Matrix: 
[[1064  122    0]
 [ 114 1069    5]
 [   4    2 1146]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1064, 1069, 1146], Sum: 3279
All_Elements: [1064, 122, 0, 114, 1069, 5, 4, 2, 1146], Sum: 3526
Accuracy: 0.9299489506522972
Done!
