Experiment_path: Random_Seeds//V2/Experiment_02_8
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_8/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_15
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000028B8542C9B0>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x0000028B85474F98>
Epoch 1
-------------------------------
loss: 0.160064  [    0/ 3474]
loss: 0.087728  [  100/ 3474]
loss: 0.111002  [  200/ 3474]
loss: 0.094398  [  300/ 3474]
loss: 0.236188  [  400/ 3474]
loss: 0.009980  [  500/ 3474]
loss: 0.048351  [  600/ 3474]
loss: 0.020600  [  700/ 3474]
loss: 0.047870  [  800/ 3474]
loss: 0.216904  [  900/ 3474]
loss: 0.040126  [ 1000/ 3474]
loss: 0.118248  [ 1100/ 3474]
loss: 0.054977  [ 1200/ 3474]
loss: 0.059626  [ 1300/ 3474]
loss: 0.036221  [ 1400/ 3474]
loss: 0.087452  [ 1500/ 3474]
loss: 0.009479  [ 1600/ 3474]
loss: 0.117950  [ 1700/ 3474]
loss: 0.029218  [ 1800/ 3474]
loss: 0.016046  [ 1900/ 3474]
loss: 0.012525  [ 2000/ 3474]
loss: 0.014289  [ 2100/ 3474]
loss: 0.025112  [ 2200/ 3474]
loss: 0.076108  [ 2300/ 3474]
loss: 0.032153  [ 2400/ 3474]
loss: 0.043570  [ 2500/ 3474]
loss: 0.027939  [ 2600/ 3474]
loss: 0.102645  [ 2700/ 3474]
loss: 0.048329  [ 2800/ 3474]
loss: 0.017564  [ 2900/ 3474]
loss: 0.257641  [ 3000/ 3474]
loss: 0.010434  [ 3100/ 3474]
loss: 0.030235  [ 3200/ 3474]
loss: 0.025768  [ 3300/ 3474]
loss: 0.089108  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.064442  [    0/ 3474]
loss: 0.010064  [  100/ 3474]
loss: 0.021044  [  200/ 3474]
loss: 0.061399  [  300/ 3474]
loss: 0.098889  [  400/ 3474]
loss: 0.006007  [  500/ 3474]
loss: 0.039849  [  600/ 3474]
loss: 0.021588  [  700/ 3474]
loss: 0.032422  [  800/ 3474]
loss: 0.220629  [  900/ 3474]
loss: 0.035713  [ 1000/ 3474]
loss: 0.073069  [ 1100/ 3474]
loss: 0.054832  [ 1200/ 3474]
loss: 0.042617  [ 1300/ 3474]
loss: 0.025587  [ 1400/ 3474]
loss: 0.079895  [ 1500/ 3474]
loss: 0.008746  [ 1600/ 3474]
loss: 0.110099  [ 1700/ 3474]
loss: 0.029499  [ 1800/ 3474]
loss: 0.013863  [ 1900/ 3474]
loss: 0.009203  [ 2000/ 3474]
loss: 0.013541  [ 2100/ 3474]
loss: 0.027249  [ 2200/ 3474]
loss: 0.079196  [ 2300/ 3474]
loss: 0.027708  [ 2400/ 3474]
loss: 0.043478  [ 2500/ 3474]
loss: 0.026503  [ 2600/ 3474]
loss: 0.098131  [ 2700/ 3474]
loss: 0.043739  [ 2800/ 3474]
loss: 0.016158  [ 2900/ 3474]
loss: 0.218932  [ 3000/ 3474]
loss: 0.011052  [ 3100/ 3474]
loss: 0.012866  [ 3200/ 3474]
loss: 0.017385  [ 3300/ 3474]
loss: 0.074463  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.044848  [    0/ 3474]
loss: 0.009479  [  100/ 3474]
loss: 0.021013  [  200/ 3474]
loss: 0.066887  [  300/ 3474]
loss: 0.058912  [  400/ 3474]
loss: 0.005838  [  500/ 3474]
loss: 0.041369  [  600/ 3474]
loss: 0.024620  [  700/ 3474]
loss: 0.017338  [  800/ 3474]
loss: 0.192755  [  900/ 3474]
loss: 0.027979  [ 1000/ 3474]
loss: 0.102695  [ 1100/ 3474]
loss: 0.068305  [ 1200/ 3474]
loss: 0.031698  [ 1300/ 3474]
loss: 0.016235  [ 1400/ 3474]
loss: 0.080757  [ 1500/ 3474]
loss: 0.009769  [ 1600/ 3474]
loss: 0.064059  [ 1700/ 3474]
loss: 0.029726  [ 1800/ 3474]
loss: 0.014264  [ 1900/ 3474]
loss: 0.008259  [ 2000/ 3474]
loss: 0.013210  [ 2100/ 3474]
loss: 0.025640  [ 2200/ 3474]
loss: 0.077409  [ 2300/ 3474]
loss: 0.019807  [ 2400/ 3474]
loss: 0.033508  [ 2500/ 3474]
loss: 0.024326  [ 2600/ 3474]
loss: 0.095453  [ 2700/ 3474]
loss: 0.032871  [ 2800/ 3474]
loss: 0.015614  [ 2900/ 3474]
loss: 0.196463  [ 3000/ 3474]
loss: 0.010059  [ 3100/ 3474]
loss: 0.014583  [ 3200/ 3474]
loss: 0.019360  [ 3300/ 3474]
loss: 0.048090  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.032203  [    0/ 3474]
loss: 0.008903  [  100/ 3474]
loss: 0.019722  [  200/ 3474]
loss: 0.063237  [  300/ 3474]
loss: 0.058471  [  400/ 3474]
loss: 0.005985  [  500/ 3474]
loss: 0.038807  [  600/ 3474]
loss: 0.024810  [  700/ 3474]
loss: 0.012313  [  800/ 3474]
loss: 0.188722  [  900/ 3474]
loss: 0.024152  [ 1000/ 3474]
loss: 0.119158  [ 1100/ 3474]
loss: 0.073506  [ 1200/ 3474]
loss: 0.028692  [ 1300/ 3474]
loss: 0.012904  [ 1400/ 3474]
loss: 0.079448  [ 1500/ 3474]
loss: 0.009792  [ 1600/ 3474]
loss: 0.046103  [ 1700/ 3474]
loss: 0.027686  [ 1800/ 3474]
loss: 0.015967  [ 1900/ 3474]
loss: 0.008465  [ 2000/ 3474]
loss: 0.012287  [ 2100/ 3474]
loss: 0.025708  [ 2200/ 3474]
loss: 0.074709  [ 2300/ 3474]
loss: 0.016576  [ 2400/ 3474]
loss: 0.023555  [ 2500/ 3474]
loss: 0.023001  [ 2600/ 3474]
loss: 0.092020  [ 2700/ 3474]
loss: 0.025638  [ 2800/ 3474]
loss: 0.015028  [ 2900/ 3474]
loss: 0.186699  [ 3000/ 3474]
loss: 0.009299  [ 3100/ 3474]
loss: 0.015106  [ 3200/ 3474]
loss: 0.025936  [ 3300/ 3474]
loss: 0.028115  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.029233  [    0/ 3474]
loss: 0.008962  [  100/ 3474]
loss: 0.020623  [  200/ 3474]
loss: 0.061304  [  300/ 3474]
loss: 0.060377  [  400/ 3474]
loss: 0.006135  [  500/ 3474]
loss: 0.032452  [  600/ 3474]
loss: 0.024445  [  700/ 3474]
loss: 0.013400  [  800/ 3474]
loss: 0.191862  [  900/ 3474]
loss: 0.019499  [ 1000/ 3474]
loss: 0.126154  [ 1100/ 3474]
loss: 0.071985  [ 1200/ 3474]
loss: 0.027198  [ 1300/ 3474]
loss: 0.011311  [ 1400/ 3474]
loss: 0.076858  [ 1500/ 3474]
loss: 0.010240  [ 1600/ 3474]
loss: 0.041565  [ 1700/ 3474]
loss: 0.025086  [ 1800/ 3474]
loss: 0.018511  [ 1900/ 3474]
loss: 0.008869  [ 2000/ 3474]
loss: 0.011521  [ 2100/ 3474]
loss: 0.026417  [ 2200/ 3474]
loss: 0.072741  [ 2300/ 3474]
loss: 0.013892  [ 2400/ 3474]
loss: 0.017504  [ 2500/ 3474]
loss: 0.022437  [ 2600/ 3474]
loss: 0.087596  [ 2700/ 3474]
loss: 0.022498  [ 2800/ 3474]
loss: 0.014380  [ 2900/ 3474]
loss: 0.178787  [ 3000/ 3474]
loss: 0.009147  [ 3100/ 3474]
loss: 0.014791  [ 3200/ 3474]
loss: 0.032051  [ 3300/ 3474]
loss: 0.016606  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.027988  [    0/ 3474]
loss: 0.008979  [  100/ 3474]
loss: 0.022136  [  200/ 3474]
loss: 0.060036  [  300/ 3474]
loss: 0.063175  [  400/ 3474]
loss: 0.006084  [  500/ 3474]
loss: 0.025911  [  600/ 3474]
loss: 0.023290  [  700/ 3474]
loss: 0.016439  [  800/ 3474]
loss: 0.196435  [  900/ 3474]
loss: 0.013183  [ 1000/ 3474]
loss: 0.127039  [ 1100/ 3474]
loss: 0.062949  [ 1200/ 3474]
loss: 0.026427  [ 1300/ 3474]
loss: 0.010992  [ 1400/ 3474]
loss: 0.070387  [ 1500/ 3474]
loss: 0.010955  [ 1600/ 3474]
loss: 0.042034  [ 1700/ 3474]
loss: 0.022184  [ 1800/ 3474]
loss: 0.020363  [ 1900/ 3474]
loss: 0.009067  [ 2000/ 3474]
loss: 0.010988  [ 2100/ 3474]
loss: 0.026908  [ 2200/ 3474]
loss: 0.071858  [ 2300/ 3474]
loss: 0.010648  [ 2400/ 3474]
loss: 0.014534  [ 2500/ 3474]
loss: 0.021847  [ 2600/ 3474]
loss: 0.078954  [ 2700/ 3474]
loss: 0.021498  [ 2800/ 3474]
loss: 0.014150  [ 2900/ 3474]
loss: 0.175451  [ 3000/ 3474]
loss: 0.009134  [ 3100/ 3474]
loss: 0.014941  [ 3200/ 3474]
loss: 0.036116  [ 3300/ 3474]
loss: 0.012124  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.029075  [    0/ 3474]
loss: 0.008902  [  100/ 3474]
loss: 0.022850  [  200/ 3474]
loss: 0.059315  [  300/ 3474]
loss: 0.063996  [  400/ 3474]
loss: 0.005552  [  500/ 3474]
loss: 0.021822  [  600/ 3474]
loss: 0.020241  [  700/ 3474]
loss: 0.017828  [  800/ 3474]
loss: 0.198820  [  900/ 3474]
loss: 0.007464  [ 1000/ 3474]
loss: 0.140818  [ 1100/ 3474]
loss: 0.048903  [ 1200/ 3474]
loss: 0.025871  [ 1300/ 3474]
loss: 0.011044  [ 1400/ 3474]
loss: 0.067164  [ 1500/ 3474]
loss: 0.011829  [ 1600/ 3474]
loss: 0.043018  [ 1700/ 3474]
loss: 0.020509  [ 1800/ 3474]
loss: 0.021678  [ 1900/ 3474]
loss: 0.009295  [ 2000/ 3474]
loss: 0.011571  [ 2100/ 3474]
loss: 0.027606  [ 2200/ 3474]
loss: 0.071437  [ 2300/ 3474]
loss: 0.007064  [ 2400/ 3474]
loss: 0.014398  [ 2500/ 3474]
loss: 0.021164  [ 2600/ 3474]
loss: 0.065833  [ 2700/ 3474]
loss: 0.021671  [ 2800/ 3474]
loss: 0.014734  [ 2900/ 3474]
loss: 0.171061  [ 3000/ 3474]
loss: 0.009067  [ 3100/ 3474]
loss: 0.014505  [ 3200/ 3474]
loss: 0.037745  [ 3300/ 3474]
loss: 0.010364  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.030193  [    0/ 3474]
loss: 0.008823  [  100/ 3474]
loss: 0.023887  [  200/ 3474]
loss: 0.058948  [  300/ 3474]
loss: 0.065175  [  400/ 3474]
loss: 0.006006  [  500/ 3474]
loss: 0.020292  [  600/ 3474]
loss: 0.016859  [  700/ 3474]
loss: 0.019331  [  800/ 3474]
loss: 0.200362  [  900/ 3474]
loss: 0.006032  [ 1000/ 3474]
loss: 0.156747  [ 1100/ 3474]
loss: 0.040080  [ 1200/ 3474]
loss: 0.025629  [ 1300/ 3474]
loss: 0.010872  [ 1400/ 3474]
loss: 0.069447  [ 1500/ 3474]
loss: 0.012051  [ 1600/ 3474]
loss: 0.039218  [ 1700/ 3474]
loss: 0.017997  [ 1800/ 3474]
loss: 0.022367  [ 1900/ 3474]
loss: 0.009503  [ 2000/ 3474]
loss: 0.012561  [ 2100/ 3474]
loss: 0.028364  [ 2200/ 3474]
loss: 0.070153  [ 2300/ 3474]
loss: 0.005589  [ 2400/ 3474]
loss: 0.014168  [ 2500/ 3474]
loss: 0.020695  [ 2600/ 3474]
loss: 0.057155  [ 2700/ 3474]
loss: 0.022254  [ 2800/ 3474]
loss: 0.015316  [ 2900/ 3474]
loss: 0.159976  [ 3000/ 3474]
loss: 0.009050  [ 3100/ 3474]
loss: 0.014121  [ 3200/ 3474]
loss: 0.038314  [ 3300/ 3474]
loss: 0.009751  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [ 0.6291241 -0.3674731]
[0 1 1 ... 1 1 2]
[2 0 0 ... 0 0 1]
Cluster 0 Occurrences: 1198; KMEANS: 1123
Cluster 1 Occurrences: 1128; KMEANS: 1163
Cluster 2 Occurrences: 1148; KMEANS: 1188
Centroids: [[1.1297851, -0.8131664], [-0.18594189, 2.3583076], [-1.4575065, -0.89045954]]
Centroids: [[-0.19264044, 2.3720398], [-1.4489884, -0.8983758], [1.1549084, -0.80407387]]
Contingency Matrix: 
[[   0   16 1182]
 [1122    0    6]
 [   1 1147    0]]
[[-1, -1, -1], [1122, 0, -1], [1, 1147, -1]]
[[-1, -1, -1], [1122, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 1, 1: 0}
New Contingency Matrix: 
[[1182    0   16]
 [   6 1122    0]
 [   0    1 1147]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1182, 1122, 1147], Sum: 3451
All_Elements: [1182, 0, 16, 6, 1122, 0, 0, 1, 1147], Sum: 3474
Accuracy: 0.9933793897524468
Done!
