Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_12
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA03C24198>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x000001FA032AD400>
Epoch 1
-------------------------------
loss: 0.214990  [    0/ 3477]
loss: 0.174988  [  100/ 3477]
loss: 0.071854  [  200/ 3477]
loss: 0.084125  [  300/ 3477]
loss: 0.031393  [  400/ 3477]
loss: 0.047645  [  500/ 3477]
loss: 0.019772  [  600/ 3477]
loss: 0.025759  [  700/ 3477]
loss: 0.021958  [  800/ 3477]
loss: 0.023507  [  900/ 3477]
loss: 0.024782  [ 1000/ 3477]
loss: 0.004360  [ 1100/ 3477]
loss: 0.023756  [ 1200/ 3477]
loss: 0.018474  [ 1300/ 3477]
loss: 0.016236  [ 1400/ 3477]
loss: 0.018238  [ 1500/ 3477]
loss: 0.013421  [ 1600/ 3477]
loss: 0.014581  [ 1700/ 3477]
loss: 0.010689  [ 1800/ 3477]
loss: 0.019000  [ 1900/ 3477]
loss: 0.022855  [ 2000/ 3477]
loss: 0.024218  [ 2100/ 3477]
loss: 0.021508  [ 2200/ 3477]
loss: 0.011287  [ 2300/ 3477]
loss: 0.010740  [ 2400/ 3477]
loss: 0.085862  [ 2500/ 3477]
loss: 0.075358  [ 2600/ 3477]
loss: 0.005340  [ 2700/ 3477]
loss: 0.234618  [ 2800/ 3477]
loss: 0.021428  [ 2900/ 3477]
loss: 0.016255  [ 3000/ 3477]
loss: 0.014542  [ 3100/ 3477]
loss: 0.089804  [ 3200/ 3477]
loss: 0.016305  [ 3300/ 3477]
loss: 0.006352  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.010059  [    0/ 3477]
loss: 0.015821  [  100/ 3477]
loss: 0.019921  [  200/ 3477]
loss: 0.031350  [  300/ 3477]
loss: 0.012460  [  400/ 3477]
loss: 0.013364  [  500/ 3477]
loss: 0.014553  [  600/ 3477]
loss: 0.020785  [  700/ 3477]
loss: 0.020687  [  800/ 3477]
loss: 0.020408  [  900/ 3477]
loss: 0.018793  [ 1000/ 3477]
loss: 0.008180  [ 1100/ 3477]
loss: 0.019419  [ 1200/ 3477]
loss: 0.014404  [ 1300/ 3477]
loss: 0.014618  [ 1400/ 3477]
loss: 0.015866  [ 1500/ 3477]
loss: 0.014208  [ 1600/ 3477]
loss: 0.013912  [ 1700/ 3477]
loss: 0.011508  [ 1800/ 3477]
loss: 0.015335  [ 1900/ 3477]
loss: 0.022712  [ 2000/ 3477]
loss: 0.017360  [ 2100/ 3477]
loss: 0.022364  [ 2200/ 3477]
loss: 0.011924  [ 2300/ 3477]
loss: 0.008986  [ 2400/ 3477]
loss: 0.076554  [ 2500/ 3477]
loss: 0.079109  [ 2600/ 3477]
loss: 0.005926  [ 2700/ 3477]
loss: 0.289709  [ 2800/ 3477]
loss: 0.020915  [ 2900/ 3477]
loss: 0.015545  [ 3000/ 3477]
loss: 0.014986  [ 3100/ 3477]
loss: 0.081609  [ 3200/ 3477]
loss: 0.016443  [ 3300/ 3477]
loss: 0.006682  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.010210  [    0/ 3477]
loss: 0.014810  [  100/ 3477]
loss: 0.018722  [  200/ 3477]
loss: 0.031381  [  300/ 3477]
loss: 0.016526  [  400/ 3477]
loss: 0.010787  [  500/ 3477]
loss: 0.013409  [  600/ 3477]
loss: 0.020219  [  700/ 3477]
loss: 0.020390  [  800/ 3477]
loss: 0.017183  [  900/ 3477]
loss: 0.017310  [ 1000/ 3477]
loss: 0.006972  [ 1100/ 3477]
loss: 0.018926  [ 1200/ 3477]
loss: 0.012414  [ 1300/ 3477]
loss: 0.013366  [ 1400/ 3477]
loss: 0.014458  [ 1500/ 3477]
loss: 0.014097  [ 1600/ 3477]
loss: 0.014508  [ 1700/ 3477]
loss: 0.011585  [ 1800/ 3477]
loss: 0.012712  [ 1900/ 3477]
loss: 0.022543  [ 2000/ 3477]
loss: 0.015868  [ 2100/ 3477]
loss: 0.022411  [ 2200/ 3477]
loss: 0.013139  [ 2300/ 3477]
loss: 0.008513  [ 2400/ 3477]
loss: 0.078358  [ 2500/ 3477]
loss: 0.076902  [ 2600/ 3477]
loss: 0.006187  [ 2700/ 3477]
loss: 0.268017  [ 2800/ 3477]
loss: 0.020567  [ 2900/ 3477]
loss: 0.015513  [ 3000/ 3477]
loss: 0.014643  [ 3100/ 3477]
loss: 0.081030  [ 3200/ 3477]
loss: 0.016561  [ 3300/ 3477]
loss: 0.007040  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.010141  [    0/ 3477]
loss: 0.015773  [  100/ 3477]
loss: 0.018404  [  200/ 3477]
loss: 0.030905  [  300/ 3477]
loss: 0.017193  [  400/ 3477]
loss: 0.009008  [  500/ 3477]
loss: 0.013636  [  600/ 3477]
loss: 0.020345  [  700/ 3477]
loss: 0.020161  [  800/ 3477]
loss: 0.016585  [  900/ 3477]
loss: 0.017466  [ 1000/ 3477]
loss: 0.005678  [ 1100/ 3477]
loss: 0.018919  [ 1200/ 3477]
loss: 0.011791  [ 1300/ 3477]
loss: 0.012923  [ 1400/ 3477]
loss: 0.013650  [ 1500/ 3477]
loss: 0.015002  [ 1600/ 3477]
loss: 0.014462  [ 1700/ 3477]
loss: 0.011348  [ 1800/ 3477]
loss: 0.011790  [ 1900/ 3477]
loss: 0.022769  [ 2000/ 3477]
loss: 0.016241  [ 2100/ 3477]
loss: 0.021742  [ 2200/ 3477]
loss: 0.013992  [ 2300/ 3477]
loss: 0.008465  [ 2400/ 3477]
loss: 0.078871  [ 2500/ 3477]
loss: 0.077229  [ 2600/ 3477]
loss: 0.005824  [ 2700/ 3477]
loss: 0.211107  [ 2800/ 3477]
loss: 0.020269  [ 2900/ 3477]
loss: 0.015339  [ 3000/ 3477]
loss: 0.013902  [ 3100/ 3477]
loss: 0.080242  [ 3200/ 3477]
loss: 0.016754  [ 3300/ 3477]
loss: 0.007172  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.010035  [    0/ 3477]
loss: 0.016154  [  100/ 3477]
loss: 0.018780  [  200/ 3477]
loss: 0.030801  [  300/ 3477]
loss: 0.016876  [  400/ 3477]
loss: 0.008028  [  500/ 3477]
loss: 0.013840  [  600/ 3477]
loss: 0.020588  [  700/ 3477]
loss: 0.019833  [  800/ 3477]
loss: 0.015896  [  900/ 3477]
loss: 0.017365  [ 1000/ 3477]
loss: 0.005039  [ 1100/ 3477]
loss: 0.019183  [ 1200/ 3477]
loss: 0.011312  [ 1300/ 3477]
loss: 0.012924  [ 1400/ 3477]
loss: 0.013754  [ 1500/ 3477]
loss: 0.015392  [ 1600/ 3477]
loss: 0.014187  [ 1700/ 3477]
loss: 0.011119  [ 1800/ 3477]
loss: 0.011242  [ 1900/ 3477]
loss: 0.022499  [ 2000/ 3477]
loss: 0.016692  [ 2100/ 3477]
loss: 0.021658  [ 2200/ 3477]
loss: 0.014084  [ 2300/ 3477]
loss: 0.008499  [ 2400/ 3477]
loss: 0.079145  [ 2500/ 3477]
loss: 0.075698  [ 2600/ 3477]
loss: 0.005299  [ 2700/ 3477]
loss: 0.173955  [ 2800/ 3477]
loss: 0.020204  [ 2900/ 3477]
loss: 0.015054  [ 3000/ 3477]
loss: 0.013508  [ 3100/ 3477]
loss: 0.079586  [ 3200/ 3477]
loss: 0.016866  [ 3300/ 3477]
loss: 0.007381  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.009879  [    0/ 3477]
loss: 0.015988  [  100/ 3477]
loss: 0.018700  [  200/ 3477]
loss: 0.030178  [  300/ 3477]
loss: 0.015934  [  400/ 3477]
loss: 0.007658  [  500/ 3477]
loss: 0.014235  [  600/ 3477]
loss: 0.020853  [  700/ 3477]
loss: 0.019545  [  800/ 3477]
loss: 0.015110  [  900/ 3477]
loss: 0.017551  [ 1000/ 3477]
loss: 0.004737  [ 1100/ 3477]
loss: 0.019626  [ 1200/ 3477]
loss: 0.010704  [ 1300/ 3477]
loss: 0.013253  [ 1400/ 3477]
loss: 0.014675  [ 1500/ 3477]
loss: 0.014143  [ 1600/ 3477]
loss: 0.013991  [ 1700/ 3477]
loss: 0.010943  [ 1800/ 3477]
loss: 0.011326  [ 1900/ 3477]
loss: 0.022474  [ 2000/ 3477]
loss: 0.017206  [ 2100/ 3477]
loss: 0.021480  [ 2200/ 3477]
loss: 0.014226  [ 2300/ 3477]
loss: 0.008645  [ 2400/ 3477]
loss: 0.079518  [ 2500/ 3477]
loss: 0.075658  [ 2600/ 3477]
loss: 0.004592  [ 2700/ 3477]
loss: 0.137275  [ 2800/ 3477]
loss: 0.020226  [ 2900/ 3477]
loss: 0.014595  [ 3000/ 3477]
loss: 0.012920  [ 3100/ 3477]
loss: 0.078076  [ 3200/ 3477]
loss: 0.016737  [ 3300/ 3477]
loss: 0.007445  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.009617  [    0/ 3477]
loss: 0.016382  [  100/ 3477]
loss: 0.018834  [  200/ 3477]
loss: 0.028388  [  300/ 3477]
loss: 0.015890  [  400/ 3477]
loss: 0.007484  [  500/ 3477]
loss: 0.014025  [  600/ 3477]
loss: 0.021197  [  700/ 3477]
loss: 0.019272  [  800/ 3477]
loss: 0.015223  [  900/ 3477]
loss: 0.017453  [ 1000/ 3477]
loss: 0.004176  [ 1100/ 3477]
loss: 0.019752  [ 1200/ 3477]
loss: 0.010130  [ 1300/ 3477]
loss: 0.013698  [ 1400/ 3477]
loss: 0.016022  [ 1500/ 3477]
loss: 0.011644  [ 1600/ 3477]
loss: 0.013584  [ 1700/ 3477]
loss: 0.010724  [ 1800/ 3477]
loss: 0.011602  [ 1900/ 3477]
loss: 0.022826  [ 2000/ 3477]
loss: 0.017545  [ 2100/ 3477]
loss: 0.021497  [ 2200/ 3477]
loss: 0.014168  [ 2300/ 3477]
loss: 0.008711  [ 2400/ 3477]
loss: 0.080270  [ 2500/ 3477]
loss: 0.075902  [ 2600/ 3477]
loss: 0.004154  [ 2700/ 3477]
loss: 0.111974  [ 2800/ 3477]
loss: 0.020244  [ 2900/ 3477]
loss: 0.014180  [ 3000/ 3477]
loss: 0.012310  [ 3100/ 3477]
loss: 0.077815  [ 3200/ 3477]
loss: 0.016433  [ 3300/ 3477]
loss: 0.007401  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.009398  [    0/ 3477]
loss: 0.015529  [  100/ 3477]
loss: 0.018814  [  200/ 3477]
loss: 0.027447  [  300/ 3477]
loss: 0.015256  [  400/ 3477]
loss: 0.007663  [  500/ 3477]
loss: 0.013904  [  600/ 3477]
loss: 0.021351  [  700/ 3477]
loss: 0.018831  [  800/ 3477]
loss: 0.015016  [  900/ 3477]
loss: 0.017274  [ 1000/ 3477]
loss: 0.004466  [ 1100/ 3477]
loss: 0.019751  [ 1200/ 3477]
loss: 0.009686  [ 1300/ 3477]
loss: 0.014259  [ 1400/ 3477]
loss: 0.017049  [ 1500/ 3477]
loss: 0.011025  [ 1600/ 3477]
loss: 0.012997  [ 1700/ 3477]
loss: 0.010412  [ 1800/ 3477]
loss: 0.011937  [ 1900/ 3477]
loss: 0.023233  [ 2000/ 3477]
loss: 0.017542  [ 2100/ 3477]
loss: 0.021355  [ 2200/ 3477]
loss: 0.014180  [ 2300/ 3477]
loss: 0.008832  [ 2400/ 3477]
loss: 0.080804  [ 2500/ 3477]
loss: 0.075653  [ 2600/ 3477]
loss: 0.003796  [ 2700/ 3477]
loss: 0.094210  [ 2800/ 3477]
loss: 0.020206  [ 2900/ 3477]
loss: 0.013829  [ 3000/ 3477]
loss: 0.011630  [ 3100/ 3477]
loss: 0.077708  [ 3200/ 3477]
loss: 0.016203  [ 3300/ 3477]
loss: 0.007439  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [0.644608  1.3993238]
[1 0 2 ... 1 1 1]
[1 2 0 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1174
Cluster 1 Occurrences: 1188; KMEANS: 1176
Cluster 2 Occurrences: 1157; KMEANS: 1127
Centroids: [[-2.1992106, 0.5333005], [0.597098, 1.0544567], [-0.7174841, -1.6773423]]
Centroids: [[-0.69493794, -1.6743599], [0.5944359, 1.0829247], [-2.2124956, 0.5393828]]
Contingency Matrix: 
[[   5    6 1121]
 [  14 1170    4]
 [1155    0    2]]
[[5, -1, 1121], [-1, -1, -1], [1155, -1, 2]]
[[-1, -1, 1121], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1121    6    5]
 [   4 1170   14]
 [   2    0 1155]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1121, 1170, 1155], Sum: 3446
All_Elements: [1121, 6, 5, 4, 1170, 14, 2, 0, 1155], Sum: 3477
Accuracy: 0.9910842680471671
Done!
