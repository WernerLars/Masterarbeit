Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_55_12
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA03732198>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x000001FA03BDDF28>
Epoch 1
-------------------------------
loss: 0.193354  [    0/ 3526]
loss: 0.188495  [  100/ 3526]
loss: 0.065444  [  200/ 3526]
loss: 0.027049  [  300/ 3526]
loss: 0.021162  [  400/ 3526]
loss: 0.062372  [  500/ 3526]
loss: 0.037158  [  600/ 3526]
loss: 0.023348  [  700/ 3526]
loss: 0.063363  [  800/ 3526]
loss: 0.013460  [  900/ 3526]
loss: 0.021852  [ 1000/ 3526]
loss: 0.030868  [ 1100/ 3526]
loss: 0.031282  [ 1200/ 3526]
loss: 0.035892  [ 1300/ 3526]
loss: 0.030596  [ 1400/ 3526]
loss: 0.046791  [ 1500/ 3526]
loss: 0.037578  [ 1600/ 3526]
loss: 0.018827  [ 1700/ 3526]
loss: 0.026617  [ 1800/ 3526]
loss: 0.020447  [ 1900/ 3526]
loss: 0.018686  [ 2000/ 3526]
loss: 0.230570  [ 2100/ 3526]
loss: 0.052559  [ 2200/ 3526]
loss: 0.016149  [ 2300/ 3526]
loss: 0.010323  [ 2400/ 3526]
loss: 0.015008  [ 2500/ 3526]
loss: 0.054332  [ 2600/ 3526]
loss: 0.026689  [ 2700/ 3526]
loss: 0.017411  [ 2800/ 3526]
loss: 0.009295  [ 2900/ 3526]
loss: 0.070200  [ 3000/ 3526]
loss: 0.161795  [ 3100/ 3526]
loss: 0.035811  [ 3200/ 3526]
loss: 0.032491  [ 3300/ 3526]
loss: 0.007442  [ 3400/ 3526]
loss: 0.042444  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.023638  [    0/ 3526]
loss: 0.027337  [  100/ 3526]
loss: 0.027642  [  200/ 3526]
loss: 0.022510  [  300/ 3526]
loss: 0.009401  [  400/ 3526]
loss: 0.057311  [  500/ 3526]
loss: 0.013280  [  600/ 3526]
loss: 0.014513  [  700/ 3526]
loss: 0.029887  [  800/ 3526]
loss: 0.008829  [  900/ 3526]
loss: 0.010473  [ 1000/ 3526]
loss: 0.031504  [ 1100/ 3526]
loss: 0.018638  [ 1200/ 3526]
loss: 0.028027  [ 1300/ 3526]
loss: 0.030903  [ 1400/ 3526]
loss: 0.028452  [ 1500/ 3526]
loss: 0.036859  [ 1600/ 3526]
loss: 0.019812  [ 1700/ 3526]
loss: 0.024764  [ 1800/ 3526]
loss: 0.019923  [ 1900/ 3526]
loss: 0.016695  [ 2000/ 3526]
loss: 0.220225  [ 2100/ 3526]
loss: 0.048386  [ 2200/ 3526]
loss: 0.016886  [ 2300/ 3526]
loss: 0.010110  [ 2400/ 3526]
loss: 0.015028  [ 2500/ 3526]
loss: 0.044261  [ 2600/ 3526]
loss: 0.019342  [ 2700/ 3526]
loss: 0.009812  [ 2800/ 3526]
loss: 0.009925  [ 2900/ 3526]
loss: 0.027353  [ 3000/ 3526]
loss: 0.175874  [ 3100/ 3526]
loss: 0.034525  [ 3200/ 3526]
loss: 0.028795  [ 3300/ 3526]
loss: 0.009060  [ 3400/ 3526]
loss: 0.043295  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.019875  [    0/ 3526]
loss: 0.013855  [  100/ 3526]
loss: 0.015489  [  200/ 3526]
loss: 0.018114  [  300/ 3526]
loss: 0.011840  [  400/ 3526]
loss: 0.048645  [  500/ 3526]
loss: 0.014289  [  600/ 3526]
loss: 0.012874  [  700/ 3526]
loss: 0.027942  [  800/ 3526]
loss: 0.008341  [  900/ 3526]
loss: 0.016007  [ 1000/ 3526]
loss: 0.029703  [ 1100/ 3526]
loss: 0.020651  [ 1200/ 3526]
loss: 0.027637  [ 1300/ 3526]
loss: 0.030721  [ 1400/ 3526]
loss: 0.022987  [ 1500/ 3526]
loss: 0.039136  [ 1600/ 3526]
loss: 0.020467  [ 1700/ 3526]
loss: 0.023335  [ 1800/ 3526]
loss: 0.014907  [ 1900/ 3526]
loss: 0.016382  [ 2000/ 3526]
loss: 0.185475  [ 2100/ 3526]
loss: 0.038735  [ 2200/ 3526]
loss: 0.016878  [ 2300/ 3526]
loss: 0.010298  [ 2400/ 3526]
loss: 0.014147  [ 2500/ 3526]
loss: 0.032291  [ 2600/ 3526]
loss: 0.016372  [ 2700/ 3526]
loss: 0.009124  [ 2800/ 3526]
loss: 0.009918  [ 2900/ 3526]
loss: 0.029650  [ 3000/ 3526]
loss: 0.174157  [ 3100/ 3526]
loss: 0.032629  [ 3200/ 3526]
loss: 0.027219  [ 3300/ 3526]
loss: 0.009469  [ 3400/ 3526]
loss: 0.041921  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.016981  [    0/ 3526]
loss: 0.011357  [  100/ 3526]
loss: 0.014840  [  200/ 3526]
loss: 0.016896  [  300/ 3526]
loss: 0.013153  [  400/ 3526]
loss: 0.042417  [  500/ 3526]
loss: 0.013040  [  600/ 3526]
loss: 0.012916  [  700/ 3526]
loss: 0.026511  [  800/ 3526]
loss: 0.009253  [  900/ 3526]
loss: 0.016074  [ 1000/ 3526]
loss: 0.025487  [ 1100/ 3526]
loss: 0.020184  [ 1200/ 3526]
loss: 0.027162  [ 1300/ 3526]
loss: 0.030420  [ 1400/ 3526]
loss: 0.021578  [ 1500/ 3526]
loss: 0.044017  [ 1600/ 3526]
loss: 0.020869  [ 1700/ 3526]
loss: 0.022316  [ 1800/ 3526]
loss: 0.013228  [ 1900/ 3526]
loss: 0.016147  [ 2000/ 3526]
loss: 0.157144  [ 2100/ 3526]
loss: 0.033785  [ 2200/ 3526]
loss: 0.016752  [ 2300/ 3526]
loss: 0.010188  [ 2400/ 3526]
loss: 0.013702  [ 2500/ 3526]
loss: 0.027110  [ 2600/ 3526]
loss: 0.015555  [ 2700/ 3526]
loss: 0.009306  [ 2800/ 3526]
loss: 0.009869  [ 2900/ 3526]
loss: 0.036208  [ 3000/ 3526]
loss: 0.170362  [ 3100/ 3526]
loss: 0.031186  [ 3200/ 3526]
loss: 0.026498  [ 3300/ 3526]
loss: 0.009404  [ 3400/ 3526]
loss: 0.040286  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.014605  [    0/ 3526]
loss: 0.015793  [  100/ 3526]
loss: 0.017198  [  200/ 3526]
loss: 0.015825  [  300/ 3526]
loss: 0.013647  [  400/ 3526]
loss: 0.039760  [  500/ 3526]
loss: 0.011718  [  600/ 3526]
loss: 0.012925  [  700/ 3526]
loss: 0.026023  [  800/ 3526]
loss: 0.010074  [  900/ 3526]
loss: 0.016328  [ 1000/ 3526]
loss: 0.021787  [ 1100/ 3526]
loss: 0.019763  [ 1200/ 3526]
loss: 0.026521  [ 1300/ 3526]
loss: 0.030182  [ 1400/ 3526]
loss: 0.021373  [ 1500/ 3526]
loss: 0.047454  [ 1600/ 3526]
loss: 0.021137  [ 1700/ 3526]
loss: 0.021759  [ 1800/ 3526]
loss: 0.012268  [ 1900/ 3526]
loss: 0.016009  [ 2000/ 3526]
loss: 0.154885  [ 2100/ 3526]
loss: 0.031779  [ 2200/ 3526]
loss: 0.016658  [ 2300/ 3526]
loss: 0.010122  [ 2400/ 3526]
loss: 0.013538  [ 2500/ 3526]
loss: 0.024229  [ 2600/ 3526]
loss: 0.015144  [ 2700/ 3526]
loss: 0.010853  [ 2800/ 3526]
loss: 0.010565  [ 2900/ 3526]
loss: 0.044865  [ 3000/ 3526]
loss: 0.161750  [ 3100/ 3526]
loss: 0.029950  [ 3200/ 3526]
loss: 0.026029  [ 3300/ 3526]
loss: 0.008984  [ 3400/ 3526]
loss: 0.038660  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.013964  [    0/ 3526]
loss: 0.019875  [  100/ 3526]
loss: 0.019494  [  200/ 3526]
loss: 0.014521  [  300/ 3526]
loss: 0.014194  [  400/ 3526]
loss: 0.037511  [  500/ 3526]
loss: 0.011147  [  600/ 3526]
loss: 0.013173  [  700/ 3526]
loss: 0.026124  [  800/ 3526]
loss: 0.010566  [  900/ 3526]
loss: 0.016799  [ 1000/ 3526]
loss: 0.019768  [ 1100/ 3526]
loss: 0.019924  [ 1200/ 3526]
loss: 0.026965  [ 1300/ 3526]
loss: 0.029775  [ 1400/ 3526]
loss: 0.021473  [ 1500/ 3526]
loss: 0.048461  [ 1600/ 3526]
loss: 0.020180  [ 1700/ 3526]
loss: 0.021596  [ 1800/ 3526]
loss: 0.011477  [ 1900/ 3526]
loss: 0.015547  [ 2000/ 3526]
loss: 0.122810  [ 2100/ 3526]
loss: 0.030616  [ 2200/ 3526]
loss: 0.016484  [ 2300/ 3526]
loss: 0.009916  [ 2400/ 3526]
loss: 0.013380  [ 2500/ 3526]
loss: 0.022302  [ 2600/ 3526]
loss: 0.015022  [ 2700/ 3526]
loss: 0.011422  [ 2800/ 3526]
loss: 0.010626  [ 2900/ 3526]
loss: 0.049464  [ 3000/ 3526]
loss: 0.155456  [ 3100/ 3526]
loss: 0.029192  [ 3200/ 3526]
loss: 0.025727  [ 3300/ 3526]
loss: 0.008293  [ 3400/ 3526]
loss: 0.036554  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.013448  [    0/ 3526]
loss: 0.020767  [  100/ 3526]
loss: 0.020747  [  200/ 3526]
loss: 0.013646  [  300/ 3526]
loss: 0.014386  [  400/ 3526]
loss: 0.034727  [  500/ 3526]
loss: 0.010667  [  600/ 3526]
loss: 0.013168  [  700/ 3526]
loss: 0.026037  [  800/ 3526]
loss: 0.010862  [  900/ 3526]
loss: 0.016947  [ 1000/ 3526]
loss: 0.018775  [ 1100/ 3526]
loss: 0.019669  [ 1200/ 3526]
loss: 0.030074  [ 1300/ 3526]
loss: 0.027673  [ 1400/ 3526]
loss: 0.021615  [ 1500/ 3526]
loss: 0.050924  [ 1600/ 3526]
loss: 0.017875  [ 1700/ 3526]
loss: 0.021339  [ 1800/ 3526]
loss: 0.010995  [ 1900/ 3526]
loss: 0.014935  [ 2000/ 3526]
loss: 0.112984  [ 2100/ 3526]
loss: 0.030055  [ 2200/ 3526]
loss: 0.016334  [ 2300/ 3526]
loss: 0.009847  [ 2400/ 3526]
loss: 0.013273  [ 2500/ 3526]
loss: 0.021216  [ 2600/ 3526]
loss: 0.015273  [ 2700/ 3526]
loss: 0.010457  [ 2800/ 3526]
loss: 0.009952  [ 2900/ 3526]
loss: 0.052794  [ 3000/ 3526]
loss: 0.152897  [ 3100/ 3526]
loss: 0.028699  [ 3200/ 3526]
loss: 0.025542  [ 3300/ 3526]
loss: 0.008894  [ 3400/ 3526]
loss: 0.034143  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.013236  [    0/ 3526]
loss: 0.020664  [  100/ 3526]
loss: 0.022041  [  200/ 3526]
loss: 0.012873  [  300/ 3526]
loss: 0.014356  [  400/ 3526]
loss: 0.032393  [  500/ 3526]
loss: 0.010312  [  600/ 3526]
loss: 0.013229  [  700/ 3526]
loss: 0.025476  [  800/ 3526]
loss: 0.011263  [  900/ 3526]
loss: 0.016691  [ 1000/ 3526]
loss: 0.017944  [ 1100/ 3526]
loss: 0.019320  [ 1200/ 3526]
loss: 0.030313  [ 1300/ 3526]
loss: 0.026535  [ 1400/ 3526]
loss: 0.021647  [ 1500/ 3526]
loss: 0.051848  [ 1600/ 3526]
loss: 0.017689  [ 1700/ 3526]
loss: 0.021655  [ 1800/ 3526]
loss: 0.011418  [ 1900/ 3526]
loss: 0.014390  [ 2000/ 3526]
loss: 0.154969  [ 2100/ 3526]
loss: 0.029106  [ 2200/ 3526]
loss: 0.016435  [ 2300/ 3526]
loss: 0.009920  [ 2400/ 3526]
loss: 0.013255  [ 2500/ 3526]
loss: 0.020293  [ 2600/ 3526]
loss: 0.015251  [ 2700/ 3526]
loss: 0.010322  [ 2800/ 3526]
loss: 0.012085  [ 2900/ 3526]
loss: 0.056483  [ 3000/ 3526]
loss: 0.150940  [ 3100/ 3526]
loss: 0.028129  [ 3200/ 3526]
loss: 0.025419  [ 3300/ 3526]
loss: 0.008727  [ 3400/ 3526]
loss: 0.031475  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [-0.78973556 -0.9565929 ]
[2 0 2 ... 0 2 2]
[1 0 1 ... 0 1 1]
Cluster 0 Occurrences: 1186; KMEANS: 1224
Cluster 1 Occurrences: 1188; KMEANS: 1142
Cluster 2 Occurrences: 1152; KMEANS: 1160
Centroids: [[-0.20101783, 1.612947], [0.44388556, 0.9422519], [-0.3130968, -1.1411223]]
Centroids: [[-0.26227713, 1.6146121], [-0.31680638, -1.1588905], [0.52677715, 0.9180561]]
Contingency Matrix: 
[[1082    0  104]
 [ 138    0 1050]
 [   4 1142    6]]
[[1082, -1, 104], [138, -1, 1050], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, 1050], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 0: 0, 1: 2}
New Contingency Matrix: 
[[1082  104    0]
 [ 138 1050    0]
 [   4    6 1142]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [1082, 1050, 1142], Sum: 3274
All_Elements: [1082, 104, 0, 138, 1050, 0, 4, 6, 1142], Sum: 3526
Accuracy: 0.9285309132161089
Done!
