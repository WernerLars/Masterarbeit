Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_50_08
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA03C24CC0>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x000001FA03BDDA58>
Epoch 1
-------------------------------
loss: 0.116275  [    0/ 3386]
loss: 0.214507  [  100/ 3386]
loss: 0.122532  [  200/ 3386]
loss: 0.186632  [  300/ 3386]
loss: 0.281310  [  400/ 3386]
loss: 0.075633  [  500/ 3386]
loss: 0.167622  [  600/ 3386]
loss: 0.045539  [  700/ 3386]
loss: 0.105754  [  800/ 3386]
loss: 0.084152  [  900/ 3386]
loss: 0.128367  [ 1000/ 3386]
loss: 0.327827  [ 1100/ 3386]
loss: 0.243675  [ 1200/ 3386]
loss: 0.029417  [ 1300/ 3386]
loss: 0.043876  [ 1400/ 3386]
loss: 0.096406  [ 1500/ 3386]
loss: 0.032214  [ 1600/ 3386]
loss: 0.023882  [ 1700/ 3386]
loss: 0.288829  [ 1800/ 3386]
loss: 0.437019  [ 1900/ 3386]
loss: 0.039034  [ 2000/ 3386]
loss: 0.165789  [ 2100/ 3386]
loss: 0.475427  [ 2200/ 3386]
loss: 0.133236  [ 2300/ 3386]
loss: 0.129959  [ 2400/ 3386]
loss: 0.046506  [ 2500/ 3386]
loss: 0.189399  [ 2600/ 3386]
loss: 0.231617  [ 2700/ 3386]
loss: 0.085656  [ 2800/ 3386]
loss: 0.074769  [ 2900/ 3386]
loss: 0.015023  [ 3000/ 3386]
loss: 0.115172  [ 3100/ 3386]
loss: 0.098650  [ 3200/ 3386]
loss: 0.120917  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.051916  [    0/ 3386]
loss: 0.119517  [  100/ 3386]
loss: 0.090932  [  200/ 3386]
loss: 0.070081  [  300/ 3386]
loss: 0.237079  [  400/ 3386]
loss: 0.066937  [  500/ 3386]
loss: 0.159345  [  600/ 3386]
loss: 0.045062  [  700/ 3386]
loss: 0.113983  [  800/ 3386]
loss: 0.050061  [  900/ 3386]
loss: 0.135089  [ 1000/ 3386]
loss: 0.299148  [ 1100/ 3386]
loss: 0.160824  [ 1200/ 3386]
loss: 0.024130  [ 1300/ 3386]
loss: 0.046831  [ 1400/ 3386]
loss: 0.071106  [ 1500/ 3386]
loss: 0.036215  [ 1600/ 3386]
loss: 0.026727  [ 1700/ 3386]
loss: 0.308077  [ 1800/ 3386]
loss: 0.486727  [ 1900/ 3386]
loss: 0.031497  [ 2000/ 3386]
loss: 0.109123  [ 2100/ 3386]
loss: 0.710180  [ 2200/ 3386]
loss: 0.095761  [ 2300/ 3386]
loss: 0.097490  [ 2400/ 3386]
loss: 0.043377  [ 2500/ 3386]
loss: 0.164472  [ 2600/ 3386]
loss: 0.209677  [ 2700/ 3386]
loss: 0.085867  [ 2800/ 3386]
loss: 0.061900  [ 2900/ 3386]
loss: 0.014659  [ 3000/ 3386]
loss: 0.117299  [ 3100/ 3386]
loss: 0.087024  [ 3200/ 3386]
loss: 0.112299  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.050567  [    0/ 3386]
loss: 0.115715  [  100/ 3386]
loss: 0.097467  [  200/ 3386]
loss: 0.106675  [  300/ 3386]
loss: 0.216468  [  400/ 3386]
loss: 0.070329  [  500/ 3386]
loss: 0.156935  [  600/ 3386]
loss: 0.045939  [  700/ 3386]
loss: 0.115344  [  800/ 3386]
loss: 0.046872  [  900/ 3386]
loss: 0.127858  [ 1000/ 3386]
loss: 0.296634  [ 1100/ 3386]
loss: 0.143860  [ 1200/ 3386]
loss: 0.023733  [ 1300/ 3386]
loss: 0.050258  [ 1400/ 3386]
loss: 0.065325  [ 1500/ 3386]
loss: 0.036410  [ 1600/ 3386]
loss: 0.027730  [ 1700/ 3386]
loss: 0.315521  [ 1800/ 3386]
loss: 0.457907  [ 1900/ 3386]
loss: 0.029450  [ 2000/ 3386]
loss: 0.099186  [ 2100/ 3386]
loss: 0.682149  [ 2200/ 3386]
loss: 0.086873  [ 2300/ 3386]
loss: 0.089612  [ 2400/ 3386]
loss: 0.044547  [ 2500/ 3386]
loss: 0.156338  [ 2600/ 3386]
loss: 0.208197  [ 2700/ 3386]
loss: 0.098484  [ 2800/ 3386]
loss: 0.062337  [ 2900/ 3386]
loss: 0.015199  [ 3000/ 3386]
loss: 0.114946  [ 3100/ 3386]
loss: 0.080565  [ 3200/ 3386]
loss: 0.110930  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.050913  [    0/ 3386]
loss: 0.111999  [  100/ 3386]
loss: 0.097383  [  200/ 3386]
loss: 0.105831  [  300/ 3386]
loss: 0.208519  [  400/ 3386]
loss: 0.071372  [  500/ 3386]
loss: 0.156643  [  600/ 3386]
loss: 0.045527  [  700/ 3386]
loss: 0.116827  [  800/ 3386]
loss: 0.045249  [  900/ 3386]
loss: 0.127533  [ 1000/ 3386]
loss: 0.300683  [ 1100/ 3386]
loss: 0.138148  [ 1200/ 3386]
loss: 0.023450  [ 1300/ 3386]
loss: 0.049616  [ 1400/ 3386]
loss: 0.064611  [ 1500/ 3386]
loss: 0.036906  [ 1600/ 3386]
loss: 0.027415  [ 1700/ 3386]
loss: 0.320625  [ 1800/ 3386]
loss: 0.433095  [ 1900/ 3386]
loss: 0.029242  [ 2000/ 3386]
loss: 0.094845  [ 2100/ 3386]
loss: 0.662224  [ 2200/ 3386]
loss: 0.084599  [ 2300/ 3386]
loss: 0.086264  [ 2400/ 3386]
loss: 0.046318  [ 2500/ 3386]
loss: 0.152553  [ 2600/ 3386]
loss: 0.209228  [ 2700/ 3386]
loss: 0.104327  [ 2800/ 3386]
loss: 0.060642  [ 2900/ 3386]
loss: 0.015790  [ 3000/ 3386]
loss: 0.114157  [ 3100/ 3386]
loss: 0.076958  [ 3200/ 3386]
loss: 0.107829  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.051157  [    0/ 3386]
loss: 0.110090  [  100/ 3386]
loss: 0.097809  [  200/ 3386]
loss: 0.101059  [  300/ 3386]
loss: 0.209817  [  400/ 3386]
loss: 0.071840  [  500/ 3386]
loss: 0.156517  [  600/ 3386]
loss: 0.045292  [  700/ 3386]
loss: 0.117548  [  800/ 3386]
loss: 0.043943  [  900/ 3386]
loss: 0.130739  [ 1000/ 3386]
loss: 0.306014  [ 1100/ 3386]
loss: 0.134242  [ 1200/ 3386]
loss: 0.022853  [ 1300/ 3386]
loss: 0.047964  [ 1400/ 3386]
loss: 0.066529  [ 1500/ 3386]
loss: 0.037337  [ 1600/ 3386]
loss: 0.027068  [ 1700/ 3386]
loss: 0.318550  [ 1800/ 3386]
loss: 0.402417  [ 1900/ 3386]
loss: 0.028472  [ 2000/ 3386]
loss: 0.090547  [ 2100/ 3386]
loss: 0.626487  [ 2200/ 3386]
loss: 0.085118  [ 2300/ 3386]
loss: 0.083418  [ 2400/ 3386]
loss: 0.048079  [ 2500/ 3386]
loss: 0.159778  [ 2600/ 3386]
loss: 0.210049  [ 2700/ 3386]
loss: 0.112885  [ 2800/ 3386]
loss: 0.061039  [ 2900/ 3386]
loss: 0.016364  [ 3000/ 3386]
loss: 0.114035  [ 3100/ 3386]
loss: 0.074475  [ 3200/ 3386]
loss: 0.105552  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.051981  [    0/ 3386]
loss: 0.111026  [  100/ 3386]
loss: 0.095792  [  200/ 3386]
loss: 0.092772  [  300/ 3386]
loss: 0.209221  [  400/ 3386]
loss: 0.073757  [  500/ 3386]
loss: 0.155898  [  600/ 3386]
loss: 0.045530  [  700/ 3386]
loss: 0.118040  [  800/ 3386]
loss: 0.042852  [  900/ 3386]
loss: 0.137710  [ 1000/ 3386]
loss: 0.313918  [ 1100/ 3386]
loss: 0.129233  [ 1200/ 3386]
loss: 0.022884  [ 1300/ 3386]
loss: 0.047152  [ 1400/ 3386]
loss: 0.067787  [ 1500/ 3386]
loss: 0.037981  [ 1600/ 3386]
loss: 0.027427  [ 1700/ 3386]
loss: 0.309193  [ 1800/ 3386]
loss: 0.384143  [ 1900/ 3386]
loss: 0.029109  [ 2000/ 3386]
loss: 0.087361  [ 2100/ 3386]
loss: 0.576439  [ 2200/ 3386]
loss: 0.081473  [ 2300/ 3386]
loss: 0.078861  [ 2400/ 3386]
loss: 0.046385  [ 2500/ 3386]
loss: 0.168365  [ 2600/ 3386]
loss: 0.209506  [ 2700/ 3386]
loss: 0.116036  [ 2800/ 3386]
loss: 0.059792  [ 2900/ 3386]
loss: 0.016408  [ 3000/ 3386]
loss: 0.115942  [ 3100/ 3386]
loss: 0.074731  [ 3200/ 3386]
loss: 0.104338  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.051941  [    0/ 3386]
loss: 0.113658  [  100/ 3386]
loss: 0.094374  [  200/ 3386]
loss: 0.084603  [  300/ 3386]
loss: 0.207171  [  400/ 3386]
loss: 0.074937  [  500/ 3386]
loss: 0.151311  [  600/ 3386]
loss: 0.045875  [  700/ 3386]
loss: 0.115397  [  800/ 3386]
loss: 0.042719  [  900/ 3386]
loss: 0.146077  [ 1000/ 3386]
loss: 0.311952  [ 1100/ 3386]
loss: 0.127264  [ 1200/ 3386]
loss: 0.023269  [ 1300/ 3386]
loss: 0.046023  [ 1400/ 3386]
loss: 0.075309  [ 1500/ 3386]
loss: 0.037152  [ 1600/ 3386]
loss: 0.028025  [ 1700/ 3386]
loss: 0.287346  [ 1800/ 3386]
loss: 0.373505  [ 1900/ 3386]
loss: 0.029412  [ 2000/ 3386]
loss: 0.087435  [ 2100/ 3386]
loss: 0.566625  [ 2200/ 3386]
loss: 0.082721  [ 2300/ 3386]
loss: 0.075604  [ 2400/ 3386]
loss: 0.045909  [ 2500/ 3386]
loss: 0.178176  [ 2600/ 3386]
loss: 0.208507  [ 2700/ 3386]
loss: 0.114001  [ 2800/ 3386]
loss: 0.059364  [ 2900/ 3386]
loss: 0.016387  [ 3000/ 3386]
loss: 0.119705  [ 3100/ 3386]
loss: 0.075226  [ 3200/ 3386]
loss: 0.104529  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.052028  [    0/ 3386]
loss: 0.114806  [  100/ 3386]
loss: 0.092684  [  200/ 3386]
loss: 0.073842  [  300/ 3386]
loss: 0.203399  [  400/ 3386]
loss: 0.076139  [  500/ 3386]
loss: 0.146861  [  600/ 3386]
loss: 0.045467  [  700/ 3386]
loss: 0.111932  [  800/ 3386]
loss: 0.043387  [  900/ 3386]
loss: 0.153045  [ 1000/ 3386]
loss: 0.308471  [ 1100/ 3386]
loss: 0.124128  [ 1200/ 3386]
loss: 0.023385  [ 1300/ 3386]
loss: 0.045909  [ 1400/ 3386]
loss: 0.085819  [ 1500/ 3386]
loss: 0.037154  [ 1600/ 3386]
loss: 0.028667  [ 1700/ 3386]
loss: 0.263878  [ 1800/ 3386]
loss: 0.357213  [ 1900/ 3386]
loss: 0.031068  [ 2000/ 3386]
loss: 0.086893  [ 2100/ 3386]
loss: 0.512420  [ 2200/ 3386]
loss: 0.081747  [ 2300/ 3386]
loss: 0.071634  [ 2400/ 3386]
loss: 0.045547  [ 2500/ 3386]
loss: 0.185587  [ 2600/ 3386]
loss: 0.207550  [ 2700/ 3386]
loss: 0.089524  [ 2800/ 3386]
loss: 0.057595  [ 2900/ 3386]
loss: 0.017862  [ 3000/ 3386]
loss: 0.118719  [ 3100/ 3386]
loss: 0.070340  [ 3200/ 3386]
loss: 0.098744  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [-0.30654943  0.3991142 ]
[0 2 0 ... 2 0 1]
[0 2 0 ... 2 0 1]
Cluster 0 Occurrences: 1079; KMEANS: 1165
Cluster 1 Occurrences: 1158; KMEANS: 1126
Cluster 2 Occurrences: 1149; KMEANS: 1095
Centroids: [[-0.71112907, 0.32608846], [3.221461, 2.397693], [-0.9235198, -0.9392722]]
Centroids: [[-0.5220545, 0.39770323], [3.3870761, 2.4213386], [-1.1905341, -1.0416415]]
Contingency Matrix: 
[[ 986    0   93]
 [  74 1081    3]
 [ 105   45  999]]
[[986, -1, 93], [-1, -1, -1], [105, -1, 999]]
[[986, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 2, 0: 0}
New Contingency Matrix: 
[[ 986    0   93]
 [  74 1081    3]
 [ 105   45  999]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [986, 1081, 999], Sum: 3066
All_Elements: [986, 0, 93, 74, 1081, 3, 105, 45, 999], Sum: 3386
Accuracy: 0.9054932073242764
Done!
