Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_21
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA03C240B8>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x000001FA03BDDE48>
Epoch 1
-------------------------------
loss: 0.152255  [    0/ 3474]
loss: 0.077668  [  100/ 3474]
loss: 0.114738  [  200/ 3474]
loss: 0.091967  [  300/ 3474]
loss: 0.188315  [  400/ 3474]
loss: 0.012570  [  500/ 3474]
loss: 0.045852  [  600/ 3474]
loss: 0.023557  [  700/ 3474]
loss: 0.054247  [  800/ 3474]
loss: 0.204935  [  900/ 3474]
loss: 0.028737  [ 1000/ 3474]
loss: 0.190814  [ 1100/ 3474]
loss: 0.048181  [ 1200/ 3474]
loss: 0.040326  [ 1300/ 3474]
loss: 0.020612  [ 1400/ 3474]
loss: 0.078127  [ 1500/ 3474]
loss: 0.009362  [ 1600/ 3474]
loss: 0.125155  [ 1700/ 3474]
loss: 0.029277  [ 1800/ 3474]
loss: 0.027767  [ 1900/ 3474]
loss: 0.008524  [ 2000/ 3474]
loss: 0.016088  [ 2100/ 3474]
loss: 0.021703  [ 2200/ 3474]
loss: 0.070003  [ 2300/ 3474]
loss: 0.016186  [ 2400/ 3474]
loss: 0.054128  [ 2500/ 3474]
loss: 0.025053  [ 2600/ 3474]
loss: 0.068452  [ 2700/ 3474]
loss: 0.036348  [ 2800/ 3474]
loss: 0.015833  [ 2900/ 3474]
loss: 0.172083  [ 3000/ 3474]
loss: 0.009747  [ 3100/ 3474]
loss: 0.013315  [ 3200/ 3474]
loss: 0.048767  [ 3300/ 3474]
loss: 0.072993  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.046911  [    0/ 3474]
loss: 0.011786  [  100/ 3474]
loss: 0.025254  [  200/ 3474]
loss: 0.062034  [  300/ 3474]
loss: 0.062118  [  400/ 3474]
loss: 0.005905  [  500/ 3474]
loss: 0.041071  [  600/ 3474]
loss: 0.012488  [  700/ 3474]
loss: 0.044822  [  800/ 3474]
loss: 0.201364  [  900/ 3474]
loss: 0.005912  [ 1000/ 3474]
loss: 0.168069  [ 1100/ 3474]
loss: 0.031076  [ 1200/ 3474]
loss: 0.030951  [ 1300/ 3474]
loss: 0.011438  [ 1400/ 3474]
loss: 0.077137  [ 1500/ 3474]
loss: 0.009733  [ 1600/ 3474]
loss: 0.082533  [ 1700/ 3474]
loss: 0.024942  [ 1800/ 3474]
loss: 0.030334  [ 1900/ 3474]
loss: 0.008213  [ 2000/ 3474]
loss: 0.016175  [ 2100/ 3474]
loss: 0.022187  [ 2200/ 3474]
loss: 0.068868  [ 2300/ 3474]
loss: 0.006513  [ 2400/ 3474]
loss: 0.051470  [ 2500/ 3474]
loss: 0.020997  [ 2600/ 3474]
loss: 0.044747  [ 2700/ 3474]
loss: 0.031703  [ 2800/ 3474]
loss: 0.016709  [ 2900/ 3474]
loss: 0.152246  [ 3000/ 3474]
loss: 0.010051  [ 3100/ 3474]
loss: 0.015753  [ 3200/ 3474]
loss: 0.049249  [ 3300/ 3474]
loss: 0.075229  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.032395  [    0/ 3474]
loss: 0.010517  [  100/ 3474]
loss: 0.020904  [  200/ 3474]
loss: 0.060178  [  300/ 3474]
loss: 0.060729  [  400/ 3474]
loss: 0.005903  [  500/ 3474]
loss: 0.040257  [  600/ 3474]
loss: 0.009798  [  700/ 3474]
loss: 0.044141  [  800/ 3474]
loss: 0.197395  [  900/ 3474]
loss: 0.006750  [ 1000/ 3474]
loss: 0.158596  [ 1100/ 3474]
loss: 0.029189  [ 1200/ 3474]
loss: 0.031780  [ 1300/ 3474]
loss: 0.012280  [ 1400/ 3474]
loss: 0.082659  [ 1500/ 3474]
loss: 0.009808  [ 1600/ 3474]
loss: 0.070958  [ 1700/ 3474]
loss: 0.024009  [ 1800/ 3474]
loss: 0.030771  [ 1900/ 3474]
loss: 0.008375  [ 2000/ 3474]
loss: 0.015569  [ 2100/ 3474]
loss: 0.022733  [ 2200/ 3474]
loss: 0.068738  [ 2300/ 3474]
loss: 0.006924  [ 2400/ 3474]
loss: 0.051855  [ 2500/ 3474]
loss: 0.019652  [ 2600/ 3474]
loss: 0.039677  [ 2700/ 3474]
loss: 0.032661  [ 2800/ 3474]
loss: 0.017062  [ 2900/ 3474]
loss: 0.149185  [ 3000/ 3474]
loss: 0.009995  [ 3100/ 3474]
loss: 0.016343  [ 3200/ 3474]
loss: 0.048858  [ 3300/ 3474]
loss: 0.075373  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.030037  [    0/ 3474]
loss: 0.010600  [  100/ 3474]
loss: 0.021541  [  200/ 3474]
loss: 0.059553  [  300/ 3474]
loss: 0.060553  [  400/ 3474]
loss: 0.005993  [  500/ 3474]
loss: 0.041209  [  600/ 3474]
loss: 0.009190  [  700/ 3474]
loss: 0.045083  [  800/ 3474]
loss: 0.195563  [  900/ 3474]
loss: 0.007348  [ 1000/ 3474]
loss: 0.140956  [ 1100/ 3474]
loss: 0.028880  [ 1200/ 3474]
loss: 0.030886  [ 1300/ 3474]
loss: 0.011950  [ 1400/ 3474]
loss: 0.083969  [ 1500/ 3474]
loss: 0.009406  [ 1600/ 3474]
loss: 0.068776  [ 1700/ 3474]
loss: 0.023251  [ 1800/ 3474]
loss: 0.030365  [ 1900/ 3474]
loss: 0.008171  [ 2000/ 3474]
loss: 0.015132  [ 2100/ 3474]
loss: 0.023035  [ 2200/ 3474]
loss: 0.069219  [ 2300/ 3474]
loss: 0.006959  [ 2400/ 3474]
loss: 0.052422  [ 2500/ 3474]
loss: 0.018896  [ 2600/ 3474]
loss: 0.039498  [ 2700/ 3474]
loss: 0.033477  [ 2800/ 3474]
loss: 0.017204  [ 2900/ 3474]
loss: 0.147674  [ 3000/ 3474]
loss: 0.009928  [ 3100/ 3474]
loss: 0.016280  [ 3200/ 3474]
loss: 0.048663  [ 3300/ 3474]
loss: 0.073947  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.030184  [    0/ 3474]
loss: 0.010441  [  100/ 3474]
loss: 0.022205  [  200/ 3474]
loss: 0.059288  [  300/ 3474]
loss: 0.060104  [  400/ 3474]
loss: 0.006138  [  500/ 3474]
loss: 0.041848  [  600/ 3474]
loss: 0.009269  [  700/ 3474]
loss: 0.044879  [  800/ 3474]
loss: 0.195983  [  900/ 3474]
loss: 0.007561  [ 1000/ 3474]
loss: 0.128626  [ 1100/ 3474]
loss: 0.028785  [ 1200/ 3474]
loss: 0.030537  [ 1300/ 3474]
loss: 0.012085  [ 1400/ 3474]
loss: 0.083550  [ 1500/ 3474]
loss: 0.009347  [ 1600/ 3474]
loss: 0.065603  [ 1700/ 3474]
loss: 0.022351  [ 1800/ 3474]
loss: 0.030224  [ 1900/ 3474]
loss: 0.008226  [ 2000/ 3474]
loss: 0.014780  [ 2100/ 3474]
loss: 0.023513  [ 2200/ 3474]
loss: 0.070064  [ 2300/ 3474]
loss: 0.007023  [ 2400/ 3474]
loss: 0.053030  [ 2500/ 3474]
loss: 0.017986  [ 2600/ 3474]
loss: 0.038756  [ 2700/ 3474]
loss: 0.034166  [ 2800/ 3474]
loss: 0.017241  [ 2900/ 3474]
loss: 0.149245  [ 3000/ 3474]
loss: 0.010031  [ 3100/ 3474]
loss: 0.015817  [ 3200/ 3474]
loss: 0.048204  [ 3300/ 3474]
loss: 0.073967  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.029680  [    0/ 3474]
loss: 0.010455  [  100/ 3474]
loss: 0.022753  [  200/ 3474]
loss: 0.059156  [  300/ 3474]
loss: 0.059084  [  400/ 3474]
loss: 0.006229  [  500/ 3474]
loss: 0.042631  [  600/ 3474]
loss: 0.009567  [  700/ 3474]
loss: 0.045427  [  800/ 3474]
loss: 0.196797  [  900/ 3474]
loss: 0.007267  [ 1000/ 3474]
loss: 0.116588  [ 1100/ 3474]
loss: 0.028843  [ 1200/ 3474]
loss: 0.030319  [ 1300/ 3474]
loss: 0.011422  [ 1400/ 3474]
loss: 0.083615  [ 1500/ 3474]
loss: 0.008942  [ 1600/ 3474]
loss: 0.063500  [ 1700/ 3474]
loss: 0.021672  [ 1800/ 3474]
loss: 0.029665  [ 1900/ 3474]
loss: 0.008177  [ 2000/ 3474]
loss: 0.014357  [ 2100/ 3474]
loss: 0.023880  [ 2200/ 3474]
loss: 0.071274  [ 2300/ 3474]
loss: 0.006772  [ 2400/ 3474]
loss: 0.053668  [ 2500/ 3474]
loss: 0.017438  [ 2600/ 3474]
loss: 0.039143  [ 2700/ 3474]
loss: 0.034955  [ 2800/ 3474]
loss: 0.017319  [ 2900/ 3474]
loss: 0.152285  [ 3000/ 3474]
loss: 0.009990  [ 3100/ 3474]
loss: 0.014980  [ 3200/ 3474]
loss: 0.047428  [ 3300/ 3474]
loss: 0.075855  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.030309  [    0/ 3474]
loss: 0.010201  [  100/ 3474]
loss: 0.023207  [  200/ 3474]
loss: 0.059187  [  300/ 3474]
loss: 0.058543  [  400/ 3474]
loss: 0.006283  [  500/ 3474]
loss: 0.043081  [  600/ 3474]
loss: 0.009713  [  700/ 3474]
loss: 0.045636  [  800/ 3474]
loss: 0.197398  [  900/ 3474]
loss: 0.007454  [ 1000/ 3474]
loss: 0.109416  [ 1100/ 3474]
loss: 0.029408  [ 1200/ 3474]
loss: 0.029682  [ 1300/ 3474]
loss: 0.012395  [ 1400/ 3474]
loss: 0.084910  [ 1500/ 3474]
loss: 0.008805  [ 1600/ 3474]
loss: 0.060057  [ 1700/ 3474]
loss: 0.021144  [ 1800/ 3474]
loss: 0.029521  [ 1900/ 3474]
loss: 0.008125  [ 2000/ 3474]
loss: 0.014120  [ 2100/ 3474]
loss: 0.024194  [ 2200/ 3474]
loss: 0.072980  [ 2300/ 3474]
loss: 0.006740  [ 2400/ 3474]
loss: 0.054185  [ 2500/ 3474]
loss: 0.016847  [ 2600/ 3474]
loss: 0.039068  [ 2700/ 3474]
loss: 0.035641  [ 2800/ 3474]
loss: 0.017454  [ 2900/ 3474]
loss: 0.154356  [ 3000/ 3474]
loss: 0.009955  [ 3100/ 3474]
loss: 0.014636  [ 3200/ 3474]
loss: 0.046701  [ 3300/ 3474]
loss: 0.076918  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.031377  [    0/ 3474]
loss: 0.010130  [  100/ 3474]
loss: 0.023727  [  200/ 3474]
loss: 0.059321  [  300/ 3474]
loss: 0.058825  [  400/ 3474]
loss: 0.006358  [  500/ 3474]
loss: 0.043309  [  600/ 3474]
loss: 0.009854  [  700/ 3474]
loss: 0.045402  [  800/ 3474]
loss: 0.196953  [  900/ 3474]
loss: 0.007635  [ 1000/ 3474]
loss: 0.101192  [ 1100/ 3474]
loss: 0.030173  [ 1200/ 3474]
loss: 0.029092  [ 1300/ 3474]
loss: 0.012488  [ 1400/ 3474]
loss: 0.084960  [ 1500/ 3474]
loss: 0.008598  [ 1600/ 3474]
loss: 0.058414  [ 1700/ 3474]
loss: 0.020588  [ 1800/ 3474]
loss: 0.029083  [ 1900/ 3474]
loss: 0.008162  [ 2000/ 3474]
loss: 0.013819  [ 2100/ 3474]
loss: 0.024604  [ 2200/ 3474]
loss: 0.073328  [ 2300/ 3474]
loss: 0.006685  [ 2400/ 3474]
loss: 0.054360  [ 2500/ 3474]
loss: 0.016452  [ 2600/ 3474]
loss: 0.038817  [ 2700/ 3474]
loss: 0.036570  [ 2800/ 3474]
loss: 0.017612  [ 2900/ 3474]
loss: 0.157560  [ 3000/ 3474]
loss: 0.009937  [ 3100/ 3474]
loss: 0.014653  [ 3200/ 3474]
loss: 0.045308  [ 3300/ 3474]
loss: 0.077709  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [-0.5745379   0.43984914]
[0 1 1 ... 1 1 2]
[1 2 2 ... 2 2 0]
Cluster 0 Occurrences: 1198; KMEANS: 1173
Cluster 1 Occurrences: 1128; KMEANS: 1186
Cluster 2 Occurrences: 1148; KMEANS: 1115
Centroids: [[-1.5151526, 0.64866096], [0.67051136, 1.2939227], [-0.41395593, -1.2505848]]
Centroids: [[-0.40567505, -1.2411275], [-1.5323802, 0.66795355], [0.680917, 1.3135597]]
Contingency Matrix: 
[[  19 1175    4]
 [  11    7 1110]
 [1143    4    1]]
[[-1, -1, -1], [11, -1, 1110], [1143, -1, 1]]
[[-1, -1, -1], [-1, -1, 1110], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 1, 2: 0, 1: 2}
New Contingency Matrix: 
[[1175    4   19]
 [   7 1110   11]
 [   4    1 1143]]
New Clustered Label Sequence: [1, 2, 0]
Diagonal_Elements: [1175, 1110, 1143], Sum: 3428
All_Elements: [1175, 4, 19, 7, 1110, 11, 4, 1, 1143], Sum: 3474
Accuracy: 0.9867587795048935
Done!
