Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise010.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Difficult1_noise010.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_57_48
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA846A1828>
Sampling rate: 24000.0
Raw: [-0.08093593 -0.05766541 -0.01986255 ... -0.05525448 -0.03568967
 -0.02908211]
Times: [   1054    1166    1249 ... 1439165 1439229 1439456]
Cluster: [3 3 3 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3448
First aligned Spike Frame: [-4.79707202e-02 -8.83141277e-02 -1.14477415e-01 -1.20070620e-01
 -1.07771810e-01 -8.38316152e-02 -5.31944576e-02 -2.65798100e-02
 -6.71143520e-03  2.42590968e-02  6.29914810e-02  1.33638321e-01
  3.34948892e-01  7.06515114e-01  1.07966210e+00  1.10962398e+00
  6.55362247e-01 -7.19265760e-04 -4.64310305e-01 -6.23655437e-01
 -6.27154873e-01 -6.07058236e-01 -5.79298390e-01 -5.10136794e-01
 -4.12813198e-01 -3.36153681e-01 -2.86889156e-01 -2.38391657e-01
 -1.98571332e-01 -1.83172365e-01 -1.85707124e-01 -1.94134424e-01
 -2.08249204e-01 -2.33590971e-01 -2.49276546e-01 -2.46246400e-01
 -2.44939654e-01 -2.59799331e-01 -2.74706204e-01 -2.68658955e-01
 -2.53059716e-01 -2.46742300e-01 -2.46057086e-01 -2.38944634e-01
 -2.31254619e-01 -2.20748865e-01 -1.79535377e-01]
Cluster 0, Occurrences: 1164
Cluster 1, Occurrences: 1155
Cluster 2, Occurrences: 1129
<torch.utils.data.dataloader.DataLoader object at 0x000001FA871FC6D8>
Epoch 1
-------------------------------
loss: 0.191293  [    0/ 3448]
loss: 0.026029  [  100/ 3448]
loss: 0.019727  [  200/ 3448]
loss: 0.034551  [  300/ 3448]
loss: 0.015941  [  400/ 3448]
loss: 0.015645  [  500/ 3448]
loss: 0.013683  [  600/ 3448]
loss: 0.010349  [  700/ 3448]
loss: 0.007773  [  800/ 3448]
loss: 0.020939  [  900/ 3448]
loss: 0.085716  [ 1000/ 3448]
loss: 0.012226  [ 1100/ 3448]
loss: 0.011164  [ 1200/ 3448]
loss: 0.124888  [ 1300/ 3448]
loss: 0.008839  [ 1400/ 3448]
loss: 0.016806  [ 1500/ 3448]
loss: 0.008540  [ 1600/ 3448]
loss: 0.013758  [ 1700/ 3448]
loss: 0.007907  [ 1800/ 3448]
loss: 0.016979  [ 1900/ 3448]
loss: 0.007885  [ 2000/ 3448]
loss: 0.003626  [ 2100/ 3448]
loss: 0.007522  [ 2200/ 3448]
loss: 0.006333  [ 2300/ 3448]
loss: 0.009996  [ 2400/ 3448]
loss: 0.007360  [ 2500/ 3448]
loss: 0.012602  [ 2600/ 3448]
loss: 0.008507  [ 2700/ 3448]
loss: 0.006764  [ 2800/ 3448]
loss: 0.002616  [ 2900/ 3448]
loss: 0.004497  [ 3000/ 3448]
loss: 0.014901  [ 3100/ 3448]
loss: 0.012148  [ 3200/ 3448]
loss: 0.010328  [ 3300/ 3448]
loss: 0.006526  [ 3400/ 3448]
Epoch 2
-------------------------------
loss: 0.022159  [    0/ 3448]
loss: 0.008856  [  100/ 3448]
loss: 0.001771  [  200/ 3448]
loss: 0.002557  [  300/ 3448]
loss: 0.006155  [  400/ 3448]
loss: 0.013850  [  500/ 3448]
loss: 0.006001  [  600/ 3448]
loss: 0.008191  [  700/ 3448]
loss: 0.003580  [  800/ 3448]
loss: 0.006886  [  900/ 3448]
loss: 0.084106  [ 1000/ 3448]
loss: 0.014881  [ 1100/ 3448]
loss: 0.009137  [ 1200/ 3448]
loss: 0.125552  [ 1300/ 3448]
loss: 0.006760  [ 1400/ 3448]
loss: 0.020004  [ 1500/ 3448]
loss: 0.004327  [ 1600/ 3448]
loss: 0.012030  [ 1700/ 3448]
loss: 0.006785  [ 1800/ 3448]
loss: 0.017078  [ 1900/ 3448]
loss: 0.008375  [ 2000/ 3448]
loss: 0.003343  [ 2100/ 3448]
loss: 0.005713  [ 2200/ 3448]
loss: 0.005822  [ 2300/ 3448]
loss: 0.008048  [ 2400/ 3448]
loss: 0.008112  [ 2500/ 3448]
loss: 0.012764  [ 2600/ 3448]
loss: 0.008987  [ 2700/ 3448]
loss: 0.006297  [ 2800/ 3448]
loss: 0.002864  [ 2900/ 3448]
loss: 0.004203  [ 3000/ 3448]
loss: 0.013104  [ 3100/ 3448]
loss: 0.012914  [ 3200/ 3448]
loss: 0.010769  [ 3300/ 3448]
loss: 0.006656  [ 3400/ 3448]
Epoch 3
-------------------------------
loss: 0.021180  [    0/ 3448]
loss: 0.008100  [  100/ 3448]
loss: 0.002170  [  200/ 3448]
loss: 0.003517  [  300/ 3448]
loss: 0.006801  [  400/ 3448]
loss: 0.013218  [  500/ 3448]
loss: 0.006528  [  600/ 3448]
loss: 0.007924  [  700/ 3448]
loss: 0.003634  [  800/ 3448]
loss: 0.005976  [  900/ 3448]
loss: 0.083853  [ 1000/ 3448]
loss: 0.015027  [ 1100/ 3448]
loss: 0.008773  [ 1200/ 3448]
loss: 0.125448  [ 1300/ 3448]
loss: 0.006540  [ 1400/ 3448]
loss: 0.020700  [ 1500/ 3448]
loss: 0.003910  [ 1600/ 3448]
loss: 0.011682  [ 1700/ 3448]
loss: 0.006964  [ 1800/ 3448]
loss: 0.017109  [ 1900/ 3448]
loss: 0.008372  [ 2000/ 3448]
loss: 0.003295  [ 2100/ 3448]
loss: 0.005400  [ 2200/ 3448]
loss: 0.005785  [ 2300/ 3448]
loss: 0.007828  [ 2400/ 3448]
loss: 0.008236  [ 2500/ 3448]
loss: 0.012591  [ 2600/ 3448]
loss: 0.009224  [ 2700/ 3448]
loss: 0.006469  [ 2800/ 3448]
loss: 0.002767  [ 2900/ 3448]
loss: 0.004080  [ 3000/ 3448]
loss: 0.012937  [ 3100/ 3448]
loss: 0.013110  [ 3200/ 3448]
loss: 0.010761  [ 3300/ 3448]
loss: 0.006825  [ 3400/ 3448]
Epoch 4
-------------------------------
loss: 0.020546  [    0/ 3448]
loss: 0.007673  [  100/ 3448]
loss: 0.003335  [  200/ 3448]
loss: 0.003678  [  300/ 3448]
loss: 0.005349  [  400/ 3448]
loss: 0.012878  [  500/ 3448]
loss: 0.006578  [  600/ 3448]
loss: 0.007851  [  700/ 3448]
loss: 0.003824  [  800/ 3448]
loss: 0.005675  [  900/ 3448]
loss: 0.084517  [ 1000/ 3448]
loss: 0.015007  [ 1100/ 3448]
loss: 0.008089  [ 1200/ 3448]
loss: 0.124897  [ 1300/ 3448]
loss: 0.006646  [ 1400/ 3448]
loss: 0.020963  [ 1500/ 3448]
loss: 0.003916  [ 1600/ 3448]
loss: 0.011788  [ 1700/ 3448]
loss: 0.007059  [ 1800/ 3448]
loss: 0.017570  [ 1900/ 3448]
loss: 0.008300  [ 2000/ 3448]
loss: 0.003230  [ 2100/ 3448]
loss: 0.005241  [ 2200/ 3448]
loss: 0.005778  [ 2300/ 3448]
loss: 0.007845  [ 2400/ 3448]
loss: 0.008270  [ 2500/ 3448]
loss: 0.012657  [ 2600/ 3448]
loss: 0.009180  [ 2700/ 3448]
loss: 0.007007  [ 2800/ 3448]
loss: 0.002399  [ 2900/ 3448]
loss: 0.004170  [ 3000/ 3448]
loss: 0.012903  [ 3100/ 3448]
loss: 0.012900  [ 3200/ 3448]
loss: 0.010788  [ 3300/ 3448]
loss: 0.006687  [ 3400/ 3448]
Epoch 5
-------------------------------
loss: 0.019894  [    0/ 3448]
loss: 0.007093  [  100/ 3448]
loss: 0.004203  [  200/ 3448]
loss: 0.004086  [  300/ 3448]
loss: 0.004490  [  400/ 3448]
loss: 0.012754  [  500/ 3448]
loss: 0.006627  [  600/ 3448]
loss: 0.007310  [  700/ 3448]
loss: 0.003982  [  800/ 3448]
loss: 0.005334  [  900/ 3448]
loss: 0.084719  [ 1000/ 3448]
loss: 0.014556  [ 1100/ 3448]
loss: 0.007502  [ 1200/ 3448]
loss: 0.124691  [ 1300/ 3448]
loss: 0.006513  [ 1400/ 3448]
loss: 0.021015  [ 1500/ 3448]
loss: 0.003880  [ 1600/ 3448]
loss: 0.011281  [ 1700/ 3448]
loss: 0.007318  [ 1800/ 3448]
loss: 0.017875  [ 1900/ 3448]
loss: 0.007759  [ 2000/ 3448]
loss: 0.003043  [ 2100/ 3448]
loss: 0.004991  [ 2200/ 3448]
loss: 0.006008  [ 2300/ 3448]
loss: 0.007330  [ 2400/ 3448]
loss: 0.008134  [ 2500/ 3448]
loss: 0.012687  [ 2600/ 3448]
loss: 0.008526  [ 2700/ 3448]
loss: 0.007437  [ 2800/ 3448]
loss: 0.002215  [ 2900/ 3448]
loss: 0.004488  [ 3000/ 3448]
loss: 0.013188  [ 3100/ 3448]
loss: 0.012477  [ 3200/ 3448]
loss: 0.010653  [ 3300/ 3448]
loss: 0.006357  [ 3400/ 3448]
Epoch 6
-------------------------------
loss: 0.019375  [    0/ 3448]
loss: 0.006554  [  100/ 3448]
loss: 0.004990  [  200/ 3448]
loss: 0.004945  [  300/ 3448]
loss: 0.003578  [  400/ 3448]
loss: 0.012961  [  500/ 3448]
loss: 0.006624  [  600/ 3448]
loss: 0.006490  [  700/ 3448]
loss: 0.004292  [  800/ 3448]
loss: 0.005188  [  900/ 3448]
loss: 0.084845  [ 1000/ 3448]
loss: 0.013931  [ 1100/ 3448]
loss: 0.007211  [ 1200/ 3448]
loss: 0.124640  [ 1300/ 3448]
loss: 0.006259  [ 1400/ 3448]
loss: 0.021238  [ 1500/ 3448]
loss: 0.003864  [ 1600/ 3448]
loss: 0.010631  [ 1700/ 3448]
loss: 0.007509  [ 1800/ 3448]
loss: 0.018789  [ 1900/ 3448]
loss: 0.007159  [ 2000/ 3448]
loss: 0.002945  [ 2100/ 3448]
loss: 0.004920  [ 2200/ 3448]
loss: 0.006299  [ 2300/ 3448]
loss: 0.007327  [ 2400/ 3448]
loss: 0.008249  [ 2500/ 3448]
loss: 0.012663  [ 2600/ 3448]
loss: 0.007945  [ 2700/ 3448]
loss: 0.007908  [ 2800/ 3448]
loss: 0.002101  [ 2900/ 3448]
loss: 0.004917  [ 3000/ 3448]
loss: 0.013494  [ 3100/ 3448]
loss: 0.011827  [ 3200/ 3448]
loss: 0.010640  [ 3300/ 3448]
loss: 0.006376  [ 3400/ 3448]
Epoch 7
-------------------------------
loss: 0.018209  [    0/ 3448]
loss: 0.005902  [  100/ 3448]
loss: 0.005867  [  200/ 3448]
loss: 0.005272  [  300/ 3448]
loss: 0.003894  [  400/ 3448]
loss: 0.013074  [  500/ 3448]
loss: 0.006790  [  600/ 3448]
loss: 0.006165  [  700/ 3448]
loss: 0.004905  [  800/ 3448]
loss: 0.004863  [  900/ 3448]
loss: 0.084590  [ 1000/ 3448]
loss: 0.013615  [ 1100/ 3448]
loss: 0.007008  [ 1200/ 3448]
loss: 0.124604  [ 1300/ 3448]
loss: 0.006029  [ 1400/ 3448]
loss: 0.021368  [ 1500/ 3448]
loss: 0.004072  [ 1600/ 3448]
loss: 0.010281  [ 1700/ 3448]
loss: 0.007590  [ 1800/ 3448]
loss: 0.019116  [ 1900/ 3448]
loss: 0.006661  [ 2000/ 3448]
loss: 0.003068  [ 2100/ 3448]
loss: 0.004746  [ 2200/ 3448]
loss: 0.006184  [ 2300/ 3448]
loss: 0.007348  [ 2400/ 3448]
loss: 0.008160  [ 2500/ 3448]
loss: 0.012547  [ 2600/ 3448]
loss: 0.007662  [ 2700/ 3448]
loss: 0.007792  [ 2800/ 3448]
loss: 0.002031  [ 2900/ 3448]
loss: 0.005147  [ 3000/ 3448]
loss: 0.013663  [ 3100/ 3448]
loss: 0.011140  [ 3200/ 3448]
loss: 0.010698  [ 3300/ 3448]
loss: 0.006420  [ 3400/ 3448]
Epoch 8
-------------------------------
loss: 0.017131  [    0/ 3448]
loss: 0.005427  [  100/ 3448]
loss: 0.005540  [  200/ 3448]
loss: 0.005768  [  300/ 3448]
loss: 0.003770  [  400/ 3448]
loss: 0.012969  [  500/ 3448]
loss: 0.006865  [  600/ 3448]
loss: 0.006172  [  700/ 3448]
loss: 0.005103  [  800/ 3448]
loss: 0.004391  [  900/ 3448]
loss: 0.083814  [ 1000/ 3448]
loss: 0.013372  [ 1100/ 3448]
loss: 0.006959  [ 1200/ 3448]
loss: 0.124503  [ 1300/ 3448]
loss: 0.005934  [ 1400/ 3448]
loss: 0.020772  [ 1500/ 3448]
loss: 0.004379  [ 1600/ 3448]
loss: 0.010516  [ 1700/ 3448]
loss: 0.008001  [ 1800/ 3448]
loss: 0.018672  [ 1900/ 3448]
loss: 0.006189  [ 2000/ 3448]
loss: 0.003038  [ 2100/ 3448]
loss: 0.004663  [ 2200/ 3448]
loss: 0.006243  [ 2300/ 3448]
loss: 0.007636  [ 2400/ 3448]
loss: 0.007957  [ 2500/ 3448]
loss: 0.012577  [ 2600/ 3448]
loss: 0.007464  [ 2700/ 3448]
loss: 0.007470  [ 2800/ 3448]
loss: 0.001970  [ 2900/ 3448]
loss: 0.005363  [ 3000/ 3448]
loss: 0.013606  [ 3100/ 3448]
loss: 0.010420  [ 3200/ 3448]
loss: 0.011172  [ 3300/ 3448]
loss: 0.006721  [ 3400/ 3448]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3448
First Spike after testing: [-0.7130932   0.57373166]
[2 2 2 ... 1 0 2]
[1 1 1 ... 0 2 1]
Cluster 0 Occurrences: 1164; KMEANS: 1127
Cluster 1 Occurrences: 1155; KMEANS: 1204
Cluster 2 Occurrences: 1129; KMEANS: 1117
Centroids: [[-0.4128478, 0.7152576], [-0.30921647, 0.23488733], [-0.8861226, 0.4406966]]
Centroids: [[-0.27558956, 0.21262996], [-0.90816075, 0.4698252], [-0.38864562, 0.7127105]]
Contingency Matrix: 
[[  20   79 1065]
 [1068   61   26]
 [  39 1064   26]]
[[-1, 79, 1065], [-1, -1, -1], [-1, 1064, 26]]
[[-1, -1, -1], [-1, -1, -1], [-1, 1064, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 0: 2, 2: 1}
New Contingency Matrix: 
[[1065   20   79]
 [  26 1068   61]
 [  26   39 1064]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1065, 1068, 1064], Sum: 3197
All_Elements: [1065, 20, 79, 26, 1068, 61, 26, 39, 1064], Sum: 3448
Accuracy: 0.9272041763341067
Done!
