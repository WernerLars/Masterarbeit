Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_35
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA03C24518>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x000001FA03BDDDD8>
Epoch 1
-------------------------------
loss: 0.262419  [    0/ 3298]
loss: 0.218012  [  100/ 3298]
loss: 0.119508  [  200/ 3298]
loss: 0.031556  [  300/ 3298]
loss: 0.019547  [  400/ 3298]
loss: 0.027767  [  500/ 3298]
loss: 0.037170  [  600/ 3298]
loss: 0.067122  [  700/ 3298]
loss: 0.025528  [  800/ 3298]
loss: 0.121833  [  900/ 3298]
loss: 0.036087  [ 1000/ 3298]
loss: 0.054696  [ 1100/ 3298]
loss: 0.070778  [ 1200/ 3298]
loss: 0.044319  [ 1300/ 3298]
loss: 0.013969  [ 1400/ 3298]
loss: 0.035417  [ 1500/ 3298]
loss: 0.010059  [ 1600/ 3298]
loss: 0.058178  [ 1700/ 3298]
loss: 0.046505  [ 1800/ 3298]
loss: 0.029068  [ 1900/ 3298]
loss: 0.010109  [ 2000/ 3298]
loss: 0.015783  [ 2100/ 3298]
loss: 0.014946  [ 2200/ 3298]
loss: 0.019519  [ 2300/ 3298]
loss: 0.015329  [ 2400/ 3298]
loss: 0.051605  [ 2500/ 3298]
loss: 0.032409  [ 2600/ 3298]
loss: 0.032086  [ 2700/ 3298]
loss: 0.239907  [ 2800/ 3298]
loss: 0.020998  [ 2900/ 3298]
loss: 0.035455  [ 3000/ 3298]
loss: 0.013763  [ 3100/ 3298]
loss: 0.077911  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.029193  [    0/ 3298]
loss: 0.024597  [  100/ 3298]
loss: 0.097194  [  200/ 3298]
loss: 0.016396  [  300/ 3298]
loss: 0.021134  [  400/ 3298]
loss: 0.023126  [  500/ 3298]
loss: 0.023275  [  600/ 3298]
loss: 0.068920  [  700/ 3298]
loss: 0.023724  [  800/ 3298]
loss: 0.127762  [  900/ 3298]
loss: 0.033329  [ 1000/ 3298]
loss: 0.066494  [ 1100/ 3298]
loss: 0.071319  [ 1200/ 3298]
loss: 0.028932  [ 1300/ 3298]
loss: 0.012823  [ 1400/ 3298]
loss: 0.020912  [ 1500/ 3298]
loss: 0.013890  [ 1600/ 3298]
loss: 0.068105  [ 1700/ 3298]
loss: 0.033933  [ 1800/ 3298]
loss: 0.023729  [ 1900/ 3298]
loss: 0.009396  [ 2000/ 3298]
loss: 0.013202  [ 2100/ 3298]
loss: 0.014357  [ 2200/ 3298]
loss: 0.018430  [ 2300/ 3298]
loss: 0.015121  [ 2400/ 3298]
loss: 0.019520  [ 2500/ 3298]
loss: 0.029753  [ 2600/ 3298]
loss: 0.043130  [ 2700/ 3298]
loss: 0.279943  [ 2800/ 3298]
loss: 0.022172  [ 2900/ 3298]
loss: 0.035517  [ 3000/ 3298]
loss: 0.015160  [ 3100/ 3298]
loss: 0.061199  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.028094  [    0/ 3298]
loss: 0.024201  [  100/ 3298]
loss: 0.103450  [  200/ 3298]
loss: 0.016872  [  300/ 3298]
loss: 0.020779  [  400/ 3298]
loss: 0.023492  [  500/ 3298]
loss: 0.020989  [  600/ 3298]
loss: 0.067139  [  700/ 3298]
loss: 0.024347  [  800/ 3298]
loss: 0.121664  [  900/ 3298]
loss: 0.029388  [ 1000/ 3298]
loss: 0.068383  [ 1100/ 3298]
loss: 0.071252  [ 1200/ 3298]
loss: 0.027288  [ 1300/ 3298]
loss: 0.012531  [ 1400/ 3298]
loss: 0.020495  [ 1500/ 3298]
loss: 0.014948  [ 1600/ 3298]
loss: 0.064415  [ 1700/ 3298]
loss: 0.031520  [ 1800/ 3298]
loss: 0.022639  [ 1900/ 3298]
loss: 0.009709  [ 2000/ 3298]
loss: 0.013201  [ 2100/ 3298]
loss: 0.012941  [ 2200/ 3298]
loss: 0.018135  [ 2300/ 3298]
loss: 0.014788  [ 2400/ 3298]
loss: 0.014696  [ 2500/ 3298]
loss: 0.029668  [ 2600/ 3298]
loss: 0.045103  [ 2700/ 3298]
loss: 0.260774  [ 2800/ 3298]
loss: 0.023865  [ 2900/ 3298]
loss: 0.035849  [ 3000/ 3298]
loss: 0.016038  [ 3100/ 3298]
loss: 0.055823  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.026485  [    0/ 3298]
loss: 0.023821  [  100/ 3298]
loss: 0.102831  [  200/ 3298]
loss: 0.016679  [  300/ 3298]
loss: 0.021584  [  400/ 3298]
loss: 0.023302  [  500/ 3298]
loss: 0.019125  [  600/ 3298]
loss: 0.067279  [  700/ 3298]
loss: 0.024533  [  800/ 3298]
loss: 0.122273  [  900/ 3298]
loss: 0.026569  [ 1000/ 3298]
loss: 0.069665  [ 1100/ 3298]
loss: 0.070709  [ 1200/ 3298]
loss: 0.027064  [ 1300/ 3298]
loss: 0.012357  [ 1400/ 3298]
loss: 0.020759  [ 1500/ 3298]
loss: 0.015119  [ 1600/ 3298]
loss: 0.060958  [ 1700/ 3298]
loss: 0.031386  [ 1800/ 3298]
loss: 0.022298  [ 1900/ 3298]
loss: 0.010232  [ 2000/ 3298]
loss: 0.013474  [ 2100/ 3298]
loss: 0.012784  [ 2200/ 3298]
loss: 0.018336  [ 2300/ 3298]
loss: 0.014492  [ 2400/ 3298]
loss: 0.012746  [ 2500/ 3298]
loss: 0.029092  [ 2600/ 3298]
loss: 0.045659  [ 2700/ 3298]
loss: 0.244431  [ 2800/ 3298]
loss: 0.024695  [ 2900/ 3298]
loss: 0.036042  [ 3000/ 3298]
loss: 0.016802  [ 3100/ 3298]
loss: 0.052990  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.025438  [    0/ 3298]
loss: 0.024163  [  100/ 3298]
loss: 0.102314  [  200/ 3298]
loss: 0.015043  [  300/ 3298]
loss: 0.019515  [  400/ 3298]
loss: 0.024341  [  500/ 3298]
loss: 0.017160  [  600/ 3298]
loss: 0.067714  [  700/ 3298]
loss: 0.024795  [  800/ 3298]
loss: 0.121113  [  900/ 3298]
loss: 0.025240  [ 1000/ 3298]
loss: 0.069452  [ 1100/ 3298]
loss: 0.070235  [ 1200/ 3298]
loss: 0.027234  [ 1300/ 3298]
loss: 0.012079  [ 1400/ 3298]
loss: 0.021049  [ 1500/ 3298]
loss: 0.015979  [ 1600/ 3298]
loss: 0.056992  [ 1700/ 3298]
loss: 0.031802  [ 1800/ 3298]
loss: 0.022429  [ 1900/ 3298]
loss: 0.010779  [ 2000/ 3298]
loss: 0.013542  [ 2100/ 3298]
loss: 0.012372  [ 2200/ 3298]
loss: 0.018330  [ 2300/ 3298]
loss: 0.014483  [ 2400/ 3298]
loss: 0.011084  [ 2500/ 3298]
loss: 0.028710  [ 2600/ 3298]
loss: 0.045639  [ 2700/ 3298]
loss: 0.236172  [ 2800/ 3298]
loss: 0.025079  [ 2900/ 3298]
loss: 0.036133  [ 3000/ 3298]
loss: 0.017366  [ 3100/ 3298]
loss: 0.051149  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.024910  [    0/ 3298]
loss: 0.024656  [  100/ 3298]
loss: 0.101614  [  200/ 3298]
loss: 0.013816  [  300/ 3298]
loss: 0.018547  [  400/ 3298]
loss: 0.024010  [  500/ 3298]
loss: 0.015642  [  600/ 3298]
loss: 0.067054  [  700/ 3298]
loss: 0.024987  [  800/ 3298]
loss: 0.120012  [  900/ 3298]
loss: 0.024727  [ 1000/ 3298]
loss: 0.068416  [ 1100/ 3298]
loss: 0.070245  [ 1200/ 3298]
loss: 0.027708  [ 1300/ 3298]
loss: 0.012007  [ 1400/ 3298]
loss: 0.020882  [ 1500/ 3298]
loss: 0.015957  [ 1600/ 3298]
loss: 0.049750  [ 1700/ 3298]
loss: 0.032470  [ 1800/ 3298]
loss: 0.022306  [ 1900/ 3298]
loss: 0.011248  [ 2000/ 3298]
loss: 0.013578  [ 2100/ 3298]
loss: 0.011862  [ 2200/ 3298]
loss: 0.018443  [ 2300/ 3298]
loss: 0.014365  [ 2400/ 3298]
loss: 0.009992  [ 2500/ 3298]
loss: 0.028228  [ 2600/ 3298]
loss: 0.045351  [ 2700/ 3298]
loss: 0.235059  [ 2800/ 3298]
loss: 0.024960  [ 2900/ 3298]
loss: 0.036164  [ 3000/ 3298]
loss: 0.017590  [ 3100/ 3298]
loss: 0.050105  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.024061  [    0/ 3298]
loss: 0.025367  [  100/ 3298]
loss: 0.101156  [  200/ 3298]
loss: 0.013128  [  300/ 3298]
loss: 0.017731  [  400/ 3298]
loss: 0.023619  [  500/ 3298]
loss: 0.014468  [  600/ 3298]
loss: 0.065992  [  700/ 3298]
loss: 0.025231  [  800/ 3298]
loss: 0.119315  [  900/ 3298]
loss: 0.024373  [ 1000/ 3298]
loss: 0.066868  [ 1100/ 3298]
loss: 0.070378  [ 1200/ 3298]
loss: 0.028274  [ 1300/ 3298]
loss: 0.011910  [ 1400/ 3298]
loss: 0.020839  [ 1500/ 3298]
loss: 0.015745  [ 1600/ 3298]
loss: 0.043192  [ 1700/ 3298]
loss: 0.032065  [ 1800/ 3298]
loss: 0.022171  [ 1900/ 3298]
loss: 0.011423  [ 2000/ 3298]
loss: 0.013436  [ 2100/ 3298]
loss: 0.011145  [ 2200/ 3298]
loss: 0.018563  [ 2300/ 3298]
loss: 0.014273  [ 2400/ 3298]
loss: 0.009644  [ 2500/ 3298]
loss: 0.028020  [ 2600/ 3298]
loss: 0.044600  [ 2700/ 3298]
loss: 0.233452  [ 2800/ 3298]
loss: 0.025418  [ 2900/ 3298]
loss: 0.035196  [ 3000/ 3298]
loss: 0.017881  [ 3100/ 3298]
loss: 0.049640  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.022809  [    0/ 3298]
loss: 0.026073  [  100/ 3298]
loss: 0.100682  [  200/ 3298]
loss: 0.012760  [  300/ 3298]
loss: 0.017132  [  400/ 3298]
loss: 0.023220  [  500/ 3298]
loss: 0.013761  [  600/ 3298]
loss: 0.063296  [  700/ 3298]
loss: 0.025446  [  800/ 3298]
loss: 0.122059  [  900/ 3298]
loss: 0.023601  [ 1000/ 3298]
loss: 0.064777  [ 1100/ 3298]
loss: 0.070425  [ 1200/ 3298]
loss: 0.028695  [ 1300/ 3298]
loss: 0.012120  [ 1400/ 3298]
loss: 0.020550  [ 1500/ 3298]
loss: 0.015643  [ 1600/ 3298]
loss: 0.038415  [ 1700/ 3298]
loss: 0.032195  [ 1800/ 3298]
loss: 0.022584  [ 1900/ 3298]
loss: 0.011755  [ 2000/ 3298]
loss: 0.012828  [ 2100/ 3298]
loss: 0.009960  [ 2200/ 3298]
loss: 0.018487  [ 2300/ 3298]
loss: 0.014034  [ 2400/ 3298]
loss: 0.009273  [ 2500/ 3298]
loss: 0.027480  [ 2600/ 3298]
loss: 0.043519  [ 2700/ 3298]
loss: 0.198935  [ 2800/ 3298]
loss: 0.026030  [ 2900/ 3298]
loss: 0.034180  [ 3000/ 3298]
loss: 0.017820  [ 3100/ 3298]
loss: 0.048744  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [1.6368821 1.1243241]
[1 0 0 ... 0 1 2]
[0 1 1 ... 1 0 2]
Cluster 0 Occurrences: 1094; KMEANS: 1090
Cluster 1 Occurrences: 1089; KMEANS: 1128
Cluster 2 Occurrences: 1115; KMEANS: 1080
Centroids: [[-1.3025514, 0.5289255], [0.9102883, 1.7476145], [-0.38586447, -1.1040491]]
Centroids: [[0.9240542, 1.7439449], [-1.3175666, 0.510714], [-0.3564167, -1.1353737]]
Contingency Matrix: 
[[   0 1074   20]
 [1083    5    1]
 [   7   49 1059]]
[[-1, 1074, 20], [-1, -1, -1], [-1, 49, 1059]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 1059]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 0: 1, 2: 2}
New Contingency Matrix: 
[[1074    0   20]
 [   5 1083    1]
 [  49    7 1059]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1074, 1083, 1059], Sum: 3216
All_Elements: [1074, 0, 20, 5, 1083, 1, 49, 7, 1059], Sum: 3298
Accuracy: 0.9751364463311097
Done!
