Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_42
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA4ABC5E10>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x000001FA03BDD940>
Epoch 1
-------------------------------
loss: 0.353517  [    0/ 3475]
loss: 0.111651  [  100/ 3475]
loss: 0.135356  [  200/ 3475]
loss: 0.224058  [  300/ 3475]
loss: 0.089864  [  400/ 3475]
loss: 0.053898  [  500/ 3475]
loss: 0.046619  [  600/ 3475]
loss: 0.083941  [  700/ 3475]
loss: 0.138611  [  800/ 3475]
loss: 0.064157  [  900/ 3475]
loss: 0.060534  [ 1000/ 3475]
loss: 0.091072  [ 1100/ 3475]
loss: 0.057014  [ 1200/ 3475]
loss: 0.038234  [ 1300/ 3475]
loss: 0.040562  [ 1400/ 3475]
loss: 0.058694  [ 1500/ 3475]
loss: 0.053972  [ 1600/ 3475]
loss: 0.013921  [ 1700/ 3475]
loss: 0.099008  [ 1800/ 3475]
loss: 0.035496  [ 1900/ 3475]
loss: 0.018423  [ 2000/ 3475]
loss: 0.053492  [ 2100/ 3475]
loss: 0.076922  [ 2200/ 3475]
loss: 0.040112  [ 2300/ 3475]
loss: 0.013727  [ 2400/ 3475]
loss: 0.075822  [ 2500/ 3475]
loss: 0.032056  [ 2600/ 3475]
loss: 0.096096  [ 2700/ 3475]
loss: 0.066706  [ 2800/ 3475]
loss: 0.034840  [ 2900/ 3475]
loss: 0.053493  [ 3000/ 3475]
loss: 0.094556  [ 3100/ 3475]
loss: 0.088171  [ 3200/ 3475]
loss: 0.027541  [ 3300/ 3475]
loss: 0.038409  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.040578  [    0/ 3475]
loss: 0.046783  [  100/ 3475]
loss: 0.050179  [  200/ 3475]
loss: 0.220578  [  300/ 3475]
loss: 0.021830  [  400/ 3475]
loss: 0.033781  [  500/ 3475]
loss: 0.014900  [  600/ 3475]
loss: 0.051658  [  700/ 3475]
loss: 0.107931  [  800/ 3475]
loss: 0.057755  [  900/ 3475]
loss: 0.035387  [ 1000/ 3475]
loss: 0.026787  [ 1100/ 3475]
loss: 0.029827  [ 1200/ 3475]
loss: 0.040418  [ 1300/ 3475]
loss: 0.038190  [ 1400/ 3475]
loss: 0.040072  [ 1500/ 3475]
loss: 0.054593  [ 1600/ 3475]
loss: 0.012118  [ 1700/ 3475]
loss: 0.102259  [ 1800/ 3475]
loss: 0.039390  [ 1900/ 3475]
loss: 0.017376  [ 2000/ 3475]
loss: 0.051639  [ 2100/ 3475]
loss: 0.075482  [ 2200/ 3475]
loss: 0.029243  [ 2300/ 3475]
loss: 0.015298  [ 2400/ 3475]
loss: 0.086893  [ 2500/ 3475]
loss: 0.027338  [ 2600/ 3475]
loss: 0.094432  [ 2700/ 3475]
loss: 0.068455  [ 2800/ 3475]
loss: 0.036836  [ 2900/ 3475]
loss: 0.051938  [ 3000/ 3475]
loss: 0.070861  [ 3100/ 3475]
loss: 0.067674  [ 3200/ 3475]
loss: 0.027216  [ 3300/ 3475]
loss: 0.029895  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.041236  [    0/ 3475]
loss: 0.036236  [  100/ 3475]
loss: 0.049488  [  200/ 3475]
loss: 0.220300  [  300/ 3475]
loss: 0.020024  [  400/ 3475]
loss: 0.029762  [  500/ 3475]
loss: 0.019066  [  600/ 3475]
loss: 0.047940  [  700/ 3475]
loss: 0.104026  [  800/ 3475]
loss: 0.053023  [  900/ 3475]
loss: 0.035555  [ 1000/ 3475]
loss: 0.021398  [ 1100/ 3475]
loss: 0.026470  [ 1200/ 3475]
loss: 0.040734  [ 1300/ 3475]
loss: 0.034046  [ 1400/ 3475]
loss: 0.037460  [ 1500/ 3475]
loss: 0.054010  [ 1600/ 3475]
loss: 0.012603  [ 1700/ 3475]
loss: 0.092943  [ 1800/ 3475]
loss: 0.038087  [ 1900/ 3475]
loss: 0.017782  [ 2000/ 3475]
loss: 0.053072  [ 2100/ 3475]
loss: 0.076450  [ 2200/ 3475]
loss: 0.027116  [ 2300/ 3475]
loss: 0.015795  [ 2400/ 3475]
loss: 0.091988  [ 2500/ 3475]
loss: 0.025248  [ 2600/ 3475]
loss: 0.092831  [ 2700/ 3475]
loss: 0.072145  [ 2800/ 3475]
loss: 0.038731  [ 2900/ 3475]
loss: 0.051585  [ 3000/ 3475]
loss: 0.066355  [ 3100/ 3475]
loss: 0.062668  [ 3200/ 3475]
loss: 0.029536  [ 3300/ 3475]
loss: 0.029951  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.047137  [    0/ 3475]
loss: 0.033153  [  100/ 3475]
loss: 0.049212  [  200/ 3475]
loss: 0.216346  [  300/ 3475]
loss: 0.019173  [  400/ 3475]
loss: 0.031010  [  500/ 3475]
loss: 0.019729  [  600/ 3475]
loss: 0.046736  [  700/ 3475]
loss: 0.106245  [  800/ 3475]
loss: 0.051029  [  900/ 3475]
loss: 0.036743  [ 1000/ 3475]
loss: 0.020263  [ 1100/ 3475]
loss: 0.026141  [ 1200/ 3475]
loss: 0.037182  [ 1300/ 3475]
loss: 0.032024  [ 1400/ 3475]
loss: 0.036799  [ 1500/ 3475]
loss: 0.053728  [ 1600/ 3475]
loss: 0.013092  [ 1700/ 3475]
loss: 0.086208  [ 1800/ 3475]
loss: 0.037577  [ 1900/ 3475]
loss: 0.018355  [ 2000/ 3475]
loss: 0.054240  [ 2100/ 3475]
loss: 0.078335  [ 2200/ 3475]
loss: 0.027625  [ 2300/ 3475]
loss: 0.016285  [ 2400/ 3475]
loss: 0.093930  [ 2500/ 3475]
loss: 0.025423  [ 2600/ 3475]
loss: 0.091541  [ 2700/ 3475]
loss: 0.076748  [ 2800/ 3475]
loss: 0.039854  [ 2900/ 3475]
loss: 0.051514  [ 3000/ 3475]
loss: 0.063520  [ 3100/ 3475]
loss: 0.058414  [ 3200/ 3475]
loss: 0.033968  [ 3300/ 3475]
loss: 0.031300  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.052496  [    0/ 3475]
loss: 0.030280  [  100/ 3475]
loss: 0.049267  [  200/ 3475]
loss: 0.213385  [  300/ 3475]
loss: 0.019184  [  400/ 3475]
loss: 0.031773  [  500/ 3475]
loss: 0.018306  [  600/ 3475]
loss: 0.047448  [  700/ 3475]
loss: 0.105054  [  800/ 3475]
loss: 0.049160  [  900/ 3475]
loss: 0.034511  [ 1000/ 3475]
loss: 0.020727  [ 1100/ 3475]
loss: 0.027113  [ 1200/ 3475]
loss: 0.032840  [ 1300/ 3475]
loss: 0.030358  [ 1400/ 3475]
loss: 0.036659  [ 1500/ 3475]
loss: 0.053241  [ 1600/ 3475]
loss: 0.013576  [ 1700/ 3475]
loss: 0.074057  [ 1800/ 3475]
loss: 0.037106  [ 1900/ 3475]
loss: 0.018683  [ 2000/ 3475]
loss: 0.055142  [ 2100/ 3475]
loss: 0.078473  [ 2200/ 3475]
loss: 0.025914  [ 2300/ 3475]
loss: 0.016733  [ 2400/ 3475]
loss: 0.096126  [ 2500/ 3475]
loss: 0.024453  [ 2600/ 3475]
loss: 0.091117  [ 2700/ 3475]
loss: 0.081758  [ 2800/ 3475]
loss: 0.039686  [ 2900/ 3475]
loss: 0.051259  [ 3000/ 3475]
loss: 0.062529  [ 3100/ 3475]
loss: 0.054814  [ 3200/ 3475]
loss: 0.036607  [ 3300/ 3475]
loss: 0.031667  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.054822  [    0/ 3475]
loss: 0.029238  [  100/ 3475]
loss: 0.049900  [  200/ 3475]
loss: 0.210220  [  300/ 3475]
loss: 0.019516  [  400/ 3475]
loss: 0.032003  [  500/ 3475]
loss: 0.018446  [  600/ 3475]
loss: 0.046464  [  700/ 3475]
loss: 0.104632  [  800/ 3475]
loss: 0.047921  [  900/ 3475]
loss: 0.034040  [ 1000/ 3475]
loss: 0.021318  [ 1100/ 3475]
loss: 0.026797  [ 1200/ 3475]
loss: 0.030314  [ 1300/ 3475]
loss: 0.029441  [ 1400/ 3475]
loss: 0.036382  [ 1500/ 3475]
loss: 0.052232  [ 1600/ 3475]
loss: 0.014096  [ 1700/ 3475]
loss: 0.066859  [ 1800/ 3475]
loss: 0.036581  [ 1900/ 3475]
loss: 0.019854  [ 2000/ 3475]
loss: 0.054690  [ 2100/ 3475]
loss: 0.078714  [ 2200/ 3475]
loss: 0.024291  [ 2300/ 3475]
loss: 0.016795  [ 2400/ 3475]
loss: 0.095991  [ 2500/ 3475]
loss: 0.022651  [ 2600/ 3475]
loss: 0.088211  [ 2700/ 3475]
loss: 0.088701  [ 2800/ 3475]
loss: 0.041320  [ 2900/ 3475]
loss: 0.051039  [ 3000/ 3475]
loss: 0.061572  [ 3100/ 3475]
loss: 0.051099  [ 3200/ 3475]
loss: 0.037318  [ 3300/ 3475]
loss: 0.033009  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.057400  [    0/ 3475]
loss: 0.027547  [  100/ 3475]
loss: 0.049887  [  200/ 3475]
loss: 0.208444  [  300/ 3475]
loss: 0.019751  [  400/ 3475]
loss: 0.030302  [  500/ 3475]
loss: 0.018800  [  600/ 3475]
loss: 0.042679  [  700/ 3475]
loss: 0.104849  [  800/ 3475]
loss: 0.046773  [  900/ 3475]
loss: 0.034091  [ 1000/ 3475]
loss: 0.018567  [ 1100/ 3475]
loss: 0.025234  [ 1200/ 3475]
loss: 0.027559  [ 1300/ 3475]
loss: 0.028937  [ 1400/ 3475]
loss: 0.037950  [ 1500/ 3475]
loss: 0.051470  [ 1600/ 3475]
loss: 0.015339  [ 1700/ 3475]
loss: 0.060849  [ 1800/ 3475]
loss: 0.035769  [ 1900/ 3475]
loss: 0.020865  [ 2000/ 3475]
loss: 0.052857  [ 2100/ 3475]
loss: 0.078182  [ 2200/ 3475]
loss: 0.023373  [ 2300/ 3475]
loss: 0.016481  [ 2400/ 3475]
loss: 0.096303  [ 2500/ 3475]
loss: 0.022189  [ 2600/ 3475]
loss: 0.084740  [ 2700/ 3475]
loss: 0.097075  [ 2800/ 3475]
loss: 0.043312  [ 2900/ 3475]
loss: 0.050861  [ 3000/ 3475]
loss: 0.061828  [ 3100/ 3475]
loss: 0.051416  [ 3200/ 3475]
loss: 0.041995  [ 3300/ 3475]
loss: 0.033679  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.059525  [    0/ 3475]
loss: 0.025669  [  100/ 3475]
loss: 0.049997  [  200/ 3475]
loss: 0.206939  [  300/ 3475]
loss: 0.020567  [  400/ 3475]
loss: 0.031111  [  500/ 3475]
loss: 0.018997  [  600/ 3475]
loss: 0.033881  [  700/ 3475]
loss: 0.104824  [  800/ 3475]
loss: 0.046810  [  900/ 3475]
loss: 0.038058  [ 1000/ 3475]
loss: 0.018839  [ 1100/ 3475]
loss: 0.025216  [ 1200/ 3475]
loss: 0.026578  [ 1300/ 3475]
loss: 0.031191  [ 1400/ 3475]
loss: 0.037908  [ 1500/ 3475]
loss: 0.050299  [ 1600/ 3475]
loss: 0.016926  [ 1700/ 3475]
loss: 0.054872  [ 1800/ 3475]
loss: 0.035100  [ 1900/ 3475]
loss: 0.020892  [ 2000/ 3475]
loss: 0.052772  [ 2100/ 3475]
loss: 0.077652  [ 2200/ 3475]
loss: 0.023435  [ 2300/ 3475]
loss: 0.016371  [ 2400/ 3475]
loss: 0.095260  [ 2500/ 3475]
loss: 0.021430  [ 2600/ 3475]
loss: 0.080633  [ 2700/ 3475]
loss: 0.104278  [ 2800/ 3475]
loss: 0.044491  [ 2900/ 3475]
loss: 0.050691  [ 3000/ 3475]
loss: 0.063581  [ 3100/ 3475]
loss: 0.050773  [ 3200/ 3475]
loss: 0.047046  [ 3300/ 3475]
loss: 0.034217  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [ 0.49752024 -0.7565893 ]
[2 1 2 ... 1 0 1]
[0 1 0 ... 1 2 1]
Cluster 0 Occurrences: 1162; KMEANS: 1139
Cluster 1 Occurrences: 1164; KMEANS: 1174
Cluster 2 Occurrences: 1149; KMEANS: 1162
Centroids: [[-1.6149104, 0.49149936], [0.96704006, 1.433404], [-0.34209266, -1.3828697]]
Centroids: [[-0.33608118, -1.3992008], [0.97506607, 1.4225935], [-1.6401778, 0.49419284]]
Contingency Matrix: 
[[  24    4 1134]
 [   6 1155    3]
 [1109   15   25]]
[[24, -1, 1134], [-1, -1, -1], [1109, -1, 25]]
[[-1, -1, -1], [-1, -1, -1], [1109, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 2, 2: 0}
New Contingency Matrix: 
[[1134    4   24]
 [   3 1155    6]
 [  25   15 1109]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1134, 1155, 1109], Sum: 3398
All_Elements: [1134, 4, 24, 3, 1155, 6, 25, 15, 1109], Sum: 3475
Accuracy: 0.977841726618705
Done!
