Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_14
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA877F62E8>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x000001FA877E09E8>
Epoch 1
-------------------------------
loss: 0.255613  [    0/ 3442]
loss: 0.103126  [  100/ 3442]
loss: 0.053394  [  200/ 3442]
loss: 0.013527  [  300/ 3442]
loss: 0.043496  [  400/ 3442]
loss: 0.026124  [  500/ 3442]
loss: 0.027478  [  600/ 3442]
loss: 0.014030  [  700/ 3442]
loss: 0.033940  [  800/ 3442]
loss: 0.053108  [  900/ 3442]
loss: 0.032171  [ 1000/ 3442]
loss: 0.003342  [ 1100/ 3442]
loss: 0.021611  [ 1200/ 3442]
loss: 0.011234  [ 1300/ 3442]
loss: 0.009411  [ 1400/ 3442]
loss: 0.021612  [ 1500/ 3442]
loss: 0.014846  [ 1600/ 3442]
loss: 0.016901  [ 1700/ 3442]
loss: 0.022800  [ 1800/ 3442]
loss: 0.049407  [ 1900/ 3442]
loss: 0.016787  [ 2000/ 3442]
loss: 0.026041  [ 2100/ 3442]
loss: 0.021149  [ 2200/ 3442]
loss: 0.010108  [ 2300/ 3442]
loss: 0.013645  [ 2400/ 3442]
loss: 0.043330  [ 2500/ 3442]
loss: 0.131114  [ 2600/ 3442]
loss: 0.066359  [ 2700/ 3442]
loss: 0.011136  [ 2800/ 3442]
loss: 0.024807  [ 2900/ 3442]
loss: 0.005664  [ 3000/ 3442]
loss: 0.020674  [ 3100/ 3442]
loss: 0.068551  [ 3200/ 3442]
loss: 0.010315  [ 3300/ 3442]
loss: 0.035293  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.043665  [    0/ 3442]
loss: 0.014754  [  100/ 3442]
loss: 0.029541  [  200/ 3442]
loss: 0.007871  [  300/ 3442]
loss: 0.036011  [  400/ 3442]
loss: 0.012571  [  500/ 3442]
loss: 0.031353  [  600/ 3442]
loss: 0.012146  [  700/ 3442]
loss: 0.029826  [  800/ 3442]
loss: 0.029808  [  900/ 3442]
loss: 0.024475  [ 1000/ 3442]
loss: 0.003989  [ 1100/ 3442]
loss: 0.020365  [ 1200/ 3442]
loss: 0.006276  [ 1300/ 3442]
loss: 0.006096  [ 1400/ 3442]
loss: 0.022158  [ 1500/ 3442]
loss: 0.012597  [ 1600/ 3442]
loss: 0.013332  [ 1700/ 3442]
loss: 0.017592  [ 1800/ 3442]
loss: 0.036522  [ 1900/ 3442]
loss: 0.018300  [ 2000/ 3442]
loss: 0.018773  [ 2100/ 3442]
loss: 0.018481  [ 2200/ 3442]
loss: 0.011304  [ 2300/ 3442]
loss: 0.013062  [ 2400/ 3442]
loss: 0.029928  [ 2500/ 3442]
loss: 0.115232  [ 2600/ 3442]
loss: 0.063686  [ 2700/ 3442]
loss: 0.009700  [ 2800/ 3442]
loss: 0.018334  [ 2900/ 3442]
loss: 0.005367  [ 3000/ 3442]
loss: 0.019592  [ 3100/ 3442]
loss: 0.105051  [ 3200/ 3442]
loss: 0.010719  [ 3300/ 3442]
loss: 0.031294  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.036471  [    0/ 3442]
loss: 0.014377  [  100/ 3442]
loss: 0.025769  [  200/ 3442]
loss: 0.007117  [  300/ 3442]
loss: 0.020320  [  400/ 3442]
loss: 0.011888  [  500/ 3442]
loss: 0.032790  [  600/ 3442]
loss: 0.011227  [  700/ 3442]
loss: 0.023681  [  800/ 3442]
loss: 0.032099  [  900/ 3442]
loss: 0.018179  [ 1000/ 3442]
loss: 0.004267  [ 1100/ 3442]
loss: 0.019646  [ 1200/ 3442]
loss: 0.006123  [ 1300/ 3442]
loss: 0.006719  [ 1400/ 3442]
loss: 0.020474  [ 1500/ 3442]
loss: 0.012278  [ 1600/ 3442]
loss: 0.010332  [ 1700/ 3442]
loss: 0.020889  [ 1800/ 3442]
loss: 0.028410  [ 1900/ 3442]
loss: 0.018299  [ 2000/ 3442]
loss: 0.013292  [ 2100/ 3442]
loss: 0.016543  [ 2200/ 3442]
loss: 0.011169  [ 2300/ 3442]
loss: 0.011994  [ 2400/ 3442]
loss: 0.027382  [ 2500/ 3442]
loss: 0.114707  [ 2600/ 3442]
loss: 0.051788  [ 2700/ 3442]
loss: 0.008549  [ 2800/ 3442]
loss: 0.011481  [ 2900/ 3442]
loss: 0.004952  [ 3000/ 3442]
loss: 0.017995  [ 3100/ 3442]
loss: 0.119490  [ 3200/ 3442]
loss: 0.011849  [ 3300/ 3442]
loss: 0.029917  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.023651  [    0/ 3442]
loss: 0.014213  [  100/ 3442]
loss: 0.027377  [  200/ 3442]
loss: 0.007346  [  300/ 3442]
loss: 0.014014  [  400/ 3442]
loss: 0.011145  [  500/ 3442]
loss: 0.031413  [  600/ 3442]
loss: 0.011075  [  700/ 3442]
loss: 0.019638  [  800/ 3442]
loss: 0.030921  [  900/ 3442]
loss: 0.013788  [ 1000/ 3442]
loss: 0.003753  [ 1100/ 3442]
loss: 0.018783  [ 1200/ 3442]
loss: 0.005342  [ 1300/ 3442]
loss: 0.009231  [ 1400/ 3442]
loss: 0.020210  [ 1500/ 3442]
loss: 0.012892  [ 1600/ 3442]
loss: 0.010105  [ 1700/ 3442]
loss: 0.019415  [ 1800/ 3442]
loss: 0.023657  [ 1900/ 3442]
loss: 0.017822  [ 2000/ 3442]
loss: 0.011774  [ 2100/ 3442]
loss: 0.015156  [ 2200/ 3442]
loss: 0.010654  [ 2300/ 3442]
loss: 0.011083  [ 2400/ 3442]
loss: 0.029773  [ 2500/ 3442]
loss: 0.119156  [ 2600/ 3442]
loss: 0.043265  [ 2700/ 3442]
loss: 0.008406  [ 2800/ 3442]
loss: 0.010397  [ 2900/ 3442]
loss: 0.004696  [ 3000/ 3442]
loss: 0.017941  [ 3100/ 3442]
loss: 0.121776  [ 3200/ 3442]
loss: 0.011911  [ 3300/ 3442]
loss: 0.029962  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.015957  [    0/ 3442]
loss: 0.014163  [  100/ 3442]
loss: 0.027467  [  200/ 3442]
loss: 0.007741  [  300/ 3442]
loss: 0.014417  [  400/ 3442]
loss: 0.011052  [  500/ 3442]
loss: 0.029229  [  600/ 3442]
loss: 0.011099  [  700/ 3442]
loss: 0.018638  [  800/ 3442]
loss: 0.029703  [  900/ 3442]
loss: 0.011594  [ 1000/ 3442]
loss: 0.003345  [ 1100/ 3442]
loss: 0.017554  [ 1200/ 3442]
loss: 0.005160  [ 1300/ 3442]
loss: 0.010599  [ 1400/ 3442]
loss: 0.020183  [ 1500/ 3442]
loss: 0.013289  [ 1600/ 3442]
loss: 0.009861  [ 1700/ 3442]
loss: 0.017592  [ 1800/ 3442]
loss: 0.019220  [ 1900/ 3442]
loss: 0.017691  [ 2000/ 3442]
loss: 0.011543  [ 2100/ 3442]
loss: 0.013998  [ 2200/ 3442]
loss: 0.011035  [ 2300/ 3442]
loss: 0.010643  [ 2400/ 3442]
loss: 0.031301  [ 2500/ 3442]
loss: 0.121416  [ 2600/ 3442]
loss: 0.039337  [ 2700/ 3442]
loss: 0.008069  [ 2800/ 3442]
loss: 0.010154  [ 2900/ 3442]
loss: 0.004790  [ 3000/ 3442]
loss: 0.017306  [ 3100/ 3442]
loss: 0.119875  [ 3200/ 3442]
loss: 0.011842  [ 3300/ 3442]
loss: 0.030341  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.012260  [    0/ 3442]
loss: 0.014456  [  100/ 3442]
loss: 0.027292  [  200/ 3442]
loss: 0.008089  [  300/ 3442]
loss: 0.015338  [  400/ 3442]
loss: 0.010817  [  500/ 3442]
loss: 0.026944  [  600/ 3442]
loss: 0.011070  [  700/ 3442]
loss: 0.018450  [  800/ 3442]
loss: 0.029284  [  900/ 3442]
loss: 0.010144  [ 1000/ 3442]
loss: 0.003170  [ 1100/ 3442]
loss: 0.016864  [ 1200/ 3442]
loss: 0.004978  [ 1300/ 3442]
loss: 0.011467  [ 1400/ 3442]
loss: 0.021318  [ 1500/ 3442]
loss: 0.012764  [ 1600/ 3442]
loss: 0.009410  [ 1700/ 3442]
loss: 0.016089  [ 1800/ 3442]
loss: 0.015955  [ 1900/ 3442]
loss: 0.017913  [ 2000/ 3442]
loss: 0.011484  [ 2100/ 3442]
loss: 0.012736  [ 2200/ 3442]
loss: 0.010634  [ 2300/ 3442]
loss: 0.010137  [ 2400/ 3442]
loss: 0.031816  [ 2500/ 3442]
loss: 0.121803  [ 2600/ 3442]
loss: 0.037164  [ 2700/ 3442]
loss: 0.007721  [ 2800/ 3442]
loss: 0.010385  [ 2900/ 3442]
loss: 0.005021  [ 3000/ 3442]
loss: 0.016464  [ 3100/ 3442]
loss: 0.119547  [ 3200/ 3442]
loss: 0.011160  [ 3300/ 3442]
loss: 0.031025  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.010675  [    0/ 3442]
loss: 0.014601  [  100/ 3442]
loss: 0.027292  [  200/ 3442]
loss: 0.008542  [  300/ 3442]
loss: 0.015965  [  400/ 3442]
loss: 0.010493  [  500/ 3442]
loss: 0.024925  [  600/ 3442]
loss: 0.010689  [  700/ 3442]
loss: 0.018486  [  800/ 3442]
loss: 0.029501  [  900/ 3442]
loss: 0.009056  [ 1000/ 3442]
loss: 0.002912  [ 1100/ 3442]
loss: 0.015487  [ 1200/ 3442]
loss: 0.004952  [ 1300/ 3442]
loss: 0.012449  [ 1400/ 3442]
loss: 0.019990  [ 1500/ 3442]
loss: 0.012213  [ 1600/ 3442]
loss: 0.009051  [ 1700/ 3442]
loss: 0.015445  [ 1800/ 3442]
loss: 0.014116  [ 1900/ 3442]
loss: 0.018013  [ 2000/ 3442]
loss: 0.011405  [ 2100/ 3442]
loss: 0.011412  [ 2200/ 3442]
loss: 0.010517  [ 2300/ 3442]
loss: 0.009978  [ 2400/ 3442]
loss: 0.032030  [ 2500/ 3442]
loss: 0.122419  [ 2600/ 3442]
loss: 0.035974  [ 2700/ 3442]
loss: 0.007213  [ 2800/ 3442]
loss: 0.010491  [ 2900/ 3442]
loss: 0.005080  [ 3000/ 3442]
loss: 0.015719  [ 3100/ 3442]
loss: 0.118998  [ 3200/ 3442]
loss: 0.010775  [ 3300/ 3442]
loss: 0.031213  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.009575  [    0/ 3442]
loss: 0.014955  [  100/ 3442]
loss: 0.027147  [  200/ 3442]
loss: 0.008690  [  300/ 3442]
loss: 0.015969  [  400/ 3442]
loss: 0.010475  [  500/ 3442]
loss: 0.023847  [  600/ 3442]
loss: 0.010463  [  700/ 3442]
loss: 0.018693  [  800/ 3442]
loss: 0.030103  [  900/ 3442]
loss: 0.008558  [ 1000/ 3442]
loss: 0.002928  [ 1100/ 3442]
loss: 0.014396  [ 1200/ 3442]
loss: 0.005287  [ 1300/ 3442]
loss: 0.012878  [ 1400/ 3442]
loss: 0.019529  [ 1500/ 3442]
loss: 0.011856  [ 1600/ 3442]
loss: 0.009330  [ 1700/ 3442]
loss: 0.014528  [ 1800/ 3442]
loss: 0.013187  [ 1900/ 3442]
loss: 0.017765  [ 2000/ 3442]
loss: 0.011154  [ 2100/ 3442]
loss: 0.010617  [ 2200/ 3442]
loss: 0.010187  [ 2300/ 3442]
loss: 0.010063  [ 2400/ 3442]
loss: 0.031893  [ 2500/ 3442]
loss: 0.121942  [ 2600/ 3442]
loss: 0.035632  [ 2700/ 3442]
loss: 0.006636  [ 2800/ 3442]
loss: 0.010204  [ 2900/ 3442]
loss: 0.005342  [ 3000/ 3442]
loss: 0.014800  [ 3100/ 3442]
loss: 0.121087  [ 3200/ 3442]
loss: 0.010139  [ 3300/ 3442]
loss: 0.032334  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [1.3529549 1.4654931]
[1 0 0 ... 1 2 0]
[0 2 2 ... 0 1 2]
Cluster 0 Occurrences: 1159; KMEANS: 1089
Cluster 1 Occurrences: 1156; KMEANS: 1118
Cluster 2 Occurrences: 1127; KMEANS: 1235
Centroids: [[-0.033640523, 1.5711738], [0.84935385, 0.9100344], [-0.068336494, -0.9550034]]
Centroids: [[0.94957453, 0.90241486], [-0.06833916, -0.9750232], [-0.07436039, 1.541739]]
Contingency Matrix: 
[[  37    0 1122]
 [1050    4  102]
 [   2 1114   11]]
[[-1, -1, -1], [1050, 4, -1], [2, 1114, -1]]
[[-1, -1, -1], [1050, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 1, 1: 0}
New Contingency Matrix: 
[[1122   37    0]
 [ 102 1050    4]
 [  11    2 1114]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1122, 1050, 1114], Sum: 3286
All_Elements: [1122, 37, 0, 102, 1050, 4, 11, 2, 1114], Sum: 3442
Accuracy: 0.9546775130737943
Done!
