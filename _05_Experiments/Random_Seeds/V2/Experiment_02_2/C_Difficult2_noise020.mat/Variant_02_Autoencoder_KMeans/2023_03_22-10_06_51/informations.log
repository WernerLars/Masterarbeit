Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_06_51
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA03ABC780>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x000001FA9FCFCF28>
Epoch 1
-------------------------------
loss: 0.374360  [    0/ 3493]
loss: 0.099282  [  100/ 3493]
loss: 0.042570  [  200/ 3493]
loss: 0.063759  [  300/ 3493]
loss: 0.056407  [  400/ 3493]
loss: 0.019256  [  500/ 3493]
loss: 0.067100  [  600/ 3493]
loss: 0.191085  [  700/ 3493]
loss: 0.068481  [  800/ 3493]
loss: 0.023388  [  900/ 3493]
loss: 0.038684  [ 1000/ 3493]
loss: 0.087662  [ 1100/ 3493]
loss: 0.027853  [ 1200/ 3493]
loss: 0.014882  [ 1300/ 3493]
loss: 0.023185  [ 1400/ 3493]
loss: 0.027256  [ 1500/ 3493]
loss: 0.026054  [ 1600/ 3493]
loss: 0.039400  [ 1700/ 3493]
loss: 0.065401  [ 1800/ 3493]
loss: 0.020669  [ 1900/ 3493]
loss: 0.009052  [ 2000/ 3493]
loss: 0.036809  [ 2100/ 3493]
loss: 0.010445  [ 2200/ 3493]
loss: 0.031440  [ 2300/ 3493]
loss: 0.013099  [ 2400/ 3493]
loss: 0.049028  [ 2500/ 3493]
loss: 0.023112  [ 2600/ 3493]
loss: 0.050008  [ 2700/ 3493]
loss: 0.036532  [ 2800/ 3493]
loss: 0.018985  [ 2900/ 3493]
loss: 0.027593  [ 3000/ 3493]
loss: 0.021026  [ 3100/ 3493]
loss: 0.029807  [ 3200/ 3493]
loss: 0.016961  [ 3300/ 3493]
loss: 0.009896  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.092331  [    0/ 3493]
loss: 0.017482  [  100/ 3493]
loss: 0.037274  [  200/ 3493]
loss: 0.034296  [  300/ 3493]
loss: 0.062288  [  400/ 3493]
loss: 0.017146  [  500/ 3493]
loss: 0.051933  [  600/ 3493]
loss: 0.141172  [  700/ 3493]
loss: 0.058927  [  800/ 3493]
loss: 0.024483  [  900/ 3493]
loss: 0.035620  [ 1000/ 3493]
loss: 0.071133  [ 1100/ 3493]
loss: 0.024502  [ 1200/ 3493]
loss: 0.010122  [ 1300/ 3493]
loss: 0.031819  [ 1400/ 3493]
loss: 0.016251  [ 1500/ 3493]
loss: 0.024580  [ 1600/ 3493]
loss: 0.022671  [ 1700/ 3493]
loss: 0.057872  [ 1800/ 3493]
loss: 0.012207  [ 1900/ 3493]
loss: 0.009315  [ 2000/ 3493]
loss: 0.019254  [ 2100/ 3493]
loss: 0.010829  [ 2200/ 3493]
loss: 0.028653  [ 2300/ 3493]
loss: 0.014200  [ 2400/ 3493]
loss: 0.071116  [ 2500/ 3493]
loss: 0.013025  [ 2600/ 3493]
loss: 0.024170  [ 2700/ 3493]
loss: 0.025323  [ 2800/ 3493]
loss: 0.011566  [ 2900/ 3493]
loss: 0.026970  [ 3000/ 3493]
loss: 0.016944  [ 3100/ 3493]
loss: 0.034561  [ 3200/ 3493]
loss: 0.015641  [ 3300/ 3493]
loss: 0.007448  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.066945  [    0/ 3493]
loss: 0.012426  [  100/ 3493]
loss: 0.035570  [  200/ 3493]
loss: 0.028049  [  300/ 3493]
loss: 0.063613  [  400/ 3493]
loss: 0.023309  [  500/ 3493]
loss: 0.047786  [  600/ 3493]
loss: 0.155852  [  700/ 3493]
loss: 0.034918  [  800/ 3493]
loss: 0.021005  [  900/ 3493]
loss: 0.042106  [ 1000/ 3493]
loss: 0.042682  [ 1100/ 3493]
loss: 0.019215  [ 1200/ 3493]
loss: 0.012498  [ 1300/ 3493]
loss: 0.033489  [ 1400/ 3493]
loss: 0.030101  [ 1500/ 3493]
loss: 0.026839  [ 1600/ 3493]
loss: 0.014435  [ 1700/ 3493]
loss: 0.057098  [ 1800/ 3493]
loss: 0.009202  [ 1900/ 3493]
loss: 0.006605  [ 2000/ 3493]
loss: 0.009737  [ 2100/ 3493]
loss: 0.014550  [ 2200/ 3493]
loss: 0.025777  [ 2300/ 3493]
loss: 0.015241  [ 2400/ 3493]
loss: 0.084352  [ 2500/ 3493]
loss: 0.007358  [ 2600/ 3493]
loss: 0.013418  [ 2700/ 3493]
loss: 0.019576  [ 2800/ 3493]
loss: 0.009502  [ 2900/ 3493]
loss: 0.036824  [ 3000/ 3493]
loss: 0.014712  [ 3100/ 3493]
loss: 0.036179  [ 3200/ 3493]
loss: 0.017229  [ 3300/ 3493]
loss: 0.008126  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.049946  [    0/ 3493]
loss: 0.012118  [  100/ 3493]
loss: 0.033193  [  200/ 3493]
loss: 0.019554  [  300/ 3493]
loss: 0.059743  [  400/ 3493]
loss: 0.023096  [  500/ 3493]
loss: 0.048457  [  600/ 3493]
loss: 0.161874  [  700/ 3493]
loss: 0.020911  [  800/ 3493]
loss: 0.019296  [  900/ 3493]
loss: 0.039936  [ 1000/ 3493]
loss: 0.033742  [ 1100/ 3493]
loss: 0.018294  [ 1200/ 3493]
loss: 0.014286  [ 1300/ 3493]
loss: 0.034237  [ 1400/ 3493]
loss: 0.041534  [ 1500/ 3493]
loss: 0.027976  [ 1600/ 3493]
loss: 0.013121  [ 1700/ 3493]
loss: 0.054279  [ 1800/ 3493]
loss: 0.010763  [ 1900/ 3493]
loss: 0.007188  [ 2000/ 3493]
loss: 0.008959  [ 2100/ 3493]
loss: 0.016135  [ 2200/ 3493]
loss: 0.024138  [ 2300/ 3493]
loss: 0.015280  [ 2400/ 3493]
loss: 0.087285  [ 2500/ 3493]
loss: 0.004230  [ 2600/ 3493]
loss: 0.011051  [ 2700/ 3493]
loss: 0.019394  [ 2800/ 3493]
loss: 0.009104  [ 2900/ 3493]
loss: 0.040897  [ 3000/ 3493]
loss: 0.014171  [ 3100/ 3493]
loss: 0.033017  [ 3200/ 3493]
loss: 0.018971  [ 3300/ 3493]
loss: 0.008444  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.047124  [    0/ 3493]
loss: 0.013189  [  100/ 3493]
loss: 0.029791  [  200/ 3493]
loss: 0.016257  [  300/ 3493]
loss: 0.057696  [  400/ 3493]
loss: 0.022715  [  500/ 3493]
loss: 0.049704  [  600/ 3493]
loss: 0.147290  [  700/ 3493]
loss: 0.015196  [  800/ 3493]
loss: 0.015791  [  900/ 3493]
loss: 0.036500  [ 1000/ 3493]
loss: 0.031490  [ 1100/ 3493]
loss: 0.017653  [ 1200/ 3493]
loss: 0.013818  [ 1300/ 3493]
loss: 0.034602  [ 1400/ 3493]
loss: 0.048776  [ 1500/ 3493]
loss: 0.029816  [ 1600/ 3493]
loss: 0.013581  [ 1700/ 3493]
loss: 0.049677  [ 1800/ 3493]
loss: 0.012746  [ 1900/ 3493]
loss: 0.007228  [ 2000/ 3493]
loss: 0.010958  [ 2100/ 3493]
loss: 0.017771  [ 2200/ 3493]
loss: 0.023281  [ 2300/ 3493]
loss: 0.015113  [ 2400/ 3493]
loss: 0.086825  [ 2500/ 3493]
loss: 0.003693  [ 2600/ 3493]
loss: 0.010644  [ 2700/ 3493]
loss: 0.019163  [ 2800/ 3493]
loss: 0.009122  [ 2900/ 3493]
loss: 0.043270  [ 3000/ 3493]
loss: 0.013946  [ 3100/ 3493]
loss: 0.030639  [ 3200/ 3493]
loss: 0.020515  [ 3300/ 3493]
loss: 0.008256  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.046404  [    0/ 3493]
loss: 0.013859  [  100/ 3493]
loss: 0.028090  [  200/ 3493]
loss: 0.013474  [  300/ 3493]
loss: 0.056819  [  400/ 3493]
loss: 0.023394  [  500/ 3493]
loss: 0.050392  [  600/ 3493]
loss: 0.128885  [  700/ 3493]
loss: 0.013644  [  800/ 3493]
loss: 0.015149  [  900/ 3493]
loss: 0.034061  [ 1000/ 3493]
loss: 0.031190  [ 1100/ 3493]
loss: 0.017534  [ 1200/ 3493]
loss: 0.013581  [ 1300/ 3493]
loss: 0.034418  [ 1400/ 3493]
loss: 0.053735  [ 1500/ 3493]
loss: 0.030403  [ 1600/ 3493]
loss: 0.014299  [ 1700/ 3493]
loss: 0.044465  [ 1800/ 3493]
loss: 0.015323  [ 1900/ 3493]
loss: 0.007589  [ 2000/ 3493]
loss: 0.012762  [ 2100/ 3493]
loss: 0.018648  [ 2200/ 3493]
loss: 0.022834  [ 2300/ 3493]
loss: 0.014822  [ 2400/ 3493]
loss: 0.086289  [ 2500/ 3493]
loss: 0.003957  [ 2600/ 3493]
loss: 0.010130  [ 2700/ 3493]
loss: 0.019231  [ 2800/ 3493]
loss: 0.009015  [ 2900/ 3493]
loss: 0.044988  [ 3000/ 3493]
loss: 0.013607  [ 3100/ 3493]
loss: 0.028527  [ 3200/ 3493]
loss: 0.021615  [ 3300/ 3493]
loss: 0.008138  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.046579  [    0/ 3493]
loss: 0.013968  [  100/ 3493]
loss: 0.027149  [  200/ 3493]
loss: 0.013298  [  300/ 3493]
loss: 0.056332  [  400/ 3493]
loss: 0.022855  [  500/ 3493]
loss: 0.051150  [  600/ 3493]
loss: 0.098599  [  700/ 3493]
loss: 0.013443  [  800/ 3493]
loss: 0.013951  [  900/ 3493]
loss: 0.030927  [ 1000/ 3493]
loss: 0.032402  [ 1100/ 3493]
loss: 0.017307  [ 1200/ 3493]
loss: 0.013605  [ 1300/ 3493]
loss: 0.034463  [ 1400/ 3493]
loss: 0.056072  [ 1500/ 3493]
loss: 0.030638  [ 1600/ 3493]
loss: 0.014444  [ 1700/ 3493]
loss: 0.039224  [ 1800/ 3493]
loss: 0.017293  [ 1900/ 3493]
loss: 0.007955  [ 2000/ 3493]
loss: 0.013908  [ 2100/ 3493]
loss: 0.018667  [ 2200/ 3493]
loss: 0.022770  [ 2300/ 3493]
loss: 0.014439  [ 2400/ 3493]
loss: 0.083281  [ 2500/ 3493]
loss: 0.004407  [ 2600/ 3493]
loss: 0.009816  [ 2700/ 3493]
loss: 0.019314  [ 2800/ 3493]
loss: 0.009561  [ 2900/ 3493]
loss: 0.046265  [ 3000/ 3493]
loss: 0.013520  [ 3100/ 3493]
loss: 0.025826  [ 3200/ 3493]
loss: 0.021975  [ 3300/ 3493]
loss: 0.007905  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.048167  [    0/ 3493]
loss: 0.013981  [  100/ 3493]
loss: 0.027076  [  200/ 3493]
loss: 0.012108  [  300/ 3493]
loss: 0.056221  [  400/ 3493]
loss: 0.022552  [  500/ 3493]
loss: 0.051300  [  600/ 3493]
loss: 0.077350  [  700/ 3493]
loss: 0.011856  [  800/ 3493]
loss: 0.013075  [  900/ 3493]
loss: 0.029248  [ 1000/ 3493]
loss: 0.034387  [ 1100/ 3493]
loss: 0.017105  [ 1200/ 3493]
loss: 0.014011  [ 1300/ 3493]
loss: 0.034139  [ 1400/ 3493]
loss: 0.056981  [ 1500/ 3493]
loss: 0.030139  [ 1600/ 3493]
loss: 0.014419  [ 1700/ 3493]
loss: 0.034646  [ 1800/ 3493]
loss: 0.018349  [ 1900/ 3493]
loss: 0.008301  [ 2000/ 3493]
loss: 0.014295  [ 2100/ 3493]
loss: 0.019543  [ 2200/ 3493]
loss: 0.022831  [ 2300/ 3493]
loss: 0.014518  [ 2400/ 3493]
loss: 0.082622  [ 2500/ 3493]
loss: 0.004831  [ 2600/ 3493]
loss: 0.009506  [ 2700/ 3493]
loss: 0.019206  [ 2800/ 3493]
loss: 0.010073  [ 2900/ 3493]
loss: 0.047409  [ 3000/ 3493]
loss: 0.013418  [ 3100/ 3493]
loss: 0.022713  [ 3200/ 3493]
loss: 0.022128  [ 3300/ 3493]
loss: 0.007713  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [-1.5354289  2.2805786]
[1 0 0 ... 1 0 2]
[1 2 2 ... 1 2 0]
Cluster 0 Occurrences: 1151; KMEANS: 1680
Cluster 1 Occurrences: 1195; KMEANS: 1182
Cluster 2 Occurrences: 1147; KMEANS: 631
Centroids: [[-0.12836312, -0.6324248], [-0.7153186, 2.0196419], [-0.35673916, 0.0042668767]]
Centroids: [[-0.29577887, -0.047591355], [-0.7209133, 2.0484033], [-0.09937177, -1.0314008]]
Contingency Matrix: 
[[ 564    2  585]
 [  15 1176    4]
 [1101    4   42]]
[[564, -1, 585], [-1, -1, -1], [1101, -1, 42]]
[[-1, -1, 585], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[ 585    2  564]
 [   4 1176   15]
 [  42    4 1101]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [585, 1176, 1101], Sum: 2862
All_Elements: [585, 2, 564, 4, 1176, 15, 42, 4, 1101], Sum: 3493
Accuracy: 0.819352991697681
Done!
