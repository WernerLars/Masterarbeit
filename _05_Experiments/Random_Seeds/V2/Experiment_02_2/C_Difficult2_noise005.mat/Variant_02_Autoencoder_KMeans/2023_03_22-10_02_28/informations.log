Experiment_path: Random_Seeds//V2/Experiment_02_2
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise005.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise005.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_2/C_Difficult2_noise005.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_02_28
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001FA9F6A6240>
Sampling rate: 24000.0
Raw: [ 0.02085333  0.02043967  0.02052644 ... -0.02218732 -0.02150573
 -0.01811243]
Times: [   1583    1934    2430 ... 1439313 1439656 1439854]
Cluster: [3 3 3 ... 2 2 1]
Number of different clusters:  3
Number of Spikes: 3364
First aligned Spike Frame: [-0.05170878 -0.0548761  -0.06029554 -0.06053219 -0.04807119 -0.02780025
 -0.01550543 -0.01702494 -0.02945104 -0.04493807 -0.07056858 -0.07003585
  0.07629654  0.43081562  0.80470191  0.96319627  0.89198123  0.73643948
  0.58987232  0.46714337  0.36345495  0.2828462   0.22743292  0.182731
  0.13931053  0.09524506  0.05136602  0.01367166 -0.01393093 -0.03985679
 -0.07387102 -0.11218435 -0.1444455  -0.16672578 -0.17809238 -0.18020802
 -0.17953732 -0.18246903 -0.18617363 -0.18205375 -0.17299738 -0.16958427
 -0.17248955 -0.17516876 -0.1727246  -0.16696514 -0.15993314]
Cluster 0, Occurrences: 1120
Cluster 1, Occurrences: 1109
Cluster 2, Occurrences: 1135
<torch.utils.data.dataloader.DataLoader object at 0x000001FA962A6630>
Epoch 1
-------------------------------
loss: 0.169832  [    0/ 3364]
loss: 0.084593  [  100/ 3364]
loss: 0.022566  [  200/ 3364]
loss: 0.013586  [  300/ 3364]
loss: 0.001184  [  400/ 3364]
loss: 0.039844  [  500/ 3364]
loss: 0.004387  [  600/ 3364]
loss: 0.006856  [  700/ 3364]
loss: 0.003495  [  800/ 3364]
loss: 0.009352  [  900/ 3364]
loss: 0.008834  [ 1000/ 3364]
loss: 0.003925  [ 1100/ 3364]
loss: 0.006652  [ 1200/ 3364]
loss: 0.006274  [ 1300/ 3364]
loss: 0.085931  [ 1400/ 3364]
loss: 0.003370  [ 1500/ 3364]
loss: 0.007554  [ 1600/ 3364]
loss: 0.005416  [ 1700/ 3364]
loss: 0.002033  [ 1800/ 3364]
loss: 0.004239  [ 1900/ 3364]
loss: 0.003826  [ 2000/ 3364]
loss: 0.004838  [ 2100/ 3364]
loss: 0.005171  [ 2200/ 3364]
loss: 0.008128  [ 2300/ 3364]
loss: 0.001223  [ 2400/ 3364]
loss: 0.002622  [ 2500/ 3364]
loss: 0.003030  [ 2600/ 3364]
loss: 0.003386  [ 2700/ 3364]
loss: 0.003627  [ 2800/ 3364]
loss: 0.001447  [ 2900/ 3364]
loss: 0.003545  [ 3000/ 3364]
loss: 0.006301  [ 3100/ 3364]
loss: 0.001879  [ 3200/ 3364]
loss: 0.000899  [ 3300/ 3364]
Epoch 2
-------------------------------
loss: 0.001539  [    0/ 3364]
loss: 0.001976  [  100/ 3364]
loss: 0.004480  [  200/ 3364]
loss: 0.001141  [  300/ 3364]
loss: 0.000768  [  400/ 3364]
loss: 0.032587  [  500/ 3364]
loss: 0.003495  [  600/ 3364]
loss: 0.006850  [  700/ 3364]
loss: 0.002426  [  800/ 3364]
loss: 0.003892  [  900/ 3364]
loss: 0.004130  [ 1000/ 3364]
loss: 0.003294  [ 1100/ 3364]
loss: 0.002528  [ 1200/ 3364]
loss: 0.005407  [ 1300/ 3364]
loss: 0.085442  [ 1400/ 3364]
loss: 0.002764  [ 1500/ 3364]
loss: 0.006626  [ 1600/ 3364]
loss: 0.003712  [ 1700/ 3364]
loss: 0.001631  [ 1800/ 3364]
loss: 0.003835  [ 1900/ 3364]
loss: 0.002961  [ 2000/ 3364]
loss: 0.004706  [ 2100/ 3364]
loss: 0.005042  [ 2200/ 3364]
loss: 0.007828  [ 2300/ 3364]
loss: 0.000956  [ 2400/ 3364]
loss: 0.002252  [ 2500/ 3364]
loss: 0.002438  [ 2600/ 3364]
loss: 0.003288  [ 2700/ 3364]
loss: 0.003575  [ 2800/ 3364]
loss: 0.001480  [ 2900/ 3364]
loss: 0.003331  [ 3000/ 3364]
loss: 0.006184  [ 3100/ 3364]
loss: 0.001880  [ 3200/ 3364]
loss: 0.000753  [ 3300/ 3364]
Epoch 3
-------------------------------
loss: 0.001533  [    0/ 3364]
loss: 0.002196  [  100/ 3364]
loss: 0.004297  [  200/ 3364]
loss: 0.000980  [  300/ 3364]
loss: 0.000850  [  400/ 3364]
loss: 0.030905  [  500/ 3364]
loss: 0.003434  [  600/ 3364]
loss: 0.005523  [  700/ 3364]
loss: 0.002645  [  800/ 3364]
loss: 0.003429  [  900/ 3364]
loss: 0.003496  [ 1000/ 3364]
loss: 0.003193  [ 1100/ 3364]
loss: 0.002433  [ 1200/ 3364]
loss: 0.004976  [ 1300/ 3364]
loss: 0.085608  [ 1400/ 3364]
loss: 0.003036  [ 1500/ 3364]
loss: 0.006303  [ 1600/ 3364]
loss: 0.003831  [ 1700/ 3364]
loss: 0.001632  [ 1800/ 3364]
loss: 0.003847  [ 1900/ 3364]
loss: 0.002928  [ 2000/ 3364]
loss: 0.004576  [ 2100/ 3364]
loss: 0.004541  [ 2200/ 3364]
loss: 0.007437  [ 2300/ 3364]
loss: 0.000963  [ 2400/ 3364]
loss: 0.002134  [ 2500/ 3364]
loss: 0.002428  [ 2600/ 3364]
loss: 0.003204  [ 2700/ 3364]
loss: 0.003590  [ 2800/ 3364]
loss: 0.001641  [ 2900/ 3364]
loss: 0.003317  [ 3000/ 3364]
loss: 0.006172  [ 3100/ 3364]
loss: 0.001872  [ 3200/ 3364]
loss: 0.000699  [ 3300/ 3364]
Epoch 4
-------------------------------
loss: 0.001522  [    0/ 3364]
loss: 0.002179  [  100/ 3364]
loss: 0.004170  [  200/ 3364]
loss: 0.000926  [  300/ 3364]
loss: 0.000824  [  400/ 3364]
loss: 0.030338  [  500/ 3364]
loss: 0.003330  [  600/ 3364]
loss: 0.004725  [  700/ 3364]
loss: 0.002709  [  800/ 3364]
loss: 0.003223  [  900/ 3364]
loss: 0.003089  [ 1000/ 3364]
loss: 0.003136  [ 1100/ 3364]
loss: 0.002325  [ 1200/ 3364]
loss: 0.004919  [ 1300/ 3364]
loss: 0.085598  [ 1400/ 3364]
loss: 0.003024  [ 1500/ 3364]
loss: 0.006278  [ 1600/ 3364]
loss: 0.003649  [ 1700/ 3364]
loss: 0.001602  [ 1800/ 3364]
loss: 0.003895  [ 1900/ 3364]
loss: 0.002836  [ 2000/ 3364]
loss: 0.004485  [ 2100/ 3364]
loss: 0.004348  [ 2200/ 3364]
loss: 0.007479  [ 2300/ 3364]
loss: 0.000945  [ 2400/ 3364]
loss: 0.002110  [ 2500/ 3364]
loss: 0.002286  [ 2600/ 3364]
loss: 0.003163  [ 2700/ 3364]
loss: 0.003449  [ 2800/ 3364]
loss: 0.001764  [ 2900/ 3364]
loss: 0.003305  [ 3000/ 3364]
loss: 0.006073  [ 3100/ 3364]
loss: 0.001895  [ 3200/ 3364]
loss: 0.000693  [ 3300/ 3364]
Epoch 5
-------------------------------
loss: 0.001508  [    0/ 3364]
loss: 0.002136  [  100/ 3364]
loss: 0.004138  [  200/ 3364]
loss: 0.000938  [  300/ 3364]
loss: 0.000849  [  400/ 3364]
loss: 0.030103  [  500/ 3364]
loss: 0.003315  [  600/ 3364]
loss: 0.004977  [  700/ 3364]
loss: 0.002848  [  800/ 3364]
loss: 0.003016  [  900/ 3364]
loss: 0.002898  [ 1000/ 3364]
loss: 0.003098  [ 1100/ 3364]
loss: 0.002290  [ 1200/ 3364]
loss: 0.004936  [ 1300/ 3364]
loss: 0.085407  [ 1400/ 3364]
loss: 0.003138  [ 1500/ 3364]
loss: 0.006276  [ 1600/ 3364]
loss: 0.003602  [ 1700/ 3364]
loss: 0.001546  [ 1800/ 3364]
loss: 0.003960  [ 1900/ 3364]
loss: 0.002763  [ 2000/ 3364]
loss: 0.004493  [ 2100/ 3364]
loss: 0.004140  [ 2200/ 3364]
loss: 0.007420  [ 2300/ 3364]
loss: 0.000969  [ 2400/ 3364]
loss: 0.002080  [ 2500/ 3364]
loss: 0.002157  [ 2600/ 3364]
loss: 0.003085  [ 2700/ 3364]
loss: 0.003445  [ 2800/ 3364]
loss: 0.001840  [ 2900/ 3364]
loss: 0.003303  [ 3000/ 3364]
loss: 0.006021  [ 3100/ 3364]
loss: 0.001904  [ 3200/ 3364]
loss: 0.000751  [ 3300/ 3364]
Epoch 6
-------------------------------
loss: 0.001510  [    0/ 3364]
loss: 0.002088  [  100/ 3364]
loss: 0.004242  [  200/ 3364]
loss: 0.001018  [  300/ 3364]
loss: 0.000951  [  400/ 3364]
loss: 0.030179  [  500/ 3364]
loss: 0.003247  [  600/ 3364]
loss: 0.004529  [  700/ 3364]
loss: 0.002794  [  800/ 3364]
loss: 0.002859  [  900/ 3364]
loss: 0.003632  [ 1000/ 3364]
loss: 0.003083  [ 1100/ 3364]
loss: 0.002278  [ 1200/ 3364]
loss: 0.004857  [ 1300/ 3364]
loss: 0.085200  [ 1400/ 3364]
loss: 0.003114  [ 1500/ 3364]
loss: 0.006364  [ 1600/ 3364]
loss: 0.003601  [ 1700/ 3364]
loss: 0.001579  [ 1800/ 3364]
loss: 0.004143  [ 1900/ 3364]
loss: 0.002740  [ 2000/ 3364]
loss: 0.004300  [ 2100/ 3364]
loss: 0.003913  [ 2200/ 3364]
loss: 0.007313  [ 2300/ 3364]
loss: 0.000981  [ 2400/ 3364]
loss: 0.002074  [ 2500/ 3364]
loss: 0.002044  [ 2600/ 3364]
loss: 0.003039  [ 2700/ 3364]
loss: 0.003415  [ 2800/ 3364]
loss: 0.001830  [ 2900/ 3364]
loss: 0.003294  [ 3000/ 3364]
loss: 0.005941  [ 3100/ 3364]
loss: 0.001883  [ 3200/ 3364]
loss: 0.000802  [ 3300/ 3364]
Epoch 7
-------------------------------
loss: 0.001474  [    0/ 3364]
loss: 0.002547  [  100/ 3364]
loss: 0.004201  [  200/ 3364]
loss: 0.000935  [  300/ 3364]
loss: 0.000990  [  400/ 3364]
loss: 0.030428  [  500/ 3364]
loss: 0.003163  [  600/ 3364]
loss: 0.003944  [  700/ 3364]
loss: 0.002628  [  800/ 3364]
loss: 0.002964  [  900/ 3364]
loss: 0.003973  [ 1000/ 3364]
loss: 0.003091  [ 1100/ 3364]
loss: 0.002264  [ 1200/ 3364]
loss: 0.004836  [ 1300/ 3364]
loss: 0.085203  [ 1400/ 3364]
loss: 0.003092  [ 1500/ 3364]
loss: 0.006458  [ 1600/ 3364]
loss: 0.003523  [ 1700/ 3364]
loss: 0.001560  [ 1800/ 3364]
loss: 0.004355  [ 1900/ 3364]
loss: 0.002715  [ 2000/ 3364]
loss: 0.004318  [ 2100/ 3364]
loss: 0.004048  [ 2200/ 3364]
loss: 0.007363  [ 2300/ 3364]
loss: 0.000988  [ 2400/ 3364]
loss: 0.002078  [ 2500/ 3364]
loss: 0.002008  [ 2600/ 3364]
loss: 0.002977  [ 2700/ 3364]
loss: 0.003569  [ 2800/ 3364]
loss: 0.001890  [ 2900/ 3364]
loss: 0.003290  [ 3000/ 3364]
loss: 0.005927  [ 3100/ 3364]
loss: 0.001865  [ 3200/ 3364]
loss: 0.000863  [ 3300/ 3364]
Epoch 8
-------------------------------
loss: 0.001472  [    0/ 3364]
loss: 0.002487  [  100/ 3364]
loss: 0.004191  [  200/ 3364]
loss: 0.000922  [  300/ 3364]
loss: 0.001019  [  400/ 3364]
loss: 0.030463  [  500/ 3364]
loss: 0.003118  [  600/ 3364]
loss: 0.003607  [  700/ 3364]
loss: 0.002513  [  800/ 3364]
loss: 0.002949  [  900/ 3364]
loss: 0.004067  [ 1000/ 3364]
loss: 0.003021  [ 1100/ 3364]
loss: 0.002275  [ 1200/ 3364]
loss: 0.004817  [ 1300/ 3364]
loss: 0.085064  [ 1400/ 3364]
loss: 0.003107  [ 1500/ 3364]
loss: 0.006508  [ 1600/ 3364]
loss: 0.003282  [ 1700/ 3364]
loss: 0.001587  [ 1800/ 3364]
loss: 0.004455  [ 1900/ 3364]
loss: 0.002704  [ 2000/ 3364]
loss: 0.004101  [ 2100/ 3364]
loss: 0.004095  [ 2200/ 3364]
loss: 0.007447  [ 2300/ 3364]
loss: 0.000993  [ 2400/ 3364]
loss: 0.002051  [ 2500/ 3364]
loss: 0.001967  [ 2600/ 3364]
loss: 0.003005  [ 2700/ 3364]
loss: 0.003528  [ 2800/ 3364]
loss: 0.002019  [ 2900/ 3364]
loss: 0.003245  [ 3000/ 3364]
loss: 0.006030  [ 3100/ 3364]
loss: 0.001914  [ 3200/ 3364]
loss: 0.000885  [ 3300/ 3364]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3364
First Spike after testing: [-0.48775885  0.426889  ]
[2 2 2 ... 1 1 0]
[0 0 0 ... 2 2 1]
Cluster 0 Occurrences: 1120; KMEANS: 1124
Cluster 1 Occurrences: 1109; KMEANS: 1130
Cluster 2 Occurrences: 1135; KMEANS: 1110
Centroids: [[0.06515801, -0.64562863], [-0.45029753, 1.4151964], [-0.46717018, 0.36264074]]
Centroids: [[-0.47842646, 0.38947216], [0.08117461, -0.67722756], [-0.46001542, 1.4283302]]
Contingency Matrix: 
[[  14 1102    4]
 [  11    4 1094]
 [1099   24   12]]
[[-1, -1, -1], [11, -1, 1094], [1099, -1, 12]]
[[-1, -1, -1], [-1, -1, 1094], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 1, 2: 0, 1: 2}
New Contingency Matrix: 
[[1102    4   14]
 [   4 1094   11]
 [  24   12 1099]]
New Clustered Label Sequence: [1, 2, 0]
Diagonal_Elements: [1102, 1094, 1099], Sum: 3295
All_Elements: [1102, 4, 14, 4, 1094, 11, 24, 12, 1099], Sum: 3364
Accuracy: 0.9794887039239001
Done!
