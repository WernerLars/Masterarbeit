Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_20
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E7385A26D8>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x000001E7328FB5F8>
Epoch 1
-------------------------------
loss: 0.189809  [    0/ 3474]
loss: 0.095564  [  100/ 3474]
loss: 0.097077  [  200/ 3474]
loss: 0.084731  [  300/ 3474]
loss: 0.173710  [  400/ 3474]
loss: 0.005655  [  500/ 3474]
loss: 0.098049  [  600/ 3474]
loss: 0.021137  [  700/ 3474]
loss: 0.067766  [  800/ 3474]
loss: 0.211872  [  900/ 3474]
loss: 0.042822  [ 1000/ 3474]
loss: 0.092932  [ 1100/ 3474]
loss: 0.056540  [ 1200/ 3474]
loss: 0.050029  [ 1300/ 3474]
loss: 0.038304  [ 1400/ 3474]
loss: 0.081329  [ 1500/ 3474]
loss: 0.009973  [ 1600/ 3474]
loss: 0.109872  [ 1700/ 3474]
loss: 0.027687  [ 1800/ 3474]
loss: 0.016231  [ 1900/ 3474]
loss: 0.012452  [ 2000/ 3474]
loss: 0.016024  [ 2100/ 3474]
loss: 0.028358  [ 2200/ 3474]
loss: 0.074829  [ 2300/ 3474]
loss: 0.032913  [ 2400/ 3474]
loss: 0.045257  [ 2500/ 3474]
loss: 0.028262  [ 2600/ 3474]
loss: 0.085488  [ 2700/ 3474]
loss: 0.050443  [ 2800/ 3474]
loss: 0.014230  [ 2900/ 3474]
loss: 0.189433  [ 3000/ 3474]
loss: 0.011922  [ 3100/ 3474]
loss: 0.029193  [ 3200/ 3474]
loss: 0.028570  [ 3300/ 3474]
loss: 0.095551  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.070098  [    0/ 3474]
loss: 0.012011  [  100/ 3474]
loss: 0.016194  [  200/ 3474]
loss: 0.064238  [  300/ 3474]
loss: 0.146959  [  400/ 3474]
loss: 0.005626  [  500/ 3474]
loss: 0.041441  [  600/ 3474]
loss: 0.014624  [  700/ 3474]
loss: 0.057773  [  800/ 3474]
loss: 0.205501  [  900/ 3474]
loss: 0.020271  [ 1000/ 3474]
loss: 0.060890  [ 1100/ 3474]
loss: 0.018618  [ 1200/ 3474]
loss: 0.029199  [ 1300/ 3474]
loss: 0.022997  [ 1400/ 3474]
loss: 0.058757  [ 1500/ 3474]
loss: 0.006427  [ 1600/ 3474]
loss: 0.090696  [ 1700/ 3474]
loss: 0.028168  [ 1800/ 3474]
loss: 0.020466  [ 1900/ 3474]
loss: 0.007980  [ 2000/ 3474]
loss: 0.018435  [ 2100/ 3474]
loss: 0.028185  [ 2200/ 3474]
loss: 0.064468  [ 2300/ 3474]
loss: 0.011757  [ 2400/ 3474]
loss: 0.042136  [ 2500/ 3474]
loss: 0.020369  [ 2600/ 3474]
loss: 0.046325  [ 2700/ 3474]
loss: 0.034571  [ 2800/ 3474]
loss: 0.017480  [ 2900/ 3474]
loss: 0.131883  [ 3000/ 3474]
loss: 0.010803  [ 3100/ 3474]
loss: 0.020470  [ 3200/ 3474]
loss: 0.041830  [ 3300/ 3474]
loss: 0.071746  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.045126  [    0/ 3474]
loss: 0.011007  [  100/ 3474]
loss: 0.017079  [  200/ 3474]
loss: 0.062339  [  300/ 3474]
loss: 0.110153  [  400/ 3474]
loss: 0.008257  [  500/ 3474]
loss: 0.032073  [  600/ 3474]
loss: 0.009869  [  700/ 3474]
loss: 0.057396  [  800/ 3474]
loss: 0.196419  [  900/ 3474]
loss: 0.009693  [ 1000/ 3474]
loss: 0.091484  [ 1100/ 3474]
loss: 0.032189  [ 1200/ 3474]
loss: 0.026661  [ 1300/ 3474]
loss: 0.018497  [ 1400/ 3474]
loss: 0.078147  [ 1500/ 3474]
loss: 0.009442  [ 1600/ 3474]
loss: 0.078219  [ 1700/ 3474]
loss: 0.029435  [ 1800/ 3474]
loss: 0.024678  [ 1900/ 3474]
loss: 0.006217  [ 2000/ 3474]
loss: 0.017255  [ 2100/ 3474]
loss: 0.028246  [ 2200/ 3474]
loss: 0.063848  [ 2300/ 3474]
loss: 0.007781  [ 2400/ 3474]
loss: 0.034314  [ 2500/ 3474]
loss: 0.018977  [ 2600/ 3474]
loss: 0.041797  [ 2700/ 3474]
loss: 0.024554  [ 2800/ 3474]
loss: 0.018194  [ 2900/ 3474]
loss: 0.163661  [ 3000/ 3474]
loss: 0.010307  [ 3100/ 3474]
loss: 0.016629  [ 3200/ 3474]
loss: 0.047675  [ 3300/ 3474]
loss: 0.053983  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.032519  [    0/ 3474]
loss: 0.010099  [  100/ 3474]
loss: 0.017292  [  200/ 3474]
loss: 0.061202  [  300/ 3474]
loss: 0.084698  [  400/ 3474]
loss: 0.007173  [  500/ 3474]
loss: 0.027802  [  600/ 3474]
loss: 0.010433  [  700/ 3474]
loss: 0.050899  [  800/ 3474]
loss: 0.188381  [  900/ 3474]
loss: 0.010112  [ 1000/ 3474]
loss: 0.126623  [ 1100/ 3474]
loss: 0.036556  [ 1200/ 3474]
loss: 0.027459  [ 1300/ 3474]
loss: 0.017409  [ 1400/ 3474]
loss: 0.078518  [ 1500/ 3474]
loss: 0.011895  [ 1600/ 3474]
loss: 0.069692  [ 1700/ 3474]
loss: 0.028821  [ 1800/ 3474]
loss: 0.025536  [ 1900/ 3474]
loss: 0.007517  [ 2000/ 3474]
loss: 0.017332  [ 2100/ 3474]
loss: 0.028226  [ 2200/ 3474]
loss: 0.064330  [ 2300/ 3474]
loss: 0.006614  [ 2400/ 3474]
loss: 0.025360  [ 2500/ 3474]
loss: 0.021675  [ 2600/ 3474]
loss: 0.047667  [ 2700/ 3474]
loss: 0.018289  [ 2800/ 3474]
loss: 0.015517  [ 2900/ 3474]
loss: 0.196593  [ 3000/ 3474]
loss: 0.010631  [ 3100/ 3474]
loss: 0.015627  [ 3200/ 3474]
loss: 0.050811  [ 3300/ 3474]
loss: 0.036094  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.030744  [    0/ 3474]
loss: 0.009624  [  100/ 3474]
loss: 0.017406  [  200/ 3474]
loss: 0.061526  [  300/ 3474]
loss: 0.073715  [  400/ 3474]
loss: 0.005804  [  500/ 3474]
loss: 0.025630  [  600/ 3474]
loss: 0.011218  [  700/ 3474]
loss: 0.041871  [  800/ 3474]
loss: 0.185259  [  900/ 3474]
loss: 0.007580  [ 1000/ 3474]
loss: 0.146942  [ 1100/ 3474]
loss: 0.035396  [ 1200/ 3474]
loss: 0.028801  [ 1300/ 3474]
loss: 0.017981  [ 1400/ 3474]
loss: 0.075161  [ 1500/ 3474]
loss: 0.012545  [ 1600/ 3474]
loss: 0.055463  [ 1700/ 3474]
loss: 0.027953  [ 1800/ 3474]
loss: 0.024871  [ 1900/ 3474]
loss: 0.008284  [ 2000/ 3474]
loss: 0.016388  [ 2100/ 3474]
loss: 0.027931  [ 2200/ 3474]
loss: 0.065542  [ 2300/ 3474]
loss: 0.006286  [ 2400/ 3474]
loss: 0.020137  [ 2500/ 3474]
loss: 0.022501  [ 2600/ 3474]
loss: 0.052299  [ 2700/ 3474]
loss: 0.016713  [ 2800/ 3474]
loss: 0.015066  [ 2900/ 3474]
loss: 0.212855  [ 3000/ 3474]
loss: 0.010870  [ 3100/ 3474]
loss: 0.015630  [ 3200/ 3474]
loss: 0.052029  [ 3300/ 3474]
loss: 0.022638  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.030679  [    0/ 3474]
loss: 0.009527  [  100/ 3474]
loss: 0.018087  [  200/ 3474]
loss: 0.061673  [  300/ 3474]
loss: 0.065955  [  400/ 3474]
loss: 0.005212  [  500/ 3474]
loss: 0.025276  [  600/ 3474]
loss: 0.011098  [  700/ 3474]
loss: 0.036625  [  800/ 3474]
loss: 0.183890  [  900/ 3474]
loss: 0.007218  [ 1000/ 3474]
loss: 0.153204  [ 1100/ 3474]
loss: 0.030787  [ 1200/ 3474]
loss: 0.027731  [ 1300/ 3474]
loss: 0.017422  [ 1400/ 3474]
loss: 0.076445  [ 1500/ 3474]
loss: 0.012344  [ 1600/ 3474]
loss: 0.050911  [ 1700/ 3474]
loss: 0.027496  [ 1800/ 3474]
loss: 0.024024  [ 1900/ 3474]
loss: 0.008561  [ 2000/ 3474]
loss: 0.016359  [ 2100/ 3474]
loss: 0.028049  [ 2200/ 3474]
loss: 0.065946  [ 2300/ 3474]
loss: 0.006124  [ 2400/ 3474]
loss: 0.017317  [ 2500/ 3474]
loss: 0.022282  [ 2600/ 3474]
loss: 0.053215  [ 2700/ 3474]
loss: 0.016588  [ 2800/ 3474]
loss: 0.015305  [ 2900/ 3474]
loss: 0.221941  [ 3000/ 3474]
loss: 0.011166  [ 3100/ 3474]
loss: 0.015967  [ 3200/ 3474]
loss: 0.051677  [ 3300/ 3474]
loss: 0.016642  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.031178  [    0/ 3474]
loss: 0.009468  [  100/ 3474]
loss: 0.018760  [  200/ 3474]
loss: 0.061666  [  300/ 3474]
loss: 0.062147  [  400/ 3474]
loss: 0.005198  [  500/ 3474]
loss: 0.025212  [  600/ 3474]
loss: 0.011315  [  700/ 3474]
loss: 0.033103  [  800/ 3474]
loss: 0.182761  [  900/ 3474]
loss: 0.006274  [ 1000/ 3474]
loss: 0.156609  [ 1100/ 3474]
loss: 0.032314  [ 1200/ 3474]
loss: 0.026709  [ 1300/ 3474]
loss: 0.018974  [ 1400/ 3474]
loss: 0.077494  [ 1500/ 3474]
loss: 0.011725  [ 1600/ 3474]
loss: 0.045521  [ 1700/ 3474]
loss: 0.026189  [ 1800/ 3474]
loss: 0.023361  [ 1900/ 3474]
loss: 0.008604  [ 2000/ 3474]
loss: 0.016766  [ 2100/ 3474]
loss: 0.027944  [ 2200/ 3474]
loss: 0.066084  [ 2300/ 3474]
loss: 0.006470  [ 2400/ 3474]
loss: 0.015729  [ 2500/ 3474]
loss: 0.022495  [ 2600/ 3474]
loss: 0.049764  [ 2700/ 3474]
loss: 0.017039  [ 2800/ 3474]
loss: 0.015298  [ 2900/ 3474]
loss: 0.225534  [ 3000/ 3474]
loss: 0.011481  [ 3100/ 3474]
loss: 0.015610  [ 3200/ 3474]
loss: 0.050844  [ 3300/ 3474]
loss: 0.013041  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.031184  [    0/ 3474]
loss: 0.009448  [  100/ 3474]
loss: 0.019629  [  200/ 3474]
loss: 0.061854  [  300/ 3474]
loss: 0.058377  [  400/ 3474]
loss: 0.005196  [  500/ 3474]
loss: 0.024766  [  600/ 3474]
loss: 0.011034  [  700/ 3474]
loss: 0.031113  [  800/ 3474]
loss: 0.183028  [  900/ 3474]
loss: 0.006580  [ 1000/ 3474]
loss: 0.148237  [ 1100/ 3474]
loss: 0.030709  [ 1200/ 3474]
loss: 0.026375  [ 1300/ 3474]
loss: 0.017993  [ 1400/ 3474]
loss: 0.077874  [ 1500/ 3474]
loss: 0.011562  [ 1600/ 3474]
loss: 0.043506  [ 1700/ 3474]
loss: 0.025725  [ 1800/ 3474]
loss: 0.023068  [ 1900/ 3474]
loss: 0.008671  [ 2000/ 3474]
loss: 0.016774  [ 2100/ 3474]
loss: 0.028002  [ 2200/ 3474]
loss: 0.066478  [ 2300/ 3474]
loss: 0.005923  [ 2400/ 3474]
loss: 0.015483  [ 2500/ 3474]
loss: 0.022372  [ 2600/ 3474]
loss: 0.048360  [ 2700/ 3474]
loss: 0.017489  [ 2800/ 3474]
loss: 0.015481  [ 2900/ 3474]
loss: 0.228082  [ 3000/ 3474]
loss: 0.011532  [ 3100/ 3474]
loss: 0.015602  [ 3200/ 3474]
loss: 0.050748  [ 3300/ 3474]
loss: 0.014668  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [-0.48060292 -0.6154235 ]
[0 1 1 ... 1 1 2]
[0 1 1 ... 1 1 2]
Cluster 0 Occurrences: 1198; KMEANS: 1185
Cluster 1 Occurrences: 1128; KMEANS: 1108
Cluster 2 Occurrences: 1148; KMEANS: 1181
Centroids: [[-0.95282555, -1.1156147], [1.459767, 2.573263], [0.76815045, -1.3772975]]
Centroids: [[-0.9741174, -1.107994], [1.4834683, 2.634776], [0.7600468, -1.3728726]]
Contingency Matrix: 
[[1176    0   22]
 [   8 1108   12]
 [   1    0 1147]]
[[-1, -1, -1], [-1, 1108, 12], [-1, 0, 1147]]
[[-1, -1, -1], [-1, 1108, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 2, 1: 1}
New Contingency Matrix: 
[[1176    0   22]
 [   8 1108   12]
 [   1    0 1147]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1176, 1108, 1147], Sum: 3431
All_Elements: [1176, 0, 22, 8, 1108, 12, 1, 0, 1147], Sum: 3474
Accuracy: 0.98762233736327
Done!
