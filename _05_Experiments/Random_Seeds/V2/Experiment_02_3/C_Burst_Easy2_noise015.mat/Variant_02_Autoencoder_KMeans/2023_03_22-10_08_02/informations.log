Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_02
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E78D945C18>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x000001E795D002B0>
Epoch 1
-------------------------------
loss: 0.237760  [    0/ 3442]
loss: 0.162660  [  100/ 3442]
loss: 0.051788  [  200/ 3442]
loss: 0.035062  [  300/ 3442]
loss: 0.043654  [  400/ 3442]
loss: 0.027396  [  500/ 3442]
loss: 0.030790  [  600/ 3442]
loss: 0.015300  [  700/ 3442]
loss: 0.035300  [  800/ 3442]
loss: 0.037268  [  900/ 3442]
loss: 0.031280  [ 1000/ 3442]
loss: 0.004881  [ 1100/ 3442]
loss: 0.024795  [ 1200/ 3442]
loss: 0.010865  [ 1300/ 3442]
loss: 0.019827  [ 1400/ 3442]
loss: 0.021220  [ 1500/ 3442]
loss: 0.016629  [ 1600/ 3442]
loss: 0.014080  [ 1700/ 3442]
loss: 0.016555  [ 1800/ 3442]
loss: 0.042853  [ 1900/ 3442]
loss: 0.014410  [ 2000/ 3442]
loss: 0.010503  [ 2100/ 3442]
loss: 0.019555  [ 2200/ 3442]
loss: 0.011358  [ 2300/ 3442]
loss: 0.014943  [ 2400/ 3442]
loss: 0.020598  [ 2500/ 3442]
loss: 0.108607  [ 2600/ 3442]
loss: 0.052767  [ 2700/ 3442]
loss: 0.007970  [ 2800/ 3442]
loss: 0.005071  [ 2900/ 3442]
loss: 0.005833  [ 3000/ 3442]
loss: 0.018489  [ 3100/ 3442]
loss: 0.137945  [ 3200/ 3442]
loss: 0.014563  [ 3300/ 3442]
loss: 0.012367  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.024915  [    0/ 3442]
loss: 0.010872  [  100/ 3442]
loss: 0.030083  [  200/ 3442]
loss: 0.005863  [  300/ 3442]
loss: 0.009470  [  400/ 3442]
loss: 0.009630  [  500/ 3442]
loss: 0.030102  [  600/ 3442]
loss: 0.011386  [  700/ 3442]
loss: 0.016317  [  800/ 3442]
loss: 0.032008  [  900/ 3442]
loss: 0.009795  [ 1000/ 3442]
loss: 0.003585  [ 1100/ 3442]
loss: 0.012272  [ 1200/ 3442]
loss: 0.008233  [ 1300/ 3442]
loss: 0.013993  [ 1400/ 3442]
loss: 0.019029  [ 1500/ 3442]
loss: 0.018076  [ 1600/ 3442]
loss: 0.010281  [ 1700/ 3442]
loss: 0.015562  [ 1800/ 3442]
loss: 0.020874  [ 1900/ 3442]
loss: 0.015824  [ 2000/ 3442]
loss: 0.011923  [ 2100/ 3442]
loss: 0.015798  [ 2200/ 3442]
loss: 0.010847  [ 2300/ 3442]
loss: 0.013102  [ 2400/ 3442]
loss: 0.029991  [ 2500/ 3442]
loss: 0.122000  [ 2600/ 3442]
loss: 0.032856  [ 2700/ 3442]
loss: 0.008855  [ 2800/ 3442]
loss: 0.007835  [ 2900/ 3442]
loss: 0.005387  [ 3000/ 3442]
loss: 0.018732  [ 3100/ 3442]
loss: 0.124724  [ 3200/ 3442]
loss: 0.013632  [ 3300/ 3442]
loss: 0.018093  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.012474  [    0/ 3442]
loss: 0.012887  [  100/ 3442]
loss: 0.030291  [  200/ 3442]
loss: 0.006367  [  300/ 3442]
loss: 0.015164  [  400/ 3442]
loss: 0.009351  [  500/ 3442]
loss: 0.028197  [  600/ 3442]
loss: 0.012721  [  700/ 3442]
loss: 0.016344  [  800/ 3442]
loss: 0.027807  [  900/ 3442]
loss: 0.009426  [ 1000/ 3442]
loss: 0.002967  [ 1100/ 3442]
loss: 0.013141  [ 1200/ 3442]
loss: 0.007162  [ 1300/ 3442]
loss: 0.013986  [ 1400/ 3442]
loss: 0.017670  [ 1500/ 3442]
loss: 0.016400  [ 1600/ 3442]
loss: 0.010650  [ 1700/ 3442]
loss: 0.015831  [ 1800/ 3442]
loss: 0.016099  [ 1900/ 3442]
loss: 0.015739  [ 2000/ 3442]
loss: 0.012636  [ 2100/ 3442]
loss: 0.014178  [ 2200/ 3442]
loss: 0.010915  [ 2300/ 3442]
loss: 0.012494  [ 2400/ 3442]
loss: 0.032125  [ 2500/ 3442]
loss: 0.125556  [ 2600/ 3442]
loss: 0.030217  [ 2700/ 3442]
loss: 0.008745  [ 2800/ 3442]
loss: 0.009022  [ 2900/ 3442]
loss: 0.005323  [ 3000/ 3442]
loss: 0.018154  [ 3100/ 3442]
loss: 0.121147  [ 3200/ 3442]
loss: 0.012825  [ 3300/ 3442]
loss: 0.019609  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.010396  [    0/ 3442]
loss: 0.013328  [  100/ 3442]
loss: 0.029457  [  200/ 3442]
loss: 0.006176  [  300/ 3442]
loss: 0.016610  [  400/ 3442]
loss: 0.008967  [  500/ 3442]
loss: 0.025374  [  600/ 3442]
loss: 0.013242  [  700/ 3442]
loss: 0.016863  [  800/ 3442]
loss: 0.026717  [  900/ 3442]
loss: 0.010015  [ 1000/ 3442]
loss: 0.002768  [ 1100/ 3442]
loss: 0.014281  [ 1200/ 3442]
loss: 0.006491  [ 1300/ 3442]
loss: 0.013028  [ 1400/ 3442]
loss: 0.018138  [ 1500/ 3442]
loss: 0.015939  [ 1600/ 3442]
loss: 0.010496  [ 1700/ 3442]
loss: 0.015422  [ 1800/ 3442]
loss: 0.015084  [ 1900/ 3442]
loss: 0.015760  [ 2000/ 3442]
loss: 0.012609  [ 2100/ 3442]
loss: 0.013142  [ 2200/ 3442]
loss: 0.010915  [ 2300/ 3442]
loss: 0.011613  [ 2400/ 3442]
loss: 0.032106  [ 2500/ 3442]
loss: 0.126012  [ 2600/ 3442]
loss: 0.029778  [ 2700/ 3442]
loss: 0.008527  [ 2800/ 3442]
loss: 0.009238  [ 2900/ 3442]
loss: 0.005229  [ 3000/ 3442]
loss: 0.017444  [ 3100/ 3442]
loss: 0.121466  [ 3200/ 3442]
loss: 0.012948  [ 3300/ 3442]
loss: 0.020506  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.011039  [    0/ 3442]
loss: 0.013474  [  100/ 3442]
loss: 0.029387  [  200/ 3442]
loss: 0.005997  [  300/ 3442]
loss: 0.016973  [  400/ 3442]
loss: 0.008906  [  500/ 3442]
loss: 0.024840  [  600/ 3442]
loss: 0.013511  [  700/ 3442]
loss: 0.017061  [  800/ 3442]
loss: 0.027114  [  900/ 3442]
loss: 0.009431  [ 1000/ 3442]
loss: 0.002823  [ 1100/ 3442]
loss: 0.015109  [ 1200/ 3442]
loss: 0.004858  [ 1300/ 3442]
loss: 0.012469  [ 1400/ 3442]
loss: 0.018053  [ 1500/ 3442]
loss: 0.014100  [ 1600/ 3442]
loss: 0.010255  [ 1700/ 3442]
loss: 0.015276  [ 1800/ 3442]
loss: 0.015006  [ 1900/ 3442]
loss: 0.015640  [ 2000/ 3442]
loss: 0.012380  [ 2100/ 3442]
loss: 0.012932  [ 2200/ 3442]
loss: 0.010853  [ 2300/ 3442]
loss: 0.010271  [ 2400/ 3442]
loss: 0.032238  [ 2500/ 3442]
loss: 0.126378  [ 2600/ 3442]
loss: 0.029323  [ 2700/ 3442]
loss: 0.008195  [ 2800/ 3442]
loss: 0.009409  [ 2900/ 3442]
loss: 0.005246  [ 3000/ 3442]
loss: 0.017204  [ 3100/ 3442]
loss: 0.122044  [ 3200/ 3442]
loss: 0.009684  [ 3300/ 3442]
loss: 0.023040  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.011530  [    0/ 3442]
loss: 0.014028  [  100/ 3442]
loss: 0.029327  [  200/ 3442]
loss: 0.006464  [  300/ 3442]
loss: 0.016569  [  400/ 3442]
loss: 0.008891  [  500/ 3442]
loss: 0.024735  [  600/ 3442]
loss: 0.012368  [  700/ 3442]
loss: 0.016854  [  800/ 3442]
loss: 0.027536  [  900/ 3442]
loss: 0.007439  [ 1000/ 3442]
loss: 0.002890  [ 1100/ 3442]
loss: 0.015457  [ 1200/ 3442]
loss: 0.004241  [ 1300/ 3442]
loss: 0.012403  [ 1400/ 3442]
loss: 0.018047  [ 1500/ 3442]
loss: 0.012638  [ 1600/ 3442]
loss: 0.010184  [ 1700/ 3442]
loss: 0.014961  [ 1800/ 3442]
loss: 0.014787  [ 1900/ 3442]
loss: 0.015617  [ 2000/ 3442]
loss: 0.012232  [ 2100/ 3442]
loss: 0.012559  [ 2200/ 3442]
loss: 0.010728  [ 2300/ 3442]
loss: 0.010119  [ 2400/ 3442]
loss: 0.032058  [ 2500/ 3442]
loss: 0.126329  [ 2600/ 3442]
loss: 0.029695  [ 2700/ 3442]
loss: 0.008111  [ 2800/ 3442]
loss: 0.008966  [ 2900/ 3442]
loss: 0.005048  [ 3000/ 3442]
loss: 0.017319  [ 3100/ 3442]
loss: 0.121757  [ 3200/ 3442]
loss: 0.009300  [ 3300/ 3442]
loss: 0.025942  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.012247  [    0/ 3442]
loss: 0.014164  [  100/ 3442]
loss: 0.029257  [  200/ 3442]
loss: 0.006488  [  300/ 3442]
loss: 0.015522  [  400/ 3442]
loss: 0.008885  [  500/ 3442]
loss: 0.025133  [  600/ 3442]
loss: 0.011439  [  700/ 3442]
loss: 0.016453  [  800/ 3442]
loss: 0.028476  [  900/ 3442]
loss: 0.007440  [ 1000/ 3442]
loss: 0.002957  [ 1100/ 3442]
loss: 0.015555  [ 1200/ 3442]
loss: 0.004170  [ 1300/ 3442]
loss: 0.012248  [ 1400/ 3442]
loss: 0.018020  [ 1500/ 3442]
loss: 0.011981  [ 1600/ 3442]
loss: 0.009913  [ 1700/ 3442]
loss: 0.014453  [ 1800/ 3442]
loss: 0.014670  [ 1900/ 3442]
loss: 0.015496  [ 2000/ 3442]
loss: 0.012162  [ 2100/ 3442]
loss: 0.012266  [ 2200/ 3442]
loss: 0.010590  [ 2300/ 3442]
loss: 0.010526  [ 2400/ 3442]
loss: 0.031453  [ 2500/ 3442]
loss: 0.125522  [ 2600/ 3442]
loss: 0.030707  [ 2700/ 3442]
loss: 0.007929  [ 2800/ 3442]
loss: 0.008663  [ 2900/ 3442]
loss: 0.004947  [ 3000/ 3442]
loss: 0.017295  [ 3100/ 3442]
loss: 0.121648  [ 3200/ 3442]
loss: 0.009529  [ 3300/ 3442]
loss: 0.027667  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.013008  [    0/ 3442]
loss: 0.014211  [  100/ 3442]
loss: 0.029681  [  200/ 3442]
loss: 0.006439  [  300/ 3442]
loss: 0.014824  [  400/ 3442]
loss: 0.008913  [  500/ 3442]
loss: 0.025298  [  600/ 3442]
loss: 0.010826  [  700/ 3442]
loss: 0.016367  [  800/ 3442]
loss: 0.029565  [  900/ 3442]
loss: 0.008117  [ 1000/ 3442]
loss: 0.002910  [ 1100/ 3442]
loss: 0.015458  [ 1200/ 3442]
loss: 0.004187  [ 1300/ 3442]
loss: 0.012121  [ 1400/ 3442]
loss: 0.018057  [ 1500/ 3442]
loss: 0.011680  [ 1600/ 3442]
loss: 0.009647  [ 1700/ 3442]
loss: 0.014052  [ 1800/ 3442]
loss: 0.014325  [ 1900/ 3442]
loss: 0.015414  [ 2000/ 3442]
loss: 0.012173  [ 2100/ 3442]
loss: 0.011812  [ 2200/ 3442]
loss: 0.010488  [ 2300/ 3442]
loss: 0.010878  [ 2400/ 3442]
loss: 0.030820  [ 2500/ 3442]
loss: 0.124721  [ 2600/ 3442]
loss: 0.032004  [ 2700/ 3442]
loss: 0.007658  [ 2800/ 3442]
loss: 0.008445  [ 2900/ 3442]
loss: 0.004926  [ 3000/ 3442]
loss: 0.017128  [ 3100/ 3442]
loss: 0.122174  [ 3200/ 3442]
loss: 0.009831  [ 3300/ 3442]
loss: 0.028711  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [0.8078017  0.26673236]
[1 0 0 ... 1 2 0]
[0 0 0 ... 0 2 0]
Cluster 0 Occurrences: 1159; KMEANS: 2329
Cluster 1 Occurrences: 1156; KMEANS: 608
Cluster 2 Occurrences: 1127; KMEANS: 505
Centroids: [[1.5343423, -0.06191989], [0.80044335, -0.18044941], [-3.4040844, 0.90114707]]
Centroids: [[1.165946, -0.1173824], [-4.2411776, 1.0094117], [-2.5141385, 0.78195935]]
Contingency Matrix: 
[[1159    0    0]
 [1151    0    5]
 [  19  608  500]]
[[-1, -1, -1], [-1, 0, 5], [-1, 608, 500]]
[[-1, -1, -1], [-1, -1, 5], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 1, 1: 2}
New Contingency Matrix: 
[[1159    0    0]
 [1151    5    0]
 [  19  500  608]]
New Clustered Label Sequence: [0, 2, 1]
Diagonal_Elements: [1159, 5, 608], Sum: 1772
All_Elements: [1159, 0, 0, 1151, 5, 0, 19, 500, 608], Sum: 3442
Accuracy: 0.5148169668797211
Done!
