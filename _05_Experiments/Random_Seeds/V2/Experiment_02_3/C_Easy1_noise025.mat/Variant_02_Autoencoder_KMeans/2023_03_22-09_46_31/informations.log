Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_31
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E732955198>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x000001E7328FBE10>
Epoch 1
-------------------------------
loss: 0.216157  [    0/ 3298]
loss: 0.422736  [  100/ 3298]
loss: 0.184946  [  200/ 3298]
loss: 0.104421  [  300/ 3298]
loss: 0.122734  [  400/ 3298]
loss: 0.093731  [  500/ 3298]
loss: 0.044957  [  600/ 3298]
loss: 0.058665  [  700/ 3298]
loss: 0.062703  [  800/ 3298]
loss: 0.083517  [  900/ 3298]
loss: 0.046329  [ 1000/ 3298]
loss: 0.085370  [ 1100/ 3298]
loss: 0.075119  [ 1200/ 3298]
loss: 0.051666  [ 1300/ 3298]
loss: 0.012929  [ 1400/ 3298]
loss: 0.031921  [ 1500/ 3298]
loss: 0.008315  [ 1600/ 3298]
loss: 0.018471  [ 1700/ 3298]
loss: 0.046671  [ 1800/ 3298]
loss: 0.027518  [ 1900/ 3298]
loss: 0.009530  [ 2000/ 3298]
loss: 0.014773  [ 2100/ 3298]
loss: 0.015167  [ 2200/ 3298]
loss: 0.026719  [ 2300/ 3298]
loss: 0.016128  [ 2400/ 3298]
loss: 0.071836  [ 2500/ 3298]
loss: 0.044940  [ 2600/ 3298]
loss: 0.024316  [ 2700/ 3298]
loss: 0.104750  [ 2800/ 3298]
loss: 0.022994  [ 2900/ 3298]
loss: 0.040868  [ 3000/ 3298]
loss: 0.030350  [ 3100/ 3298]
loss: 0.100524  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.017543  [    0/ 3298]
loss: 0.021854  [  100/ 3298]
loss: 0.109416  [  200/ 3298]
loss: 0.015155  [  300/ 3298]
loss: 0.006952  [  400/ 3298]
loss: 0.023111  [  500/ 3298]
loss: 0.027075  [  600/ 3298]
loss: 0.063996  [  700/ 3298]
loss: 0.025984  [  800/ 3298]
loss: 0.027093  [  900/ 3298]
loss: 0.042144  [ 1000/ 3298]
loss: 0.039361  [ 1100/ 3298]
loss: 0.067824  [ 1200/ 3298]
loss: 0.041307  [ 1300/ 3298]
loss: 0.014407  [ 1400/ 3298]
loss: 0.016910  [ 1500/ 3298]
loss: 0.014522  [ 1600/ 3298]
loss: 0.010635  [ 1700/ 3298]
loss: 0.030854  [ 1800/ 3298]
loss: 0.025809  [ 1900/ 3298]
loss: 0.010434  [ 2000/ 3298]
loss: 0.013836  [ 2100/ 3298]
loss: 0.018012  [ 2200/ 3298]
loss: 0.023085  [ 2300/ 3298]
loss: 0.016100  [ 2400/ 3298]
loss: 0.027752  [ 2500/ 3298]
loss: 0.042084  [ 2600/ 3298]
loss: 0.028857  [ 2700/ 3298]
loss: 0.139863  [ 2800/ 3298]
loss: 0.020692  [ 2900/ 3298]
loss: 0.039294  [ 3000/ 3298]
loss: 0.022143  [ 3100/ 3298]
loss: 0.087032  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.013658  [    0/ 3298]
loss: 0.021967  [  100/ 3298]
loss: 0.109471  [  200/ 3298]
loss: 0.019416  [  300/ 3298]
loss: 0.007535  [  400/ 3298]
loss: 0.023176  [  500/ 3298]
loss: 0.020528  [  600/ 3298]
loss: 0.062558  [  700/ 3298]
loss: 0.024594  [  800/ 3298]
loss: 0.022043  [  900/ 3298]
loss: 0.035110  [ 1000/ 3298]
loss: 0.019509  [ 1100/ 3298]
loss: 0.065185  [ 1200/ 3298]
loss: 0.029823  [ 1300/ 3298]
loss: 0.015393  [ 1400/ 3298]
loss: 0.021907  [ 1500/ 3298]
loss: 0.016286  [ 1600/ 3298]
loss: 0.014101  [ 1700/ 3298]
loss: 0.029484  [ 1800/ 3298]
loss: 0.022814  [ 1900/ 3298]
loss: 0.010289  [ 2000/ 3298]
loss: 0.015135  [ 2100/ 3298]
loss: 0.017290  [ 2200/ 3298]
loss: 0.022172  [ 2300/ 3298]
loss: 0.016099  [ 2400/ 3298]
loss: 0.017934  [ 2500/ 3298]
loss: 0.033207  [ 2600/ 3298]
loss: 0.034215  [ 2700/ 3298]
loss: 0.172509  [ 2800/ 3298]
loss: 0.026073  [ 2900/ 3298]
loss: 0.036669  [ 3000/ 3298]
loss: 0.020138  [ 3100/ 3298]
loss: 0.069972  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.016190  [    0/ 3298]
loss: 0.025417  [  100/ 3298]
loss: 0.109469  [  200/ 3298]
loss: 0.019821  [  300/ 3298]
loss: 0.008418  [  400/ 3298]
loss: 0.024367  [  500/ 3298]
loss: 0.023119  [  600/ 3298]
loss: 0.061863  [  700/ 3298]
loss: 0.023816  [  800/ 3298]
loss: 0.020322  [  900/ 3298]
loss: 0.030753  [ 1000/ 3298]
loss: 0.017197  [ 1100/ 3298]
loss: 0.065994  [ 1200/ 3298]
loss: 0.023790  [ 1300/ 3298]
loss: 0.014719  [ 1400/ 3298]
loss: 0.022348  [ 1500/ 3298]
loss: 0.016028  [ 1600/ 3298]
loss: 0.015382  [ 1700/ 3298]
loss: 0.027884  [ 1800/ 3298]
loss: 0.022306  [ 1900/ 3298]
loss: 0.010763  [ 2000/ 3298]
loss: 0.015980  [ 2100/ 3298]
loss: 0.016465  [ 2200/ 3298]
loss: 0.021125  [ 2300/ 3298]
loss: 0.016493  [ 2400/ 3298]
loss: 0.012972  [ 2500/ 3298]
loss: 0.029137  [ 2600/ 3298]
loss: 0.037729  [ 2700/ 3298]
loss: 0.179707  [ 2800/ 3298]
loss: 0.025890  [ 2900/ 3298]
loss: 0.036065  [ 3000/ 3298]
loss: 0.019195  [ 3100/ 3298]
loss: 0.059831  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.018770  [    0/ 3298]
loss: 0.025760  [  100/ 3298]
loss: 0.105907  [  200/ 3298]
loss: 0.018902  [  300/ 3298]
loss: 0.008304  [  400/ 3298]
loss: 0.025281  [  500/ 3298]
loss: 0.016323  [  600/ 3298]
loss: 0.062180  [  700/ 3298]
loss: 0.023257  [  800/ 3298]
loss: 0.018489  [  900/ 3298]
loss: 0.026725  [ 1000/ 3298]
loss: 0.022964  [ 1100/ 3298]
loss: 0.067015  [ 1200/ 3298]
loss: 0.021943  [ 1300/ 3298]
loss: 0.014279  [ 1400/ 3298]
loss: 0.021423  [ 1500/ 3298]
loss: 0.016078  [ 1600/ 3298]
loss: 0.015389  [ 1700/ 3298]
loss: 0.028502  [ 1800/ 3298]
loss: 0.021997  [ 1900/ 3298]
loss: 0.010592  [ 2000/ 3298]
loss: 0.016691  [ 2100/ 3298]
loss: 0.014351  [ 2200/ 3298]
loss: 0.020467  [ 2300/ 3298]
loss: 0.015954  [ 2400/ 3298]
loss: 0.009801  [ 2500/ 3298]
loss: 0.027234  [ 2600/ 3298]
loss: 0.039041  [ 2700/ 3298]
loss: 0.187084  [ 2800/ 3298]
loss: 0.025939  [ 2900/ 3298]
loss: 0.035499  [ 3000/ 3298]
loss: 0.019350  [ 3100/ 3298]
loss: 0.055549  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.020784  [    0/ 3298]
loss: 0.023402  [  100/ 3298]
loss: 0.104298  [  200/ 3298]
loss: 0.017305  [  300/ 3298]
loss: 0.007829  [  400/ 3298]
loss: 0.027772  [  500/ 3298]
loss: 0.015495  [  600/ 3298]
loss: 0.061888  [  700/ 3298]
loss: 0.023554  [  800/ 3298]
loss: 0.015897  [  900/ 3298]
loss: 0.024819  [ 1000/ 3298]
loss: 0.028515  [ 1100/ 3298]
loss: 0.067008  [ 1200/ 3298]
loss: 0.021629  [ 1300/ 3298]
loss: 0.014723  [ 1400/ 3298]
loss: 0.022222  [ 1500/ 3298]
loss: 0.014452  [ 1600/ 3298]
loss: 0.015146  [ 1700/ 3298]
loss: 0.028854  [ 1800/ 3298]
loss: 0.021672  [ 1900/ 3298]
loss: 0.010386  [ 2000/ 3298]
loss: 0.016815  [ 2100/ 3298]
loss: 0.012607  [ 2200/ 3298]
loss: 0.019788  [ 2300/ 3298]
loss: 0.015615  [ 2400/ 3298]
loss: 0.008439  [ 2500/ 3298]
loss: 0.026835  [ 2600/ 3298]
loss: 0.039591  [ 2700/ 3298]
loss: 0.202526  [ 2800/ 3298]
loss: 0.026299  [ 2900/ 3298]
loss: 0.034515  [ 3000/ 3298]
loss: 0.018667  [ 3100/ 3298]
loss: 0.054032  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.022145  [    0/ 3298]
loss: 0.023303  [  100/ 3298]
loss: 0.102774  [  200/ 3298]
loss: 0.017128  [  300/ 3298]
loss: 0.007489  [  400/ 3298]
loss: 0.025559  [  500/ 3298]
loss: 0.014049  [  600/ 3298]
loss: 0.060967  [  700/ 3298]
loss: 0.023440  [  800/ 3298]
loss: 0.015654  [  900/ 3298]
loss: 0.023710  [ 1000/ 3298]
loss: 0.027371  [ 1100/ 3298]
loss: 0.067479  [ 1200/ 3298]
loss: 0.022204  [ 1300/ 3298]
loss: 0.014477  [ 1400/ 3298]
loss: 0.023063  [ 1500/ 3298]
loss: 0.014524  [ 1600/ 3298]
loss: 0.015324  [ 1700/ 3298]
loss: 0.028557  [ 1800/ 3298]
loss: 0.021401  [ 1900/ 3298]
loss: 0.010290  [ 2000/ 3298]
loss: 0.016951  [ 2100/ 3298]
loss: 0.011118  [ 2200/ 3298]
loss: 0.019467  [ 2300/ 3298]
loss: 0.014937  [ 2400/ 3298]
loss: 0.007770  [ 2500/ 3298]
loss: 0.026506  [ 2600/ 3298]
loss: 0.040111  [ 2700/ 3298]
loss: 0.188240  [ 2800/ 3298]
loss: 0.026662  [ 2900/ 3298]
loss: 0.033194  [ 3000/ 3298]
loss: 0.018141  [ 3100/ 3298]
loss: 0.053267  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.022775  [    0/ 3298]
loss: 0.024304  [  100/ 3298]
loss: 0.102334  [  200/ 3298]
loss: 0.016912  [  300/ 3298]
loss: 0.007073  [  400/ 3298]
loss: 0.025758  [  500/ 3298]
loss: 0.012917  [  600/ 3298]
loss: 0.059639  [  700/ 3298]
loss: 0.023692  [  800/ 3298]
loss: 0.014684  [  900/ 3298]
loss: 0.022785  [ 1000/ 3298]
loss: 0.028279  [ 1100/ 3298]
loss: 0.067697  [ 1200/ 3298]
loss: 0.023229  [ 1300/ 3298]
loss: 0.013980  [ 1400/ 3298]
loss: 0.023392  [ 1500/ 3298]
loss: 0.014304  [ 1600/ 3298]
loss: 0.015309  [ 1700/ 3298]
loss: 0.028570  [ 1800/ 3298]
loss: 0.021140  [ 1900/ 3298]
loss: 0.010466  [ 2000/ 3298]
loss: 0.016689  [ 2100/ 3298]
loss: 0.009960  [ 2200/ 3298]
loss: 0.019374  [ 2300/ 3298]
loss: 0.014266  [ 2400/ 3298]
loss: 0.007241  [ 2500/ 3298]
loss: 0.025960  [ 2600/ 3298]
loss: 0.040736  [ 2700/ 3298]
loss: 0.180932  [ 2800/ 3298]
loss: 0.026849  [ 2900/ 3298]
loss: 0.032133  [ 3000/ 3298]
loss: 0.018058  [ 3100/ 3298]
loss: 0.052803  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [1.2909172 3.721013 ]
[1 0 0 ... 0 1 2]
[0 1 1 ... 1 0 2]
Cluster 0 Occurrences: 1094; KMEANS: 1083
Cluster 1 Occurrences: 1089; KMEANS: 1060
Cluster 2 Occurrences: 1115; KMEANS: 1155
Centroids: [[-1.1958733, -1.4810157], [0.6853422, 3.6031964], [1.4902979, -1.1336337]]
Centroids: [[0.69402295, 3.6270826], [-1.2651461, -1.4915893], [1.4624784, -1.1319461]]
Contingency Matrix: 
[[   0 1052   42]
 [1079    5    5]
 [   4    3 1108]]
[[0, 1052, -1], [1079, 5, -1], [-1, -1, -1]]
[[-1, 1052, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 2, 1: 0, 0: 1}
New Contingency Matrix: 
[[1052    0   42]
 [   5 1079    5]
 [   3    4 1108]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1052, 1079, 1108], Sum: 3239
All_Elements: [1052, 0, 42, 5, 1079, 5, 3, 4, 1108], Sum: 3298
Accuracy: 0.9821103699211643
Done!
