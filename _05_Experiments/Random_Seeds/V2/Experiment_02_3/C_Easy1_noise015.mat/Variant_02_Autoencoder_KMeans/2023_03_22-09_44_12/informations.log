Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_12
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E7327E34A8>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x000001E7328FBE10>
Epoch 1
-------------------------------
loss: 0.182637  [    0/ 3477]
loss: 0.306414  [  100/ 3477]
loss: 0.124004  [  200/ 3477]
loss: 0.060313  [  300/ 3477]
loss: 0.022586  [  400/ 3477]
loss: 0.021013  [  500/ 3477]
loss: 0.059786  [  600/ 3477]
loss: 0.044798  [  700/ 3477]
loss: 0.031172  [  800/ 3477]
loss: 0.040888  [  900/ 3477]
loss: 0.025639  [ 1000/ 3477]
loss: 0.008105  [ 1100/ 3477]
loss: 0.025508  [ 1200/ 3477]
loss: 0.022597  [ 1300/ 3477]
loss: 0.016302  [ 1400/ 3477]
loss: 0.027179  [ 1500/ 3477]
loss: 0.012268  [ 1600/ 3477]
loss: 0.011911  [ 1700/ 3477]
loss: 0.008900  [ 1800/ 3477]
loss: 0.016709  [ 1900/ 3477]
loss: 0.024572  [ 2000/ 3477]
loss: 0.024421  [ 2100/ 3477]
loss: 0.021993  [ 2200/ 3477]
loss: 0.011626  [ 2300/ 3477]
loss: 0.011388  [ 2400/ 3477]
loss: 0.096004  [ 2500/ 3477]
loss: 0.084554  [ 2600/ 3477]
loss: 0.009466  [ 2700/ 3477]
loss: 0.081173  [ 2800/ 3477]
loss: 0.020654  [ 2900/ 3477]
loss: 0.018926  [ 3000/ 3477]
loss: 0.012456  [ 3100/ 3477]
loss: 0.110131  [ 3200/ 3477]
loss: 0.009612  [ 3300/ 3477]
loss: 0.006978  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.015526  [    0/ 3477]
loss: 0.031681  [  100/ 3477]
loss: 0.032533  [  200/ 3477]
loss: 0.033375  [  300/ 3477]
loss: 0.012472  [  400/ 3477]
loss: 0.008828  [  500/ 3477]
loss: 0.016768  [  600/ 3477]
loss: 0.024257  [  700/ 3477]
loss: 0.016753  [  800/ 3477]
loss: 0.026003  [  900/ 3477]
loss: 0.021129  [ 1000/ 3477]
loss: 0.007140  [ 1100/ 3477]
loss: 0.021033  [ 1200/ 3477]
loss: 0.018916  [ 1300/ 3477]
loss: 0.015918  [ 1400/ 3477]
loss: 0.025304  [ 1500/ 3477]
loss: 0.010024  [ 1600/ 3477]
loss: 0.012478  [ 1700/ 3477]
loss: 0.006362  [ 1800/ 3477]
loss: 0.016447  [ 1900/ 3477]
loss: 0.024015  [ 2000/ 3477]
loss: 0.026034  [ 2100/ 3477]
loss: 0.020908  [ 2200/ 3477]
loss: 0.011067  [ 2300/ 3477]
loss: 0.009247  [ 2400/ 3477]
loss: 0.084247  [ 2500/ 3477]
loss: 0.082906  [ 2600/ 3477]
loss: 0.008047  [ 2700/ 3477]
loss: 0.110946  [ 2800/ 3477]
loss: 0.019850  [ 2900/ 3477]
loss: 0.018371  [ 3000/ 3477]
loss: 0.011766  [ 3100/ 3477]
loss: 0.108403  [ 3200/ 3477]
loss: 0.010084  [ 3300/ 3477]
loss: 0.007015  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.013536  [    0/ 3477]
loss: 0.028529  [  100/ 3477]
loss: 0.025275  [  200/ 3477]
loss: 0.030524  [  300/ 3477]
loss: 0.018924  [  400/ 3477]
loss: 0.009938  [  500/ 3477]
loss: 0.018478  [  600/ 3477]
loss: 0.024168  [  700/ 3477]
loss: 0.014206  [  800/ 3477]
loss: 0.023686  [  900/ 3477]
loss: 0.021254  [ 1000/ 3477]
loss: 0.006572  [ 1100/ 3477]
loss: 0.020235  [ 1200/ 3477]
loss: 0.018997  [ 1300/ 3477]
loss: 0.016499  [ 1400/ 3477]
loss: 0.022615  [ 1500/ 3477]
loss: 0.011892  [ 1600/ 3477]
loss: 0.013770  [ 1700/ 3477]
loss: 0.006675  [ 1800/ 3477]
loss: 0.011960  [ 1900/ 3477]
loss: 0.022777  [ 2000/ 3477]
loss: 0.026221  [ 2100/ 3477]
loss: 0.021457  [ 2200/ 3477]
loss: 0.011822  [ 2300/ 3477]
loss: 0.007746  [ 2400/ 3477]
loss: 0.074637  [ 2500/ 3477]
loss: 0.083450  [ 2600/ 3477]
loss: 0.007204  [ 2700/ 3477]
loss: 0.134762  [ 2800/ 3477]
loss: 0.019695  [ 2900/ 3477]
loss: 0.017188  [ 3000/ 3477]
loss: 0.010825  [ 3100/ 3477]
loss: 0.099527  [ 3200/ 3477]
loss: 0.010900  [ 3300/ 3477]
loss: 0.010236  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.008901  [    0/ 3477]
loss: 0.020222  [  100/ 3477]
loss: 0.020921  [  200/ 3477]
loss: 0.030492  [  300/ 3477]
loss: 0.018255  [  400/ 3477]
loss: 0.008416  [  500/ 3477]
loss: 0.018243  [  600/ 3477]
loss: 0.026229  [  700/ 3477]
loss: 0.012552  [  800/ 3477]
loss: 0.020252  [  900/ 3477]
loss: 0.019621  [ 1000/ 3477]
loss: 0.004891  [ 1100/ 3477]
loss: 0.020973  [ 1200/ 3477]
loss: 0.016052  [ 1300/ 3477]
loss: 0.015411  [ 1400/ 3477]
loss: 0.019661  [ 1500/ 3477]
loss: 0.012747  [ 1600/ 3477]
loss: 0.013495  [ 1700/ 3477]
loss: 0.007018  [ 1800/ 3477]
loss: 0.010007  [ 1900/ 3477]
loss: 0.023238  [ 2000/ 3477]
loss: 0.025996  [ 2100/ 3477]
loss: 0.022173  [ 2200/ 3477]
loss: 0.013322  [ 2300/ 3477]
loss: 0.006888  [ 2400/ 3477]
loss: 0.076942  [ 2500/ 3477]
loss: 0.082306  [ 2600/ 3477]
loss: 0.004916  [ 2700/ 3477]
loss: 0.124958  [ 2800/ 3477]
loss: 0.021365  [ 2900/ 3477]
loss: 0.015822  [ 3000/ 3477]
loss: 0.009869  [ 3100/ 3477]
loss: 0.091886  [ 3200/ 3477]
loss: 0.012120  [ 3300/ 3477]
loss: 0.013067  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.007380  [    0/ 3477]
loss: 0.016519  [  100/ 3477]
loss: 0.019754  [  200/ 3477]
loss: 0.029980  [  300/ 3477]
loss: 0.013289  [  400/ 3477]
loss: 0.007110  [  500/ 3477]
loss: 0.018023  [  600/ 3477]
loss: 0.025846  [  700/ 3477]
loss: 0.012405  [  800/ 3477]
loss: 0.018426  [  900/ 3477]
loss: 0.018324  [ 1000/ 3477]
loss: 0.003102  [ 1100/ 3477]
loss: 0.021619  [ 1200/ 3477]
loss: 0.013426  [ 1300/ 3477]
loss: 0.014285  [ 1400/ 3477]
loss: 0.019455  [ 1500/ 3477]
loss: 0.012110  [ 1600/ 3477]
loss: 0.013341  [ 1700/ 3477]
loss: 0.007167  [ 1800/ 3477]
loss: 0.009512  [ 1900/ 3477]
loss: 0.022987  [ 2000/ 3477]
loss: 0.025916  [ 2100/ 3477]
loss: 0.021225  [ 2200/ 3477]
loss: 0.013657  [ 2300/ 3477]
loss: 0.006901  [ 2400/ 3477]
loss: 0.079335  [ 2500/ 3477]
loss: 0.082584  [ 2600/ 3477]
loss: 0.004307  [ 2700/ 3477]
loss: 0.115135  [ 2800/ 3477]
loss: 0.021340  [ 2900/ 3477]
loss: 0.015095  [ 3000/ 3477]
loss: 0.009597  [ 3100/ 3477]
loss: 0.088796  [ 3200/ 3477]
loss: 0.012599  [ 3300/ 3477]
loss: 0.014319  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.005924  [    0/ 3477]
loss: 0.015244  [  100/ 3477]
loss: 0.019754  [  200/ 3477]
loss: 0.028902  [  300/ 3477]
loss: 0.012992  [  400/ 3477]
loss: 0.006661  [  500/ 3477]
loss: 0.017242  [  600/ 3477]
loss: 0.024981  [  700/ 3477]
loss: 0.012405  [  800/ 3477]
loss: 0.017776  [  900/ 3477]
loss: 0.017660  [ 1000/ 3477]
loss: 0.002684  [ 1100/ 3477]
loss: 0.021990  [ 1200/ 3477]
loss: 0.012222  [ 1300/ 3477]
loss: 0.013829  [ 1400/ 3477]
loss: 0.018262  [ 1500/ 3477]
loss: 0.011092  [ 1600/ 3477]
loss: 0.013333  [ 1700/ 3477]
loss: 0.007347  [ 1800/ 3477]
loss: 0.009477  [ 1900/ 3477]
loss: 0.022888  [ 2000/ 3477]
loss: 0.025360  [ 2100/ 3477]
loss: 0.021913  [ 2200/ 3477]
loss: 0.013457  [ 2300/ 3477]
loss: 0.007504  [ 2400/ 3477]
loss: 0.080467  [ 2500/ 3477]
loss: 0.081327  [ 2600/ 3477]
loss: 0.003829  [ 2700/ 3477]
loss: 0.109715  [ 2800/ 3477]
loss: 0.021422  [ 2900/ 3477]
loss: 0.014590  [ 3000/ 3477]
loss: 0.009543  [ 3100/ 3477]
loss: 0.088647  [ 3200/ 3477]
loss: 0.012716  [ 3300/ 3477]
loss: 0.015318  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.005316  [    0/ 3477]
loss: 0.015329  [  100/ 3477]
loss: 0.019587  [  200/ 3477]
loss: 0.028149  [  300/ 3477]
loss: 0.012519  [  400/ 3477]
loss: 0.006469  [  500/ 3477]
loss: 0.018097  [  600/ 3477]
loss: 0.024038  [  700/ 3477]
loss: 0.012017  [  800/ 3477]
loss: 0.017186  [  900/ 3477]
loss: 0.017018  [ 1000/ 3477]
loss: 0.002540  [ 1100/ 3477]
loss: 0.022246  [ 1200/ 3477]
loss: 0.011560  [ 1300/ 3477]
loss: 0.013339  [ 1400/ 3477]
loss: 0.018481  [ 1500/ 3477]
loss: 0.010862  [ 1600/ 3477]
loss: 0.013273  [ 1700/ 3477]
loss: 0.008048  [ 1800/ 3477]
loss: 0.009617  [ 1900/ 3477]
loss: 0.022703  [ 2000/ 3477]
loss: 0.024801  [ 2100/ 3477]
loss: 0.022352  [ 2200/ 3477]
loss: 0.013320  [ 2300/ 3477]
loss: 0.007805  [ 2400/ 3477]
loss: 0.080944  [ 2500/ 3477]
loss: 0.079621  [ 2600/ 3477]
loss: 0.003272  [ 2700/ 3477]
loss: 0.103913  [ 2800/ 3477]
loss: 0.020946  [ 2900/ 3477]
loss: 0.013672  [ 3000/ 3477]
loss: 0.008766  [ 3100/ 3477]
loss: 0.085570  [ 3200/ 3477]
loss: 0.013382  [ 3300/ 3477]
loss: 0.015462  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.004758  [    0/ 3477]
loss: 0.015798  [  100/ 3477]
loss: 0.020082  [  200/ 3477]
loss: 0.027230  [  300/ 3477]
loss: 0.012941  [  400/ 3477]
loss: 0.006674  [  500/ 3477]
loss: 0.017208  [  600/ 3477]
loss: 0.023204  [  700/ 3477]
loss: 0.011661  [  800/ 3477]
loss: 0.016587  [  900/ 3477]
loss: 0.015278  [ 1000/ 3477]
loss: 0.002638  [ 1100/ 3477]
loss: 0.023049  [ 1200/ 3477]
loss: 0.011360  [ 1300/ 3477]
loss: 0.013102  [ 1400/ 3477]
loss: 0.018335  [ 1500/ 3477]
loss: 0.010616  [ 1600/ 3477]
loss: 0.013162  [ 1700/ 3477]
loss: 0.007697  [ 1800/ 3477]
loss: 0.009736  [ 1900/ 3477]
loss: 0.022436  [ 2000/ 3477]
loss: 0.024374  [ 2100/ 3477]
loss: 0.024293  [ 2200/ 3477]
loss: 0.013264  [ 2300/ 3477]
loss: 0.007513  [ 2400/ 3477]
loss: 0.080292  [ 2500/ 3477]
loss: 0.080437  [ 2600/ 3477]
loss: 0.003077  [ 2700/ 3477]
loss: 0.104124  [ 2800/ 3477]
loss: 0.021948  [ 2900/ 3477]
loss: 0.013020  [ 3000/ 3477]
loss: 0.008731  [ 3100/ 3477]
loss: 0.084233  [ 3200/ 3477]
loss: 0.012988  [ 3300/ 3477]
loss: 0.011761  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [1.2509733 3.2472377]
[1 0 2 ... 1 1 1]
[1 2 0 ... 1 1 1]
Cluster 0 Occurrences: 1132; KMEANS: 1174
Cluster 1 Occurrences: 1188; KMEANS: 1178
Cluster 2 Occurrences: 1157; KMEANS: 1125
Centroids: [[-0.6848283, -1.1364744], [0.9082017, 2.2033901], [1.2137548, -1.126905]]
Centroids: [[1.2062496, -1.1264024], [0.9165386, 2.2278192], [-0.7002551, -1.1330358]]
Contingency Matrix: 
[[  12    0 1120]
 [  10 1173    5]
 [1152    5    0]]
[[12, -1, 1120], [-1, -1, -1], [1152, -1, 0]]
[[-1, -1, 1120], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1120    0   12]
 [   5 1173   10]
 [   0    5 1152]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1120, 1173, 1152], Sum: 3445
All_Elements: [1120, 0, 12, 5, 1173, 10, 0, 5, 1152], Sum: 3477
Accuracy: 0.9907966637906241
Done!
