Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_49_58
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E7385A26D8>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x000001E7328FBCF8>
Epoch 1
-------------------------------
loss: 0.136752  [    0/ 3386]
loss: 0.341682  [  100/ 3386]
loss: 0.240298  [  200/ 3386]
loss: 0.162582  [  300/ 3386]
loss: 0.254786  [  400/ 3386]
loss: 0.078563  [  500/ 3386]
loss: 0.182468  [  600/ 3386]
loss: 0.120481  [  700/ 3386]
loss: 0.180735  [  800/ 3386]
loss: 0.030092  [  900/ 3386]
loss: 0.302610  [ 1000/ 3386]
loss: 0.140681  [ 1100/ 3386]
loss: 0.081832  [ 1200/ 3386]
loss: 0.025717  [ 1300/ 3386]
loss: 0.074587  [ 1400/ 3386]
loss: 0.043897  [ 1500/ 3386]
loss: 0.040738  [ 1600/ 3386]
loss: 0.066946  [ 1700/ 3386]
loss: 0.173124  [ 1800/ 3386]
loss: 0.205429  [ 1900/ 3386]
loss: 0.129076  [ 2000/ 3386]
loss: 0.131605  [ 2100/ 3386]
loss: 0.049919  [ 2200/ 3386]
loss: 0.197679  [ 2300/ 3386]
loss: 0.061633  [ 2400/ 3386]
loss: 0.045161  [ 2500/ 3386]
loss: 0.088568  [ 2600/ 3386]
loss: 0.220712  [ 2700/ 3386]
loss: 0.134361  [ 2800/ 3386]
loss: 0.249608  [ 2900/ 3386]
loss: 0.022338  [ 3000/ 3386]
loss: 0.112427  [ 3100/ 3386]
loss: 0.081973  [ 3200/ 3386]
loss: 0.178948  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.063920  [    0/ 3386]
loss: 0.129640  [  100/ 3386]
loss: 0.083065  [  200/ 3386]
loss: 0.112305  [  300/ 3386]
loss: 0.244579  [  400/ 3386]
loss: 0.082811  [  500/ 3386]
loss: 0.180588  [  600/ 3386]
loss: 0.074973  [  700/ 3386]
loss: 0.129549  [  800/ 3386]
loss: 0.057371  [  900/ 3386]
loss: 0.266026  [ 1000/ 3386]
loss: 0.183958  [ 1100/ 3386]
loss: 0.145288  [ 1200/ 3386]
loss: 0.016102  [ 1300/ 3386]
loss: 0.056647  [ 1400/ 3386]
loss: 0.043025  [ 1500/ 3386]
loss: 0.044720  [ 1600/ 3386]
loss: 0.036472  [ 1700/ 3386]
loss: 0.190141  [ 1800/ 3386]
loss: 0.226661  [ 1900/ 3386]
loss: 0.050603  [ 2000/ 3386]
loss: 0.124507  [ 2100/ 3386]
loss: 0.182604  [ 2200/ 3386]
loss: 0.208171  [ 2300/ 3386]
loss: 0.055489  [ 2400/ 3386]
loss: 0.048989  [ 2500/ 3386]
loss: 0.094625  [ 2600/ 3386]
loss: 0.202178  [ 2700/ 3386]
loss: 0.097481  [ 2800/ 3386]
loss: 0.161786  [ 2900/ 3386]
loss: 0.017753  [ 3000/ 3386]
loss: 0.161885  [ 3100/ 3386]
loss: 0.107119  [ 3200/ 3386]
loss: 0.232415  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.058030  [    0/ 3386]
loss: 0.143392  [  100/ 3386]
loss: 0.086277  [  200/ 3386]
loss: 0.054168  [  300/ 3386]
loss: 0.201436  [  400/ 3386]
loss: 0.094146  [  500/ 3386]
loss: 0.163567  [  600/ 3386]
loss: 0.054401  [  700/ 3386]
loss: 0.112661  [  800/ 3386]
loss: 0.052408  [  900/ 3386]
loss: 0.294772  [ 1000/ 3386]
loss: 0.185116  [ 1100/ 3386]
loss: 0.142030  [ 1200/ 3386]
loss: 0.016485  [ 1300/ 3386]
loss: 0.055557  [ 1400/ 3386]
loss: 0.042334  [ 1500/ 3386]
loss: 0.055406  [ 1600/ 3386]
loss: 0.028758  [ 1700/ 3386]
loss: 0.196589  [ 1800/ 3386]
loss: 0.264487  [ 1900/ 3386]
loss: 0.044278  [ 2000/ 3386]
loss: 0.104206  [ 2100/ 3386]
loss: 0.231639  [ 2200/ 3386]
loss: 0.197948  [ 2300/ 3386]
loss: 0.058914  [ 2400/ 3386]
loss: 0.051005  [ 2500/ 3386]
loss: 0.085808  [ 2600/ 3386]
loss: 0.180105  [ 2700/ 3386]
loss: 0.058431  [ 2800/ 3386]
loss: 0.124771  [ 2900/ 3386]
loss: 0.016348  [ 3000/ 3386]
loss: 0.174119  [ 3100/ 3386]
loss: 0.132297  [ 3200/ 3386]
loss: 0.208511  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.056716  [    0/ 3386]
loss: 0.138736  [  100/ 3386]
loss: 0.086594  [  200/ 3386]
loss: 0.032550  [  300/ 3386]
loss: 0.172250  [  400/ 3386]
loss: 0.090231  [  500/ 3386]
loss: 0.142264  [  600/ 3386]
loss: 0.047933  [  700/ 3386]
loss: 0.113293  [  800/ 3386]
loss: 0.053171  [  900/ 3386]
loss: 0.278367  [ 1000/ 3386]
loss: 0.185439  [ 1100/ 3386]
loss: 0.156490  [ 1200/ 3386]
loss: 0.018617  [ 1300/ 3386]
loss: 0.054092  [ 1400/ 3386]
loss: 0.042576  [ 1500/ 3386]
loss: 0.049555  [ 1600/ 3386]
loss: 0.028030  [ 1700/ 3386]
loss: 0.199377  [ 1800/ 3386]
loss: 0.276858  [ 1900/ 3386]
loss: 0.051584  [ 2000/ 3386]
loss: 0.107847  [ 2100/ 3386]
loss: 0.248582  [ 2200/ 3386]
loss: 0.190998  [ 2300/ 3386]
loss: 0.063366  [ 2400/ 3386]
loss: 0.053389  [ 2500/ 3386]
loss: 0.083314  [ 2600/ 3386]
loss: 0.174974  [ 2700/ 3386]
loss: 0.047044  [ 2800/ 3386]
loss: 0.099710  [ 2900/ 3386]
loss: 0.015287  [ 3000/ 3386]
loss: 0.175161  [ 3100/ 3386]
loss: 0.156739  [ 3200/ 3386]
loss: 0.212883  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.057002  [    0/ 3386]
loss: 0.132436  [  100/ 3386]
loss: 0.085999  [  200/ 3386]
loss: 0.028553  [  300/ 3386]
loss: 0.167910  [  400/ 3386]
loss: 0.088177  [  500/ 3386]
loss: 0.130121  [  600/ 3386]
loss: 0.044771  [  700/ 3386]
loss: 0.108213  [  800/ 3386]
loss: 0.055436  [  900/ 3386]
loss: 0.262002  [ 1000/ 3386]
loss: 0.187249  [ 1100/ 3386]
loss: 0.155343  [ 1200/ 3386]
loss: 0.019628  [ 1300/ 3386]
loss: 0.053870  [ 1400/ 3386]
loss: 0.044037  [ 1500/ 3386]
loss: 0.047950  [ 1600/ 3386]
loss: 0.026941  [ 1700/ 3386]
loss: 0.190775  [ 1800/ 3386]
loss: 0.271639  [ 1900/ 3386]
loss: 0.052685  [ 2000/ 3386]
loss: 0.103691  [ 2100/ 3386]
loss: 0.247781  [ 2200/ 3386]
loss: 0.185476  [ 2300/ 3386]
loss: 0.065956  [ 2400/ 3386]
loss: 0.054917  [ 2500/ 3386]
loss: 0.073332  [ 2600/ 3386]
loss: 0.173645  [ 2700/ 3386]
loss: 0.044113  [ 2800/ 3386]
loss: 0.092396  [ 2900/ 3386]
loss: 0.014962  [ 3000/ 3386]
loss: 0.171629  [ 3100/ 3386]
loss: 0.175073  [ 3200/ 3386]
loss: 0.208503  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.058035  [    0/ 3386]
loss: 0.130968  [  100/ 3386]
loss: 0.084882  [  200/ 3386]
loss: 0.031870  [  300/ 3386]
loss: 0.171808  [  400/ 3386]
loss: 0.085819  [  500/ 3386]
loss: 0.119924  [  600/ 3386]
loss: 0.043403  [  700/ 3386]
loss: 0.102573  [  800/ 3386]
loss: 0.056899  [  900/ 3386]
loss: 0.241981  [ 1000/ 3386]
loss: 0.184989  [ 1100/ 3386]
loss: 0.155596  [ 1200/ 3386]
loss: 0.020288  [ 1300/ 3386]
loss: 0.054365  [ 1400/ 3386]
loss: 0.044985  [ 1500/ 3386]
loss: 0.043023  [ 1600/ 3386]
loss: 0.027674  [ 1700/ 3386]
loss: 0.183990  [ 1800/ 3386]
loss: 0.255679  [ 1900/ 3386]
loss: 0.051079  [ 2000/ 3386]
loss: 0.100185  [ 2100/ 3386]
loss: 0.240617  [ 2200/ 3386]
loss: 0.176970  [ 2300/ 3386]
loss: 0.065338  [ 2400/ 3386]
loss: 0.055150  [ 2500/ 3386]
loss: 0.072160  [ 2600/ 3386]
loss: 0.174233  [ 2700/ 3386]
loss: 0.042695  [ 2800/ 3386]
loss: 0.086519  [ 2900/ 3386]
loss: 0.015410  [ 3000/ 3386]
loss: 0.170245  [ 3100/ 3386]
loss: 0.182936  [ 3200/ 3386]
loss: 0.201463  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.058818  [    0/ 3386]
loss: 0.127524  [  100/ 3386]
loss: 0.084794  [  200/ 3386]
loss: 0.033437  [  300/ 3386]
loss: 0.174215  [  400/ 3386]
loss: 0.084068  [  500/ 3386]
loss: 0.111641  [  600/ 3386]
loss: 0.043445  [  700/ 3386]
loss: 0.098271  [  800/ 3386]
loss: 0.058422  [  900/ 3386]
loss: 0.224510  [ 1000/ 3386]
loss: 0.183630  [ 1100/ 3386]
loss: 0.155113  [ 1200/ 3386]
loss: 0.021138  [ 1300/ 3386]
loss: 0.054602  [ 1400/ 3386]
loss: 0.045714  [ 1500/ 3386]
loss: 0.041867  [ 1600/ 3386]
loss: 0.027368  [ 1700/ 3386]
loss: 0.179124  [ 1800/ 3386]
loss: 0.248194  [ 1900/ 3386]
loss: 0.049078  [ 2000/ 3386]
loss: 0.102586  [ 2100/ 3386]
loss: 0.250212  [ 2200/ 3386]
loss: 0.166307  [ 2300/ 3386]
loss: 0.065042  [ 2400/ 3386]
loss: 0.055271  [ 2500/ 3386]
loss: 0.069629  [ 2600/ 3386]
loss: 0.175609  [ 2700/ 3386]
loss: 0.041124  [ 2800/ 3386]
loss: 0.082769  [ 2900/ 3386]
loss: 0.015398  [ 3000/ 3386]
loss: 0.168388  [ 3100/ 3386]
loss: 0.184262  [ 3200/ 3386]
loss: 0.191231  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.060002  [    0/ 3386]
loss: 0.126741  [  100/ 3386]
loss: 0.084445  [  200/ 3386]
loss: 0.034622  [  300/ 3386]
loss: 0.178037  [  400/ 3386]
loss: 0.082010  [  500/ 3386]
loss: 0.109009  [  600/ 3386]
loss: 0.041078  [  700/ 3386]
loss: 0.097888  [  800/ 3386]
loss: 0.059016  [  900/ 3386]
loss: 0.209802  [ 1000/ 3386]
loss: 0.184159  [ 1100/ 3386]
loss: 0.153181  [ 1200/ 3386]
loss: 0.022062  [ 1300/ 3386]
loss: 0.054578  [ 1400/ 3386]
loss: 0.045793  [ 1500/ 3386]
loss: 0.037191  [ 1600/ 3386]
loss: 0.026890  [ 1700/ 3386]
loss: 0.174700  [ 1800/ 3386]
loss: 0.244003  [ 1900/ 3386]
loss: 0.048226  [ 2000/ 3386]
loss: 0.103965  [ 2100/ 3386]
loss: 0.232891  [ 2200/ 3386]
loss: 0.160183  [ 2300/ 3386]
loss: 0.064280  [ 2400/ 3386]
loss: 0.054536  [ 2500/ 3386]
loss: 0.062858  [ 2600/ 3386]
loss: 0.176315  [ 2700/ 3386]
loss: 0.039287  [ 2800/ 3386]
loss: 0.083893  [ 2900/ 3386]
loss: 0.014796  [ 3000/ 3386]
loss: 0.160509  [ 3100/ 3386]
loss: 0.164991  [ 3200/ 3386]
loss: 0.173928  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [-1.066852  -1.9616208]
[0 2 0 ... 2 0 1]
[0 2 2 ... 2 0 1]
Cluster 0 Occurrences: 1079; KMEANS: 952
Cluster 1 Occurrences: 1158; KMEANS: 1103
Cluster 2 Occurrences: 1149; KMEANS: 1331
Centroids: [[-0.90633065, -2.2158167], [-0.15077402, 3.7972288], [1.6197126, -0.72182685]]
Centroids: [[-1.1080253, -2.3764982], [-0.17721535, 3.9470823], [1.4716991, -0.6868973]]
Contingency Matrix: 
[[ 949    0  130]
 [   3 1098   57]
 [   0    5 1144]]
[[949, 0, -1], [3, 1098, -1], [-1, -1, -1]]
[[949, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 2, 1: 1, 0: 0}
New Contingency Matrix: 
[[ 949    0  130]
 [   3 1098   57]
 [   0    5 1144]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [949, 1098, 1144], Sum: 3191
All_Elements: [949, 0, 130, 3, 1098, 57, 0, 5, 1144], Sum: 3386
Accuracy: 0.9424099232132309
Done!
