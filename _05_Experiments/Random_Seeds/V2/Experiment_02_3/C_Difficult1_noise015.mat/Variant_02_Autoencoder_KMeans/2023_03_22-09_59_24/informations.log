Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Difficult1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_59_24
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E7877A7588>
Sampling rate: 24000.0
Raw: [ 0.04887081  0.02693095 -0.0154249  ... -0.09301659 -0.11629005
 -0.14613101]
Times: [    340     491     641 ... 1439047 1439065 1439816]
Cluster: [1 1 1 ... 3 2 2]
Number of different clusters:  3
Number of Spikes: 3472
First aligned Spike Frame: [ 0.12751554  0.12305882  0.10482977  0.09479529  0.10214978  0.11675932
  0.11777927  0.09307299  0.04670706 -0.00574343 -0.06143573 -0.14637617
 -0.20942665 -0.00208103  0.52241508  0.81651544  0.46446121 -0.19226425
 -0.60927882 -0.6713583  -0.57871227 -0.49011309 -0.4269388  -0.3668903
 -0.30523219 -0.24747124 -0.19738203 -0.15189972 -0.10449507 -0.05533325
 -0.01452429  0.01008816  0.02570853  0.04365027  0.06334113  0.07980397
  0.08484457  0.07688513  0.06142919  0.04320028  0.02240626  0.00477291
 -0.00393242 -0.00135684  0.00575182  0.0026944  -0.01541647]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1172
Cluster 2, Occurrences: 1141
<torch.utils.data.dataloader.DataLoader object at 0x000001E78753FFD0>
Epoch 1
-------------------------------
loss: 0.128547  [    0/ 3472]
loss: 0.132975  [  100/ 3472]
loss: 0.016471  [  200/ 3472]
loss: 0.025956  [  300/ 3472]
loss: 0.047276  [  400/ 3472]
loss: 0.015439  [  500/ 3472]
loss: 0.021799  [  600/ 3472]
loss: 0.022387  [  700/ 3472]
loss: 0.042586  [  800/ 3472]
loss: 0.027380  [  900/ 3472]
loss: 0.012700  [ 1000/ 3472]
loss: 0.031744  [ 1100/ 3472]
loss: 0.024060  [ 1200/ 3472]
loss: 0.024114  [ 1300/ 3472]
loss: 0.005284  [ 1400/ 3472]
loss: 0.059580  [ 1500/ 3472]
loss: 0.022025  [ 1600/ 3472]
loss: 0.010567  [ 1700/ 3472]
loss: 0.027096  [ 1800/ 3472]
loss: 0.171723  [ 1900/ 3472]
loss: 0.020674  [ 2000/ 3472]
loss: 0.013378  [ 2100/ 3472]
loss: 0.010684  [ 2200/ 3472]
loss: 0.013028  [ 2300/ 3472]
loss: 0.028489  [ 2400/ 3472]
loss: 0.010853  [ 2500/ 3472]
loss: 0.019482  [ 2600/ 3472]
loss: 0.008052  [ 2700/ 3472]
loss: 0.021934  [ 2800/ 3472]
loss: 0.030889  [ 2900/ 3472]
loss: 0.008553  [ 3000/ 3472]
loss: 0.025810  [ 3100/ 3472]
loss: 0.014262  [ 3200/ 3472]
loss: 0.011239  [ 3300/ 3472]
loss: 0.008347  [ 3400/ 3472]
Epoch 2
-------------------------------
loss: 0.013695  [    0/ 3472]
loss: 0.007343  [  100/ 3472]
loss: 0.008663  [  200/ 3472]
loss: 0.018293  [  300/ 3472]
loss: 0.009660  [  400/ 3472]
loss: 0.016007  [  500/ 3472]
loss: 0.010578  [  600/ 3472]
loss: 0.003859  [  700/ 3472]
loss: 0.033618  [  800/ 3472]
loss: 0.027524  [  900/ 3472]
loss: 0.010873  [ 1000/ 3472]
loss: 0.024633  [ 1100/ 3472]
loss: 0.022942  [ 1200/ 3472]
loss: 0.009902  [ 1300/ 3472]
loss: 0.002833  [ 1400/ 3472]
loss: 0.040779  [ 1500/ 3472]
loss: 0.020993  [ 1600/ 3472]
loss: 0.011680  [ 1700/ 3472]
loss: 0.023956  [ 1800/ 3472]
loss: 0.142433  [ 1900/ 3472]
loss: 0.043218  [ 2000/ 3472]
loss: 0.011959  [ 2100/ 3472]
loss: 0.010734  [ 2200/ 3472]
loss: 0.005297  [ 2300/ 3472]
loss: 0.009136  [ 2400/ 3472]
loss: 0.011555  [ 2500/ 3472]
loss: 0.016439  [ 2600/ 3472]
loss: 0.009660  [ 2700/ 3472]
loss: 0.024827  [ 2800/ 3472]
loss: 0.029256  [ 2900/ 3472]
loss: 0.008481  [ 3000/ 3472]
loss: 0.023352  [ 3100/ 3472]
loss: 0.014144  [ 3200/ 3472]
loss: 0.008199  [ 3300/ 3472]
loss: 0.008220  [ 3400/ 3472]
Epoch 3
-------------------------------
loss: 0.010823  [    0/ 3472]
loss: 0.007100  [  100/ 3472]
loss: 0.009716  [  200/ 3472]
loss: 0.018002  [  300/ 3472]
loss: 0.007447  [  400/ 3472]
loss: 0.015677  [  500/ 3472]
loss: 0.011801  [  600/ 3472]
loss: 0.004978  [  700/ 3472]
loss: 0.035605  [  800/ 3472]
loss: 0.028050  [  900/ 3472]
loss: 0.010249  [ 1000/ 3472]
loss: 0.025843  [ 1100/ 3472]
loss: 0.024044  [ 1200/ 3472]
loss: 0.007119  [ 1300/ 3472]
loss: 0.003101  [ 1400/ 3472]
loss: 0.035558  [ 1500/ 3472]
loss: 0.019755  [ 1600/ 3472]
loss: 0.011239  [ 1700/ 3472]
loss: 0.027166  [ 1800/ 3472]
loss: 0.136031  [ 1900/ 3472]
loss: 0.047463  [ 2000/ 3472]
loss: 0.013075  [ 2100/ 3472]
loss: 0.010850  [ 2200/ 3472]
loss: 0.005133  [ 2300/ 3472]
loss: 0.007951  [ 2400/ 3472]
loss: 0.012385  [ 2500/ 3472]
loss: 0.015674  [ 2600/ 3472]
loss: 0.010475  [ 2700/ 3472]
loss: 0.025562  [ 2800/ 3472]
loss: 0.029788  [ 2900/ 3472]
loss: 0.008745  [ 3000/ 3472]
loss: 0.023104  [ 3100/ 3472]
loss: 0.013999  [ 3200/ 3472]
loss: 0.008476  [ 3300/ 3472]
loss: 0.008264  [ 3400/ 3472]
Epoch 4
-------------------------------
loss: 0.009496  [    0/ 3472]
loss: 0.006962  [  100/ 3472]
loss: 0.009973  [  200/ 3472]
loss: 0.017935  [  300/ 3472]
loss: 0.006933  [  400/ 3472]
loss: 0.015523  [  500/ 3472]
loss: 0.011903  [  600/ 3472]
loss: 0.005250  [  700/ 3472]
loss: 0.036027  [  800/ 3472]
loss: 0.028442  [  900/ 3472]
loss: 0.009938  [ 1000/ 3472]
loss: 0.026256  [ 1100/ 3472]
loss: 0.024194  [ 1200/ 3472]
loss: 0.006484  [ 1300/ 3472]
loss: 0.003146  [ 1400/ 3472]
loss: 0.031686  [ 1500/ 3472]
loss: 0.019386  [ 1600/ 3472]
loss: 0.010870  [ 1700/ 3472]
loss: 0.028015  [ 1800/ 3472]
loss: 0.134108  [ 1900/ 3472]
loss: 0.048768  [ 2000/ 3472]
loss: 0.013356  [ 2100/ 3472]
loss: 0.011047  [ 2200/ 3472]
loss: 0.005200  [ 2300/ 3472]
loss: 0.007447  [ 2400/ 3472]
loss: 0.012771  [ 2500/ 3472]
loss: 0.015446  [ 2600/ 3472]
loss: 0.010749  [ 2700/ 3472]
loss: 0.025285  [ 2800/ 3472]
loss: 0.030343  [ 2900/ 3472]
loss: 0.008884  [ 3000/ 3472]
loss: 0.023634  [ 3100/ 3472]
loss: 0.013738  [ 3200/ 3472]
loss: 0.008785  [ 3300/ 3472]
loss: 0.008045  [ 3400/ 3472]
Epoch 5
-------------------------------
loss: 0.008749  [    0/ 3472]
loss: 0.006816  [  100/ 3472]
loss: 0.010062  [  200/ 3472]
loss: 0.017994  [  300/ 3472]
loss: 0.006515  [  400/ 3472]
loss: 0.015381  [  500/ 3472]
loss: 0.011953  [  600/ 3472]
loss: 0.005226  [  700/ 3472]
loss: 0.036197  [  800/ 3472]
loss: 0.028743  [  900/ 3472]
loss: 0.009847  [ 1000/ 3472]
loss: 0.026396  [ 1100/ 3472]
loss: 0.024214  [ 1200/ 3472]
loss: 0.006263  [ 1300/ 3472]
loss: 0.003099  [ 1400/ 3472]
loss: 0.028205  [ 1500/ 3472]
loss: 0.019133  [ 1600/ 3472]
loss: 0.010607  [ 1700/ 3472]
loss: 0.027924  [ 1800/ 3472]
loss: 0.133668  [ 1900/ 3472]
loss: 0.048950  [ 2000/ 3472]
loss: 0.013345  [ 2100/ 3472]
loss: 0.011170  [ 2200/ 3472]
loss: 0.005204  [ 2300/ 3472]
loss: 0.007248  [ 2400/ 3472]
loss: 0.012937  [ 2500/ 3472]
loss: 0.015290  [ 2600/ 3472]
loss: 0.010800  [ 2700/ 3472]
loss: 0.024997  [ 2800/ 3472]
loss: 0.030702  [ 2900/ 3472]
loss: 0.008915  [ 3000/ 3472]
loss: 0.024397  [ 3100/ 3472]
loss: 0.013709  [ 3200/ 3472]
loss: 0.009046  [ 3300/ 3472]
loss: 0.007836  [ 3400/ 3472]
Epoch 6
-------------------------------
loss: 0.008327  [    0/ 3472]
loss: 0.006834  [  100/ 3472]
loss: 0.010118  [  200/ 3472]
loss: 0.018019  [  300/ 3472]
loss: 0.006080  [  400/ 3472]
loss: 0.015325  [  500/ 3472]
loss: 0.012007  [  600/ 3472]
loss: 0.005122  [  700/ 3472]
loss: 0.036353  [  800/ 3472]
loss: 0.028794  [  900/ 3472]
loss: 0.009818  [ 1000/ 3472]
loss: 0.026485  [ 1100/ 3472]
loss: 0.024203  [ 1200/ 3472]
loss: 0.006162  [ 1300/ 3472]
loss: 0.003085  [ 1400/ 3472]
loss: 0.024683  [ 1500/ 3472]
loss: 0.018957  [ 1600/ 3472]
loss: 0.010454  [ 1700/ 3472]
loss: 0.027795  [ 1800/ 3472]
loss: 0.133483  [ 1900/ 3472]
loss: 0.048991  [ 2000/ 3472]
loss: 0.013257  [ 2100/ 3472]
loss: 0.011228  [ 2200/ 3472]
loss: 0.005188  [ 2300/ 3472]
loss: 0.007005  [ 2400/ 3472]
loss: 0.013093  [ 2500/ 3472]
loss: 0.014481  [ 2600/ 3472]
loss: 0.010701  [ 2700/ 3472]
loss: 0.024721  [ 2800/ 3472]
loss: 0.031121  [ 2900/ 3472]
loss: 0.008954  [ 3000/ 3472]
loss: 0.025370  [ 3100/ 3472]
loss: 0.013591  [ 3200/ 3472]
loss: 0.009043  [ 3300/ 3472]
loss: 0.007725  [ 3400/ 3472]
Epoch 7
-------------------------------
loss: 0.008182  [    0/ 3472]
loss: 0.006770  [  100/ 3472]
loss: 0.010098  [  200/ 3472]
loss: 0.018086  [  300/ 3472]
loss: 0.005778  [  400/ 3472]
loss: 0.015307  [  500/ 3472]
loss: 0.012096  [  600/ 3472]
loss: 0.005011  [  700/ 3472]
loss: 0.036380  [  800/ 3472]
loss: 0.028735  [  900/ 3472]
loss: 0.009856  [ 1000/ 3472]
loss: 0.026509  [ 1100/ 3472]
loss: 0.024223  [ 1200/ 3472]
loss: 0.006077  [ 1300/ 3472]
loss: 0.003021  [ 1400/ 3472]
loss: 0.022018  [ 1500/ 3472]
loss: 0.018731  [ 1600/ 3472]
loss: 0.010357  [ 1700/ 3472]
loss: 0.027675  [ 1800/ 3472]
loss: 0.133185  [ 1900/ 3472]
loss: 0.048947  [ 2000/ 3472]
loss: 0.013094  [ 2100/ 3472]
loss: 0.011228  [ 2200/ 3472]
loss: 0.005149  [ 2300/ 3472]
loss: 0.007087  [ 2400/ 3472]
loss: 0.013013  [ 2500/ 3472]
loss: 0.013503  [ 2600/ 3472]
loss: 0.010641  [ 2700/ 3472]
loss: 0.024725  [ 2800/ 3472]
loss: 0.031234  [ 2900/ 3472]
loss: 0.009628  [ 3000/ 3472]
loss: 0.021870  [ 3100/ 3472]
loss: 0.013501  [ 3200/ 3472]
loss: 0.008933  [ 3300/ 3472]
loss: 0.007482  [ 3400/ 3472]
Epoch 8
-------------------------------
loss: 0.008009  [    0/ 3472]
loss: 0.006899  [  100/ 3472]
loss: 0.010055  [  200/ 3472]
loss: 0.018141  [  300/ 3472]
loss: 0.005699  [  400/ 3472]
loss: 0.015368  [  500/ 3472]
loss: 0.012075  [  600/ 3472]
loss: 0.004921  [  700/ 3472]
loss: 0.036159  [  800/ 3472]
loss: 0.028978  [  900/ 3472]
loss: 0.009868  [ 1000/ 3472]
loss: 0.026510  [ 1100/ 3472]
loss: 0.024204  [ 1200/ 3472]
loss: 0.006116  [ 1300/ 3472]
loss: 0.003067  [ 1400/ 3472]
loss: 0.019902  [ 1500/ 3472]
loss: 0.018566  [ 1600/ 3472]
loss: 0.010288  [ 1700/ 3472]
loss: 0.027331  [ 1800/ 3472]
loss: 0.133172  [ 1900/ 3472]
loss: 0.048702  [ 2000/ 3472]
loss: 0.012984  [ 2100/ 3472]
loss: 0.011245  [ 2200/ 3472]
loss: 0.005109  [ 2300/ 3472]
loss: 0.006992  [ 2400/ 3472]
loss: 0.013010  [ 2500/ 3472]
loss: 0.012928  [ 2600/ 3472]
loss: 0.010545  [ 2700/ 3472]
loss: 0.024547  [ 2800/ 3472]
loss: 0.031267  [ 2900/ 3472]
loss: 0.009610  [ 3000/ 3472]
loss: 0.019766  [ 3100/ 3472]
loss: 0.013605  [ 3200/ 3472]
loss: 0.008875  [ 3300/ 3472]
loss: 0.007419  [ 3400/ 3472]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3472
First Spike after testing: [-0.0638018  -0.21629184]
[0 0 0 ... 2 1 1]
[2 2 2 ... 0 2 2]
Cluster 0 Occurrences: 1159; KMEANS: 1271
Cluster 1 Occurrences: 1172; KMEANS: 1053
Cluster 2 Occurrences: 1141; KMEANS: 1148
Centroids: [[-0.41133082, -0.08382362], [-0.59129995, 0.3568273], [-1.1957457, -0.073118106]]
Centroids: [[-1.2605133, -0.07854628], [-0.4691688, 0.50329906], [-0.38147607, -0.16769975]]
Contingency Matrix: 
[[123 232 804]
 [220 725 227]
 [928  96 117]]
[[-1, 232, 804], [-1, 725, 227], [-1, -1, -1]]
[[-1, -1, -1], [-1, 725, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 0: 2, 1: 1}
New Contingency Matrix: 
[[804 232 123]
 [227 725 220]
 [117  96 928]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [804, 725, 928], Sum: 2457
All_Elements: [804, 232, 123, 227, 725, 220, 117, 96, 928], Sum: 3472
Accuracy: 0.7076612903225806
Done!
