Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E732042518>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x000001E7328FBBE0>
Epoch 1
-------------------------------
loss: 0.412524  [    0/ 3475]
loss: 0.142039  [  100/ 3475]
loss: 0.248091  [  200/ 3475]
loss: 0.229645  [  300/ 3475]
loss: 0.148005  [  400/ 3475]
loss: 0.033950  [  500/ 3475]
loss: 0.069310  [  600/ 3475]
loss: 0.161065  [  700/ 3475]
loss: 0.120060  [  800/ 3475]
loss: 0.064086  [  900/ 3475]
loss: 0.043636  [ 1000/ 3475]
loss: 0.094293  [ 1100/ 3475]
loss: 0.028062  [ 1200/ 3475]
loss: 0.023159  [ 1300/ 3475]
loss: 0.063838  [ 1400/ 3475]
loss: 0.049714  [ 1500/ 3475]
loss: 0.029070  [ 1600/ 3475]
loss: 0.030750  [ 1700/ 3475]
loss: 0.085114  [ 1800/ 3475]
loss: 0.030150  [ 1900/ 3475]
loss: 0.020439  [ 2000/ 3475]
loss: 0.040069  [ 2100/ 3475]
loss: 0.106026  [ 2200/ 3475]
loss: 0.062181  [ 2300/ 3475]
loss: 0.016984  [ 2400/ 3475]
loss: 0.078351  [ 2500/ 3475]
loss: 0.032963  [ 2600/ 3475]
loss: 0.069019  [ 2700/ 3475]
loss: 0.093494  [ 2800/ 3475]
loss: 0.064569  [ 2900/ 3475]
loss: 0.055162  [ 3000/ 3475]
loss: 0.115461  [ 3100/ 3475]
loss: 0.106674  [ 3200/ 3475]
loss: 0.066507  [ 3300/ 3475]
loss: 0.064146  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.097674  [    0/ 3475]
loss: 0.067114  [  100/ 3475]
loss: 0.054002  [  200/ 3475]
loss: 0.219656  [  300/ 3475]
loss: 0.071587  [  400/ 3475]
loss: 0.031760  [  500/ 3475]
loss: 0.024177  [  600/ 3475]
loss: 0.063340  [  700/ 3475]
loss: 0.117714  [  800/ 3475]
loss: 0.059011  [  900/ 3475]
loss: 0.049040  [ 1000/ 3475]
loss: 0.040897  [ 1100/ 3475]
loss: 0.024951  [ 1200/ 3475]
loss: 0.031398  [ 1300/ 3475]
loss: 0.049500  [ 1400/ 3475]
loss: 0.052052  [ 1500/ 3475]
loss: 0.040289  [ 1600/ 3475]
loss: 0.020440  [ 1700/ 3475]
loss: 0.065155  [ 1800/ 3475]
loss: 0.034388  [ 1900/ 3475]
loss: 0.019396  [ 2000/ 3475]
loss: 0.039755  [ 2100/ 3475]
loss: 0.087610  [ 2200/ 3475]
loss: 0.031478  [ 2300/ 3475]
loss: 0.016920  [ 2400/ 3475]
loss: 0.086237  [ 2500/ 3475]
loss: 0.031912  [ 2600/ 3475]
loss: 0.066361  [ 2700/ 3475]
loss: 0.099702  [ 2800/ 3475]
loss: 0.032012  [ 2900/ 3475]
loss: 0.050731  [ 3000/ 3475]
loss: 0.100712  [ 3100/ 3475]
loss: 0.057443  [ 3200/ 3475]
loss: 0.050374  [ 3300/ 3475]
loss: 0.035848  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.057325  [    0/ 3475]
loss: 0.045553  [  100/ 3475]
loss: 0.053460  [  200/ 3475]
loss: 0.221316  [  300/ 3475]
loss: 0.035467  [  400/ 3475]
loss: 0.027114  [  500/ 3475]
loss: 0.019027  [  600/ 3475]
loss: 0.023146  [  700/ 3475]
loss: 0.105818  [  800/ 3475]
loss: 0.059886  [  900/ 3475]
loss: 0.043687  [ 1000/ 3475]
loss: 0.028497  [ 1100/ 3475]
loss: 0.028140  [ 1200/ 3475]
loss: 0.034674  [ 1300/ 3475]
loss: 0.053791  [ 1400/ 3475]
loss: 0.041103  [ 1500/ 3475]
loss: 0.038391  [ 1600/ 3475]
loss: 0.018311  [ 1700/ 3475]
loss: 0.070455  [ 1800/ 3475]
loss: 0.038081  [ 1900/ 3475]
loss: 0.020602  [ 2000/ 3475]
loss: 0.035445  [ 2100/ 3475]
loss: 0.081826  [ 2200/ 3475]
loss: 0.024594  [ 2300/ 3475]
loss: 0.017672  [ 2400/ 3475]
loss: 0.094246  [ 2500/ 3475]
loss: 0.033151  [ 2600/ 3475]
loss: 0.064055  [ 2700/ 3475]
loss: 0.085900  [ 2800/ 3475]
loss: 0.025347  [ 2900/ 3475]
loss: 0.050195  [ 3000/ 3475]
loss: 0.085348  [ 3100/ 3475]
loss: 0.044905  [ 3200/ 3475]
loss: 0.037287  [ 3300/ 3475]
loss: 0.030188  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.049780  [    0/ 3475]
loss: 0.037593  [  100/ 3475]
loss: 0.051520  [  200/ 3475]
loss: 0.219057  [  300/ 3475]
loss: 0.026165  [  400/ 3475]
loss: 0.025451  [  500/ 3475]
loss: 0.019465  [  600/ 3475]
loss: 0.014626  [  700/ 3475]
loss: 0.104427  [  800/ 3475]
loss: 0.060234  [  900/ 3475]
loss: 0.041458  [ 1000/ 3475]
loss: 0.024993  [ 1100/ 3475]
loss: 0.028903  [ 1200/ 3475]
loss: 0.036683  [ 1300/ 3475]
loss: 0.055939  [ 1400/ 3475]
loss: 0.039511  [ 1500/ 3475]
loss: 0.035611  [ 1600/ 3475]
loss: 0.017731  [ 1700/ 3475]
loss: 0.080827  [ 1800/ 3475]
loss: 0.037579  [ 1900/ 3475]
loss: 0.020336  [ 2000/ 3475]
loss: 0.031643  [ 2100/ 3475]
loss: 0.079567  [ 2200/ 3475]
loss: 0.025188  [ 2300/ 3475]
loss: 0.017303  [ 2400/ 3475]
loss: 0.099082  [ 2500/ 3475]
loss: 0.031661  [ 2600/ 3475]
loss: 0.065793  [ 2700/ 3475]
loss: 0.080772  [ 2800/ 3475]
loss: 0.025489  [ 2900/ 3475]
loss: 0.050302  [ 3000/ 3475]
loss: 0.083963  [ 3100/ 3475]
loss: 0.046604  [ 3200/ 3475]
loss: 0.031364  [ 3300/ 3475]
loss: 0.030013  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.051833  [    0/ 3475]
loss: 0.033148  [  100/ 3475]
loss: 0.051038  [  200/ 3475]
loss: 0.220557  [  300/ 3475]
loss: 0.021290  [  400/ 3475]
loss: 0.024651  [  500/ 3475]
loss: 0.019111  [  600/ 3475]
loss: 0.013119  [  700/ 3475]
loss: 0.104261  [  800/ 3475]
loss: 0.053648  [  900/ 3475]
loss: 0.039843  [ 1000/ 3475]
loss: 0.023568  [ 1100/ 3475]
loss: 0.025303  [ 1200/ 3475]
loss: 0.039962  [ 1300/ 3475]
loss: 0.047014  [ 1400/ 3475]
loss: 0.039787  [ 1500/ 3475]
loss: 0.032121  [ 1600/ 3475]
loss: 0.017226  [ 1700/ 3475]
loss: 0.085895  [ 1800/ 3475]
loss: 0.035853  [ 1900/ 3475]
loss: 0.020331  [ 2000/ 3475]
loss: 0.027345  [ 2100/ 3475]
loss: 0.079992  [ 2200/ 3475]
loss: 0.025027  [ 2300/ 3475]
loss: 0.016383  [ 2400/ 3475]
loss: 0.099276  [ 2500/ 3475]
loss: 0.030521  [ 2600/ 3475]
loss: 0.075040  [ 2700/ 3475]
loss: 0.070219  [ 2800/ 3475]
loss: 0.032008  [ 2900/ 3475]
loss: 0.050344  [ 3000/ 3475]
loss: 0.076585  [ 3100/ 3475]
loss: 0.050024  [ 3200/ 3475]
loss: 0.028692  [ 3300/ 3475]
loss: 0.030367  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.051851  [    0/ 3475]
loss: 0.026878  [  100/ 3475]
loss: 0.050534  [  200/ 3475]
loss: 0.220439  [  300/ 3475]
loss: 0.022511  [  400/ 3475]
loss: 0.023234  [  500/ 3475]
loss: 0.016341  [  600/ 3475]
loss: 0.021266  [  700/ 3475]
loss: 0.104180  [  800/ 3475]
loss: 0.047689  [  900/ 3475]
loss: 0.037533  [ 1000/ 3475]
loss: 0.025426  [ 1100/ 3475]
loss: 0.023896  [ 1200/ 3475]
loss: 0.041978  [ 1300/ 3475]
loss: 0.036947  [ 1400/ 3475]
loss: 0.040830  [ 1500/ 3475]
loss: 0.031576  [ 1600/ 3475]
loss: 0.016853  [ 1700/ 3475]
loss: 0.082771  [ 1800/ 3475]
loss: 0.034748  [ 1900/ 3475]
loss: 0.020835  [ 2000/ 3475]
loss: 0.028882  [ 2100/ 3475]
loss: 0.083711  [ 2200/ 3475]
loss: 0.028656  [ 2300/ 3475]
loss: 0.015780  [ 2400/ 3475]
loss: 0.098545  [ 2500/ 3475]
loss: 0.028755  [ 2600/ 3475]
loss: 0.084863  [ 2700/ 3475]
loss: 0.069846  [ 2800/ 3475]
loss: 0.038434  [ 2900/ 3475]
loss: 0.050107  [ 3000/ 3475]
loss: 0.076776  [ 3100/ 3475]
loss: 0.050428  [ 3200/ 3475]
loss: 0.025926  [ 3300/ 3475]
loss: 0.030371  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.050500  [    0/ 3475]
loss: 0.022904  [  100/ 3475]
loss: 0.050837  [  200/ 3475]
loss: 0.218327  [  300/ 3475]
loss: 0.022755  [  400/ 3475]
loss: 0.023248  [  500/ 3475]
loss: 0.013412  [  600/ 3475]
loss: 0.027941  [  700/ 3475]
loss: 0.103734  [  800/ 3475]
loss: 0.045199  [  900/ 3475]
loss: 0.034944  [ 1000/ 3475]
loss: 0.024543  [ 1100/ 3475]
loss: 0.024424  [ 1200/ 3475]
loss: 0.042939  [ 1300/ 3475]
loss: 0.032320  [ 1400/ 3475]
loss: 0.041423  [ 1500/ 3475]
loss: 0.032641  [ 1600/ 3475]
loss: 0.015701  [ 1700/ 3475]
loss: 0.078996  [ 1800/ 3475]
loss: 0.034118  [ 1900/ 3475]
loss: 0.020875  [ 2000/ 3475]
loss: 0.029153  [ 2100/ 3475]
loss: 0.084433  [ 2200/ 3475]
loss: 0.030642  [ 2300/ 3475]
loss: 0.015794  [ 2400/ 3475]
loss: 0.095987  [ 2500/ 3475]
loss: 0.028689  [ 2600/ 3475]
loss: 0.092054  [ 2700/ 3475]
loss: 0.065120  [ 2800/ 3475]
loss: 0.040251  [ 2900/ 3475]
loss: 0.050200  [ 3000/ 3475]
loss: 0.076270  [ 3100/ 3475]
loss: 0.052622  [ 3200/ 3475]
loss: 0.025320  [ 3300/ 3475]
loss: 0.030541  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.051219  [    0/ 3475]
loss: 0.021476  [  100/ 3475]
loss: 0.050737  [  200/ 3475]
loss: 0.214610  [  300/ 3475]
loss: 0.022824  [  400/ 3475]
loss: 0.023453  [  500/ 3475]
loss: 0.012762  [  600/ 3475]
loss: 0.037959  [  700/ 3475]
loss: 0.103663  [  800/ 3475]
loss: 0.044008  [  900/ 3475]
loss: 0.033520  [ 1000/ 3475]
loss: 0.024450  [ 1100/ 3475]
loss: 0.024391  [ 1200/ 3475]
loss: 0.041561  [ 1300/ 3475]
loss: 0.029734  [ 1400/ 3475]
loss: 0.041683  [ 1500/ 3475]
loss: 0.032689  [ 1600/ 3475]
loss: 0.015040  [ 1700/ 3475]
loss: 0.074890  [ 1800/ 3475]
loss: 0.033619  [ 1900/ 3475]
loss: 0.020383  [ 2000/ 3475]
loss: 0.029666  [ 2100/ 3475]
loss: 0.083488  [ 2200/ 3475]
loss: 0.030906  [ 2300/ 3475]
loss: 0.015645  [ 2400/ 3475]
loss: 0.095690  [ 2500/ 3475]
loss: 0.029081  [ 2600/ 3475]
loss: 0.098122  [ 2700/ 3475]
loss: 0.062595  [ 2800/ 3475]
loss: 0.040895  [ 2900/ 3475]
loss: 0.050303  [ 3000/ 3475]
loss: 0.076714  [ 3100/ 3475]
loss: 0.051863  [ 3200/ 3475]
loss: 0.026023  [ 3300/ 3475]
loss: 0.029818  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [ 0.9440721  -0.35109413]
[2 1 2 ... 1 0 1]
[0 1 0 ... 1 2 1]
Cluster 0 Occurrences: 1162; KMEANS: 1203
Cluster 1 Occurrences: 1164; KMEANS: 1152
Cluster 2 Occurrences: 1149; KMEANS: 1120
Centroids: [[-1.030016, -1.3250289], [-0.23195828, 2.46471], [1.6693139, -1.0241475]]
Centroids: [[1.6372675, -1.0276735], [-0.2407713, 2.4899461], [-1.1081256, -1.3211014]]
Contingency Matrix: 
[[  54    0 1108]
 [  10 1147    7]
 [1139    5    5]]
[[54, -1, 1108], [-1, -1, -1], [1139, -1, 5]]
[[-1, -1, 1108], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[1108    0   54]
 [   7 1147   10]
 [   5    5 1139]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1108, 1147, 1139], Sum: 3394
All_Elements: [1108, 0, 54, 7, 1147, 10, 5, 5, 1139], Sum: 3475
Accuracy: 0.9766906474820144
Done!
