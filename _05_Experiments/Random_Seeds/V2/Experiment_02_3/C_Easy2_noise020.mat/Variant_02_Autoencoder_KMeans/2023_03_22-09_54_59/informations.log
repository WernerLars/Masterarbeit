Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_54_59
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E7327D5978>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x000001E733637860>
Epoch 1
-------------------------------
loss: 0.199726  [    0/ 3526]
loss: 0.268422  [  100/ 3526]
loss: 0.173972  [  200/ 3526]
loss: 0.114792  [  300/ 3526]
loss: 0.023648  [  400/ 3526]
loss: 0.061790  [  500/ 3526]
loss: 0.037085  [  600/ 3526]
loss: 0.022616  [  700/ 3526]
loss: 0.065957  [  800/ 3526]
loss: 0.011664  [  900/ 3526]
loss: 0.032421  [ 1000/ 3526]
loss: 0.030524  [ 1100/ 3526]
loss: 0.042895  [ 1200/ 3526]
loss: 0.030990  [ 1300/ 3526]
loss: 0.027482  [ 1400/ 3526]
loss: 0.037905  [ 1500/ 3526]
loss: 0.021415  [ 1600/ 3526]
loss: 0.023302  [ 1700/ 3526]
loss: 0.026272  [ 1800/ 3526]
loss: 0.013612  [ 1900/ 3526]
loss: 0.023111  [ 2000/ 3526]
loss: 0.164485  [ 2100/ 3526]
loss: 0.020499  [ 2200/ 3526]
loss: 0.016360  [ 2300/ 3526]
loss: 0.009563  [ 2400/ 3526]
loss: 0.017889  [ 2500/ 3526]
loss: 0.025676  [ 2600/ 3526]
loss: 0.023534  [ 2700/ 3526]
loss: 0.012869  [ 2800/ 3526]
loss: 0.011566  [ 2900/ 3526]
loss: 0.072077  [ 3000/ 3526]
loss: 0.170221  [ 3100/ 3526]
loss: 0.030899  [ 3200/ 3526]
loss: 0.021204  [ 3300/ 3526]
loss: 0.040546  [ 3400/ 3526]
loss: 0.044557  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.015814  [    0/ 3526]
loss: 0.052551  [  100/ 3526]
loss: 0.053240  [  200/ 3526]
loss: 0.004187  [  300/ 3526]
loss: 0.012488  [  400/ 3526]
loss: 0.049005  [  500/ 3526]
loss: 0.013860  [  600/ 3526]
loss: 0.011769  [  700/ 3526]
loss: 0.028580  [  800/ 3526]
loss: 0.012390  [  900/ 3526]
loss: 0.016402  [ 1000/ 3526]
loss: 0.014250  [ 1100/ 3526]
loss: 0.025986  [ 1200/ 3526]
loss: 0.028355  [ 1300/ 3526]
loss: 0.029817  [ 1400/ 3526]
loss: 0.020374  [ 1500/ 3526]
loss: 0.040748  [ 1600/ 3526]
loss: 0.021015  [ 1700/ 3526]
loss: 0.020802  [ 1800/ 3526]
loss: 0.014688  [ 1900/ 3526]
loss: 0.017105  [ 2000/ 3526]
loss: 0.166800  [ 2100/ 3526]
loss: 0.020276  [ 2200/ 3526]
loss: 0.016263  [ 2300/ 3526]
loss: 0.008428  [ 2400/ 3526]
loss: 0.016484  [ 2500/ 3526]
loss: 0.015698  [ 2600/ 3526]
loss: 0.015292  [ 2700/ 3526]
loss: 0.009771  [ 2800/ 3526]
loss: 0.009795  [ 2900/ 3526]
loss: 0.056264  [ 3000/ 3526]
loss: 0.166714  [ 3100/ 3526]
loss: 0.029064  [ 3200/ 3526]
loss: 0.020779  [ 3300/ 3526]
loss: 0.040353  [ 3400/ 3526]
loss: 0.041862  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.014248  [    0/ 3526]
loss: 0.047854  [  100/ 3526]
loss: 0.051563  [  200/ 3526]
loss: 0.004056  [  300/ 3526]
loss: 0.015278  [  400/ 3526]
loss: 0.040487  [  500/ 3526]
loss: 0.011402  [  600/ 3526]
loss: 0.013481  [  700/ 3526]
loss: 0.031176  [  800/ 3526]
loss: 0.011991  [  900/ 3526]
loss: 0.017297  [ 1000/ 3526]
loss: 0.012864  [ 1100/ 3526]
loss: 0.029281  [ 1200/ 3526]
loss: 0.026528  [ 1300/ 3526]
loss: 0.028394  [ 1400/ 3526]
loss: 0.019706  [ 1500/ 3526]
loss: 0.043832  [ 1600/ 3526]
loss: 0.020558  [ 1700/ 3526]
loss: 0.020736  [ 1800/ 3526]
loss: 0.014318  [ 1900/ 3526]
loss: 0.016296  [ 2000/ 3526]
loss: 0.125999  [ 2100/ 3526]
loss: 0.021435  [ 2200/ 3526]
loss: 0.015875  [ 2300/ 3526]
loss: 0.008302  [ 2400/ 3526]
loss: 0.015707  [ 2500/ 3526]
loss: 0.014291  [ 2600/ 3526]
loss: 0.013986  [ 2700/ 3526]
loss: 0.009414  [ 2800/ 3526]
loss: 0.009722  [ 2900/ 3526]
loss: 0.057553  [ 3000/ 3526]
loss: 0.161842  [ 3100/ 3526]
loss: 0.028228  [ 3200/ 3526]
loss: 0.020020  [ 3300/ 3526]
loss: 0.035752  [ 3400/ 3526]
loss: 0.040035  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.011698  [    0/ 3526]
loss: 0.043725  [  100/ 3526]
loss: 0.048551  [  200/ 3526]
loss: 0.005320  [  300/ 3526]
loss: 0.015947  [  400/ 3526]
loss: 0.037157  [  500/ 3526]
loss: 0.010638  [  600/ 3526]
loss: 0.014298  [  700/ 3526]
loss: 0.032509  [  800/ 3526]
loss: 0.012381  [  900/ 3526]
loss: 0.018593  [ 1000/ 3526]
loss: 0.013511  [ 1100/ 3526]
loss: 0.030151  [ 1200/ 3526]
loss: 0.026026  [ 1300/ 3526]
loss: 0.027582  [ 1400/ 3526]
loss: 0.019006  [ 1500/ 3526]
loss: 0.044946  [ 1600/ 3526]
loss: 0.020685  [ 1700/ 3526]
loss: 0.020611  [ 1800/ 3526]
loss: 0.013055  [ 1900/ 3526]
loss: 0.015756  [ 2000/ 3526]
loss: 0.122503  [ 2100/ 3526]
loss: 0.023373  [ 2200/ 3526]
loss: 0.015907  [ 2300/ 3526]
loss: 0.008358  [ 2400/ 3526]
loss: 0.014735  [ 2500/ 3526]
loss: 0.014388  [ 2600/ 3526]
loss: 0.013205  [ 2700/ 3526]
loss: 0.009194  [ 2800/ 3526]
loss: 0.009839  [ 2900/ 3526]
loss: 0.061610  [ 3000/ 3526]
loss: 0.161015  [ 3100/ 3526]
loss: 0.028754  [ 3200/ 3526]
loss: 0.020021  [ 3300/ 3526]
loss: 0.032286  [ 3400/ 3526]
loss: 0.040076  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.010479  [    0/ 3526]
loss: 0.039773  [  100/ 3526]
loss: 0.044731  [  200/ 3526]
loss: 0.006988  [  300/ 3526]
loss: 0.015836  [  400/ 3526]
loss: 0.036283  [  500/ 3526]
loss: 0.010434  [  600/ 3526]
loss: 0.014101  [  700/ 3526]
loss: 0.031255  [  800/ 3526]
loss: 0.013066  [  900/ 3526]
loss: 0.018571  [ 1000/ 3526]
loss: 0.014734  [ 1100/ 3526]
loss: 0.029912  [ 1200/ 3526]
loss: 0.025884  [ 1300/ 3526]
loss: 0.027702  [ 1400/ 3526]
loss: 0.018829  [ 1500/ 3526]
loss: 0.044850  [ 1600/ 3526]
loss: 0.020566  [ 1700/ 3526]
loss: 0.020807  [ 1800/ 3526]
loss: 0.012164  [ 1900/ 3526]
loss: 0.015668  [ 2000/ 3526]
loss: 0.119460  [ 2100/ 3526]
loss: 0.024576  [ 2200/ 3526]
loss: 0.015964  [ 2300/ 3526]
loss: 0.008372  [ 2400/ 3526]
loss: 0.014339  [ 2500/ 3526]
loss: 0.015146  [ 2600/ 3526]
loss: 0.012954  [ 2700/ 3526]
loss: 0.009150  [ 2800/ 3526]
loss: 0.009875  [ 2900/ 3526]
loss: 0.065053  [ 3000/ 3526]
loss: 0.162212  [ 3100/ 3526]
loss: 0.028966  [ 3200/ 3526]
loss: 0.020685  [ 3300/ 3526]
loss: 0.029741  [ 3400/ 3526]
loss: 0.040866  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.010116  [    0/ 3526]
loss: 0.034985  [  100/ 3526]
loss: 0.040600  [  200/ 3526]
loss: 0.008000  [  300/ 3526]
loss: 0.015678  [  400/ 3526]
loss: 0.036639  [  500/ 3526]
loss: 0.010663  [  600/ 3526]
loss: 0.013816  [  700/ 3526]
loss: 0.030638  [  800/ 3526]
loss: 0.013402  [  900/ 3526]
loss: 0.018654  [ 1000/ 3526]
loss: 0.015851  [ 1100/ 3526]
loss: 0.029511  [ 1200/ 3526]
loss: 0.026012  [ 1300/ 3526]
loss: 0.028083  [ 1400/ 3526]
loss: 0.018813  [ 1500/ 3526]
loss: 0.044747  [ 1600/ 3526]
loss: 0.020394  [ 1700/ 3526]
loss: 0.021007  [ 1800/ 3526]
loss: 0.011519  [ 1900/ 3526]
loss: 0.015652  [ 2000/ 3526]
loss: 0.111508  [ 2100/ 3526]
loss: 0.025509  [ 2200/ 3526]
loss: 0.016154  [ 2300/ 3526]
loss: 0.008439  [ 2400/ 3526]
loss: 0.014159  [ 2500/ 3526]
loss: 0.015531  [ 2600/ 3526]
loss: 0.013323  [ 2700/ 3526]
loss: 0.009094  [ 2800/ 3526]
loss: 0.009833  [ 2900/ 3526]
loss: 0.066461  [ 3000/ 3526]
loss: 0.162695  [ 3100/ 3526]
loss: 0.028910  [ 3200/ 3526]
loss: 0.021392  [ 3300/ 3526]
loss: 0.028276  [ 3400/ 3526]
loss: 0.041534  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.010103  [    0/ 3526]
loss: 0.031398  [  100/ 3526]
loss: 0.037491  [  200/ 3526]
loss: 0.009057  [  300/ 3526]
loss: 0.015491  [  400/ 3526]
loss: 0.037389  [  500/ 3526]
loss: 0.010671  [  600/ 3526]
loss: 0.013498  [  700/ 3526]
loss: 0.030392  [  800/ 3526]
loss: 0.013673  [  900/ 3526]
loss: 0.018154  [ 1000/ 3526]
loss: 0.016459  [ 1100/ 3526]
loss: 0.028846  [ 1200/ 3526]
loss: 0.026137  [ 1300/ 3526]
loss: 0.028264  [ 1400/ 3526]
loss: 0.018824  [ 1500/ 3526]
loss: 0.044483  [ 1600/ 3526]
loss: 0.020156  [ 1700/ 3526]
loss: 0.021364  [ 1800/ 3526]
loss: 0.011114  [ 1900/ 3526]
loss: 0.015671  [ 2000/ 3526]
loss: 0.095760  [ 2100/ 3526]
loss: 0.026041  [ 2200/ 3526]
loss: 0.016292  [ 2300/ 3526]
loss: 0.008475  [ 2400/ 3526]
loss: 0.013966  [ 2500/ 3526]
loss: 0.016066  [ 2600/ 3526]
loss: 0.013886  [ 2700/ 3526]
loss: 0.009022  [ 2800/ 3526]
loss: 0.009858  [ 2900/ 3526]
loss: 0.066395  [ 3000/ 3526]
loss: 0.163647  [ 3100/ 3526]
loss: 0.028714  [ 3200/ 3526]
loss: 0.021968  [ 3300/ 3526]
loss: 0.025934  [ 3400/ 3526]
loss: 0.042002  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.010261  [    0/ 3526]
loss: 0.028248  [  100/ 3526]
loss: 0.035079  [  200/ 3526]
loss: 0.009678  [  300/ 3526]
loss: 0.015268  [  400/ 3526]
loss: 0.038413  [  500/ 3526]
loss: 0.010986  [  600/ 3526]
loss: 0.013176  [  700/ 3526]
loss: 0.030048  [  800/ 3526]
loss: 0.013765  [  900/ 3526]
loss: 0.017820  [ 1000/ 3526]
loss: 0.016998  [ 1100/ 3526]
loss: 0.027977  [ 1200/ 3526]
loss: 0.026269  [ 1300/ 3526]
loss: 0.028656  [ 1400/ 3526]
loss: 0.018753  [ 1500/ 3526]
loss: 0.044553  [ 1600/ 3526]
loss: 0.019783  [ 1700/ 3526]
loss: 0.021412  [ 1800/ 3526]
loss: 0.010846  [ 1900/ 3526]
loss: 0.015865  [ 2000/ 3526]
loss: 0.093905  [ 2100/ 3526]
loss: 0.026468  [ 2200/ 3526]
loss: 0.016444  [ 2300/ 3526]
loss: 0.008472  [ 2400/ 3526]
loss: 0.013877  [ 2500/ 3526]
loss: 0.016268  [ 2600/ 3526]
loss: 0.013706  [ 2700/ 3526]
loss: 0.009230  [ 2800/ 3526]
loss: 0.009873  [ 2900/ 3526]
loss: 0.065827  [ 3000/ 3526]
loss: 0.164782  [ 3100/ 3526]
loss: 0.028128  [ 3200/ 3526]
loss: 0.022527  [ 3300/ 3526]
loss: 0.025481  [ 3400/ 3526]
loss: 0.042064  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [-1.8891131   0.73855233]
[2 0 2 ... 0 2 2]
[2 0 1 ... 0 2 1]
Cluster 0 Occurrences: 1186; KMEANS: 2390
Cluster 1 Occurrences: 1188; KMEANS: 597
Cluster 2 Occurrences: 1152; KMEANS: 539
Centroids: [[1.4225874, -0.13496765], [0.7670162, -0.19507506], [-2.968731, 0.70979035]]
Centroids: [[1.0911858, -0.16227506], [-3.8347287, 0.79059464], [-2.1153536, 0.6339704]]
Contingency Matrix: 
[[1186    0    0]
 [1187    0    1]
 [  17  597  538]]
[[-1, 0, 0], [-1, -1, -1], [-1, 597, 538]]
[[-1, -1, 0], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 2: 1, 0: 2}
New Contingency Matrix: 
[[   0 1186    0]
 [   1 1187    0]
 [ 538   17  597]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [0, 1187, 597], Sum: 1784
All_Elements: [0, 1186, 0, 1, 1187, 0, 538, 17, 597], Sum: 3526
Accuracy: 0.5059557572319909
Done!
