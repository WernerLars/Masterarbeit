Experiment_path: Random_Seeds//V2/Experiment_02_3
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_3/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_06_38
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E7A1AF8470>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x000001E795D00240>
Epoch 1
-------------------------------
loss: 0.352901  [    0/ 3493]
loss: 0.178647  [  100/ 3493]
loss: 0.060304  [  200/ 3493]
loss: 0.070471  [  300/ 3493]
loss: 0.069592  [  400/ 3493]
loss: 0.038112  [  500/ 3493]
loss: 0.060169  [  600/ 3493]
loss: 0.203077  [  700/ 3493]
loss: 0.068091  [  800/ 3493]
loss: 0.020086  [  900/ 3493]
loss: 0.040441  [ 1000/ 3493]
loss: 0.088694  [ 1100/ 3493]
loss: 0.024663  [ 1200/ 3493]
loss: 0.011771  [ 1300/ 3493]
loss: 0.026770  [ 1400/ 3493]
loss: 0.017058  [ 1500/ 3493]
loss: 0.027274  [ 1600/ 3493]
loss: 0.041500  [ 1700/ 3493]
loss: 0.094323  [ 1800/ 3493]
loss: 0.014946  [ 1900/ 3493]
loss: 0.011182  [ 2000/ 3493]
loss: 0.015475  [ 2100/ 3493]
loss: 0.007436  [ 2200/ 3493]
loss: 0.027840  [ 2300/ 3493]
loss: 0.012505  [ 2400/ 3493]
loss: 0.062216  [ 2500/ 3493]
loss: 0.009967  [ 2600/ 3493]
loss: 0.018318  [ 2700/ 3493]
loss: 0.035239  [ 2800/ 3493]
loss: 0.009909  [ 2900/ 3493]
loss: 0.027214  [ 3000/ 3493]
loss: 0.018464  [ 3100/ 3493]
loss: 0.032142  [ 3200/ 3493]
loss: 0.023653  [ 3300/ 3493]
loss: 0.010997  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.070111  [    0/ 3493]
loss: 0.023433  [  100/ 3493]
loss: 0.024390  [  200/ 3493]
loss: 0.020282  [  300/ 3493]
loss: 0.052264  [  400/ 3493]
loss: 0.030531  [  500/ 3493]
loss: 0.049755  [  600/ 3493]
loss: 0.182103  [  700/ 3493]
loss: 0.012369  [  800/ 3493]
loss: 0.015857  [  900/ 3493]
loss: 0.029692  [ 1000/ 3493]
loss: 0.026962  [ 1100/ 3493]
loss: 0.017189  [ 1200/ 3493]
loss: 0.006170  [ 1300/ 3493]
loss: 0.030414  [ 1400/ 3493]
loss: 0.048558  [ 1500/ 3493]
loss: 0.027641  [ 1600/ 3493]
loss: 0.031158  [ 1700/ 3493]
loss: 0.073882  [ 1800/ 3493]
loss: 0.013711  [ 1900/ 3493]
loss: 0.012654  [ 2000/ 3493]
loss: 0.018003  [ 2100/ 3493]
loss: 0.009572  [ 2200/ 3493]
loss: 0.023233  [ 2300/ 3493]
loss: 0.011102  [ 2400/ 3493]
loss: 0.068739  [ 2500/ 3493]
loss: 0.002923  [ 2600/ 3493]
loss: 0.016872  [ 2700/ 3493]
loss: 0.030103  [ 2800/ 3493]
loss: 0.007773  [ 2900/ 3493]
loss: 0.033524  [ 3000/ 3493]
loss: 0.014000  [ 3100/ 3493]
loss: 0.027952  [ 3200/ 3493]
loss: 0.026357  [ 3300/ 3493]
loss: 0.012751  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.054222  [    0/ 3493]
loss: 0.021177  [  100/ 3493]
loss: 0.017141  [  200/ 3493]
loss: 0.017009  [  300/ 3493]
loss: 0.049717  [  400/ 3493]
loss: 0.030944  [  500/ 3493]
loss: 0.051781  [  600/ 3493]
loss: 0.160902  [  700/ 3493]
loss: 0.007622  [  800/ 3493]
loss: 0.015631  [  900/ 3493]
loss: 0.027259  [ 1000/ 3493]
loss: 0.016295  [ 1100/ 3493]
loss: 0.017940  [ 1200/ 3493]
loss: 0.007372  [ 1300/ 3493]
loss: 0.024167  [ 1400/ 3493]
loss: 0.055202  [ 1500/ 3493]
loss: 0.028270  [ 1600/ 3493]
loss: 0.030519  [ 1700/ 3493]
loss: 0.073533  [ 1800/ 3493]
loss: 0.014930  [ 1900/ 3493]
loss: 0.014810  [ 2000/ 3493]
loss: 0.017112  [ 2100/ 3493]
loss: 0.009836  [ 2200/ 3493]
loss: 0.022294  [ 2300/ 3493]
loss: 0.013355  [ 2400/ 3493]
loss: 0.071478  [ 2500/ 3493]
loss: 0.003465  [ 2600/ 3493]
loss: 0.017161  [ 2700/ 3493]
loss: 0.028238  [ 2800/ 3493]
loss: 0.007940  [ 2900/ 3493]
loss: 0.033890  [ 3000/ 3493]
loss: 0.013686  [ 3100/ 3493]
loss: 0.025644  [ 3200/ 3493]
loss: 0.026670  [ 3300/ 3493]
loss: 0.012797  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.046801  [    0/ 3493]
loss: 0.021043  [  100/ 3493]
loss: 0.016195  [  200/ 3493]
loss: 0.015658  [  300/ 3493]
loss: 0.049793  [  400/ 3493]
loss: 0.031460  [  500/ 3493]
loss: 0.052142  [  600/ 3493]
loss: 0.156344  [  700/ 3493]
loss: 0.007542  [  800/ 3493]
loss: 0.014703  [  900/ 3493]
loss: 0.026354  [ 1000/ 3493]
loss: 0.013947  [ 1100/ 3493]
loss: 0.018095  [ 1200/ 3493]
loss: 0.008505  [ 1300/ 3493]
loss: 0.023592  [ 1400/ 3493]
loss: 0.055333  [ 1500/ 3493]
loss: 0.028344  [ 1600/ 3493]
loss: 0.030316  [ 1700/ 3493]
loss: 0.071071  [ 1800/ 3493]
loss: 0.015063  [ 1900/ 3493]
loss: 0.015825  [ 2000/ 3493]
loss: 0.015693  [ 2100/ 3493]
loss: 0.010590  [ 2200/ 3493]
loss: 0.021598  [ 2300/ 3493]
loss: 0.014079  [ 2400/ 3493]
loss: 0.072319  [ 2500/ 3493]
loss: 0.004333  [ 2600/ 3493]
loss: 0.017059  [ 2700/ 3493]
loss: 0.027738  [ 2800/ 3493]
loss: 0.007973  [ 2900/ 3493]
loss: 0.032540  [ 3000/ 3493]
loss: 0.013864  [ 3100/ 3493]
loss: 0.019822  [ 3200/ 3493]
loss: 0.026815  [ 3300/ 3493]
loss: 0.012691  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.042367  [    0/ 3493]
loss: 0.021553  [  100/ 3493]
loss: 0.015172  [  200/ 3493]
loss: 0.015162  [  300/ 3493]
loss: 0.048223  [  400/ 3493]
loss: 0.032938  [  500/ 3493]
loss: 0.052373  [  600/ 3493]
loss: 0.152686  [  700/ 3493]
loss: 0.007590  [  800/ 3493]
loss: 0.013250  [  900/ 3493]
loss: 0.025360  [ 1000/ 3493]
loss: 0.013647  [ 1100/ 3493]
loss: 0.017958  [ 1200/ 3493]
loss: 0.007967  [ 1300/ 3493]
loss: 0.024836  [ 1400/ 3493]
loss: 0.055222  [ 1500/ 3493]
loss: 0.027925  [ 1600/ 3493]
loss: 0.029664  [ 1700/ 3493]
loss: 0.070856  [ 1800/ 3493]
loss: 0.014940  [ 1900/ 3493]
loss: 0.016256  [ 2000/ 3493]
loss: 0.015063  [ 2100/ 3493]
loss: 0.010806  [ 2200/ 3493]
loss: 0.021358  [ 2300/ 3493]
loss: 0.014464  [ 2400/ 3493]
loss: 0.069464  [ 2500/ 3493]
loss: 0.005062  [ 2600/ 3493]
loss: 0.017486  [ 2700/ 3493]
loss: 0.027853  [ 2800/ 3493]
loss: 0.007905  [ 2900/ 3493]
loss: 0.031070  [ 3000/ 3493]
loss: 0.014014  [ 3100/ 3493]
loss: 0.017364  [ 3200/ 3493]
loss: 0.026895  [ 3300/ 3493]
loss: 0.012597  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.040004  [    0/ 3493]
loss: 0.021973  [  100/ 3493]
loss: 0.015030  [  200/ 3493]
loss: 0.014665  [  300/ 3493]
loss: 0.048181  [  400/ 3493]
loss: 0.034717  [  500/ 3493]
loss: 0.052862  [  600/ 3493]
loss: 0.149061  [  700/ 3493]
loss: 0.007639  [  800/ 3493]
loss: 0.012261  [  900/ 3493]
loss: 0.024788  [ 1000/ 3493]
loss: 0.015445  [ 1100/ 3493]
loss: 0.017801  [ 1200/ 3493]
loss: 0.008026  [ 1300/ 3493]
loss: 0.025907  [ 1400/ 3493]
loss: 0.054712  [ 1500/ 3493]
loss: 0.027364  [ 1600/ 3493]
loss: 0.029328  [ 1700/ 3493]
loss: 0.066798  [ 1800/ 3493]
loss: 0.015003  [ 1900/ 3493]
loss: 0.016498  [ 2000/ 3493]
loss: 0.014023  [ 2100/ 3493]
loss: 0.010549  [ 2200/ 3493]
loss: 0.021105  [ 2300/ 3493]
loss: 0.014729  [ 2400/ 3493]
loss: 0.070052  [ 2500/ 3493]
loss: 0.005369  [ 2600/ 3493]
loss: 0.017335  [ 2700/ 3493]
loss: 0.026911  [ 2800/ 3493]
loss: 0.007899  [ 2900/ 3493]
loss: 0.030874  [ 3000/ 3493]
loss: 0.014008  [ 3100/ 3493]
loss: 0.014713  [ 3200/ 3493]
loss: 0.026667  [ 3300/ 3493]
loss: 0.012284  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.039343  [    0/ 3493]
loss: 0.021889  [  100/ 3493]
loss: 0.015388  [  200/ 3493]
loss: 0.014613  [  300/ 3493]
loss: 0.048642  [  400/ 3493]
loss: 0.035519  [  500/ 3493]
loss: 0.053434  [  600/ 3493]
loss: 0.148246  [  700/ 3493]
loss: 0.007814  [  800/ 3493]
loss: 0.011731  [  900/ 3493]
loss: 0.024786  [ 1000/ 3493]
loss: 0.017043  [ 1100/ 3493]
loss: 0.017676  [ 1200/ 3493]
loss: 0.008122  [ 1300/ 3493]
loss: 0.027348  [ 1400/ 3493]
loss: 0.054862  [ 1500/ 3493]
loss: 0.027214  [ 1600/ 3493]
loss: 0.029003  [ 1700/ 3493]
loss: 0.066370  [ 1800/ 3493]
loss: 0.014888  [ 1900/ 3493]
loss: 0.016561  [ 2000/ 3493]
loss: 0.013088  [ 2100/ 3493]
loss: 0.010451  [ 2200/ 3493]
loss: 0.021074  [ 2300/ 3493]
loss: 0.014931  [ 2400/ 3493]
loss: 0.070211  [ 2500/ 3493]
loss: 0.005555  [ 2600/ 3493]
loss: 0.017163  [ 2700/ 3493]
loss: 0.026694  [ 2800/ 3493]
loss: 0.007809  [ 2900/ 3493]
loss: 0.030774  [ 3000/ 3493]
loss: 0.014112  [ 3100/ 3493]
loss: 0.013436  [ 3200/ 3493]
loss: 0.026427  [ 3300/ 3493]
loss: 0.012142  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.038506  [    0/ 3493]
loss: 0.021752  [  100/ 3493]
loss: 0.015070  [  200/ 3493]
loss: 0.014558  [  300/ 3493]
loss: 0.049208  [  400/ 3493]
loss: 0.036185  [  500/ 3493]
loss: 0.054500  [  600/ 3493]
loss: 0.149351  [  700/ 3493]
loss: 0.007827  [  800/ 3493]
loss: 0.011274  [  900/ 3493]
loss: 0.024717  [ 1000/ 3493]
loss: 0.018868  [ 1100/ 3493]
loss: 0.017600  [ 1200/ 3493]
loss: 0.007954  [ 1300/ 3493]
loss: 0.028259  [ 1400/ 3493]
loss: 0.054724  [ 1500/ 3493]
loss: 0.027179  [ 1600/ 3493]
loss: 0.028558  [ 1700/ 3493]
loss: 0.063520  [ 1800/ 3493]
loss: 0.014906  [ 1900/ 3493]
loss: 0.016727  [ 2000/ 3493]
loss: 0.012223  [ 2100/ 3493]
loss: 0.010522  [ 2200/ 3493]
loss: 0.021171  [ 2300/ 3493]
loss: 0.014964  [ 2400/ 3493]
loss: 0.071044  [ 2500/ 3493]
loss: 0.005474  [ 2600/ 3493]
loss: 0.017519  [ 2700/ 3493]
loss: 0.026827  [ 2800/ 3493]
loss: 0.007899  [ 2900/ 3493]
loss: 0.031010  [ 3000/ 3493]
loss: 0.014019  [ 3100/ 3493]
loss: 0.012861  [ 3200/ 3493]
loss: 0.026217  [ 3300/ 3493]
loss: 0.012234  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [1.5479589  0.20995176]
[1 0 0 ... 1 0 2]
[1 2 2 ... 1 2 0]
Cluster 0 Occurrences: 1151; KMEANS: 1283
Cluster 1 Occurrences: 1195; KMEANS: 1180
Cluster 2 Occurrences: 1147; KMEANS: 1030
Centroids: [[-0.83523804, 0.35506546], [1.8154458, -0.53811663], [-0.19164358, 0.3539963]]
Centroids: [[-0.17061922, 0.31889105], [1.8426431, -0.5486789], [-0.9389622, 0.39802775]]
Contingency Matrix: 
[[ 247    1  903]
 [  15 1176    4]
 [1021    3  123]]
[[247, -1, 903], [-1, -1, -1], [1021, -1, 123]]
[[-1, -1, 903], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[ 903    1  247]
 [   4 1176   15]
 [ 123    3 1021]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [903, 1176, 1021], Sum: 3100
All_Elements: [903, 1, 247, 4, 1176, 15, 123, 3, 1021], Sum: 3493
Accuracy: 0.8874892642427713
Done!
