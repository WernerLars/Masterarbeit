Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_07_04
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B952CD7F0>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x0000019B89ADF3C8>
Epoch 1
-------------------------------
loss: 0.352398  [    0/ 3493]
loss: 0.147586  [  100/ 3493]
loss: 0.040753  [  200/ 3493]
loss: 0.041255  [  300/ 3493]
loss: 0.073444  [  400/ 3493]
loss: 0.035142  [  500/ 3493]
loss: 0.063512  [  600/ 3493]
loss: 0.166668  [  700/ 3493]
loss: 0.061307  [  800/ 3493]
loss: 0.018580  [  900/ 3493]
loss: 0.042314  [ 1000/ 3493]
loss: 0.048849  [ 1100/ 3493]
loss: 0.021738  [ 1200/ 3493]
loss: 0.014866  [ 1300/ 3493]
loss: 0.021167  [ 1400/ 3493]
loss: 0.025119  [ 1500/ 3493]
loss: 0.028609  [ 1600/ 3493]
loss: 0.045245  [ 1700/ 3493]
loss: 0.062525  [ 1800/ 3493]
loss: 0.013406  [ 1900/ 3493]
loss: 0.013656  [ 2000/ 3493]
loss: 0.015156  [ 2100/ 3493]
loss: 0.013285  [ 2200/ 3493]
loss: 0.031699  [ 2300/ 3493]
loss: 0.015765  [ 2400/ 3493]
loss: 0.086116  [ 2500/ 3493]
loss: 0.005538  [ 2600/ 3493]
loss: 0.022239  [ 2700/ 3493]
loss: 0.038618  [ 2800/ 3493]
loss: 0.009711  [ 2900/ 3493]
loss: 0.033667  [ 3000/ 3493]
loss: 0.014885  [ 3100/ 3493]
loss: 0.024080  [ 3200/ 3493]
loss: 0.017588  [ 3300/ 3493]
loss: 0.008982  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.069209  [    0/ 3493]
loss: 0.018027  [  100/ 3493]
loss: 0.042060  [  200/ 3493]
loss: 0.018474  [  300/ 3493]
loss: 0.059111  [  400/ 3493]
loss: 0.014562  [  500/ 3493]
loss: 0.054172  [  600/ 3493]
loss: 0.215362  [  700/ 3493]
loss: 0.021480  [  800/ 3493]
loss: 0.013755  [  900/ 3493]
loss: 0.052342  [ 1000/ 3493]
loss: 0.026477  [ 1100/ 3493]
loss: 0.019253  [ 1200/ 3493]
loss: 0.017048  [ 1300/ 3493]
loss: 0.027369  [ 1400/ 3493]
loss: 0.044225  [ 1500/ 3493]
loss: 0.030642  [ 1600/ 3493]
loss: 0.017342  [ 1700/ 3493]
loss: 0.047246  [ 1800/ 3493]
loss: 0.012244  [ 1900/ 3493]
loss: 0.007888  [ 2000/ 3493]
loss: 0.014627  [ 2100/ 3493]
loss: 0.015140  [ 2200/ 3493]
loss: 0.028633  [ 2300/ 3493]
loss: 0.014646  [ 2400/ 3493]
loss: 0.092335  [ 2500/ 3493]
loss: 0.004228  [ 2600/ 3493]
loss: 0.010908  [ 2700/ 3493]
loss: 0.020581  [ 2800/ 3493]
loss: 0.007956  [ 2900/ 3493]
loss: 0.038861  [ 3000/ 3493]
loss: 0.013462  [ 3100/ 3493]
loss: 0.020758  [ 3200/ 3493]
loss: 0.017018  [ 3300/ 3493]
loss: 0.010012  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.055847  [    0/ 3493]
loss: 0.015364  [  100/ 3493]
loss: 0.032467  [  200/ 3493]
loss: 0.017303  [  300/ 3493]
loss: 0.057962  [  400/ 3493]
loss: 0.016342  [  500/ 3493]
loss: 0.049690  [  600/ 3493]
loss: 0.192208  [  700/ 3493]
loss: 0.016417  [  800/ 3493]
loss: 0.012850  [  900/ 3493]
loss: 0.041763  [ 1000/ 3493]
loss: 0.024854  [ 1100/ 3493]
loss: 0.019229  [ 1200/ 3493]
loss: 0.014864  [ 1300/ 3493]
loss: 0.028186  [ 1400/ 3493]
loss: 0.048088  [ 1500/ 3493]
loss: 0.030726  [ 1600/ 3493]
loss: 0.015380  [ 1700/ 3493]
loss: 0.049249  [ 1800/ 3493]
loss: 0.012530  [ 1900/ 3493]
loss: 0.009291  [ 2000/ 3493]
loss: 0.016245  [ 2100/ 3493]
loss: 0.015029  [ 2200/ 3493]
loss: 0.025687  [ 2300/ 3493]
loss: 0.015199  [ 2400/ 3493]
loss: 0.090484  [ 2500/ 3493]
loss: 0.004908  [ 2600/ 3493]
loss: 0.010478  [ 2700/ 3493]
loss: 0.017461  [ 2800/ 3493]
loss: 0.007443  [ 2900/ 3493]
loss: 0.037905  [ 3000/ 3493]
loss: 0.013528  [ 3100/ 3493]
loss: 0.018765  [ 3200/ 3493]
loss: 0.018665  [ 3300/ 3493]
loss: 0.010834  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.052499  [    0/ 3493]
loss: 0.017451  [  100/ 3493]
loss: 0.027083  [  200/ 3493]
loss: 0.017218  [  300/ 3493]
loss: 0.057429  [  400/ 3493]
loss: 0.015721  [  500/ 3493]
loss: 0.050220  [  600/ 3493]
loss: 0.175678  [  700/ 3493]
loss: 0.014083  [  800/ 3493]
loss: 0.012355  [  900/ 3493]
loss: 0.036869  [ 1000/ 3493]
loss: 0.025827  [ 1100/ 3493]
loss: 0.019009  [ 1200/ 3493]
loss: 0.013729  [ 1300/ 3493]
loss: 0.027181  [ 1400/ 3493]
loss: 0.050180  [ 1500/ 3493]
loss: 0.030192  [ 1600/ 3493]
loss: 0.016467  [ 1700/ 3493]
loss: 0.048775  [ 1800/ 3493]
loss: 0.013114  [ 1900/ 3493]
loss: 0.010591  [ 2000/ 3493]
loss: 0.016295  [ 2100/ 3493]
loss: 0.014404  [ 2200/ 3493]
loss: 0.023916  [ 2300/ 3493]
loss: 0.015101  [ 2400/ 3493]
loss: 0.087684  [ 2500/ 3493]
loss: 0.006343  [ 2600/ 3493]
loss: 0.011947  [ 2700/ 3493]
loss: 0.017737  [ 2800/ 3493]
loss: 0.007457  [ 2900/ 3493]
loss: 0.035822  [ 3000/ 3493]
loss: 0.014008  [ 3100/ 3493]
loss: 0.017406  [ 3200/ 3493]
loss: 0.020041  [ 3300/ 3493]
loss: 0.011319  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.053079  [    0/ 3493]
loss: 0.018902  [  100/ 3493]
loss: 0.023801  [  200/ 3493]
loss: 0.017470  [  300/ 3493]
loss: 0.056998  [  400/ 3493]
loss: 0.014475  [  500/ 3493]
loss: 0.050421  [  600/ 3493]
loss: 0.165129  [  700/ 3493]
loss: 0.012705  [  800/ 3493]
loss: 0.012255  [  900/ 3493]
loss: 0.033867  [ 1000/ 3493]
loss: 0.027368  [ 1100/ 3493]
loss: 0.019961  [ 1200/ 3493]
loss: 0.013380  [ 1300/ 3493]
loss: 0.026461  [ 1400/ 3493]
loss: 0.050249  [ 1500/ 3493]
loss: 0.029042  [ 1600/ 3493]
loss: 0.018207  [ 1700/ 3493]
loss: 0.047323  [ 1800/ 3493]
loss: 0.013490  [ 1900/ 3493]
loss: 0.011100  [ 2000/ 3493]
loss: 0.015768  [ 2100/ 3493]
loss: 0.014176  [ 2200/ 3493]
loss: 0.023286  [ 2300/ 3493]
loss: 0.015390  [ 2400/ 3493]
loss: 0.084868  [ 2500/ 3493]
loss: 0.006999  [ 2600/ 3493]
loss: 0.012530  [ 2700/ 3493]
loss: 0.018257  [ 2800/ 3493]
loss: 0.007454  [ 2900/ 3493]
loss: 0.035048  [ 3000/ 3493]
loss: 0.014267  [ 3100/ 3493]
loss: 0.016190  [ 3200/ 3493]
loss: 0.021292  [ 3300/ 3493]
loss: 0.011710  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.053789  [    0/ 3493]
loss: 0.019410  [  100/ 3493]
loss: 0.021945  [  200/ 3493]
loss: 0.017430  [  300/ 3493]
loss: 0.056864  [  400/ 3493]
loss: 0.013552  [  500/ 3493]
loss: 0.051197  [  600/ 3493]
loss: 0.158518  [  700/ 3493]
loss: 0.011610  [  800/ 3493]
loss: 0.012280  [  900/ 3493]
loss: 0.032244  [ 1000/ 3493]
loss: 0.028936  [ 1100/ 3493]
loss: 0.020090  [ 1200/ 3493]
loss: 0.013269  [ 1300/ 3493]
loss: 0.025650  [ 1400/ 3493]
loss: 0.052295  [ 1500/ 3493]
loss: 0.027773  [ 1600/ 3493]
loss: 0.019626  [ 1700/ 3493]
loss: 0.046974  [ 1800/ 3493]
loss: 0.013686  [ 1900/ 3493]
loss: 0.011635  [ 2000/ 3493]
loss: 0.014868  [ 2100/ 3493]
loss: 0.014002  [ 2200/ 3493]
loss: 0.022913  [ 2300/ 3493]
loss: 0.015368  [ 2400/ 3493]
loss: 0.083681  [ 2500/ 3493]
loss: 0.007362  [ 2600/ 3493]
loss: 0.013277  [ 2700/ 3493]
loss: 0.018629  [ 2800/ 3493]
loss: 0.007505  [ 2900/ 3493]
loss: 0.034999  [ 3000/ 3493]
loss: 0.014267  [ 3100/ 3493]
loss: 0.015117  [ 3200/ 3493]
loss: 0.022206  [ 3300/ 3493]
loss: 0.011658  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.055003  [    0/ 3493]
loss: 0.019615  [  100/ 3493]
loss: 0.020653  [  200/ 3493]
loss: 0.017031  [  300/ 3493]
loss: 0.056773  [  400/ 3493]
loss: 0.013143  [  500/ 3493]
loss: 0.051908  [  600/ 3493]
loss: 0.152558  [  700/ 3493]
loss: 0.010702  [  800/ 3493]
loss: 0.012257  [  900/ 3493]
loss: 0.030560  [ 1000/ 3493]
loss: 0.029990  [ 1100/ 3493]
loss: 0.019965  [ 1200/ 3493]
loss: 0.012815  [ 1300/ 3493]
loss: 0.025228  [ 1400/ 3493]
loss: 0.052750  [ 1500/ 3493]
loss: 0.027162  [ 1600/ 3493]
loss: 0.020484  [ 1700/ 3493]
loss: 0.046440  [ 1800/ 3493]
loss: 0.013966  [ 1900/ 3493]
loss: 0.012379  [ 2000/ 3493]
loss: 0.014245  [ 2100/ 3493]
loss: 0.013706  [ 2200/ 3493]
loss: 0.022290  [ 2300/ 3493]
loss: 0.015275  [ 2400/ 3493]
loss: 0.082496  [ 2500/ 3493]
loss: 0.007594  [ 2600/ 3493]
loss: 0.013537  [ 2700/ 3493]
loss: 0.019164  [ 2800/ 3493]
loss: 0.007726  [ 2900/ 3493]
loss: 0.034586  [ 3000/ 3493]
loss: 0.014474  [ 3100/ 3493]
loss: 0.014475  [ 3200/ 3493]
loss: 0.022922  [ 3300/ 3493]
loss: 0.011537  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.055695  [    0/ 3493]
loss: 0.019947  [  100/ 3493]
loss: 0.019598  [  200/ 3493]
loss: 0.016560  [  300/ 3493]
loss: 0.056852  [  400/ 3493]
loss: 0.012955  [  500/ 3493]
loss: 0.053102  [  600/ 3493]
loss: 0.146676  [  700/ 3493]
loss: 0.009988  [  800/ 3493]
loss: 0.012146  [  900/ 3493]
loss: 0.029524  [ 1000/ 3493]
loss: 0.030879  [ 1100/ 3493]
loss: 0.019113  [ 1200/ 3493]
loss: 0.012744  [ 1300/ 3493]
loss: 0.025074  [ 1400/ 3493]
loss: 0.053241  [ 1500/ 3493]
loss: 0.026386  [ 1600/ 3493]
loss: 0.021258  [ 1700/ 3493]
loss: 0.045849  [ 1800/ 3493]
loss: 0.013908  [ 1900/ 3493]
loss: 0.013442  [ 2000/ 3493]
loss: 0.013924  [ 2100/ 3493]
loss: 0.013578  [ 2200/ 3493]
loss: 0.021886  [ 2300/ 3493]
loss: 0.015171  [ 2400/ 3493]
loss: 0.082013  [ 2500/ 3493]
loss: 0.008041  [ 2600/ 3493]
loss: 0.013755  [ 2700/ 3493]
loss: 0.019939  [ 2800/ 3493]
loss: 0.007814  [ 2900/ 3493]
loss: 0.034668  [ 3000/ 3493]
loss: 0.014514  [ 3100/ 3493]
loss: 0.013914  [ 3200/ 3493]
loss: 0.023477  [ 3300/ 3493]
loss: 0.011374  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [ 2.9943185 -1.4978869]
[1 0 0 ... 1 0 2]
[1 0 0 ... 1 0 2]
Cluster 0 Occurrences: 1151; KMEANS: 1007
Cluster 1 Occurrences: 1195; KMEANS: 1167
Cluster 2 Occurrences: 1147; KMEANS: 1319
Centroids: [[-0.9734102, -0.10782585], [2.4375262, -0.67563057], [-0.32052743, -0.43907008]]
Centroids: [[-1.0955758, -0.03221594], [2.4905431, -0.6874824], [-0.28689566, -0.45516762]]
Contingency Matrix: 
[[ 875    2  274]
 [   2 1162   31]
 [ 130    3 1014]]
[[875, -1, 274], [-1, -1, -1], [130, -1, 1014]]
[[875, -1, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 2, 0: 0}
New Contingency Matrix: 
[[ 875    2  274]
 [   2 1162   31]
 [ 130    3 1014]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [875, 1162, 1014], Sum: 3051
All_Elements: [875, 2, 274, 2, 1162, 31, 130, 3, 1014], Sum: 3493
Accuracy: 0.8734612081305468
Done!
