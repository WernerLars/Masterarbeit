Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_55_13
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B30A5F2B0>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x0000019B2610BFD0>
Epoch 1
-------------------------------
loss: 0.208386  [    0/ 3526]
loss: 0.241100  [  100/ 3526]
loss: 0.116139  [  200/ 3526]
loss: 0.042917  [  300/ 3526]
loss: 0.026392  [  400/ 3526]
loss: 0.048477  [  500/ 3526]
loss: 0.037928  [  600/ 3526]
loss: 0.021694  [  700/ 3526]
loss: 0.058154  [  800/ 3526]
loss: 0.011074  [  900/ 3526]
loss: 0.038696  [ 1000/ 3526]
loss: 0.024051  [ 1100/ 3526]
loss: 0.049165  [ 1200/ 3526]
loss: 0.029052  [ 1300/ 3526]
loss: 0.029290  [ 1400/ 3526]
loss: 0.023009  [ 1500/ 3526]
loss: 0.042046  [ 1600/ 3526]
loss: 0.022963  [ 1700/ 3526]
loss: 0.024069  [ 1800/ 3526]
loss: 0.025220  [ 1900/ 3526]
loss: 0.015241  [ 2000/ 3526]
loss: 0.080810  [ 2100/ 3526]
loss: 0.027981  [ 2200/ 3526]
loss: 0.016745  [ 2300/ 3526]
loss: 0.008579  [ 2400/ 3526]
loss: 0.015761  [ 2500/ 3526]
loss: 0.023880  [ 2600/ 3526]
loss: 0.019875  [ 2700/ 3526]
loss: 0.007542  [ 2800/ 3526]
loss: 0.010421  [ 2900/ 3526]
loss: 0.032174  [ 3000/ 3526]
loss: 0.169019  [ 3100/ 3526]
loss: 0.030432  [ 3200/ 3526]
loss: 0.027819  [ 3300/ 3526]
loss: 0.053394  [ 3400/ 3526]
loss: 0.044183  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.014204  [    0/ 3526]
loss: 0.050615  [  100/ 3526]
loss: 0.044071  [  200/ 3526]
loss: 0.006705  [  300/ 3526]
loss: 0.016496  [  400/ 3526]
loss: 0.038584  [  500/ 3526]
loss: 0.013575  [  600/ 3526]
loss: 0.014759  [  700/ 3526]
loss: 0.032032  [  800/ 3526]
loss: 0.012103  [  900/ 3526]
loss: 0.018145  [ 1000/ 3526]
loss: 0.018938  [ 1100/ 3526]
loss: 0.028574  [ 1200/ 3526]
loss: 0.025883  [ 1300/ 3526]
loss: 0.029255  [ 1400/ 3526]
loss: 0.019696  [ 1500/ 3526]
loss: 0.041653  [ 1600/ 3526]
loss: 0.019321  [ 1700/ 3526]
loss: 0.020952  [ 1800/ 3526]
loss: 0.017143  [ 1900/ 3526]
loss: 0.015947  [ 2000/ 3526]
loss: 0.113862  [ 2100/ 3526]
loss: 0.022268  [ 2200/ 3526]
loss: 0.016442  [ 2300/ 3526]
loss: 0.008560  [ 2400/ 3526]
loss: 0.017040  [ 2500/ 3526]
loss: 0.018423  [ 2600/ 3526]
loss: 0.017030  [ 2700/ 3526]
loss: 0.007711  [ 2800/ 3526]
loss: 0.009453  [ 2900/ 3526]
loss: 0.040130  [ 3000/ 3526]
loss: 0.165281  [ 3100/ 3526]
loss: 0.028154  [ 3200/ 3526]
loss: 0.026656  [ 3300/ 3526]
loss: 0.051855  [ 3400/ 3526]
loss: 0.041418  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.013719  [    0/ 3526]
loss: 0.054203  [  100/ 3526]
loss: 0.047988  [  200/ 3526]
loss: 0.004475  [  300/ 3526]
loss: 0.016461  [  400/ 3526]
loss: 0.038899  [  500/ 3526]
loss: 0.011822  [  600/ 3526]
loss: 0.014775  [  700/ 3526]
loss: 0.030873  [  800/ 3526]
loss: 0.012001  [  900/ 3526]
loss: 0.016881  [ 1000/ 3526]
loss: 0.016803  [ 1100/ 3526]
loss: 0.029456  [ 1200/ 3526]
loss: 0.023206  [ 1300/ 3526]
loss: 0.029100  [ 1400/ 3526]
loss: 0.019735  [ 1500/ 3526]
loss: 0.044017  [ 1600/ 3526]
loss: 0.019247  [ 1700/ 3526]
loss: 0.021719  [ 1800/ 3526]
loss: 0.014064  [ 1900/ 3526]
loss: 0.015468  [ 2000/ 3526]
loss: 0.122992  [ 2100/ 3526]
loss: 0.019120  [ 2200/ 3526]
loss: 0.016599  [ 2300/ 3526]
loss: 0.008843  [ 2400/ 3526]
loss: 0.016193  [ 2500/ 3526]
loss: 0.015876  [ 2600/ 3526]
loss: 0.015991  [ 2700/ 3526]
loss: 0.007802  [ 2800/ 3526]
loss: 0.009385  [ 2900/ 3526]
loss: 0.050278  [ 3000/ 3526]
loss: 0.159161  [ 3100/ 3526]
loss: 0.027669  [ 3200/ 3526]
loss: 0.023809  [ 3300/ 3526]
loss: 0.042454  [ 3400/ 3526]
loss: 0.039841  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.009645  [    0/ 3526]
loss: 0.039114  [  100/ 3526]
loss: 0.043057  [  200/ 3526]
loss: 0.006579  [  300/ 3526]
loss: 0.015867  [  400/ 3526]
loss: 0.037115  [  500/ 3526]
loss: 0.010422  [  600/ 3526]
loss: 0.014805  [  700/ 3526]
loss: 0.030132  [  800/ 3526]
loss: 0.014084  [  900/ 3526]
loss: 0.018001  [ 1000/ 3526]
loss: 0.015719  [ 1100/ 3526]
loss: 0.030508  [ 1200/ 3526]
loss: 0.022729  [ 1300/ 3526]
loss: 0.028642  [ 1400/ 3526]
loss: 0.020054  [ 1500/ 3526]
loss: 0.044526  [ 1600/ 3526]
loss: 0.020024  [ 1700/ 3526]
loss: 0.022415  [ 1800/ 3526]
loss: 0.012352  [ 1900/ 3526]
loss: 0.015213  [ 2000/ 3526]
loss: 0.127673  [ 2100/ 3526]
loss: 0.021194  [ 2200/ 3526]
loss: 0.016498  [ 2300/ 3526]
loss: 0.008880  [ 2400/ 3526]
loss: 0.015206  [ 2500/ 3526]
loss: 0.015468  [ 2600/ 3526]
loss: 0.014752  [ 2700/ 3526]
loss: 0.007768  [ 2800/ 3526]
loss: 0.009653  [ 2900/ 3526]
loss: 0.058558  [ 3000/ 3526]
loss: 0.156846  [ 3100/ 3526]
loss: 0.028087  [ 3200/ 3526]
loss: 0.022790  [ 3300/ 3526]
loss: 0.035666  [ 3400/ 3526]
loss: 0.040712  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.010367  [    0/ 3526]
loss: 0.035513  [  100/ 3526]
loss: 0.039986  [  200/ 3526]
loss: 0.008815  [  300/ 3526]
loss: 0.015153  [  400/ 3526]
loss: 0.036050  [  500/ 3526]
loss: 0.010402  [  600/ 3526]
loss: 0.014543  [  700/ 3526]
loss: 0.029796  [  800/ 3526]
loss: 0.014364  [  900/ 3526]
loss: 0.017521  [ 1000/ 3526]
loss: 0.015835  [ 1100/ 3526]
loss: 0.029862  [ 1200/ 3526]
loss: 0.023152  [ 1300/ 3526]
loss: 0.028806  [ 1400/ 3526]
loss: 0.020102  [ 1500/ 3526]
loss: 0.043953  [ 1600/ 3526]
loss: 0.020102  [ 1700/ 3526]
loss: 0.022323  [ 1800/ 3526]
loss: 0.011609  [ 1900/ 3526]
loss: 0.015013  [ 2000/ 3526]
loss: 0.143076  [ 2100/ 3526]
loss: 0.022951  [ 2200/ 3526]
loss: 0.016582  [ 2300/ 3526]
loss: 0.008944  [ 2400/ 3526]
loss: 0.014359  [ 2500/ 3526]
loss: 0.015905  [ 2600/ 3526]
loss: 0.014169  [ 2700/ 3526]
loss: 0.007740  [ 2800/ 3526]
loss: 0.010287  [ 2900/ 3526]
loss: 0.062817  [ 3000/ 3526]
loss: 0.157448  [ 3100/ 3526]
loss: 0.028417  [ 3200/ 3526]
loss: 0.023217  [ 3300/ 3526]
loss: 0.031984  [ 3400/ 3526]
loss: 0.041891  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.011102  [    0/ 3526]
loss: 0.032820  [  100/ 3526]
loss: 0.037109  [  200/ 3526]
loss: 0.010038  [  300/ 3526]
loss: 0.014754  [  400/ 3526]
loss: 0.036050  [  500/ 3526]
loss: 0.010871  [  600/ 3526]
loss: 0.014135  [  700/ 3526]
loss: 0.029558  [  800/ 3526]
loss: 0.014572  [  900/ 3526]
loss: 0.017517  [ 1000/ 3526]
loss: 0.016257  [ 1100/ 3526]
loss: 0.029282  [ 1200/ 3526]
loss: 0.023559  [ 1300/ 3526]
loss: 0.029106  [ 1400/ 3526]
loss: 0.019850  [ 1500/ 3526]
loss: 0.043557  [ 1600/ 3526]
loss: 0.020041  [ 1700/ 3526]
loss: 0.022091  [ 1800/ 3526]
loss: 0.011064  [ 1900/ 3526]
loss: 0.015025  [ 2000/ 3526]
loss: 0.148663  [ 2100/ 3526]
loss: 0.024897  [ 2200/ 3526]
loss: 0.016591  [ 2300/ 3526]
loss: 0.009041  [ 2400/ 3526]
loss: 0.014029  [ 2500/ 3526]
loss: 0.016780  [ 2600/ 3526]
loss: 0.013895  [ 2700/ 3526]
loss: 0.007785  [ 2800/ 3526]
loss: 0.010215  [ 2900/ 3526]
loss: 0.063869  [ 3000/ 3526]
loss: 0.158204  [ 3100/ 3526]
loss: 0.028375  [ 3200/ 3526]
loss: 0.023712  [ 3300/ 3526]
loss: 0.028776  [ 3400/ 3526]
loss: 0.042543  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.011375  [    0/ 3526]
loss: 0.028939  [  100/ 3526]
loss: 0.034707  [  200/ 3526]
loss: 0.010977  [  300/ 3526]
loss: 0.014442  [  400/ 3526]
loss: 0.036642  [  500/ 3526]
loss: 0.011466  [  600/ 3526]
loss: 0.013778  [  700/ 3526]
loss: 0.029233  [  800/ 3526]
loss: 0.014193  [  900/ 3526]
loss: 0.017209  [ 1000/ 3526]
loss: 0.016396  [ 1100/ 3526]
loss: 0.028514  [ 1200/ 3526]
loss: 0.023825  [ 1300/ 3526]
loss: 0.029626  [ 1400/ 3526]
loss: 0.019664  [ 1500/ 3526]
loss: 0.043157  [ 1600/ 3526]
loss: 0.019662  [ 1700/ 3526]
loss: 0.022123  [ 1800/ 3526]
loss: 0.010664  [ 1900/ 3526]
loss: 0.015089  [ 2000/ 3526]
loss: 0.156572  [ 2100/ 3526]
loss: 0.026253  [ 2200/ 3526]
loss: 0.016683  [ 2300/ 3526]
loss: 0.009105  [ 2400/ 3526]
loss: 0.013783  [ 2500/ 3526]
loss: 0.017275  [ 2600/ 3526]
loss: 0.013968  [ 2700/ 3526]
loss: 0.008044  [ 2800/ 3526]
loss: 0.009923  [ 2900/ 3526]
loss: 0.063774  [ 3000/ 3526]
loss: 0.159138  [ 3100/ 3526]
loss: 0.028362  [ 3200/ 3526]
loss: 0.024178  [ 3300/ 3526]
loss: 0.026589  [ 3400/ 3526]
loss: 0.043153  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.011492  [    0/ 3526]
loss: 0.026248  [  100/ 3526]
loss: 0.032806  [  200/ 3526]
loss: 0.011539  [  300/ 3526]
loss: 0.014272  [  400/ 3526]
loss: 0.037191  [  500/ 3526]
loss: 0.011743  [  600/ 3526]
loss: 0.013584  [  700/ 3526]
loss: 0.028872  [  800/ 3526]
loss: 0.014397  [  900/ 3526]
loss: 0.016922  [ 1000/ 3526]
loss: 0.016877  [ 1100/ 3526]
loss: 0.027717  [ 1200/ 3526]
loss: 0.023908  [ 1300/ 3526]
loss: 0.029988  [ 1400/ 3526]
loss: 0.019441  [ 1500/ 3526]
loss: 0.042831  [ 1600/ 3526]
loss: 0.019415  [ 1700/ 3526]
loss: 0.023367  [ 1800/ 3526]
loss: 0.010422  [ 1900/ 3526]
loss: 0.015094  [ 2000/ 3526]
loss: 0.166319  [ 2100/ 3526]
loss: 0.026981  [ 2200/ 3526]
loss: 0.016771  [ 2300/ 3526]
loss: 0.009162  [ 2400/ 3526]
loss: 0.013656  [ 2500/ 3526]
loss: 0.017811  [ 2600/ 3526]
loss: 0.014120  [ 2700/ 3526]
loss: 0.008211  [ 2800/ 3526]
loss: 0.009650  [ 2900/ 3526]
loss: 0.063919  [ 3000/ 3526]
loss: 0.159710  [ 3100/ 3526]
loss: 0.028272  [ 3200/ 3526]
loss: 0.024409  [ 3300/ 3526]
loss: 0.024845  [ 3400/ 3526]
loss: 0.043368  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [-1.1405354  -0.21786377]
[2 0 2 ... 0 2 2]
[1 2 1 ... 2 1 1]
Cluster 0 Occurrences: 1186; KMEANS: 1237
Cluster 1 Occurrences: 1188; KMEANS: 1125
Cluster 2 Occurrences: 1152; KMEANS: 1164
Centroids: [[1.4603195, -0.83499336], [0.6495984, -0.18296552], [-1.6861212, 0.14509095]]
Centroids: [[0.5999564, -0.14537969], [-1.7230452, 0.16447665], [1.5099056, -0.89838654]]
Contingency Matrix: 
[[ 141    0 1045]
 [1069    1  118]
 [  27 1124    1]]
[[141, -1, 1045], [1069, -1, 118], [-1, -1, -1]]
[[-1, -1, 1045], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 1, 1: 0, 0: 2}
New Contingency Matrix: 
[[1045  141    0]
 [ 118 1069    1]
 [   1   27 1124]]
New Clustered Label Sequence: [2, 0, 1]
Diagonal_Elements: [1045, 1069, 1124], Sum: 3238
All_Elements: [1045, 141, 0, 118, 1069, 1, 1, 27, 1124], Sum: 3526
Accuracy: 0.918321043675553
Done!
