Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_50_09
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B30AF6550>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000019B26154E80>
Epoch 1
-------------------------------
loss: 0.149341  [    0/ 3386]
loss: 0.258059  [  100/ 3386]
loss: 0.200667  [  200/ 3386]
loss: 0.164212  [  300/ 3386]
loss: 0.277879  [  400/ 3386]
loss: 0.072402  [  500/ 3386]
loss: 0.160845  [  600/ 3386]
loss: 0.084517  [  700/ 3386]
loss: 0.158993  [  800/ 3386]
loss: 0.080207  [  900/ 3386]
loss: 0.227841  [ 1000/ 3386]
loss: 0.358605  [ 1100/ 3386]
loss: 0.235152  [ 1200/ 3386]
loss: 0.025359  [ 1300/ 3386]
loss: 0.046882  [ 1400/ 3386]
loss: 0.102347  [ 1500/ 3386]
loss: 0.036954  [ 1600/ 3386]
loss: 0.025265  [ 1700/ 3386]
loss: 0.275931  [ 1800/ 3386]
loss: 0.538680  [ 1900/ 3386]
loss: 0.030234  [ 2000/ 3386]
loss: 0.142362  [ 2100/ 3386]
loss: 0.595920  [ 2200/ 3386]
loss: 0.111540  [ 2300/ 3386]
loss: 0.133442  [ 2400/ 3386]
loss: 0.049591  [ 2500/ 3386]
loss: 0.183305  [ 2600/ 3386]
loss: 0.226624  [ 2700/ 3386]
loss: 0.098305  [ 2800/ 3386]
loss: 0.076102  [ 2900/ 3386]
loss: 0.014820  [ 3000/ 3386]
loss: 0.116536  [ 3100/ 3386]
loss: 0.110171  [ 3200/ 3386]
loss: 0.108062  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.054752  [    0/ 3386]
loss: 0.125974  [  100/ 3386]
loss: 0.095122  [  200/ 3386]
loss: 0.129602  [  300/ 3386]
loss: 0.212786  [  400/ 3386]
loss: 0.070508  [  500/ 3386]
loss: 0.158730  [  600/ 3386]
loss: 0.046342  [  700/ 3386]
loss: 0.111270  [  800/ 3386]
loss: 0.053959  [  900/ 3386]
loss: 0.139810  [ 1000/ 3386]
loss: 0.313766  [ 1100/ 3386]
loss: 0.159694  [ 1200/ 3386]
loss: 0.023819  [ 1300/ 3386]
loss: 0.046659  [ 1400/ 3386]
loss: 0.071974  [ 1500/ 3386]
loss: 0.033794  [ 1600/ 3386]
loss: 0.027871  [ 1700/ 3386]
loss: 0.308866  [ 1800/ 3386]
loss: 0.488472  [ 1900/ 3386]
loss: 0.028143  [ 2000/ 3386]
loss: 0.102954  [ 2100/ 3386]
loss: 0.692299  [ 2200/ 3386]
loss: 0.086457  [ 2300/ 3386]
loss: 0.107916  [ 2400/ 3386]
loss: 0.042727  [ 2500/ 3386]
loss: 0.158181  [ 2600/ 3386]
loss: 0.214698  [ 2700/ 3386]
loss: 0.087018  [ 2800/ 3386]
loss: 0.066807  [ 2900/ 3386]
loss: 0.016346  [ 3000/ 3386]
loss: 0.114877  [ 3100/ 3386]
loss: 0.097391  [ 3200/ 3386]
loss: 0.104822  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.052939  [    0/ 3386]
loss: 0.121508  [  100/ 3386]
loss: 0.094403  [  200/ 3386]
loss: 0.157755  [  300/ 3386]
loss: 0.199115  [  400/ 3386]
loss: 0.071349  [  500/ 3386]
loss: 0.155432  [  600/ 3386]
loss: 0.046452  [  700/ 3386]
loss: 0.113074  [  800/ 3386]
loss: 0.049207  [  900/ 3386]
loss: 0.132633  [ 1000/ 3386]
loss: 0.301793  [ 1100/ 3386]
loss: 0.144905  [ 1200/ 3386]
loss: 0.024591  [ 1300/ 3386]
loss: 0.047570  [ 1400/ 3386]
loss: 0.065111  [ 1500/ 3386]
loss: 0.034243  [ 1600/ 3386]
loss: 0.029461  [ 1700/ 3386]
loss: 0.316603  [ 1800/ 3386]
loss: 0.459545  [ 1900/ 3386]
loss: 0.028765  [ 2000/ 3386]
loss: 0.090715  [ 2100/ 3386]
loss: 0.635186  [ 2200/ 3386]
loss: 0.082576  [ 2300/ 3386]
loss: 0.098093  [ 2400/ 3386]
loss: 0.041957  [ 2500/ 3386]
loss: 0.155013  [ 2600/ 3386]
loss: 0.214042  [ 2700/ 3386]
loss: 0.086943  [ 2800/ 3386]
loss: 0.067876  [ 2900/ 3386]
loss: 0.017120  [ 3000/ 3386]
loss: 0.112414  [ 3100/ 3386]
loss: 0.089148  [ 3200/ 3386]
loss: 0.105632  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.052148  [    0/ 3386]
loss: 0.122193  [  100/ 3386]
loss: 0.096375  [  200/ 3386]
loss: 0.172374  [  300/ 3386]
loss: 0.195018  [  400/ 3386]
loss: 0.071025  [  500/ 3386]
loss: 0.155612  [  600/ 3386]
loss: 0.046172  [  700/ 3386]
loss: 0.113208  [  800/ 3386]
loss: 0.047739  [  900/ 3386]
loss: 0.132911  [ 1000/ 3386]
loss: 0.297849  [ 1100/ 3386]
loss: 0.141236  [ 1200/ 3386]
loss: 0.024424  [ 1300/ 3386]
loss: 0.047387  [ 1400/ 3386]
loss: 0.063392  [ 1500/ 3386]
loss: 0.035320  [ 1600/ 3386]
loss: 0.029709  [ 1700/ 3386]
loss: 0.313996  [ 1800/ 3386]
loss: 0.453990  [ 1900/ 3386]
loss: 0.028574  [ 2000/ 3386]
loss: 0.088092  [ 2100/ 3386]
loss: 0.606193  [ 2200/ 3386]
loss: 0.081859  [ 2300/ 3386]
loss: 0.091517  [ 2400/ 3386]
loss: 0.042306  [ 2500/ 3386]
loss: 0.155500  [ 2600/ 3386]
loss: 0.214231  [ 2700/ 3386]
loss: 0.088128  [ 2800/ 3386]
loss: 0.064170  [ 2900/ 3386]
loss: 0.017087  [ 3000/ 3386]
loss: 0.111727  [ 3100/ 3386]
loss: 0.084767  [ 3200/ 3386]
loss: 0.106021  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.051932  [    0/ 3386]
loss: 0.121147  [  100/ 3386]
loss: 0.096490  [  200/ 3386]
loss: 0.172626  [  300/ 3386]
loss: 0.191876  [  400/ 3386]
loss: 0.070365  [  500/ 3386]
loss: 0.156850  [  600/ 3386]
loss: 0.046505  [  700/ 3386]
loss: 0.114795  [  800/ 3386]
loss: 0.047311  [  900/ 3386]
loss: 0.136192  [ 1000/ 3386]
loss: 0.300878  [ 1100/ 3386]
loss: 0.143046  [ 1200/ 3386]
loss: 0.023443  [ 1300/ 3386]
loss: 0.046168  [ 1400/ 3386]
loss: 0.064866  [ 1500/ 3386]
loss: 0.036147  [ 1600/ 3386]
loss: 0.030043  [ 1700/ 3386]
loss: 0.300509  [ 1800/ 3386]
loss: 0.457873  [ 1900/ 3386]
loss: 0.029743  [ 2000/ 3386]
loss: 0.087113  [ 2100/ 3386]
loss: 0.562445  [ 2200/ 3386]
loss: 0.082685  [ 2300/ 3386]
loss: 0.084755  [ 2400/ 3386]
loss: 0.042350  [ 2500/ 3386]
loss: 0.161806  [ 2600/ 3386]
loss: 0.213926  [ 2700/ 3386]
loss: 0.071336  [ 2800/ 3386]
loss: 0.058641  [ 2900/ 3386]
loss: 0.017774  [ 3000/ 3386]
loss: 0.108941  [ 3100/ 3386]
loss: 0.084878  [ 3200/ 3386]
loss: 0.102412  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.053303  [    0/ 3386]
loss: 0.118099  [  100/ 3386]
loss: 0.083730  [  200/ 3386]
loss: 0.098011  [  300/ 3386]
loss: 0.192344  [  400/ 3386]
loss: 0.070410  [  500/ 3386]
loss: 0.154610  [  600/ 3386]
loss: 0.047562  [  700/ 3386]
loss: 0.117238  [  800/ 3386]
loss: 0.047269  [  900/ 3386]
loss: 0.140752  [ 1000/ 3386]
loss: 0.293712  [ 1100/ 3386]
loss: 0.138519  [ 1200/ 3386]
loss: 0.023892  [ 1300/ 3386]
loss: 0.047298  [ 1400/ 3386]
loss: 0.063185  [ 1500/ 3386]
loss: 0.037330  [ 1600/ 3386]
loss: 0.032231  [ 1700/ 3386]
loss: 0.267546  [ 1800/ 3386]
loss: 0.443702  [ 1900/ 3386]
loss: 0.024837  [ 2000/ 3386]
loss: 0.083345  [ 2100/ 3386]
loss: 0.554381  [ 2200/ 3386]
loss: 0.079979  [ 2300/ 3386]
loss: 0.074703  [ 2400/ 3386]
loss: 0.043136  [ 2500/ 3386]
loss: 0.161565  [ 2600/ 3386]
loss: 0.213396  [ 2700/ 3386]
loss: 0.068402  [ 2800/ 3386]
loss: 0.055043  [ 2900/ 3386]
loss: 0.017886  [ 3000/ 3386]
loss: 0.107769  [ 3100/ 3386]
loss: 0.072788  [ 3200/ 3386]
loss: 0.103070  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.054206  [    0/ 3386]
loss: 0.117895  [  100/ 3386]
loss: 0.074886  [  200/ 3386]
loss: 0.077432  [  300/ 3386]
loss: 0.189048  [  400/ 3386]
loss: 0.070054  [  500/ 3386]
loss: 0.154188  [  600/ 3386]
loss: 0.046240  [  700/ 3386]
loss: 0.120978  [  800/ 3386]
loss: 0.047702  [  900/ 3386]
loss: 0.141070  [ 1000/ 3386]
loss: 0.289865  [ 1100/ 3386]
loss: 0.138012  [ 1200/ 3386]
loss: 0.023464  [ 1300/ 3386]
loss: 0.048820  [ 1400/ 3386]
loss: 0.065943  [ 1500/ 3386]
loss: 0.037866  [ 1600/ 3386]
loss: 0.031713  [ 1700/ 3386]
loss: 0.214111  [ 1800/ 3386]
loss: 0.428673  [ 1900/ 3386]
loss: 0.022244  [ 2000/ 3386]
loss: 0.083644  [ 2100/ 3386]
loss: 0.502978  [ 2200/ 3386]
loss: 0.080332  [ 2300/ 3386]
loss: 0.069575  [ 2400/ 3386]
loss: 0.044475  [ 2500/ 3386]
loss: 0.144880  [ 2600/ 3386]
loss: 0.207905  [ 2700/ 3386]
loss: 0.070713  [ 2800/ 3386]
loss: 0.055511  [ 2900/ 3386]
loss: 0.018371  [ 3000/ 3386]
loss: 0.108069  [ 3100/ 3386]
loss: 0.066123  [ 3200/ 3386]
loss: 0.102998  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.052017  [    0/ 3386]
loss: 0.116951  [  100/ 3386]
loss: 0.074113  [  200/ 3386]
loss: 0.253607  [  300/ 3386]
loss: 0.195470  [  400/ 3386]
loss: 0.071265  [  500/ 3386]
loss: 0.154345  [  600/ 3386]
loss: 0.044984  [  700/ 3386]
loss: 0.118138  [  800/ 3386]
loss: 0.050113  [  900/ 3386]
loss: 0.145805  [ 1000/ 3386]
loss: 0.267801  [ 1100/ 3386]
loss: 0.143852  [ 1200/ 3386]
loss: 0.022845  [ 1300/ 3386]
loss: 0.048999  [ 1400/ 3386]
loss: 0.066423  [ 1500/ 3386]
loss: 0.037311  [ 1600/ 3386]
loss: 0.031584  [ 1700/ 3386]
loss: 0.183645  [ 1800/ 3386]
loss: 0.393003  [ 1900/ 3386]
loss: 0.022584  [ 2000/ 3386]
loss: 0.085785  [ 2100/ 3386]
loss: 0.445889  [ 2200/ 3386]
loss: 0.080849  [ 2300/ 3386]
loss: 0.068729  [ 2400/ 3386]
loss: 0.044370  [ 2500/ 3386]
loss: 0.124542  [ 2600/ 3386]
loss: 0.204912  [ 2700/ 3386]
loss: 0.065488  [ 2800/ 3386]
loss: 0.054580  [ 2900/ 3386]
loss: 0.018377  [ 3000/ 3386]
loss: 0.108623  [ 3100/ 3386]
loss: 0.059393  [ 3200/ 3386]
loss: 0.102309  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [-0.38338485 -0.13031209]
[0 2 0 ... 2 0 1]
[0 0 0 ... 0 0 2]
Cluster 0 Occurrences: 1079; KMEANS: 2250
Cluster 1 Occurrences: 1158; KMEANS: 630
Cluster 2 Occurrences: 1149; KMEANS: 506
Centroids: [[-0.951802, 0.29554355], [4.9358363, 4.0530014], [-1.3375022, -0.43812025]]
Centroids: [[-1.1866245, -0.08651998], [3.6867, 3.2785556], [6.9154105, 5.2135315]]
Contingency Matrix: 
[[1079    0    0]
 [  56  597  505]
 [1115   33    1]]
[[-1, 0, 0], [-1, 597, 505], [-1, -1, -1]]
[[-1, -1, 0], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 1: 1, 0: 2}
New Contingency Matrix: 
[[   0    0 1079]
 [ 505  597   56]
 [   1   33 1115]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [0, 597, 1115], Sum: 1712
All_Elements: [0, 0, 1079, 505, 597, 56, 1, 33, 1115], Sum: 3386
Accuracy: 0.505611340815121
Done!
