Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Burst_Easy2_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-10_08_19
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B96632D68>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
<torch.utils.data.dataloader.DataLoader object at 0x0000019B89ADFBA8>
Epoch 1
-------------------------------
loss: 0.227482  [    0/ 3442]
loss: 0.145158  [  100/ 3442]
loss: 0.057995  [  200/ 3442]
loss: 0.015944  [  300/ 3442]
loss: 0.031079  [  400/ 3442]
loss: 0.037439  [  500/ 3442]
loss: 0.031280  [  600/ 3442]
loss: 0.019299  [  700/ 3442]
loss: 0.022719  [  800/ 3442]
loss: 0.042124  [  900/ 3442]
loss: 0.037130  [ 1000/ 3442]
loss: 0.003682  [ 1100/ 3442]
loss: 0.023419  [ 1200/ 3442]
loss: 0.011003  [ 1300/ 3442]
loss: 0.017763  [ 1400/ 3442]
loss: 0.021600  [ 1500/ 3442]
loss: 0.017695  [ 1600/ 3442]
loss: 0.015086  [ 1700/ 3442]
loss: 0.019494  [ 1800/ 3442]
loss: 0.032901  [ 1900/ 3442]
loss: 0.019840  [ 2000/ 3442]
loss: 0.018915  [ 2100/ 3442]
loss: 0.019569  [ 2200/ 3442]
loss: 0.011362  [ 2300/ 3442]
loss: 0.020108  [ 2400/ 3442]
loss: 0.036901  [ 2500/ 3442]
loss: 0.127636  [ 2600/ 3442]
loss: 0.029711  [ 2700/ 3442]
loss: 0.010914  [ 2800/ 3442]
loss: 0.008209  [ 2900/ 3442]
loss: 0.005920  [ 3000/ 3442]
loss: 0.019508  [ 3100/ 3442]
loss: 0.117376  [ 3200/ 3442]
loss: 0.016240  [ 3300/ 3442]
loss: 0.017103  [ 3400/ 3442]
Epoch 2
-------------------------------
loss: 0.016389  [    0/ 3442]
loss: 0.010922  [  100/ 3442]
loss: 0.031740  [  200/ 3442]
loss: 0.006622  [  300/ 3442]
loss: 0.018726  [  400/ 3442]
loss: 0.009684  [  500/ 3442]
loss: 0.025088  [  600/ 3442]
loss: 0.015279  [  700/ 3442]
loss: 0.017204  [  800/ 3442]
loss: 0.022950  [  900/ 3442]
loss: 0.018374  [ 1000/ 3442]
loss: 0.003038  [ 1100/ 3442]
loss: 0.016202  [ 1200/ 3442]
loss: 0.007994  [ 1300/ 3442]
loss: 0.012671  [ 1400/ 3442]
loss: 0.020776  [ 1500/ 3442]
loss: 0.021856  [ 1600/ 3442]
loss: 0.011684  [ 1700/ 3442]
loss: 0.016457  [ 1800/ 3442]
loss: 0.021913  [ 1900/ 3442]
loss: 0.015737  [ 2000/ 3442]
loss: 0.012416  [ 2100/ 3442]
loss: 0.017274  [ 2200/ 3442]
loss: 0.010644  [ 2300/ 3442]
loss: 0.016832  [ 2400/ 3442]
loss: 0.034575  [ 2500/ 3442]
loss: 0.125505  [ 2600/ 3442]
loss: 0.030503  [ 2700/ 3442]
loss: 0.009216  [ 2800/ 3442]
loss: 0.009027  [ 2900/ 3442]
loss: 0.005287  [ 3000/ 3442]
loss: 0.019158  [ 3100/ 3442]
loss: 0.119137  [ 3200/ 3442]
loss: 0.016774  [ 3300/ 3442]
loss: 0.017472  [ 3400/ 3442]
Epoch 3
-------------------------------
loss: 0.013984  [    0/ 3442]
loss: 0.011169  [  100/ 3442]
loss: 0.031118  [  200/ 3442]
loss: 0.006632  [  300/ 3442]
loss: 0.019200  [  400/ 3442]
loss: 0.009706  [  500/ 3442]
loss: 0.023936  [  600/ 3442]
loss: 0.014221  [  700/ 3442]
loss: 0.017982  [  800/ 3442]
loss: 0.023608  [  900/ 3442]
loss: 0.014251  [ 1000/ 3442]
loss: 0.002996  [ 1100/ 3442]
loss: 0.015333  [ 1200/ 3442]
loss: 0.008075  [ 1300/ 3442]
loss: 0.012527  [ 1400/ 3442]
loss: 0.020328  [ 1500/ 3442]
loss: 0.019977  [ 1600/ 3442]
loss: 0.011396  [ 1700/ 3442]
loss: 0.016843  [ 1800/ 3442]
loss: 0.018821  [ 1900/ 3442]
loss: 0.015598  [ 2000/ 3442]
loss: 0.011709  [ 2100/ 3442]
loss: 0.015653  [ 2200/ 3442]
loss: 0.010355  [ 2300/ 3442]
loss: 0.015928  [ 2400/ 3442]
loss: 0.033807  [ 2500/ 3442]
loss: 0.125518  [ 2600/ 3442]
loss: 0.030995  [ 2700/ 3442]
loss: 0.008765  [ 2800/ 3442]
loss: 0.009813  [ 2900/ 3442]
loss: 0.005399  [ 3000/ 3442]
loss: 0.018932  [ 3100/ 3442]
loss: 0.120303  [ 3200/ 3442]
loss: 0.014950  [ 3300/ 3442]
loss: 0.018949  [ 3400/ 3442]
Epoch 4
-------------------------------
loss: 0.013014  [    0/ 3442]
loss: 0.012086  [  100/ 3442]
loss: 0.030204  [  200/ 3442]
loss: 0.006712  [  300/ 3442]
loss: 0.019472  [  400/ 3442]
loss: 0.009789  [  500/ 3442]
loss: 0.022815  [  600/ 3442]
loss: 0.013857  [  700/ 3442]
loss: 0.018132  [  800/ 3442]
loss: 0.023591  [  900/ 3442]
loss: 0.014617  [ 1000/ 3442]
loss: 0.003463  [ 1100/ 3442]
loss: 0.017464  [ 1200/ 3442]
loss: 0.005939  [ 1300/ 3442]
loss: 0.012586  [ 1400/ 3442]
loss: 0.020451  [ 1500/ 3442]
loss: 0.015527  [ 1600/ 3442]
loss: 0.011006  [ 1700/ 3442]
loss: 0.017247  [ 1800/ 3442]
loss: 0.017125  [ 1900/ 3442]
loss: 0.015318  [ 2000/ 3442]
loss: 0.011335  [ 2100/ 3442]
loss: 0.015667  [ 2200/ 3442]
loss: 0.010147  [ 2300/ 3442]
loss: 0.010550  [ 2400/ 3442]
loss: 0.032872  [ 2500/ 3442]
loss: 0.126941  [ 2600/ 3442]
loss: 0.030964  [ 2700/ 3442]
loss: 0.008455  [ 2800/ 3442]
loss: 0.009979  [ 2900/ 3442]
loss: 0.005524  [ 3000/ 3442]
loss: 0.019043  [ 3100/ 3442]
loss: 0.121951  [ 3200/ 3442]
loss: 0.011568  [ 3300/ 3442]
loss: 0.023653  [ 3400/ 3442]
Epoch 5
-------------------------------
loss: 0.012027  [    0/ 3442]
loss: 0.012948  [  100/ 3442]
loss: 0.030114  [  200/ 3442]
loss: 0.007181  [  300/ 3442]
loss: 0.018304  [  400/ 3442]
loss: 0.009930  [  500/ 3442]
loss: 0.023417  [  600/ 3442]
loss: 0.012360  [  700/ 3442]
loss: 0.017293  [  800/ 3442]
loss: 0.024371  [  900/ 3442]
loss: 0.012547  [ 1000/ 3442]
loss: 0.003807  [ 1100/ 3442]
loss: 0.016581  [ 1200/ 3442]
loss: 0.004529  [ 1300/ 3442]
loss: 0.012809  [ 1400/ 3442]
loss: 0.020473  [ 1500/ 3442]
loss: 0.013458  [ 1600/ 3442]
loss: 0.010857  [ 1700/ 3442]
loss: 0.017185  [ 1800/ 3442]
loss: 0.016480  [ 1900/ 3442]
loss: 0.015003  [ 2000/ 3442]
loss: 0.011213  [ 2100/ 3442]
loss: 0.015432  [ 2200/ 3442]
loss: 0.010143  [ 2300/ 3442]
loss: 0.011301  [ 2400/ 3442]
loss: 0.032362  [ 2500/ 3442]
loss: 0.127259  [ 2600/ 3442]
loss: 0.030682  [ 2700/ 3442]
loss: 0.008491  [ 2800/ 3442]
loss: 0.009400  [ 2900/ 3442]
loss: 0.005380  [ 3000/ 3442]
loss: 0.019351  [ 3100/ 3442]
loss: 0.122768  [ 3200/ 3442]
loss: 0.010322  [ 3300/ 3442]
loss: 0.030136  [ 3400/ 3442]
Epoch 6
-------------------------------
loss: 0.012628  [    0/ 3442]
loss: 0.013600  [  100/ 3442]
loss: 0.030329  [  200/ 3442]
loss: 0.007571  [  300/ 3442]
loss: 0.016829  [  400/ 3442]
loss: 0.009940  [  500/ 3442]
loss: 0.023842  [  600/ 3442]
loss: 0.011489  [  700/ 3442]
loss: 0.016720  [  800/ 3442]
loss: 0.025214  [  900/ 3442]
loss: 0.012154  [ 1000/ 3442]
loss: 0.004578  [ 1100/ 3442]
loss: 0.016637  [ 1200/ 3442]
loss: 0.003960  [ 1300/ 3442]
loss: 0.012765  [ 1400/ 3442]
loss: 0.020264  [ 1500/ 3442]
loss: 0.013090  [ 1600/ 3442]
loss: 0.010559  [ 1700/ 3442]
loss: 0.017053  [ 1800/ 3442]
loss: 0.016392  [ 1900/ 3442]
loss: 0.014778  [ 2000/ 3442]
loss: 0.011029  [ 2100/ 3442]
loss: 0.015088  [ 2200/ 3442]
loss: 0.010429  [ 2300/ 3442]
loss: 0.012281  [ 2400/ 3442]
loss: 0.031796  [ 2500/ 3442]
loss: 0.126979  [ 2600/ 3442]
loss: 0.031094  [ 2700/ 3442]
loss: 0.008498  [ 2800/ 3442]
loss: 0.008929  [ 2900/ 3442]
loss: 0.005302  [ 3000/ 3442]
loss: 0.019305  [ 3100/ 3442]
loss: 0.123319  [ 3200/ 3442]
loss: 0.010622  [ 3300/ 3442]
loss: 0.031195  [ 3400/ 3442]
Epoch 7
-------------------------------
loss: 0.014223  [    0/ 3442]
loss: 0.014339  [  100/ 3442]
loss: 0.030532  [  200/ 3442]
loss: 0.008061  [  300/ 3442]
loss: 0.015478  [  400/ 3442]
loss: 0.009773  [  500/ 3442]
loss: 0.024248  [  600/ 3442]
loss: 0.011114  [  700/ 3442]
loss: 0.016491  [  800/ 3442]
loss: 0.026641  [  900/ 3442]
loss: 0.010418  [ 1000/ 3442]
loss: 0.004398  [ 1100/ 3442]
loss: 0.015943  [ 1200/ 3442]
loss: 0.004017  [ 1300/ 3442]
loss: 0.012311  [ 1400/ 3442]
loss: 0.019880  [ 1500/ 3442]
loss: 0.012724  [ 1600/ 3442]
loss: 0.010288  [ 1700/ 3442]
loss: 0.016700  [ 1800/ 3442]
loss: 0.015960  [ 1900/ 3442]
loss: 0.014727  [ 2000/ 3442]
loss: 0.011042  [ 2100/ 3442]
loss: 0.014786  [ 2200/ 3442]
loss: 0.010143  [ 2300/ 3442]
loss: 0.012375  [ 2400/ 3442]
loss: 0.030874  [ 2500/ 3442]
loss: 0.126305  [ 2600/ 3442]
loss: 0.032003  [ 2700/ 3442]
loss: 0.008447  [ 2800/ 3442]
loss: 0.008508  [ 2900/ 3442]
loss: 0.005310  [ 3000/ 3442]
loss: 0.019255  [ 3100/ 3442]
loss: 0.124045  [ 3200/ 3442]
loss: 0.011193  [ 3300/ 3442]
loss: 0.031644  [ 3400/ 3442]
Epoch 8
-------------------------------
loss: 0.015307  [    0/ 3442]
loss: 0.014866  [  100/ 3442]
loss: 0.030600  [  200/ 3442]
loss: 0.007235  [  300/ 3442]
loss: 0.014514  [  400/ 3442]
loss: 0.009813  [  500/ 3442]
loss: 0.024365  [  600/ 3442]
loss: 0.010935  [  700/ 3442]
loss: 0.016363  [  800/ 3442]
loss: 0.027687  [  900/ 3442]
loss: 0.009343  [ 1000/ 3442]
loss: 0.003926  [ 1100/ 3442]
loss: 0.015088  [ 1200/ 3442]
loss: 0.004227  [ 1300/ 3442]
loss: 0.011920  [ 1400/ 3442]
loss: 0.019400  [ 1500/ 3442]
loss: 0.012558  [ 1600/ 3442]
loss: 0.010008  [ 1700/ 3442]
loss: 0.016325  [ 1800/ 3442]
loss: 0.015288  [ 1900/ 3442]
loss: 0.014614  [ 2000/ 3442]
loss: 0.011219  [ 2100/ 3442]
loss: 0.013343  [ 2200/ 3442]
loss: 0.009763  [ 2300/ 3442]
loss: 0.011974  [ 2400/ 3442]
loss: 0.029816  [ 2500/ 3442]
loss: 0.125763  [ 2600/ 3442]
loss: 0.032964  [ 2700/ 3442]
loss: 0.008188  [ 2800/ 3442]
loss: 0.008259  [ 2900/ 3442]
loss: 0.005306  [ 3000/ 3442]
loss: 0.019129  [ 3100/ 3442]
loss: 0.124662  [ 3200/ 3442]
loss: 0.011563  [ 3300/ 3442]
loss: 0.033088  [ 3400/ 3442]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3442
First Spike after testing: [ 0.3807169 -0.5655677]
[1 0 0 ... 1 2 0]
[1 2 2 ... 1 0 1]
Cluster 0 Occurrences: 1159; KMEANS: 1115
Cluster 1 Occurrences: 1156; KMEANS: 1107
Cluster 2 Occurrences: 1127; KMEANS: 1220
Centroids: [[1.4109274, -0.56727076], [0.53071153, -0.08671243], [-1.6061516, -0.5367251]]
Centroids: [[-1.6282185, -0.531489], [0.47320583, -0.05700756], [1.4182453, -0.57940817]]
Contingency Matrix: 
[[   0   32 1127]
 [   4 1064   88]
 [1111   11    5]]
[[-1, -1, -1], [4, 1064, -1], [1111, 11, -1]]
[[-1, -1, -1], [-1, 1064, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 2, 2: 0, 1: 1}
New Contingency Matrix: 
[[1127   32    0]
 [  88 1064    4]
 [   5   11 1111]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1127, 1064, 1111], Sum: 3302
All_Elements: [1127, 32, 0, 88, 1064, 4, 5, 11, 1111], Sum: 3442
Accuracy: 0.9593259732713538
Done!
