Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_47_43
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B2610BE10>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x0000019B26154E10>
Epoch 1
-------------------------------
loss: 0.341254  [    0/ 3475]
loss: 0.144524  [  100/ 3475]
loss: 0.162245  [  200/ 3475]
loss: 0.221497  [  300/ 3475]
loss: 0.188573  [  400/ 3475]
loss: 0.043477  [  500/ 3475]
loss: 0.062248  [  600/ 3475]
loss: 0.081270  [  700/ 3475]
loss: 0.123896  [  800/ 3475]
loss: 0.128741  [  900/ 3475]
loss: 0.062022  [ 1000/ 3475]
loss: 0.048769  [ 1100/ 3475]
loss: 0.057117  [ 1200/ 3475]
loss: 0.044098  [ 1300/ 3475]
loss: 0.060184  [ 1400/ 3475]
loss: 0.054007  [ 1500/ 3475]
loss: 0.046760  [ 1600/ 3475]
loss: 0.012915  [ 1700/ 3475]
loss: 0.130490  [ 1800/ 3475]
loss: 0.039884  [ 1900/ 3475]
loss: 0.018941  [ 2000/ 3475]
loss: 0.043863  [ 2100/ 3475]
loss: 0.079627  [ 2200/ 3475]
loss: 0.027294  [ 2300/ 3475]
loss: 0.014557  [ 2400/ 3475]
loss: 0.079552  [ 2500/ 3475]
loss: 0.034199  [ 2600/ 3475]
loss: 0.094948  [ 2700/ 3475]
loss: 0.058446  [ 2800/ 3475]
loss: 0.034989  [ 2900/ 3475]
loss: 0.049910  [ 3000/ 3475]
loss: 0.076603  [ 3100/ 3475]
loss: 0.073540  [ 3200/ 3475]
loss: 0.016048  [ 3300/ 3475]
loss: 0.028038  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.029411  [    0/ 3475]
loss: 0.036431  [  100/ 3475]
loss: 0.059678  [  200/ 3475]
loss: 0.221010  [  300/ 3475]
loss: 0.033335  [  400/ 3475]
loss: 0.029851  [  500/ 3475]
loss: 0.019860  [  600/ 3475]
loss: 0.042350  [  700/ 3475]
loss: 0.103506  [  800/ 3475]
loss: 0.059554  [  900/ 3475]
loss: 0.052046  [ 1000/ 3475]
loss: 0.029008  [ 1100/ 3475]
loss: 0.029369  [ 1200/ 3475]
loss: 0.046622  [ 1300/ 3475]
loss: 0.048332  [ 1400/ 3475]
loss: 0.040266  [ 1500/ 3475]
loss: 0.049296  [ 1600/ 3475]
loss: 0.012063  [ 1700/ 3475]
loss: 0.113346  [ 1800/ 3475]
loss: 0.039360  [ 1900/ 3475]
loss: 0.019338  [ 2000/ 3475]
loss: 0.045439  [ 2100/ 3475]
loss: 0.078843  [ 2200/ 3475]
loss: 0.021444  [ 2300/ 3475]
loss: 0.015121  [ 2400/ 3475]
loss: 0.081977  [ 2500/ 3475]
loss: 0.036399  [ 2600/ 3475]
loss: 0.085392  [ 2700/ 3475]
loss: 0.060139  [ 2800/ 3475]
loss: 0.031822  [ 2900/ 3475]
loss: 0.050434  [ 3000/ 3475]
loss: 0.070441  [ 3100/ 3475]
loss: 0.065330  [ 3200/ 3475]
loss: 0.017543  [ 3300/ 3475]
loss: 0.027914  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.027990  [    0/ 3475]
loss: 0.035075  [  100/ 3475]
loss: 0.060334  [  200/ 3475]
loss: 0.221395  [  300/ 3475]
loss: 0.029842  [  400/ 3475]
loss: 0.029581  [  500/ 3475]
loss: 0.015571  [  600/ 3475]
loss: 0.041768  [  700/ 3475]
loss: 0.102761  [  800/ 3475]
loss: 0.058870  [  900/ 3475]
loss: 0.051028  [ 1000/ 3475]
loss: 0.027534  [ 1100/ 3475]
loss: 0.028506  [ 1200/ 3475]
loss: 0.041367  [ 1300/ 3475]
loss: 0.045806  [ 1400/ 3475]
loss: 0.037153  [ 1500/ 3475]
loss: 0.051889  [ 1600/ 3475]
loss: 0.011709  [ 1700/ 3475]
loss: 0.109838  [ 1800/ 3475]
loss: 0.038142  [ 1900/ 3475]
loss: 0.019412  [ 2000/ 3475]
loss: 0.044383  [ 2100/ 3475]
loss: 0.078340  [ 2200/ 3475]
loss: 0.020148  [ 2300/ 3475]
loss: 0.015398  [ 2400/ 3475]
loss: 0.080574  [ 2500/ 3475]
loss: 0.035892  [ 2600/ 3475]
loss: 0.075416  [ 2700/ 3475]
loss: 0.063853  [ 2800/ 3475]
loss: 0.031488  [ 2900/ 3475]
loss: 0.051143  [ 3000/ 3475]
loss: 0.067958  [ 3100/ 3475]
loss: 0.061839  [ 3200/ 3475]
loss: 0.017246  [ 3300/ 3475]
loss: 0.027677  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.028130  [    0/ 3475]
loss: 0.034839  [  100/ 3475]
loss: 0.059535  [  200/ 3475]
loss: 0.221019  [  300/ 3475]
loss: 0.030700  [  400/ 3475]
loss: 0.028659  [  500/ 3475]
loss: 0.015784  [  600/ 3475]
loss: 0.046126  [  700/ 3475]
loss: 0.102794  [  800/ 3475]
loss: 0.059800  [  900/ 3475]
loss: 0.052003  [ 1000/ 3475]
loss: 0.026307  [ 1100/ 3475]
loss: 0.028426  [ 1200/ 3475]
loss: 0.054794  [ 1300/ 3475]
loss: 0.043338  [ 1400/ 3475]
loss: 0.035363  [ 1500/ 3475]
loss: 0.054472  [ 1600/ 3475]
loss: 0.011050  [ 1700/ 3475]
loss: 0.109903  [ 1800/ 3475]
loss: 0.037917  [ 1900/ 3475]
loss: 0.019422  [ 2000/ 3475]
loss: 0.044142  [ 2100/ 3475]
loss: 0.080274  [ 2200/ 3475]
loss: 0.020467  [ 2300/ 3475]
loss: 0.015929  [ 2400/ 3475]
loss: 0.080038  [ 2500/ 3475]
loss: 0.035241  [ 2600/ 3475]
loss: 0.060275  [ 2700/ 3475]
loss: 0.065681  [ 2800/ 3475]
loss: 0.030003  [ 2900/ 3475]
loss: 0.051406  [ 3000/ 3475]
loss: 0.063875  [ 3100/ 3475]
loss: 0.054890  [ 3200/ 3475]
loss: 0.010601  [ 3300/ 3475]
loss: 0.026953  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.027512  [    0/ 3475]
loss: 0.034535  [  100/ 3475]
loss: 0.060426  [  200/ 3475]
loss: 0.222142  [  300/ 3475]
loss: 0.029634  [  400/ 3475]
loss: 0.030303  [  500/ 3475]
loss: 0.016765  [  600/ 3475]
loss: 0.057437  [  700/ 3475]
loss: 0.102980  [  800/ 3475]
loss: 0.060309  [  900/ 3475]
loss: 0.051450  [ 1000/ 3475]
loss: 0.025982  [ 1100/ 3475]
loss: 0.027340  [ 1200/ 3475]
loss: 0.048856  [ 1300/ 3475]
loss: 0.041965  [ 1400/ 3475]
loss: 0.035661  [ 1500/ 3475]
loss: 0.054052  [ 1600/ 3475]
loss: 0.011798  [ 1700/ 3475]
loss: 0.112255  [ 1800/ 3475]
loss: 0.037722  [ 1900/ 3475]
loss: 0.018726  [ 2000/ 3475]
loss: 0.044735  [ 2100/ 3475]
loss: 0.080364  [ 2200/ 3475]
loss: 0.018300  [ 2300/ 3475]
loss: 0.015449  [ 2400/ 3475]
loss: 0.079774  [ 2500/ 3475]
loss: 0.035111  [ 2600/ 3475]
loss: 0.069858  [ 2700/ 3475]
loss: 0.056256  [ 2800/ 3475]
loss: 0.029016  [ 2900/ 3475]
loss: 0.051065  [ 3000/ 3475]
loss: 0.060929  [ 3100/ 3475]
loss: 0.049593  [ 3200/ 3475]
loss: 0.006786  [ 3300/ 3475]
loss: 0.025962  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.024984  [    0/ 3475]
loss: 0.034819  [  100/ 3475]
loss: 0.060622  [  200/ 3475]
loss: 0.222865  [  300/ 3475]
loss: 0.026609  [  400/ 3475]
loss: 0.031136  [  500/ 3475]
loss: 0.019077  [  600/ 3475]
loss: 0.068240  [  700/ 3475]
loss: 0.104156  [  800/ 3475]
loss: 0.060274  [  900/ 3475]
loss: 0.055333  [ 1000/ 3475]
loss: 0.026508  [ 1100/ 3475]
loss: 0.026937  [ 1200/ 3475]
loss: 0.054206  [ 1300/ 3475]
loss: 0.040817  [ 1400/ 3475]
loss: 0.036920  [ 1500/ 3475]
loss: 0.051287  [ 1600/ 3475]
loss: 0.012429  [ 1700/ 3475]
loss: 0.105891  [ 1800/ 3475]
loss: 0.037844  [ 1900/ 3475]
loss: 0.017893  [ 2000/ 3475]
loss: 0.045262  [ 2100/ 3475]
loss: 0.078383  [ 2200/ 3475]
loss: 0.017329  [ 2300/ 3475]
loss: 0.015285  [ 2400/ 3475]
loss: 0.079215  [ 2500/ 3475]
loss: 0.035296  [ 2600/ 3475]
loss: 0.088442  [ 2700/ 3475]
loss: 0.047673  [ 2800/ 3475]
loss: 0.028318  [ 2900/ 3475]
loss: 0.050952  [ 3000/ 3475]
loss: 0.061406  [ 3100/ 3475]
loss: 0.046831  [ 3200/ 3475]
loss: 0.007463  [ 3300/ 3475]
loss: 0.024884  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.022843  [    0/ 3475]
loss: 0.035298  [  100/ 3475]
loss: 0.058470  [  200/ 3475]
loss: 0.222309  [  300/ 3475]
loss: 0.021561  [  400/ 3475]
loss: 0.031116  [  500/ 3475]
loss: 0.023882  [  600/ 3475]
loss: 0.057303  [  700/ 3475]
loss: 0.104727  [  800/ 3475]
loss: 0.057198  [  900/ 3475]
loss: 0.048896  [ 1000/ 3475]
loss: 0.024620  [ 1100/ 3475]
loss: 0.023752  [ 1200/ 3475]
loss: 0.039016  [ 1300/ 3475]
loss: 0.036971  [ 1400/ 3475]
loss: 0.037546  [ 1500/ 3475]
loss: 0.058611  [ 1600/ 3475]
loss: 0.013482  [ 1700/ 3475]
loss: 0.099928  [ 1800/ 3475]
loss: 0.038001  [ 1900/ 3475]
loss: 0.017962  [ 2000/ 3475]
loss: 0.044777  [ 2100/ 3475]
loss: 0.080510  [ 2200/ 3475]
loss: 0.017046  [ 2300/ 3475]
loss: 0.015071  [ 2400/ 3475]
loss: 0.079547  [ 2500/ 3475]
loss: 0.036234  [ 2600/ 3475]
loss: 0.098204  [ 2700/ 3475]
loss: 0.045219  [ 2800/ 3475]
loss: 0.028790  [ 2900/ 3475]
loss: 0.051048  [ 3000/ 3475]
loss: 0.060863  [ 3100/ 3475]
loss: 0.044023  [ 3200/ 3475]
loss: 0.008711  [ 3300/ 3475]
loss: 0.024423  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.020988  [    0/ 3475]
loss: 0.036249  [  100/ 3475]
loss: 0.059601  [  200/ 3475]
loss: 0.220226  [  300/ 3475]
loss: 0.018389  [  400/ 3475]
loss: 0.031412  [  500/ 3475]
loss: 0.020162  [  600/ 3475]
loss: 0.057737  [  700/ 3475]
loss: 0.104610  [  800/ 3475]
loss: 0.055436  [  900/ 3475]
loss: 0.051467  [ 1000/ 3475]
loss: 0.025578  [ 1100/ 3475]
loss: 0.025908  [ 1200/ 3475]
loss: 0.045009  [ 1300/ 3475]
loss: 0.034764  [ 1400/ 3475]
loss: 0.039217  [ 1500/ 3475]
loss: 0.048701  [ 1600/ 3475]
loss: 0.012821  [ 1700/ 3475]
loss: 0.088504  [ 1800/ 3475]
loss: 0.037005  [ 1900/ 3475]
loss: 0.016930  [ 2000/ 3475]
loss: 0.036567  [ 2100/ 3475]
loss: 0.078179  [ 2200/ 3475]
loss: 0.016934  [ 2300/ 3475]
loss: 0.015157  [ 2400/ 3475]
loss: 0.078632  [ 2500/ 3475]
loss: 0.035272  [ 2600/ 3475]
loss: 0.112828  [ 2700/ 3475]
loss: 0.037279  [ 2800/ 3475]
loss: 0.028045  [ 2900/ 3475]
loss: 0.051021  [ 3000/ 3475]
loss: 0.065268  [ 3100/ 3475]
loss: 0.046620  [ 3200/ 3475]
loss: 0.013498  [ 3300/ 3475]
loss: 0.025273  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [-0.21517307 -1.3561716 ]
[2 1 2 ... 1 0 1]
[1 0 1 ... 2 1 0]
Cluster 0 Occurrences: 1162; KMEANS: 631
Cluster 1 Occurrences: 1164; KMEANS: 2340
Cluster 2 Occurrences: 1149; KMEANS: 504
Centroids: [[-0.8826747, 0.15053447], [3.0617802, 2.6848605], [-1.2597854, -1.0315561]]
Centroids: [[2.2992072, 2.15748], [-1.0653164, -0.43029028], [4.2317286, 3.492758]]
Contingency Matrix: 
[[   0 1162    0]
 [ 627   33  504]
 [   4 1145    0]]
[[-1, -1, -1], [627, -1, 504], [4, -1, 0]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, 0]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 1, 1: 0, 2: 2}
New Contingency Matrix: 
[[1162    0    0]
 [  33  627  504]
 [1145    4    0]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1162, 627, 0], Sum: 1789
All_Elements: [1162, 0, 0, 33, 627, 504, 1145, 4, 0], Sum: 3475
Accuracy: 0.5148201438848921
Done!
