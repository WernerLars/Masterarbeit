Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Easy1_noise025.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_46_35
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B257DE048>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
<torch.utils.data.dataloader.DataLoader object at 0x0000019B26154FD0>
Epoch 1
-------------------------------
loss: 0.203598  [    0/ 3298]
loss: 0.256255  [  100/ 3298]
loss: 0.126213  [  200/ 3298]
loss: 0.054244  [  300/ 3298]
loss: 0.039923  [  400/ 3298]
loss: 0.037756  [  500/ 3298]
loss: 0.029788  [  600/ 3298]
loss: 0.082281  [  700/ 3298]
loss: 0.030044  [  800/ 3298]
loss: 0.107313  [  900/ 3298]
loss: 0.033477  [ 1000/ 3298]
loss: 0.054975  [ 1100/ 3298]
loss: 0.073594  [ 1200/ 3298]
loss: 0.036363  [ 1300/ 3298]
loss: 0.012909  [ 1400/ 3298]
loss: 0.025884  [ 1500/ 3298]
loss: 0.014712  [ 1600/ 3298]
loss: 0.036329  [ 1700/ 3298]
loss: 0.040672  [ 1800/ 3298]
loss: 0.026569  [ 1900/ 3298]
loss: 0.010692  [ 2000/ 3298]
loss: 0.013534  [ 2100/ 3298]
loss: 0.014459  [ 2200/ 3298]
loss: 0.022338  [ 2300/ 3298]
loss: 0.015336  [ 2400/ 3298]
loss: 0.028749  [ 2500/ 3298]
loss: 0.033204  [ 2600/ 3298]
loss: 0.042931  [ 2700/ 3298]
loss: 0.266476  [ 2800/ 3298]
loss: 0.020065  [ 2900/ 3298]
loss: 0.038212  [ 3000/ 3298]
loss: 0.023422  [ 3100/ 3298]
loss: 0.068955  [ 3200/ 3298]
Epoch 2
-------------------------------
loss: 0.022279  [    0/ 3298]
loss: 0.021904  [  100/ 3298]
loss: 0.108256  [  200/ 3298]
loss: 0.016275  [  300/ 3298]
loss: 0.022499  [  400/ 3298]
loss: 0.021457  [  500/ 3298]
loss: 0.021598  [  600/ 3298]
loss: 0.063576  [  700/ 3298]
loss: 0.025142  [  800/ 3298]
loss: 0.109871  [  900/ 3298]
loss: 0.032775  [ 1000/ 3298]
loss: 0.049872  [ 1100/ 3298]
loss: 0.067077  [ 1200/ 3298]
loss: 0.023289  [ 1300/ 3298]
loss: 0.010911  [ 1400/ 3298]
loss: 0.022078  [ 1500/ 3298]
loss: 0.019040  [ 1600/ 3298]
loss: 0.044386  [ 1700/ 3298]
loss: 0.035786  [ 1800/ 3298]
loss: 0.022419  [ 1900/ 3298]
loss: 0.009652  [ 2000/ 3298]
loss: 0.013557  [ 2100/ 3298]
loss: 0.015590  [ 2200/ 3298]
loss: 0.021383  [ 2300/ 3298]
loss: 0.017294  [ 2400/ 3298]
loss: 0.014574  [ 2500/ 3298]
loss: 0.029913  [ 2600/ 3298]
loss: 0.046991  [ 2700/ 3298]
loss: 0.275975  [ 2800/ 3298]
loss: 0.021563  [ 2900/ 3298]
loss: 0.037010  [ 3000/ 3298]
loss: 0.024129  [ 3100/ 3298]
loss: 0.055813  [ 3200/ 3298]
Epoch 3
-------------------------------
loss: 0.019637  [    0/ 3298]
loss: 0.020455  [  100/ 3298]
loss: 0.107516  [  200/ 3298]
loss: 0.014253  [  300/ 3298]
loss: 0.018978  [  400/ 3298]
loss: 0.019873  [  500/ 3298]
loss: 0.021519  [  600/ 3298]
loss: 0.063692  [  700/ 3298]
loss: 0.023762  [  800/ 3298]
loss: 0.101830  [  900/ 3298]
loss: 0.033708  [ 1000/ 3298]
loss: 0.042256  [ 1100/ 3298]
loss: 0.068166  [ 1200/ 3298]
loss: 0.024710  [ 1300/ 3298]
loss: 0.010539  [ 1400/ 3298]
loss: 0.024097  [ 1500/ 3298]
loss: 0.017499  [ 1600/ 3298]
loss: 0.041723  [ 1700/ 3298]
loss: 0.034785  [ 1800/ 3298]
loss: 0.022382  [ 1900/ 3298]
loss: 0.009199  [ 2000/ 3298]
loss: 0.013924  [ 2100/ 3298]
loss: 0.015658  [ 2200/ 3298]
loss: 0.018895  [ 2300/ 3298]
loss: 0.017689  [ 2400/ 3298]
loss: 0.013819  [ 2500/ 3298]
loss: 0.028823  [ 2600/ 3298]
loss: 0.046721  [ 2700/ 3298]
loss: 0.249158  [ 2800/ 3298]
loss: 0.022021  [ 2900/ 3298]
loss: 0.036592  [ 3000/ 3298]
loss: 0.024369  [ 3100/ 3298]
loss: 0.053784  [ 3200/ 3298]
Epoch 4
-------------------------------
loss: 0.017293  [    0/ 3298]
loss: 0.019534  [  100/ 3298]
loss: 0.107660  [  200/ 3298]
loss: 0.012660  [  300/ 3298]
loss: 0.015760  [  400/ 3298]
loss: 0.018868  [  500/ 3298]
loss: 0.022110  [  600/ 3298]
loss: 0.062617  [  700/ 3298]
loss: 0.022842  [  800/ 3298]
loss: 0.084466  [  900/ 3298]
loss: 0.033072  [ 1000/ 3298]
loss: 0.028202  [ 1100/ 3298]
loss: 0.069142  [ 1200/ 3298]
loss: 0.025847  [ 1300/ 3298]
loss: 0.010448  [ 1400/ 3298]
loss: 0.023162  [ 1500/ 3298]
loss: 0.015457  [ 1600/ 3298]
loss: 0.030981  [ 1700/ 3298]
loss: 0.036958  [ 1800/ 3298]
loss: 0.022034  [ 1900/ 3298]
loss: 0.009488  [ 2000/ 3298]
loss: 0.014163  [ 2100/ 3298]
loss: 0.015693  [ 2200/ 3298]
loss: 0.017013  [ 2300/ 3298]
loss: 0.017762  [ 2400/ 3298]
loss: 0.015669  [ 2500/ 3298]
loss: 0.028097  [ 2600/ 3298]
loss: 0.046422  [ 2700/ 3298]
loss: 0.202086  [ 2800/ 3298]
loss: 0.021887  [ 2900/ 3298]
loss: 0.036358  [ 3000/ 3298]
loss: 0.024467  [ 3100/ 3298]
loss: 0.053217  [ 3200/ 3298]
Epoch 5
-------------------------------
loss: 0.018950  [    0/ 3298]
loss: 0.021691  [  100/ 3298]
loss: 0.106554  [  200/ 3298]
loss: 0.012268  [  300/ 3298]
loss: 0.012030  [  400/ 3298]
loss: 0.019126  [  500/ 3298]
loss: 0.022115  [  600/ 3298]
loss: 0.061549  [  700/ 3298]
loss: 0.022295  [  800/ 3298]
loss: 0.062315  [  900/ 3298]
loss: 0.032865  [ 1000/ 3298]
loss: 0.014463  [ 1100/ 3298]
loss: 0.071188  [ 1200/ 3298]
loss: 0.026472  [ 1300/ 3298]
loss: 0.011021  [ 1400/ 3298]
loss: 0.021820  [ 1500/ 3298]
loss: 0.014070  [ 1600/ 3298]
loss: 0.021213  [ 1700/ 3298]
loss: 0.039493  [ 1800/ 3298]
loss: 0.021714  [ 1900/ 3298]
loss: 0.009551  [ 2000/ 3298]
loss: 0.014916  [ 2100/ 3298]
loss: 0.015564  [ 2200/ 3298]
loss: 0.017622  [ 2300/ 3298]
loss: 0.016971  [ 2400/ 3298]
loss: 0.017249  [ 2500/ 3298]
loss: 0.028133  [ 2600/ 3298]
loss: 0.045120  [ 2700/ 3298]
loss: 0.172657  [ 2800/ 3298]
loss: 0.021419  [ 2900/ 3298]
loss: 0.035922  [ 3000/ 3298]
loss: 0.024173  [ 3100/ 3298]
loss: 0.054157  [ 3200/ 3298]
Epoch 6
-------------------------------
loss: 0.021939  [    0/ 3298]
loss: 0.022853  [  100/ 3298]
loss: 0.104426  [  200/ 3298]
loss: 0.013385  [  300/ 3298]
loss: 0.009303  [  400/ 3298]
loss: 0.019589  [  500/ 3298]
loss: 0.022477  [  600/ 3298]
loss: 0.061242  [  700/ 3298]
loss: 0.022171  [  800/ 3298]
loss: 0.043310  [  900/ 3298]
loss: 0.033050  [ 1000/ 3298]
loss: 0.010934  [ 1100/ 3298]
loss: 0.072315  [ 1200/ 3298]
loss: 0.026345  [ 1300/ 3298]
loss: 0.011452  [ 1400/ 3298]
loss: 0.020355  [ 1500/ 3298]
loss: 0.014653  [ 1600/ 3298]
loss: 0.017810  [ 1700/ 3298]
loss: 0.040970  [ 1800/ 3298]
loss: 0.021528  [ 1900/ 3298]
loss: 0.009927  [ 2000/ 3298]
loss: 0.015963  [ 2100/ 3298]
loss: 0.014357  [ 2200/ 3298]
loss: 0.017974  [ 2300/ 3298]
loss: 0.015382  [ 2400/ 3298]
loss: 0.019764  [ 2500/ 3298]
loss: 0.028981  [ 2600/ 3298]
loss: 0.044640  [ 2700/ 3298]
loss: 0.157252  [ 2800/ 3298]
loss: 0.022311  [ 2900/ 3298]
loss: 0.034875  [ 3000/ 3298]
loss: 0.024691  [ 3100/ 3298]
loss: 0.053621  [ 3200/ 3298]
Epoch 7
-------------------------------
loss: 0.023918  [    0/ 3298]
loss: 0.023697  [  100/ 3298]
loss: 0.103870  [  200/ 3298]
loss: 0.014198  [  300/ 3298]
loss: 0.008678  [  400/ 3298]
loss: 0.019790  [  500/ 3298]
loss: 0.022505  [  600/ 3298]
loss: 0.061508  [  700/ 3298]
loss: 0.022256  [  800/ 3298]
loss: 0.030733  [  900/ 3298]
loss: 0.033366  [ 1000/ 3298]
loss: 0.012459  [ 1100/ 3298]
loss: 0.072950  [ 1200/ 3298]
loss: 0.025801  [ 1300/ 3298]
loss: 0.011742  [ 1400/ 3298]
loss: 0.019497  [ 1500/ 3298]
loss: 0.015123  [ 1600/ 3298]
loss: 0.016980  [ 1700/ 3298]
loss: 0.038410  [ 1800/ 3298]
loss: 0.021651  [ 1900/ 3298]
loss: 0.010204  [ 2000/ 3298]
loss: 0.015815  [ 2100/ 3298]
loss: 0.013066  [ 2200/ 3298]
loss: 0.018329  [ 2300/ 3298]
loss: 0.015010  [ 2400/ 3298]
loss: 0.017223  [ 2500/ 3298]
loss: 0.029806  [ 2600/ 3298]
loss: 0.044735  [ 2700/ 3298]
loss: 0.141833  [ 2800/ 3298]
loss: 0.022901  [ 2900/ 3298]
loss: 0.034049  [ 3000/ 3298]
loss: 0.024964  [ 3100/ 3298]
loss: 0.053091  [ 3200/ 3298]
Epoch 8
-------------------------------
loss: 0.025858  [    0/ 3298]
loss: 0.023553  [  100/ 3298]
loss: 0.103835  [  200/ 3298]
loss: 0.014323  [  300/ 3298]
loss: 0.007666  [  400/ 3298]
loss: 0.020494  [  500/ 3298]
loss: 0.022197  [  600/ 3298]
loss: 0.061858  [  700/ 3298]
loss: 0.022390  [  800/ 3298]
loss: 0.022673  [  900/ 3298]
loss: 0.034207  [ 1000/ 3298]
loss: 0.014851  [ 1100/ 3298]
loss: 0.073848  [ 1200/ 3298]
loss: 0.025103  [ 1300/ 3298]
loss: 0.011945  [ 1400/ 3298]
loss: 0.020266  [ 1500/ 3298]
loss: 0.014624  [ 1600/ 3298]
loss: 0.018727  [ 1700/ 3298]
loss: 0.037160  [ 1800/ 3298]
loss: 0.021357  [ 1900/ 3298]
loss: 0.010197  [ 2000/ 3298]
loss: 0.015914  [ 2100/ 3298]
loss: 0.011298  [ 2200/ 3298]
loss: 0.018694  [ 2300/ 3298]
loss: 0.014698  [ 2400/ 3298]
loss: 0.016886  [ 2500/ 3298]
loss: 0.031196  [ 2600/ 3298]
loss: 0.044941  [ 2700/ 3298]
loss: 0.135075  [ 2800/ 3298]
loss: 0.023534  [ 2900/ 3298]
loss: 0.033048  [ 3000/ 3298]
loss: 0.025116  [ 3100/ 3298]
loss: 0.052859  [ 3200/ 3298]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3298
First Spike after testing: [ 1.7636025  -0.07713778]
[1 0 0 ... 0 1 2]
[1 2 0 ... 2 1 0]
Cluster 0 Occurrences: 1094; KMEANS: 1213
Cluster 1 Occurrences: 1089; KMEANS: 1083
Cluster 2 Occurrences: 1115; KMEANS: 1002
Centroids: [[-1.4184921, 0.25587478], [2.086336, 0.7593883], [-0.50386846, -1.5836389]]
Centroids: [[-0.5034165, -1.5032581], [2.1113696, 0.75184673], [-1.5145639, 0.34964624]]
Contingency Matrix: 
[[ 106    0  988]
 [   1 1078   10]
 [1106    5    4]]
[[-1, 0, 988], [-1, 1078, 10], [-1, -1, -1]]
[[-1, -1, 988], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 1: 1, 0: 2}
New Contingency Matrix: 
[[ 988    0  106]
 [  10 1078    1]
 [   4    5 1106]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [988, 1078, 1106], Sum: 3172
All_Elements: [988, 0, 106, 10, 1078, 1, 4, 5, 1106], Sum: 3298
Accuracy: 0.9617950272892662
Done!
