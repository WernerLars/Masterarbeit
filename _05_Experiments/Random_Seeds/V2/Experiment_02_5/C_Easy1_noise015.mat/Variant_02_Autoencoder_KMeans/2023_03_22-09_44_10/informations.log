Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Easy1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_44_10
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B2610B438>
Sampling rate: 24000.0
Raw: [-0.11561686 -0.09151516 -0.07003629 ...  0.13067092  0.07286933
  0.02376508]
Times: [   1418    2718    2965 ... 1438324 1439204 1439256]
Cluster: [2 1 3 ... 2 2 2]
Number of different clusters:  3
Number of Spikes: 3477
First aligned Spike Frame: [-0.21672249 -0.20435022 -0.20773448 -0.23066605 -0.25048766 -0.24897994
 -0.235203   -0.22454461 -0.22637624 -0.23567647 -0.24458052 -0.29008047
 -0.46277163 -0.78005294 -1.10886208 -1.22520407 -0.93276888 -0.30507988
  0.28404034  0.5598609   0.56326036  0.46868005  0.38002586  0.308291
  0.2337485   0.15145072  0.07073965  0.00289921 -0.04579903 -0.0801131
 -0.10431654 -0.10729234 -0.08281733 -0.04721634 -0.02197862 -0.01600473
 -0.0234669  -0.0435982  -0.07322802 -0.10283475 -0.12412902 -0.14133481
 -0.1572087  -0.1697764  -0.17533489 -0.18293644 -0.19999581]
Cluster 0, Occurrences: 1132
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1157
<torch.utils.data.dataloader.DataLoader object at 0x0000019B26154198>
Epoch 1
-------------------------------
loss: 0.170888  [    0/ 3477]
loss: 0.194805  [  100/ 3477]
loss: 0.082651  [  200/ 3477]
loss: 0.094818  [  300/ 3477]
loss: 0.031780  [  400/ 3477]
loss: 0.053526  [  500/ 3477]
loss: 0.021552  [  600/ 3477]
loss: 0.026524  [  700/ 3477]
loss: 0.024934  [  800/ 3477]
loss: 0.020004  [  900/ 3477]
loss: 0.023960  [ 1000/ 3477]
loss: 0.005329  [ 1100/ 3477]
loss: 0.020553  [ 1200/ 3477]
loss: 0.020676  [ 1300/ 3477]
loss: 0.016234  [ 1400/ 3477]
loss: 0.019447  [ 1500/ 3477]
loss: 0.015997  [ 1600/ 3477]
loss: 0.014261  [ 1700/ 3477]
loss: 0.011664  [ 1800/ 3477]
loss: 0.012833  [ 1900/ 3477]
loss: 0.019271  [ 2000/ 3477]
loss: 0.020440  [ 2100/ 3477]
loss: 0.022378  [ 2200/ 3477]
loss: 0.013093  [ 2300/ 3477]
loss: 0.007657  [ 2400/ 3477]
loss: 0.076196  [ 2500/ 3477]
loss: 0.069733  [ 2600/ 3477]
loss: 0.006981  [ 2700/ 3477]
loss: 0.216538  [ 2800/ 3477]
loss: 0.018156  [ 2900/ 3477]
loss: 0.018268  [ 3000/ 3477]
loss: 0.013634  [ 3100/ 3477]
loss: 0.104092  [ 3200/ 3477]
loss: 0.015035  [ 3300/ 3477]
loss: 0.006545  [ 3400/ 3477]
Epoch 2
-------------------------------
loss: 0.011262  [    0/ 3477]
loss: 0.025015  [  100/ 3477]
loss: 0.023323  [  200/ 3477]
loss: 0.030995  [  300/ 3477]
loss: 0.018173  [  400/ 3477]
loss: 0.008695  [  500/ 3477]
loss: 0.014272  [  600/ 3477]
loss: 0.024035  [  700/ 3477]
loss: 0.019114  [  800/ 3477]
loss: 0.019445  [  900/ 3477]
loss: 0.018546  [ 1000/ 3477]
loss: 0.004587  [ 1100/ 3477]
loss: 0.016889  [ 1200/ 3477]
loss: 0.017450  [ 1300/ 3477]
loss: 0.013254  [ 1400/ 3477]
loss: 0.017712  [ 1500/ 3477]
loss: 0.020159  [ 1600/ 3477]
loss: 0.013832  [ 1700/ 3477]
loss: 0.008589  [ 1800/ 3477]
loss: 0.009620  [ 1900/ 3477]
loss: 0.019063  [ 2000/ 3477]
loss: 0.015461  [ 2100/ 3477]
loss: 0.022887  [ 2200/ 3477]
loss: 0.013082  [ 2300/ 3477]
loss: 0.006490  [ 2400/ 3477]
loss: 0.072643  [ 2500/ 3477]
loss: 0.067630  [ 2600/ 3477]
loss: 0.004648  [ 2700/ 3477]
loss: 0.182042  [ 2800/ 3477]
loss: 0.019141  [ 2900/ 3477]
loss: 0.017546  [ 3000/ 3477]
loss: 0.011981  [ 3100/ 3477]
loss: 0.097007  [ 3200/ 3477]
loss: 0.013427  [ 3300/ 3477]
loss: 0.007567  [ 3400/ 3477]
Epoch 3
-------------------------------
loss: 0.008584  [    0/ 3477]
loss: 0.019823  [  100/ 3477]
loss: 0.018984  [  200/ 3477]
loss: 0.032487  [  300/ 3477]
loss: 0.018415  [  400/ 3477]
loss: 0.007129  [  500/ 3477]
loss: 0.012187  [  600/ 3477]
loss: 0.023568  [  700/ 3477]
loss: 0.017426  [  800/ 3477]
loss: 0.018111  [  900/ 3477]
loss: 0.012836  [ 1000/ 3477]
loss: 0.003894  [ 1100/ 3477]
loss: 0.017287  [ 1200/ 3477]
loss: 0.014748  [ 1300/ 3477]
loss: 0.012312  [ 1400/ 3477]
loss: 0.016092  [ 1500/ 3477]
loss: 0.023457  [ 1600/ 3477]
loss: 0.011832  [ 1700/ 3477]
loss: 0.006739  [ 1800/ 3477]
loss: 0.011057  [ 1900/ 3477]
loss: 0.020649  [ 2000/ 3477]
loss: 0.012433  [ 2100/ 3477]
loss: 0.022891  [ 2200/ 3477]
loss: 0.013406  [ 2300/ 3477]
loss: 0.007572  [ 2400/ 3477]
loss: 0.076548  [ 2500/ 3477]
loss: 0.065241  [ 2600/ 3477]
loss: 0.004159  [ 2700/ 3477]
loss: 0.140801  [ 2800/ 3477]
loss: 0.019795  [ 2900/ 3477]
loss: 0.016640  [ 3000/ 3477]
loss: 0.011648  [ 3100/ 3477]
loss: 0.091550  [ 3200/ 3477]
loss: 0.011925  [ 3300/ 3477]
loss: 0.008057  [ 3400/ 3477]
Epoch 4
-------------------------------
loss: 0.007725  [    0/ 3477]
loss: 0.017593  [  100/ 3477]
loss: 0.015344  [  200/ 3477]
loss: 0.031537  [  300/ 3477]
loss: 0.016749  [  400/ 3477]
loss: 0.006646  [  500/ 3477]
loss: 0.010264  [  600/ 3477]
loss: 0.021970  [  700/ 3477]
loss: 0.017184  [  800/ 3477]
loss: 0.017667  [  900/ 3477]
loss: 0.009487  [ 1000/ 3477]
loss: 0.003179  [ 1100/ 3477]
loss: 0.017149  [ 1200/ 3477]
loss: 0.012276  [ 1300/ 3477]
loss: 0.012014  [ 1400/ 3477]
loss: 0.017539  [ 1500/ 3477]
loss: 0.015303  [ 1600/ 3477]
loss: 0.011013  [ 1700/ 3477]
loss: 0.006352  [ 1800/ 3477]
loss: 0.011965  [ 1900/ 3477]
loss: 0.022533  [ 2000/ 3477]
loss: 0.009849  [ 2100/ 3477]
loss: 0.022277  [ 2200/ 3477]
loss: 0.013803  [ 2300/ 3477]
loss: 0.008601  [ 2400/ 3477]
loss: 0.080114  [ 2500/ 3477]
loss: 0.066830  [ 2600/ 3477]
loss: 0.003646  [ 2700/ 3477]
loss: 0.085467  [ 2800/ 3477]
loss: 0.020263  [ 2900/ 3477]
loss: 0.016007  [ 3000/ 3477]
loss: 0.011773  [ 3100/ 3477]
loss: 0.087772  [ 3200/ 3477]
loss: 0.010010  [ 3300/ 3477]
loss: 0.008469  [ 3400/ 3477]
Epoch 5
-------------------------------
loss: 0.008403  [    0/ 3477]
loss: 0.015130  [  100/ 3477]
loss: 0.014236  [  200/ 3477]
loss: 0.030599  [  300/ 3477]
loss: 0.017406  [  400/ 3477]
loss: 0.006311  [  500/ 3477]
loss: 0.007871  [  600/ 3477]
loss: 0.020655  [  700/ 3477]
loss: 0.018399  [  800/ 3477]
loss: 0.017127  [  900/ 3477]
loss: 0.008820  [ 1000/ 3477]
loss: 0.002440  [ 1100/ 3477]
loss: 0.017085  [ 1200/ 3477]
loss: 0.010690  [ 1300/ 3477]
loss: 0.012207  [ 1400/ 3477]
loss: 0.018182  [ 1500/ 3477]
loss: 0.013793  [ 1600/ 3477]
loss: 0.009821  [ 1700/ 3477]
loss: 0.006742  [ 1800/ 3477]
loss: 0.011594  [ 1900/ 3477]
loss: 0.022658  [ 2000/ 3477]
loss: 0.008120  [ 2100/ 3477]
loss: 0.022116  [ 2200/ 3477]
loss: 0.014058  [ 2300/ 3477]
loss: 0.008781  [ 2400/ 3477]
loss: 0.081199  [ 2500/ 3477]
loss: 0.065197  [ 2600/ 3477]
loss: 0.003466  [ 2700/ 3477]
loss: 0.055512  [ 2800/ 3477]
loss: 0.020185  [ 2900/ 3477]
loss: 0.015684  [ 3000/ 3477]
loss: 0.012498  [ 3100/ 3477]
loss: 0.086975  [ 3200/ 3477]
loss: 0.008362  [ 3300/ 3477]
loss: 0.008947  [ 3400/ 3477]
Epoch 6
-------------------------------
loss: 0.008975  [    0/ 3477]
loss: 0.015020  [  100/ 3477]
loss: 0.014271  [  200/ 3477]
loss: 0.029610  [  300/ 3477]
loss: 0.017323  [  400/ 3477]
loss: 0.006461  [  500/ 3477]
loss: 0.006783  [  600/ 3477]
loss: 0.019728  [  700/ 3477]
loss: 0.019531  [  800/ 3477]
loss: 0.015837  [  900/ 3477]
loss: 0.009009  [ 1000/ 3477]
loss: 0.002273  [ 1100/ 3477]
loss: 0.016962  [ 1200/ 3477]
loss: 0.010449  [ 1300/ 3477]
loss: 0.012170  [ 1400/ 3477]
loss: 0.017993  [ 1500/ 3477]
loss: 0.012402  [ 1600/ 3477]
loss: 0.009487  [ 1700/ 3477]
loss: 0.007500  [ 1800/ 3477]
loss: 0.011038  [ 1900/ 3477]
loss: 0.022308  [ 2000/ 3477]
loss: 0.006580  [ 2100/ 3477]
loss: 0.022066  [ 2200/ 3477]
loss: 0.014085  [ 2300/ 3477]
loss: 0.008569  [ 2400/ 3477]
loss: 0.080922  [ 2500/ 3477]
loss: 0.065309  [ 2600/ 3477]
loss: 0.003433  [ 2700/ 3477]
loss: 0.043372  [ 2800/ 3477]
loss: 0.019940  [ 2900/ 3477]
loss: 0.014982  [ 3000/ 3477]
loss: 0.013237  [ 3100/ 3477]
loss: 0.087268  [ 3200/ 3477]
loss: 0.007252  [ 3300/ 3477]
loss: 0.009137  [ 3400/ 3477]
Epoch 7
-------------------------------
loss: 0.009836  [    0/ 3477]
loss: 0.015646  [  100/ 3477]
loss: 0.014456  [  200/ 3477]
loss: 0.030034  [  300/ 3477]
loss: 0.017071  [  400/ 3477]
loss: 0.007010  [  500/ 3477]
loss: 0.006434  [  600/ 3477]
loss: 0.019264  [  700/ 3477]
loss: 0.020278  [  800/ 3477]
loss: 0.014670  [  900/ 3477]
loss: 0.009794  [ 1000/ 3477]
loss: 0.002075  [ 1100/ 3477]
loss: 0.016721  [ 1200/ 3477]
loss: 0.010462  [ 1300/ 3477]
loss: 0.012226  [ 1400/ 3477]
loss: 0.018556  [ 1500/ 3477]
loss: 0.012063  [ 1600/ 3477]
loss: 0.009115  [ 1700/ 3477]
loss: 0.008418  [ 1800/ 3477]
loss: 0.010079  [ 1900/ 3477]
loss: 0.021876  [ 2000/ 3477]
loss: 0.005416  [ 2100/ 3477]
loss: 0.021998  [ 2200/ 3477]
loss: 0.013989  [ 2300/ 3477]
loss: 0.008427  [ 2400/ 3477]
loss: 0.080625  [ 2500/ 3477]
loss: 0.066238  [ 2600/ 3477]
loss: 0.003478  [ 2700/ 3477]
loss: 0.038557  [ 2800/ 3477]
loss: 0.019832  [ 2900/ 3477]
loss: 0.014447  [ 3000/ 3477]
loss: 0.013816  [ 3100/ 3477]
loss: 0.086613  [ 3200/ 3477]
loss: 0.006530  [ 3300/ 3477]
loss: 0.009228  [ 3400/ 3477]
Epoch 8
-------------------------------
loss: 0.010695  [    0/ 3477]
loss: 0.016150  [  100/ 3477]
loss: 0.015049  [  200/ 3477]
loss: 0.029669  [  300/ 3477]
loss: 0.017494  [  400/ 3477]
loss: 0.007411  [  500/ 3477]
loss: 0.006504  [  600/ 3477]
loss: 0.018666  [  700/ 3477]
loss: 0.020573  [  800/ 3477]
loss: 0.014239  [  900/ 3477]
loss: 0.011001  [ 1000/ 3477]
loss: 0.001991  [ 1100/ 3477]
loss: 0.016420  [ 1200/ 3477]
loss: 0.010476  [ 1300/ 3477]
loss: 0.012452  [ 1400/ 3477]
loss: 0.018759  [ 1500/ 3477]
loss: 0.012050  [ 1600/ 3477]
loss: 0.008889  [ 1700/ 3477]
loss: 0.009054  [ 1800/ 3477]
loss: 0.009733  [ 1900/ 3477]
loss: 0.021578  [ 2000/ 3477]
loss: 0.004725  [ 2100/ 3477]
loss: 0.022115  [ 2200/ 3477]
loss: 0.013976  [ 2300/ 3477]
loss: 0.008350  [ 2400/ 3477]
loss: 0.080267  [ 2500/ 3477]
loss: 0.067012  [ 2600/ 3477]
loss: 0.003611  [ 2700/ 3477]
loss: 0.034636  [ 2800/ 3477]
loss: 0.019500  [ 2900/ 3477]
loss: 0.014232  [ 3000/ 3477]
loss: 0.014208  [ 3100/ 3477]
loss: 0.086177  [ 3200/ 3477]
loss: 0.006206  [ 3300/ 3477]
loss: 0.008859  [ 3400/ 3477]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3477
First Spike after testing: [2.0089958 0.8689453]
[1 0 2 ... 1 1 1]
[0 1 2 ... 0 0 0]
Cluster 0 Occurrences: 1132; KMEANS: 1186
Cluster 1 Occurrences: 1188; KMEANS: 1117
Cluster 2 Occurrences: 1157; KMEANS: 1174
Centroids: [[-1.6394465, 0.5602521], [1.655293, 0.4921995], [-1.2446666, -1.294753]]
Centroids: [[1.6660705, 0.4898203], [-1.657387, 0.57798344], [-1.2385885, -1.2824746]]
Contingency Matrix: 
[[   1 1111   20]
 [1181    4    3]
 [   4    2 1151]]
[[-1, 1111, 20], [-1, -1, -1], [-1, 2, 1151]]
[[-1, 1111, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 2: 2, 0: 1}
New Contingency Matrix: 
[[1111    1   20]
 [   4 1181    3]
 [   2    4 1151]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [1111, 1181, 1151], Sum: 3443
All_Elements: [1111, 1, 20, 4, 1181, 3, 2, 4, 1151], Sum: 3477
Accuracy: 0.9902214552775381
Done!
