Experiment_path: Random_Seeds//V2/Experiment_02_5
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Random_Seeds//V2/Experiment_02_5/C_Easy1_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_22-09_45_19
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000019B2B6814E0>
Sampling rate: 24000.0
Raw: [-0.20218342 -0.1653919  -0.13236941 ...  0.26695674  0.20113134
  0.13708332]
Times: [    553     927    1270 ... 1437880 1438309 1439004]
Cluster: [1 2 2 ... 2 2 3]
Number of different clusters:  3
Number of Spikes: 3474
First aligned Spike Frame: [-0.02428298 -0.07468906 -0.10332709 -0.10788142 -0.10649267 -0.11021489
 -0.10987225 -0.08885562 -0.04921868 -0.01240992  0.01146155  0.01660937
  0.02581569  0.2202783   0.78693477  1.36742658  1.33473907  0.72217426
  0.12183007 -0.12754948 -0.13495181 -0.08662948 -0.04057795  0.00340961
  0.02448001  0.00850378 -0.01157346  0.00458874  0.04572819  0.06172643
  0.0301382  -0.01498516 -0.0270755  -0.00657047  0.0093092   0.00369654
 -0.00788818 -0.00582791  0.0080957   0.01954062  0.01611345 -0.00497206
 -0.0357219  -0.0657767  -0.0887014  -0.1049796  -0.12649457]
Cluster 0, Occurrences: 1198
Cluster 1, Occurrences: 1128
Cluster 2, Occurrences: 1148
<torch.utils.data.dataloader.DataLoader object at 0x0000019B26154FD0>
Epoch 1
-------------------------------
loss: 0.194394  [    0/ 3474]
loss: 0.106877  [  100/ 3474]
loss: 0.124598  [  200/ 3474]
loss: 0.106374  [  300/ 3474]
loss: 0.265706  [  400/ 3474]
loss: 0.007968  [  500/ 3474]
loss: 0.048022  [  600/ 3474]
loss: 0.023102  [  700/ 3474]
loss: 0.053960  [  800/ 3474]
loss: 0.207392  [  900/ 3474]
loss: 0.033116  [ 1000/ 3474]
loss: 0.166652  [ 1100/ 3474]
loss: 0.051552  [ 1200/ 3474]
loss: 0.071720  [ 1300/ 3474]
loss: 0.030566  [ 1400/ 3474]
loss: 0.081414  [ 1500/ 3474]
loss: 0.009079  [ 1600/ 3474]
loss: 0.117152  [ 1700/ 3474]
loss: 0.029277  [ 1800/ 3474]
loss: 0.022315  [ 1900/ 3474]
loss: 0.007059  [ 2000/ 3474]
loss: 0.020834  [ 2100/ 3474]
loss: 0.022963  [ 2200/ 3474]
loss: 0.063703  [ 2300/ 3474]
loss: 0.027067  [ 2400/ 3474]
loss: 0.052841  [ 2500/ 3474]
loss: 0.020615  [ 2600/ 3474]
loss: 0.054540  [ 2700/ 3474]
loss: 0.046666  [ 2800/ 3474]
loss: 0.016807  [ 2900/ 3474]
loss: 0.143786  [ 3000/ 3474]
loss: 0.010001  [ 3100/ 3474]
loss: 0.017492  [ 3200/ 3474]
loss: 0.039571  [ 3300/ 3474]
loss: 0.104217  [ 3400/ 3474]
Epoch 2
-------------------------------
loss: 0.033192  [    0/ 3474]
loss: 0.010743  [  100/ 3474]
loss: 0.018909  [  200/ 3474]
loss: 0.061686  [  300/ 3474]
loss: 0.076146  [  400/ 3474]
loss: 0.006561  [  500/ 3474]
loss: 0.043126  [  600/ 3474]
loss: 0.013534  [  700/ 3474]
loss: 0.036952  [  800/ 3474]
loss: 0.197207  [  900/ 3474]
loss: 0.010915  [ 1000/ 3474]
loss: 0.134997  [ 1100/ 3474]
loss: 0.027676  [ 1200/ 3474]
loss: 0.029774  [ 1300/ 3474]
loss: 0.015787  [ 1400/ 3474]
loss: 0.076428  [ 1500/ 3474]
loss: 0.007349  [ 1600/ 3474]
loss: 0.075703  [ 1700/ 3474]
loss: 0.030165  [ 1800/ 3474]
loss: 0.019076  [ 1900/ 3474]
loss: 0.007168  [ 2000/ 3474]
loss: 0.019130  [ 2100/ 3474]
loss: 0.024924  [ 2200/ 3474]
loss: 0.068480  [ 2300/ 3474]
loss: 0.010610  [ 2400/ 3474]
loss: 0.046991  [ 2500/ 3474]
loss: 0.020583  [ 2600/ 3474]
loss: 0.041597  [ 2700/ 3474]
loss: 0.041248  [ 2800/ 3474]
loss: 0.017899  [ 2900/ 3474]
loss: 0.204066  [ 3000/ 3474]
loss: 0.009931  [ 3100/ 3474]
loss: 0.014527  [ 3200/ 3474]
loss: 0.031040  [ 3300/ 3474]
loss: 0.065039  [ 3400/ 3474]
Epoch 3
-------------------------------
loss: 0.030149  [    0/ 3474]
loss: 0.010172  [  100/ 3474]
loss: 0.018566  [  200/ 3474]
loss: 0.062839  [  300/ 3474]
loss: 0.072261  [  400/ 3474]
loss: 0.007271  [  500/ 3474]
loss: 0.041893  [  600/ 3474]
loss: 0.011704  [  700/ 3474]
loss: 0.022917  [  800/ 3474]
loss: 0.187069  [  900/ 3474]
loss: 0.009516  [ 1000/ 3474]
loss: 0.139422  [ 1100/ 3474]
loss: 0.033020  [ 1200/ 3474]
loss: 0.025671  [ 1300/ 3474]
loss: 0.013464  [ 1400/ 3474]
loss: 0.076364  [ 1500/ 3474]
loss: 0.008141  [ 1600/ 3474]
loss: 0.063679  [ 1700/ 3474]
loss: 0.029260  [ 1800/ 3474]
loss: 0.014450  [ 1900/ 3474]
loss: 0.007445  [ 2000/ 3474]
loss: 0.017742  [ 2100/ 3474]
loss: 0.024724  [ 2200/ 3474]
loss: 0.070108  [ 2300/ 3474]
loss: 0.008006  [ 2400/ 3474]
loss: 0.031972  [ 2500/ 3474]
loss: 0.020515  [ 2600/ 3474]
loss: 0.047803  [ 2700/ 3474]
loss: 0.029951  [ 2800/ 3474]
loss: 0.017571  [ 2900/ 3474]
loss: 0.207235  [ 3000/ 3474]
loss: 0.009824  [ 3100/ 3474]
loss: 0.014473  [ 3200/ 3474]
loss: 0.023830  [ 3300/ 3474]
loss: 0.034485  [ 3400/ 3474]
Epoch 4
-------------------------------
loss: 0.030568  [    0/ 3474]
loss: 0.009921  [  100/ 3474]
loss: 0.018871  [  200/ 3474]
loss: 0.063381  [  300/ 3474]
loss: 0.067982  [  400/ 3474]
loss: 0.006870  [  500/ 3474]
loss: 0.037963  [  600/ 3474]
loss: 0.011323  [  700/ 3474]
loss: 0.014721  [  800/ 3474]
loss: 0.184743  [  900/ 3474]
loss: 0.008665  [ 1000/ 3474]
loss: 0.142028  [ 1100/ 3474]
loss: 0.033469  [ 1200/ 3474]
loss: 0.024965  [ 1300/ 3474]
loss: 0.012216  [ 1400/ 3474]
loss: 0.075515  [ 1500/ 3474]
loss: 0.009083  [ 1600/ 3474]
loss: 0.058051  [ 1700/ 3474]
loss: 0.028113  [ 1800/ 3474]
loss: 0.014814  [ 1900/ 3474]
loss: 0.007831  [ 2000/ 3474]
loss: 0.017039  [ 2100/ 3474]
loss: 0.025044  [ 2200/ 3474]
loss: 0.070678  [ 2300/ 3474]
loss: 0.007303  [ 2400/ 3474]
loss: 0.021737  [ 2500/ 3474]
loss: 0.020213  [ 2600/ 3474]
loss: 0.050596  [ 2700/ 3474]
loss: 0.022742  [ 2800/ 3474]
loss: 0.017420  [ 2900/ 3474]
loss: 0.202279  [ 3000/ 3474]
loss: 0.009638  [ 3100/ 3474]
loss: 0.014355  [ 3200/ 3474]
loss: 0.026998  [ 3300/ 3474]
loss: 0.021843  [ 3400/ 3474]
Epoch 5
-------------------------------
loss: 0.029919  [    0/ 3474]
loss: 0.009653  [  100/ 3474]
loss: 0.019447  [  200/ 3474]
loss: 0.062946  [  300/ 3474]
loss: 0.063661  [  400/ 3474]
loss: 0.006486  [  500/ 3474]
loss: 0.033802  [  600/ 3474]
loss: 0.010726  [  700/ 3474]
loss: 0.014209  [  800/ 3474]
loss: 0.185911  [  900/ 3474]
loss: 0.007672  [ 1000/ 3474]
loss: 0.150430  [ 1100/ 3474]
loss: 0.032258  [ 1200/ 3474]
loss: 0.025340  [ 1300/ 3474]
loss: 0.011211  [ 1400/ 3474]
loss: 0.078087  [ 1500/ 3474]
loss: 0.009449  [ 1600/ 3474]
loss: 0.054420  [ 1700/ 3474]
loss: 0.027298  [ 1800/ 3474]
loss: 0.015963  [ 1900/ 3474]
loss: 0.008054  [ 2000/ 3474]
loss: 0.016959  [ 2100/ 3474]
loss: 0.025202  [ 2200/ 3474]
loss: 0.070702  [ 2300/ 3474]
loss: 0.006966  [ 2400/ 3474]
loss: 0.017373  [ 2500/ 3474]
loss: 0.019942  [ 2600/ 3474]
loss: 0.050134  [ 2700/ 3474]
loss: 0.020562  [ 2800/ 3474]
loss: 0.017192  [ 2900/ 3474]
loss: 0.203136  [ 3000/ 3474]
loss: 0.009607  [ 3100/ 3474]
loss: 0.014040  [ 3200/ 3474]
loss: 0.030208  [ 3300/ 3474]
loss: 0.016548  [ 3400/ 3474]
Epoch 6
-------------------------------
loss: 0.029911  [    0/ 3474]
loss: 0.009521  [  100/ 3474]
loss: 0.019582  [  200/ 3474]
loss: 0.062312  [  300/ 3474]
loss: 0.060409  [  400/ 3474]
loss: 0.006131  [  500/ 3474]
loss: 0.031069  [  600/ 3474]
loss: 0.010433  [  700/ 3474]
loss: 0.015559  [  800/ 3474]
loss: 0.186374  [  900/ 3474]
loss: 0.007268  [ 1000/ 3474]
loss: 0.162231  [ 1100/ 3474]
loss: 0.030900  [ 1200/ 3474]
loss: 0.025376  [ 1300/ 3474]
loss: 0.010564  [ 1400/ 3474]
loss: 0.078490  [ 1500/ 3474]
loss: 0.009709  [ 1600/ 3474]
loss: 0.051422  [ 1700/ 3474]
loss: 0.026663  [ 1800/ 3474]
loss: 0.017122  [ 1900/ 3474]
loss: 0.008229  [ 2000/ 3474]
loss: 0.016879  [ 2100/ 3474]
loss: 0.025421  [ 2200/ 3474]
loss: 0.070176  [ 2300/ 3474]
loss: 0.006759  [ 2400/ 3474]
loss: 0.015131  [ 2500/ 3474]
loss: 0.019784  [ 2600/ 3474]
loss: 0.049655  [ 2700/ 3474]
loss: 0.020066  [ 2800/ 3474]
loss: 0.017102  [ 2900/ 3474]
loss: 0.203769  [ 3000/ 3474]
loss: 0.009710  [ 3100/ 3474]
loss: 0.014376  [ 3200/ 3474]
loss: 0.032917  [ 3300/ 3474]
loss: 0.013525  [ 3400/ 3474]
Epoch 7
-------------------------------
loss: 0.029992  [    0/ 3474]
loss: 0.009523  [  100/ 3474]
loss: 0.019791  [  200/ 3474]
loss: 0.062129  [  300/ 3474]
loss: 0.057584  [  400/ 3474]
loss: 0.005914  [  500/ 3474]
loss: 0.028862  [  600/ 3474]
loss: 0.010227  [  700/ 3474]
loss: 0.016846  [  800/ 3474]
loss: 0.187347  [  900/ 3474]
loss: 0.007286  [ 1000/ 3474]
loss: 0.160969  [ 1100/ 3474]
loss: 0.029345  [ 1200/ 3474]
loss: 0.025864  [ 1300/ 3474]
loss: 0.010618  [ 1400/ 3474]
loss: 0.078617  [ 1500/ 3474]
loss: 0.009708  [ 1600/ 3474]
loss: 0.049114  [ 1700/ 3474]
loss: 0.026109  [ 1800/ 3474]
loss: 0.018113  [ 1900/ 3474]
loss: 0.008377  [ 2000/ 3474]
loss: 0.016825  [ 2100/ 3474]
loss: 0.025710  [ 2200/ 3474]
loss: 0.069704  [ 2300/ 3474]
loss: 0.006614  [ 2400/ 3474]
loss: 0.013579  [ 2500/ 3474]
loss: 0.019545  [ 2600/ 3474]
loss: 0.047507  [ 2700/ 3474]
loss: 0.020048  [ 2800/ 3474]
loss: 0.016907  [ 2900/ 3474]
loss: 0.203763  [ 3000/ 3474]
loss: 0.009778  [ 3100/ 3474]
loss: 0.014932  [ 3200/ 3474]
loss: 0.034561  [ 3300/ 3474]
loss: 0.011685  [ 3400/ 3474]
Epoch 8
-------------------------------
loss: 0.030308  [    0/ 3474]
loss: 0.009439  [  100/ 3474]
loss: 0.020372  [  200/ 3474]
loss: 0.062209  [  300/ 3474]
loss: 0.056798  [  400/ 3474]
loss: 0.005733  [  500/ 3474]
loss: 0.027214  [  600/ 3474]
loss: 0.010094  [  700/ 3474]
loss: 0.017767  [  800/ 3474]
loss: 0.187584  [  900/ 3474]
loss: 0.007605  [ 1000/ 3474]
loss: 0.174347  [ 1100/ 3474]
loss: 0.028442  [ 1200/ 3474]
loss: 0.026209  [ 1300/ 3474]
loss: 0.010819  [ 1400/ 3474]
loss: 0.078834  [ 1500/ 3474]
loss: 0.009735  [ 1600/ 3474]
loss: 0.047659  [ 1700/ 3474]
loss: 0.025450  [ 1800/ 3474]
loss: 0.019001  [ 1900/ 3474]
loss: 0.008342  [ 2000/ 3474]
loss: 0.016630  [ 2100/ 3474]
loss: 0.026265  [ 2200/ 3474]
loss: 0.069604  [ 2300/ 3474]
loss: 0.006420  [ 2400/ 3474]
loss: 0.012873  [ 2500/ 3474]
loss: 0.019389  [ 2600/ 3474]
loss: 0.046442  [ 2700/ 3474]
loss: 0.020369  [ 2800/ 3474]
loss: 0.016876  [ 2900/ 3474]
loss: 0.204315  [ 3000/ 3474]
loss: 0.009822  [ 3100/ 3474]
loss: 0.013496  [ 3200/ 3474]
loss: 0.036393  [ 3300/ 3474]
loss: 0.010773  [ 3400/ 3474]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3474
First Spike after testing: [-0.4797086 -0.2676869]
[0 1 1 ... 1 1 2]
[0 1 1 ... 1 1 0]
Cluster 0 Occurrences: 1198; KMEANS: 1214
Cluster 1 Occurrences: 1128; KMEANS: 1122
Cluster 2 Occurrences: 1148; KMEANS: 1138
Centroids: [[-1.3921971, 0.33717588], [1.9759599, 0.5031132], [-0.62835675, -1.4265678]]
Centroids: [[-0.6166167, -1.3768158], [1.9928218, 0.50066954], [-1.4478879, 0.38967642]]
Contingency Matrix: 
[[  72    0 1126]
 [   1 1121    6]
 [1141    1    6]]
[[-1, 0, 1126], [-1, 1121, 6], [-1, -1, -1]]
[[-1, -1, -1], [-1, 1121, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 0: 2, 1: 1}
New Contingency Matrix: 
[[1126    0   72]
 [   6 1121    1]
 [   6    1 1141]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1126, 1121, 1141], Sum: 3388
All_Elements: [1126, 0, 72, 6, 1121, 1, 6, 1, 1141], Sum: 3474
Accuracy: 0.97524467472654
Done!
