Experiment_path: Base_Line/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Base_Line/Experiment_02/C_Easy1_noise040.mat/Variant_02_Autoencoder_KMeans/2023_03_21-00_04_40
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001DE9A129588>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x000001DE9A1830F0>
Epoch 1
-------------------------------
loss: 0.132828  [    0/ 3386]
loss: 0.254378  [  100/ 3386]
loss: 0.125997  [  200/ 3386]
loss: 0.180754  [  300/ 3386]
loss: 0.265385  [  400/ 3386]
loss: 0.074530  [  500/ 3386]
loss: 0.176513  [  600/ 3386]
loss: 0.084895  [  700/ 3386]
loss: 0.139376  [  800/ 3386]
loss: 0.079513  [  900/ 3386]
loss: 0.339560  [ 1000/ 3386]
loss: 0.278191  [ 1100/ 3386]
loss: 0.177481  [ 1200/ 3386]
loss: 0.027117  [ 1300/ 3386]
loss: 0.054528  [ 1400/ 3386]
loss: 0.106118  [ 1500/ 3386]
loss: 0.046063  [ 1600/ 3386]
loss: 0.031000  [ 1700/ 3386]
loss: 0.228362  [ 1800/ 3386]
loss: 0.611385  [ 1900/ 3386]
loss: 0.057592  [ 2000/ 3386]
loss: 0.087281  [ 2100/ 3386]
loss: 0.453684  [ 2200/ 3386]
loss: 0.076683  [ 2300/ 3386]
loss: 0.092593  [ 2400/ 3386]
loss: 0.048201  [ 2500/ 3386]
loss: 0.184749  [ 2600/ 3386]
loss: 0.228601  [ 2700/ 3386]
loss: 0.058802  [ 2800/ 3386]
loss: 0.055084  [ 2900/ 3386]
loss: 0.017073  [ 3000/ 3386]
loss: 0.113624  [ 3100/ 3386]
loss: 0.141680  [ 3200/ 3386]
loss: 0.112028  [ 3300/ 3386]
Epoch 2
-------------------------------
loss: 0.052489  [    0/ 3386]
loss: 0.113538  [  100/ 3386]
loss: 0.085570  [  200/ 3386]
loss: 0.204131  [  300/ 3386]
loss: 0.186525  [  400/ 3386]
loss: 0.067579  [  500/ 3386]
loss: 0.162524  [  600/ 3386]
loss: 0.044290  [  700/ 3386]
loss: 0.115835  [  800/ 3386]
loss: 0.060705  [  900/ 3386]
loss: 0.156190  [ 1000/ 3386]
loss: 0.308060  [ 1100/ 3386]
loss: 0.155546  [ 1200/ 3386]
loss: 0.025009  [ 1300/ 3386]
loss: 0.047754  [ 1400/ 3386]
loss: 0.074798  [ 1500/ 3386]
loss: 0.037366  [ 1600/ 3386]
loss: 0.028700  [ 1700/ 3386]
loss: 0.253440  [ 1800/ 3386]
loss: 0.709434  [ 1900/ 3386]
loss: 0.035947  [ 2000/ 3386]
loss: 0.083113  [ 2100/ 3386]
loss: 0.474620  [ 2200/ 3386]
loss: 0.089469  [ 2300/ 3386]
loss: 0.091826  [ 2400/ 3386]
loss: 0.048483  [ 2500/ 3386]
loss: 0.164175  [ 2600/ 3386]
loss: 0.219767  [ 2700/ 3386]
loss: 0.071606  [ 2800/ 3386]
loss: 0.051913  [ 2900/ 3386]
loss: 0.016084  [ 3000/ 3386]
loss: 0.119088  [ 3100/ 3386]
loss: 0.127885  [ 3200/ 3386]
loss: 0.113405  [ 3300/ 3386]
Epoch 3
-------------------------------
loss: 0.053383  [    0/ 3386]
loss: 0.113575  [  100/ 3386]
loss: 0.089251  [  200/ 3386]
loss: 0.211830  [  300/ 3386]
loss: 0.196549  [  400/ 3386]
loss: 0.069892  [  500/ 3386]
loss: 0.164812  [  600/ 3386]
loss: 0.043256  [  700/ 3386]
loss: 0.114942  [  800/ 3386]
loss: 0.057111  [  900/ 3386]
loss: 0.140620  [ 1000/ 3386]
loss: 0.297672  [ 1100/ 3386]
loss: 0.154757  [ 1200/ 3386]
loss: 0.025537  [ 1300/ 3386]
loss: 0.049370  [ 1400/ 3386]
loss: 0.070705  [ 1500/ 3386]
loss: 0.037204  [ 1600/ 3386]
loss: 0.027571  [ 1700/ 3386]
loss: 0.242061  [ 1800/ 3386]
loss: 0.687400  [ 1900/ 3386]
loss: 0.033703  [ 2000/ 3386]
loss: 0.083180  [ 2100/ 3386]
loss: 0.464171  [ 2200/ 3386]
loss: 0.091561  [ 2300/ 3386]
loss: 0.091314  [ 2400/ 3386]
loss: 0.047790  [ 2500/ 3386]
loss: 0.157679  [ 2600/ 3386]
loss: 0.219144  [ 2700/ 3386]
loss: 0.078246  [ 2800/ 3386]
loss: 0.052227  [ 2900/ 3386]
loss: 0.015899  [ 3000/ 3386]
loss: 0.116896  [ 3100/ 3386]
loss: 0.125447  [ 3200/ 3386]
loss: 0.110146  [ 3300/ 3386]
Epoch 4
-------------------------------
loss: 0.054347  [    0/ 3386]
loss: 0.112992  [  100/ 3386]
loss: 0.086131  [  200/ 3386]
loss: 0.205927  [  300/ 3386]
loss: 0.199057  [  400/ 3386]
loss: 0.070728  [  500/ 3386]
loss: 0.166645  [  600/ 3386]
loss: 0.042788  [  700/ 3386]
loss: 0.114865  [  800/ 3386]
loss: 0.054783  [  900/ 3386]
loss: 0.136399  [ 1000/ 3386]
loss: 0.285251  [ 1100/ 3386]
loss: 0.155118  [ 1200/ 3386]
loss: 0.025624  [ 1300/ 3386]
loss: 0.048912  [ 1400/ 3386]
loss: 0.067406  [ 1500/ 3386]
loss: 0.037148  [ 1600/ 3386]
loss: 0.026022  [ 1700/ 3386]
loss: 0.226828  [ 1800/ 3386]
loss: 0.676687  [ 1900/ 3386]
loss: 0.033877  [ 2000/ 3386]
loss: 0.085205  [ 2100/ 3386]
loss: 0.452883  [ 2200/ 3386]
loss: 0.090093  [ 2300/ 3386]
loss: 0.090491  [ 2400/ 3386]
loss: 0.048148  [ 2500/ 3386]
loss: 0.142046  [ 2600/ 3386]
loss: 0.218864  [ 2700/ 3386]
loss: 0.077145  [ 2800/ 3386]
loss: 0.056026  [ 2900/ 3386]
loss: 0.016244  [ 3000/ 3386]
loss: 0.115523  [ 3100/ 3386]
loss: 0.122875  [ 3200/ 3386]
loss: 0.106356  [ 3300/ 3386]
Epoch 5
-------------------------------
loss: 0.053713  [    0/ 3386]
loss: 0.113051  [  100/ 3386]
loss: 0.084230  [  200/ 3386]
loss: 0.216991  [  300/ 3386]
loss: 0.201421  [  400/ 3386]
loss: 0.072278  [  500/ 3386]
loss: 0.167539  [  600/ 3386]
loss: 0.043166  [  700/ 3386]
loss: 0.113171  [  800/ 3386]
loss: 0.055395  [  900/ 3386]
loss: 0.137422  [ 1000/ 3386]
loss: 0.264205  [ 1100/ 3386]
loss: 0.154630  [ 1200/ 3386]
loss: 0.025704  [ 1300/ 3386]
loss: 0.049619  [ 1400/ 3386]
loss: 0.062275  [ 1500/ 3386]
loss: 0.037420  [ 1600/ 3386]
loss: 0.026303  [ 1700/ 3386]
loss: 0.214761  [ 1800/ 3386]
loss: 0.638350  [ 1900/ 3386]
loss: 0.034340  [ 2000/ 3386]
loss: 0.087575  [ 2100/ 3386]
loss: 0.435370  [ 2200/ 3386]
loss: 0.092638  [ 2300/ 3386]
loss: 0.085729  [ 2400/ 3386]
loss: 0.051075  [ 2500/ 3386]
loss: 0.126868  [ 2600/ 3386]
loss: 0.214185  [ 2700/ 3386]
loss: 0.073280  [ 2800/ 3386]
loss: 0.056778  [ 2900/ 3386]
loss: 0.015976  [ 3000/ 3386]
loss: 0.119755  [ 3100/ 3386]
loss: 0.112564  [ 3200/ 3386]
loss: 0.102987  [ 3300/ 3386]
Epoch 6
-------------------------------
loss: 0.052978  [    0/ 3386]
loss: 0.113776  [  100/ 3386]
loss: 0.074127  [  200/ 3386]
loss: 0.246348  [  300/ 3386]
loss: 0.201768  [  400/ 3386]
loss: 0.072409  [  500/ 3386]
loss: 0.167369  [  600/ 3386]
loss: 0.042491  [  700/ 3386]
loss: 0.112326  [  800/ 3386]
loss: 0.057874  [  900/ 3386]
loss: 0.131370  [ 1000/ 3386]
loss: 0.249711  [ 1100/ 3386]
loss: 0.152478  [ 1200/ 3386]
loss: 0.025827  [ 1300/ 3386]
loss: 0.049258  [ 1400/ 3386]
loss: 0.057514  [ 1500/ 3386]
loss: 0.037055  [ 1600/ 3386]
loss: 0.025680  [ 1700/ 3386]
loss: 0.186910  [ 1800/ 3386]
loss: 0.625331  [ 1900/ 3386]
loss: 0.033903  [ 2000/ 3386]
loss: 0.087918  [ 2100/ 3386]
loss: 0.410227  [ 2200/ 3386]
loss: 0.094567  [ 2300/ 3386]
loss: 0.077693  [ 2400/ 3386]
loss: 0.054538  [ 2500/ 3386]
loss: 0.108433  [ 2600/ 3386]
loss: 0.211379  [ 2700/ 3386]
loss: 0.075023  [ 2800/ 3386]
loss: 0.060507  [ 2900/ 3386]
loss: 0.015087  [ 3000/ 3386]
loss: 0.115305  [ 3100/ 3386]
loss: 0.109184  [ 3200/ 3386]
loss: 0.102016  [ 3300/ 3386]
Epoch 7
-------------------------------
loss: 0.050434  [    0/ 3386]
loss: 0.113213  [  100/ 3386]
loss: 0.078069  [  200/ 3386]
loss: 0.174649  [  300/ 3386]
loss: 0.199209  [  400/ 3386]
loss: 0.070950  [  500/ 3386]
loss: 0.166041  [  600/ 3386]
loss: 0.042124  [  700/ 3386]
loss: 0.110418  [  800/ 3386]
loss: 0.058643  [  900/ 3386]
loss: 0.133483  [ 1000/ 3386]
loss: 0.234125  [ 1100/ 3386]
loss: 0.152772  [ 1200/ 3386]
loss: 0.023722  [ 1300/ 3386]
loss: 0.051487  [ 1400/ 3386]
loss: 0.051059  [ 1500/ 3386]
loss: 0.038945  [ 1600/ 3386]
loss: 0.028024  [ 1700/ 3386]
loss: 0.177990  [ 1800/ 3386]
loss: 0.368013  [ 1900/ 3386]
loss: 0.033472  [ 2000/ 3386]
loss: 0.085121  [ 2100/ 3386]
loss: 0.691626  [ 2200/ 3386]
loss: 0.084505  [ 2300/ 3386]
loss: 0.070738  [ 2400/ 3386]
loss: 0.050293  [ 2500/ 3386]
loss: 0.095639  [ 2600/ 3386]
loss: 0.203488  [ 2700/ 3386]
loss: 0.067762  [ 2800/ 3386]
loss: 0.063512  [ 2900/ 3386]
loss: 0.014811  [ 3000/ 3386]
loss: 0.116600  [ 3100/ 3386]
loss: 0.106180  [ 3200/ 3386]
loss: 0.100277  [ 3300/ 3386]
Epoch 8
-------------------------------
loss: 0.049236  [    0/ 3386]
loss: 0.112611  [  100/ 3386]
loss: 0.073788  [  200/ 3386]
loss: 0.104808  [  300/ 3386]
loss: 0.198842  [  400/ 3386]
loss: 0.073762  [  500/ 3386]
loss: 0.163586  [  600/ 3386]
loss: 0.042934  [  700/ 3386]
loss: 0.110009  [  800/ 3386]
loss: 0.057943  [  900/ 3386]
loss: 0.123679  [ 1000/ 3386]
loss: 0.222590  [ 1100/ 3386]
loss: 0.147552  [ 1200/ 3386]
loss: 0.024013  [ 1300/ 3386]
loss: 0.052037  [ 1400/ 3386]
loss: 0.048481  [ 1500/ 3386]
loss: 0.038204  [ 1600/ 3386]
loss: 0.027293  [ 1700/ 3386]
loss: 0.171488  [ 1800/ 3386]
loss: 0.350097  [ 1900/ 3386]
loss: 0.033252  [ 2000/ 3386]
loss: 0.084185  [ 2100/ 3386]
loss: 0.451545  [ 2200/ 3386]
loss: 0.086780  [ 2300/ 3386]
loss: 0.063085  [ 2400/ 3386]
loss: 0.053933  [ 2500/ 3386]
loss: 0.078885  [ 2600/ 3386]
loss: 0.199344  [ 2700/ 3386]
loss: 0.067502  [ 2800/ 3386]
loss: 0.064439  [ 2900/ 3386]
loss: 0.015217  [ 3000/ 3386]
loss: 0.116616  [ 3100/ 3386]
loss: 0.097187  [ 3200/ 3386]
loss: 0.098505  [ 3300/ 3386]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3386
First Spike after testing: [-0.04308486  0.03856529]
[0 2 0 ... 2 0 1]
[1 1 1 ... 2 1 0]
Cluster 0 Occurrences: 1079; KMEANS: 1074
Cluster 1 Occurrences: 1158; KMEANS: 1167
Cluster 2 Occurrences: 1149; KMEANS: 1145
Centroids: [[0.2015384, 0.32709125], [0.36427015, -3.0535793], [0.86667347, 1.3295423]]
Centroids: [[0.36091265, -3.2419748], [-0.060183797, 0.27629346], [1.1508356, 1.313517]]
Contingency Matrix: 
[[   0  856  223]
 [1068   82    8]
 [   6  229  914]]
[[-1, 856, 223], [-1, -1, -1], [-1, 229, 914]]
[[-1, 856, -1], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 0, 2: 2, 0: 1}
New Contingency Matrix: 
[[ 856    0  223]
 [  82 1068    8]
 [ 229    6  914]]
New Clustered Label Sequence: [1, 0, 2]
Diagonal_Elements: [856, 1068, 914], Sum: 2838
All_Elements: [856, 0, 223, 82, 1068, 8, 229, 6, 914], Sum: 3386
Accuracy: 0.8381571175428234
Done!
