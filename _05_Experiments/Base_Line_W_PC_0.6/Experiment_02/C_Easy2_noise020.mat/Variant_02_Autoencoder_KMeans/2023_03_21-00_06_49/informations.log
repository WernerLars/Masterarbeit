Experiment_path: Base_Line_W_PC_0.6/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Base_Line_W_PC_0.6/Experiment_02/C_Easy2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_21-00_06_49
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000182140161D0>
Sampling rate: 24000.0
Raw: [ 0.06217714  0.08667759  0.11027728 ... -0.20242181 -0.23729255
 -0.22686598]
Times: [    275    1209    1637 ... 1439335 1439493 1439555]
Cluster: [3 1 3 ... 1 3 3]
Number of different clusters:  3
Number of Spikes: 3526
First aligned Spike Frame: [ 0.1985413   0.13105152  0.07019694  0.01293704 -0.04549478 -0.09355401
 -0.10898392 -0.08319484 -0.04338644 -0.02286395 -0.01669682  0.03736978
  0.228401    0.55158241  0.86822633  1.017223    0.95590368  0.7885242
  0.62729572  0.50651951  0.42415885  0.36744116  0.32697735  0.30083782
  0.28884086  0.28564604  0.27020338  0.23197964  0.18793799  0.15404375
  0.12614683  0.08867524  0.0478996   0.02814512  0.02523451  0.01117923
 -0.03609381 -0.11393271 -0.18622402 -0.21752562 -0.20411432 -0.1633565
 -0.106174   -0.0312361   0.06793406  0.17242405  0.24704307]
Cluster 0, Occurrences: 1186
Cluster 1, Occurrences: 1188
Cluster 2, Occurrences: 1152
<torch.utils.data.dataloader.DataLoader object at 0x0000018214CFA710>
Epoch 1
-------------------------------
loss: 0.135419  [    0/ 3526]
loss: 0.208260  [  100/ 3526]
loss: 0.090072  [  200/ 3526]
loss: 0.050869  [  300/ 3526]
loss: 0.021989  [  400/ 3526]
loss: 0.054945  [  500/ 3526]
loss: 0.042947  [  600/ 3526]
loss: 0.018465  [  700/ 3526]
loss: 0.054781  [  800/ 3526]
loss: 0.012536  [  900/ 3526]
loss: 0.038801  [ 1000/ 3526]
loss: 0.024284  [ 1100/ 3526]
loss: 0.054391  [ 1200/ 3526]
loss: 0.037395  [ 1300/ 3526]
loss: 0.031358  [ 1400/ 3526]
loss: 0.030909  [ 1500/ 3526]
loss: 0.042711  [ 1600/ 3526]
loss: 0.028805  [ 1700/ 3526]
loss: 0.028568  [ 1800/ 3526]
loss: 0.016703  [ 1900/ 3526]
loss: 0.017674  [ 2000/ 3526]
loss: 0.120497  [ 2100/ 3526]
loss: 0.030001  [ 2200/ 3526]
loss: 0.016692  [ 2300/ 3526]
loss: 0.008849  [ 2400/ 3526]
loss: 0.017078  [ 2500/ 3526]
loss: 0.028911  [ 2600/ 3526]
loss: 0.016558  [ 2700/ 3526]
loss: 0.009348  [ 2800/ 3526]
loss: 0.010209  [ 2900/ 3526]
loss: 0.070786  [ 3000/ 3526]
loss: 0.165492  [ 3100/ 3526]
loss: 0.029851  [ 3200/ 3526]
loss: 0.025606  [ 3300/ 3526]
loss: 0.038733  [ 3400/ 3526]
loss: 0.039849  [ 3500/ 3526]
Epoch 2
-------------------------------
loss: 0.015765  [    0/ 3526]
loss: 0.045792  [  100/ 3526]
loss: 0.050448  [  200/ 3526]
loss: 0.005515  [  300/ 3526]
loss: 0.015477  [  400/ 3526]
loss: 0.038377  [  500/ 3526]
loss: 0.011469  [  600/ 3526]
loss: 0.014219  [  700/ 3526]
loss: 0.029889  [  800/ 3526]
loss: 0.013090  [  900/ 3526]
loss: 0.017683  [ 1000/ 3526]
loss: 0.016604  [ 1100/ 3526]
loss: 0.030219  [ 1200/ 3526]
loss: 0.024480  [ 1300/ 3526]
loss: 0.028445  [ 1400/ 3526]
loss: 0.018019  [ 1500/ 3526]
loss: 0.045819  [ 1600/ 3526]
loss: 0.022658  [ 1700/ 3526]
loss: 0.019178  [ 1800/ 3526]
loss: 0.015128  [ 1900/ 3526]
loss: 0.017027  [ 2000/ 3526]
loss: 0.133492  [ 2100/ 3526]
loss: 0.022541  [ 2200/ 3526]
loss: 0.015940  [ 2300/ 3526]
loss: 0.008496  [ 2400/ 3526]
loss: 0.016014  [ 2500/ 3526]
loss: 0.016626  [ 2600/ 3526]
loss: 0.015779  [ 2700/ 3526]
loss: 0.008790  [ 2800/ 3526]
loss: 0.010061  [ 2900/ 3526]
loss: 0.056171  [ 3000/ 3526]
loss: 0.160895  [ 3100/ 3526]
loss: 0.027147  [ 3200/ 3526]
loss: 0.022989  [ 3300/ 3526]
loss: 0.042365  [ 3400/ 3526]
loss: 0.039127  [ 3500/ 3526]
Epoch 3
-------------------------------
loss: 0.012482  [    0/ 3526]
loss: 0.044666  [  100/ 3526]
loss: 0.048268  [  200/ 3526]
loss: 0.005369  [  300/ 3526]
loss: 0.015586  [  400/ 3526]
loss: 0.037323  [  500/ 3526]
loss: 0.010397  [  600/ 3526]
loss: 0.013916  [  700/ 3526]
loss: 0.028911  [  800/ 3526]
loss: 0.012014  [  900/ 3526]
loss: 0.017102  [ 1000/ 3526]
loss: 0.015151  [ 1100/ 3526]
loss: 0.030366  [ 1200/ 3526]
loss: 0.025068  [ 1300/ 3526]
loss: 0.028089  [ 1400/ 3526]
loss: 0.018632  [ 1500/ 3526]
loss: 0.045398  [ 1600/ 3526]
loss: 0.021850  [ 1700/ 3526]
loss: 0.019273  [ 1800/ 3526]
loss: 0.016484  [ 1900/ 3526]
loss: 0.016367  [ 2000/ 3526]
loss: 0.137538  [ 2100/ 3526]
loss: 0.023449  [ 2200/ 3526]
loss: 0.016012  [ 2300/ 3526]
loss: 0.008533  [ 2400/ 3526]
loss: 0.015696  [ 2500/ 3526]
loss: 0.015681  [ 2600/ 3526]
loss: 0.014239  [ 2700/ 3526]
loss: 0.008439  [ 2800/ 3526]
loss: 0.009996  [ 2900/ 3526]
loss: 0.049826  [ 3000/ 3526]
loss: 0.161194  [ 3100/ 3526]
loss: 0.027025  [ 3200/ 3526]
loss: 0.023389  [ 3300/ 3526]
loss: 0.040170  [ 3400/ 3526]
loss: 0.039902  [ 3500/ 3526]
Epoch 4
-------------------------------
loss: 0.011458  [    0/ 3526]
loss: 0.043197  [  100/ 3526]
loss: 0.046111  [  200/ 3526]
loss: 0.005691  [  300/ 3526]
loss: 0.015204  [  400/ 3526]
loss: 0.038437  [  500/ 3526]
loss: 0.010253  [  600/ 3526]
loss: 0.013407  [  700/ 3526]
loss: 0.028787  [  800/ 3526]
loss: 0.011500  [  900/ 3526]
loss: 0.017339  [ 1000/ 3526]
loss: 0.013973  [ 1100/ 3526]
loss: 0.029928  [ 1200/ 3526]
loss: 0.025369  [ 1300/ 3526]
loss: 0.028177  [ 1400/ 3526]
loss: 0.018555  [ 1500/ 3526]
loss: 0.044732  [ 1600/ 3526]
loss: 0.021396  [ 1700/ 3526]
loss: 0.019382  [ 1800/ 3526]
loss: 0.016661  [ 1900/ 3526]
loss: 0.016130  [ 2000/ 3526]
loss: 0.144833  [ 2100/ 3526]
loss: 0.025182  [ 2200/ 3526]
loss: 0.016113  [ 2300/ 3526]
loss: 0.008579  [ 2400/ 3526]
loss: 0.015281  [ 2500/ 3526]
loss: 0.016335  [ 2600/ 3526]
loss: 0.013613  [ 2700/ 3526]
loss: 0.008289  [ 2800/ 3526]
loss: 0.009913  [ 2900/ 3526]
loss: 0.048699  [ 3000/ 3526]
loss: 0.162421  [ 3100/ 3526]
loss: 0.027355  [ 3200/ 3526]
loss: 0.023280  [ 3300/ 3526]
loss: 0.034860  [ 3400/ 3526]
loss: 0.040845  [ 3500/ 3526]
Epoch 5
-------------------------------
loss: 0.010492  [    0/ 3526]
loss: 0.038136  [  100/ 3526]
loss: 0.042310  [  200/ 3526]
loss: 0.006940  [  300/ 3526]
loss: 0.015236  [  400/ 3526]
loss: 0.038736  [  500/ 3526]
loss: 0.010486  [  600/ 3526]
loss: 0.013264  [  700/ 3526]
loss: 0.029159  [  800/ 3526]
loss: 0.011552  [  900/ 3526]
loss: 0.017515  [ 1000/ 3526]
loss: 0.014925  [ 1100/ 3526]
loss: 0.028961  [ 1200/ 3526]
loss: 0.025794  [ 1300/ 3526]
loss: 0.028381  [ 1400/ 3526]
loss: 0.018625  [ 1500/ 3526]
loss: 0.043898  [ 1600/ 3526]
loss: 0.021167  [ 1700/ 3526]
loss: 0.019772  [ 1800/ 3526]
loss: 0.015944  [ 1900/ 3526]
loss: 0.015599  [ 2000/ 3526]
loss: 0.151573  [ 2100/ 3526]
loss: 0.026749  [ 2200/ 3526]
loss: 0.016154  [ 2300/ 3526]
loss: 0.008561  [ 2400/ 3526]
loss: 0.014630  [ 2500/ 3526]
loss: 0.017144  [ 2600/ 3526]
loss: 0.013476  [ 2700/ 3526]
loss: 0.008422  [ 2800/ 3526]
loss: 0.009770  [ 2900/ 3526]
loss: 0.050376  [ 3000/ 3526]
loss: 0.163645  [ 3100/ 3526]
loss: 0.028070  [ 3200/ 3526]
loss: 0.023262  [ 3300/ 3526]
loss: 0.030243  [ 3400/ 3526]
loss: 0.041836  [ 3500/ 3526]
Epoch 6
-------------------------------
loss: 0.010151  [    0/ 3526]
loss: 0.033938  [  100/ 3526]
loss: 0.038432  [  200/ 3526]
loss: 0.009217  [  300/ 3526]
loss: 0.015126  [  400/ 3526]
loss: 0.038861  [  500/ 3526]
loss: 0.010707  [  600/ 3526]
loss: 0.013033  [  700/ 3526]
loss: 0.029297  [  800/ 3526]
loss: 0.011677  [  900/ 3526]
loss: 0.017870  [ 1000/ 3526]
loss: 0.016124  [ 1100/ 3526]
loss: 0.028144  [ 1200/ 3526]
loss: 0.025952  [ 1300/ 3526]
loss: 0.028539  [ 1400/ 3526]
loss: 0.018919  [ 1500/ 3526]
loss: 0.043462  [ 1600/ 3526]
loss: 0.021243  [ 1700/ 3526]
loss: 0.019814  [ 1800/ 3526]
loss: 0.014520  [ 1900/ 3526]
loss: 0.015456  [ 2000/ 3526]
loss: 0.180358  [ 2100/ 3526]
loss: 0.028298  [ 2200/ 3526]
loss: 0.016240  [ 2300/ 3526]
loss: 0.008625  [ 2400/ 3526]
loss: 0.013827  [ 2500/ 3526]
loss: 0.017374  [ 2600/ 3526]
loss: 0.013497  [ 2700/ 3526]
loss: 0.008301  [ 2800/ 3526]
loss: 0.009742  [ 2900/ 3526]
loss: 0.052375  [ 3000/ 3526]
loss: 0.164204  [ 3100/ 3526]
loss: 0.028317  [ 3200/ 3526]
loss: 0.023289  [ 3300/ 3526]
loss: 0.024562  [ 3400/ 3526]
loss: 0.042748  [ 3500/ 3526]
Epoch 7
-------------------------------
loss: 0.010604  [    0/ 3526]
loss: 0.031308  [  100/ 3526]
loss: 0.035729  [  200/ 3526]
loss: 0.010659  [  300/ 3526]
loss: 0.015278  [  400/ 3526]
loss: 0.039282  [  500/ 3526]
loss: 0.010896  [  600/ 3526]
loss: 0.012885  [  700/ 3526]
loss: 0.029430  [  800/ 3526]
loss: 0.012250  [  900/ 3526]
loss: 0.017355  [ 1000/ 3526]
loss: 0.016637  [ 1100/ 3526]
loss: 0.027241  [ 1200/ 3526]
loss: 0.026005  [ 1300/ 3526]
loss: 0.028743  [ 1400/ 3526]
loss: 0.019079  [ 1500/ 3526]
loss: 0.043004  [ 1600/ 3526]
loss: 0.021032  [ 1700/ 3526]
loss: 0.019614  [ 1800/ 3526]
loss: 0.013582  [ 1900/ 3526]
loss: 0.015195  [ 2000/ 3526]
loss: 0.221059  [ 2100/ 3526]
loss: 0.029384  [ 2200/ 3526]
loss: 0.016274  [ 2300/ 3526]
loss: 0.008641  [ 2400/ 3526]
loss: 0.013414  [ 2500/ 3526]
loss: 0.018186  [ 2600/ 3526]
loss: 0.013514  [ 2700/ 3526]
loss: 0.008576  [ 2800/ 3526]
loss: 0.009864  [ 2900/ 3526]
loss: 0.054421  [ 3000/ 3526]
loss: 0.165209  [ 3100/ 3526]
loss: 0.028705  [ 3200/ 3526]
loss: 0.023469  [ 3300/ 3526]
loss: 0.022320  [ 3400/ 3526]
loss: 0.043224  [ 3500/ 3526]
Epoch 8
-------------------------------
loss: 0.010753  [    0/ 3526]
loss: 0.026302  [  100/ 3526]
loss: 0.033137  [  200/ 3526]
loss: 0.011472  [  300/ 3526]
loss: 0.014966  [  400/ 3526]
loss: 0.039310  [  500/ 3526]
loss: 0.011101  [  600/ 3526]
loss: 0.012896  [  700/ 3526]
loss: 0.029626  [  800/ 3526]
loss: 0.012344  [  900/ 3526]
loss: 0.017137  [ 1000/ 3526]
loss: 0.017377  [ 1100/ 3526]
loss: 0.026783  [ 1200/ 3526]
loss: 0.026039  [ 1300/ 3526]
loss: 0.028870  [ 1400/ 3526]
loss: 0.019090  [ 1500/ 3526]
loss: 0.042646  [ 1600/ 3526]
loss: 0.020729  [ 1700/ 3526]
loss: 0.019523  [ 1800/ 3526]
loss: 0.012825  [ 1900/ 3526]
loss: 0.015299  [ 2000/ 3526]
loss: 0.223111  [ 2100/ 3526]
loss: 0.030380  [ 2200/ 3526]
loss: 0.016363  [ 2300/ 3526]
loss: 0.008622  [ 2400/ 3526]
loss: 0.013222  [ 2500/ 3526]
loss: 0.018556  [ 2600/ 3526]
loss: 0.013634  [ 2700/ 3526]
loss: 0.008795  [ 2800/ 3526]
loss: 0.009827  [ 2900/ 3526]
loss: 0.055537  [ 3000/ 3526]
loss: 0.165836  [ 3100/ 3526]
loss: 0.028715  [ 3200/ 3526]
loss: 0.023722  [ 3300/ 3526]
loss: 0.020401  [ 3400/ 3526]
loss: 0.043642  [ 3500/ 3526]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3526
First Spike after testing: [-0.09417858  1.6173676 ]
[2 0 2 ... 0 2 2]
[2 0 1 ... 0 2 1]
Cluster 0 Occurrences: 1186; KMEANS: 2383
Cluster 1 Occurrences: 1188; KMEANS: 533
Cluster 2 Occurrences: 1152; KMEANS: 610
Centroids: [[1.0830917, -0.7840996], [0.6640659, -0.40544704], [-0.5742043, 2.2740848]]
Centroids: [[0.87376636, -0.5918603], [-0.9147285, 2.9307017], [-0.2994448, 1.7319208]]
Contingency Matrix: 
[[1186    0    0]
 [1186    0    2]
 [  11  533  608]]
[[-1, -1, -1], [-1, 0, 2], [-1, 533, 608]]
[[-1, -1, -1], [-1, 0, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {0: 0, 2: 2, 1: 1}
New Contingency Matrix: 
[[1186    0    0]
 [1186    0    2]
 [  11  533  608]]
New Clustered Label Sequence: [0, 1, 2]
Diagonal_Elements: [1186, 0, 608], Sum: 1794
All_Elements: [1186, 0, 0, 1186, 0, 2, 11, 533, 608], Sum: 3526
Accuracy: 0.5087918321043675
Done!
