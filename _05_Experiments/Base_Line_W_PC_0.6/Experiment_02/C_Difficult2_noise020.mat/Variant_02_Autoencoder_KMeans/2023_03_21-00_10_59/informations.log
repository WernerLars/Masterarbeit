Experiment_path: Base_Line_W_PC_0.6/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise020.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Base_Line_W_PC_0.6/Experiment_02/C_Difficult2_noise020.mat/Variant_02_Autoencoder_KMeans/2023_03_21-00_10_59
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001821E676DA0>
Sampling rate: 24000.0
Raw: [-0.05920843 -0.02398302  0.01513494 ...  0.2971695   0.32984394
  0.35872829]
Times: [    337    1080    1305 ... 1438651 1438787 1439662]
Cluster: [2 1 1 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3493
First aligned Spike Frame: [ 0.50880334  0.56984686  0.60721022  0.60769692  0.58122704  0.55003969
  0.51479324  0.46436685  0.40848987  0.36206071  0.31750134  0.26828304
  0.23270096  0.2305818   0.25904633  0.30599383  0.36680145  0.45670025
  0.60261795  0.8012213   1.02149976  1.23478943  1.38977263  1.39868415
  1.211664    0.88028336  0.50425138  0.15449729 -0.12937778 -0.32272009
 -0.40685817 -0.38921932 -0.31829776 -0.24412685 -0.18860857 -0.1442941
 -0.0976923  -0.0504865  -0.01384986  0.00955437  0.03047694  0.05600466
  0.07308225  0.06101434  0.01148826 -0.0607151  -0.13636803]
Cluster 0, Occurrences: 1151
Cluster 1, Occurrences: 1195
Cluster 2, Occurrences: 1147
<torch.utils.data.dataloader.DataLoader object at 0x00000182864F56A0>
Epoch 1
-------------------------------
loss: 0.338477  [    0/ 3493]
loss: 0.091786  [  100/ 3493]
loss: 0.038642  [  200/ 3493]
loss: 0.065592  [  300/ 3493]
loss: 0.097617  [  400/ 3493]
loss: 0.026192  [  500/ 3493]
loss: 0.061688  [  600/ 3493]
loss: 0.245171  [  700/ 3493]
loss: 0.048115  [  800/ 3493]
loss: 0.018874  [  900/ 3493]
loss: 0.054416  [ 1000/ 3493]
loss: 0.042344  [ 1100/ 3493]
loss: 0.024562  [ 1200/ 3493]
loss: 0.014258  [ 1300/ 3493]
loss: 0.029531  [ 1400/ 3493]
loss: 0.046528  [ 1500/ 3493]
loss: 0.028874  [ 1600/ 3493]
loss: 0.018784  [ 1700/ 3493]
loss: 0.058110  [ 1800/ 3493]
loss: 0.009963  [ 1900/ 3493]
loss: 0.008137  [ 2000/ 3493]
loss: 0.014979  [ 2100/ 3493]
loss: 0.013135  [ 2200/ 3493]
loss: 0.024352  [ 2300/ 3493]
loss: 0.013568  [ 2400/ 3493]
loss: 0.086218  [ 2500/ 3493]
loss: 0.005534  [ 2600/ 3493]
loss: 0.008715  [ 2700/ 3493]
loss: 0.018376  [ 2800/ 3493]
loss: 0.008423  [ 2900/ 3493]
loss: 0.042477  [ 3000/ 3493]
loss: 0.013789  [ 3100/ 3493]
loss: 0.026895  [ 3200/ 3493]
loss: 0.018830  [ 3300/ 3493]
loss: 0.009135  [ 3400/ 3493]
Epoch 2
-------------------------------
loss: 0.041757  [    0/ 3493]
loss: 0.015880  [  100/ 3493]
loss: 0.030916  [  200/ 3493]
loss: 0.018259  [  300/ 3493]
loss: 0.060863  [  400/ 3493]
loss: 0.023308  [  500/ 3493]
loss: 0.051259  [  600/ 3493]
loss: 0.188070  [  700/ 3493]
loss: 0.017082  [  800/ 3493]
loss: 0.014026  [  900/ 3493]
loss: 0.040538  [ 1000/ 3493]
loss: 0.028415  [ 1100/ 3493]
loss: 0.019450  [ 1200/ 3493]
loss: 0.015457  [ 1300/ 3493]
loss: 0.032403  [ 1400/ 3493]
loss: 0.052568  [ 1500/ 3493]
loss: 0.030810  [ 1600/ 3493]
loss: 0.013612  [ 1700/ 3493]
loss: 0.048590  [ 1800/ 3493]
loss: 0.010549  [ 1900/ 3493]
loss: 0.009044  [ 2000/ 3493]
loss: 0.019400  [ 2100/ 3493]
loss: 0.014765  [ 2200/ 3493]
loss: 0.023056  [ 2300/ 3493]
loss: 0.014735  [ 2400/ 3493]
loss: 0.088000  [ 2500/ 3493]
loss: 0.002916  [ 2600/ 3493]
loss: 0.008984  [ 2700/ 3493]
loss: 0.017480  [ 2800/ 3493]
loss: 0.007822  [ 2900/ 3493]
loss: 0.041896  [ 3000/ 3493]
loss: 0.013879  [ 3100/ 3493]
loss: 0.022646  [ 3200/ 3493]
loss: 0.020101  [ 3300/ 3493]
loss: 0.009891  [ 3400/ 3493]
Epoch 3
-------------------------------
loss: 0.044917  [    0/ 3493]
loss: 0.016638  [  100/ 3493]
loss: 0.027207  [  200/ 3493]
loss: 0.017347  [  300/ 3493]
loss: 0.058200  [  400/ 3493]
loss: 0.022286  [  500/ 3493]
loss: 0.051336  [  600/ 3493]
loss: 0.175230  [  700/ 3493]
loss: 0.015755  [  800/ 3493]
loss: 0.012528  [  900/ 3493]
loss: 0.036991  [ 1000/ 3493]
loss: 0.032088  [ 1100/ 3493]
loss: 0.019762  [ 1200/ 3493]
loss: 0.015314  [ 1300/ 3493]
loss: 0.031967  [ 1400/ 3493]
loss: 0.053148  [ 1500/ 3493]
loss: 0.030809  [ 1600/ 3493]
loss: 0.014389  [ 1700/ 3493]
loss: 0.046860  [ 1800/ 3493]
loss: 0.010878  [ 1900/ 3493]
loss: 0.009588  [ 2000/ 3493]
loss: 0.020215  [ 2100/ 3493]
loss: 0.015114  [ 2200/ 3493]
loss: 0.022892  [ 2300/ 3493]
loss: 0.015029  [ 2400/ 3493]
loss: 0.086184  [ 2500/ 3493]
loss: 0.003663  [ 2600/ 3493]
loss: 0.009188  [ 2700/ 3493]
loss: 0.017449  [ 2800/ 3493]
loss: 0.007561  [ 2900/ 3493]
loss: 0.041248  [ 3000/ 3493]
loss: 0.014130  [ 3100/ 3493]
loss: 0.019022  [ 3200/ 3493]
loss: 0.020977  [ 3300/ 3493]
loss: 0.010372  [ 3400/ 3493]
Epoch 4
-------------------------------
loss: 0.046230  [    0/ 3493]
loss: 0.017139  [  100/ 3493]
loss: 0.025252  [  200/ 3493]
loss: 0.016904  [  300/ 3493]
loss: 0.057208  [  400/ 3493]
loss: 0.021397  [  500/ 3493]
loss: 0.051397  [  600/ 3493]
loss: 0.165302  [  700/ 3493]
loss: 0.015177  [  800/ 3493]
loss: 0.011503  [  900/ 3493]
loss: 0.034512  [ 1000/ 3493]
loss: 0.033557  [ 1100/ 3493]
loss: 0.019737  [ 1200/ 3493]
loss: 0.014726  [ 1300/ 3493]
loss: 0.030995  [ 1400/ 3493]
loss: 0.052866  [ 1500/ 3493]
loss: 0.031597  [ 1600/ 3493]
loss: 0.015262  [ 1700/ 3493]
loss: 0.044620  [ 1800/ 3493]
loss: 0.011291  [ 1900/ 3493]
loss: 0.009935  [ 2000/ 3493]
loss: 0.019959  [ 2100/ 3493]
loss: 0.015947  [ 2200/ 3493]
loss: 0.022627  [ 2300/ 3493]
loss: 0.014808  [ 2400/ 3493]
loss: 0.083907  [ 2500/ 3493]
loss: 0.004840  [ 2600/ 3493]
loss: 0.009337  [ 2700/ 3493]
loss: 0.017802  [ 2800/ 3493]
loss: 0.007565  [ 2900/ 3493]
loss: 0.040941  [ 3000/ 3493]
loss: 0.014486  [ 3100/ 3493]
loss: 0.015052  [ 3200/ 3493]
loss: 0.021525  [ 3300/ 3493]
loss: 0.010368  [ 3400/ 3493]
Epoch 5
-------------------------------
loss: 0.047069  [    0/ 3493]
loss: 0.017343  [  100/ 3493]
loss: 0.024141  [  200/ 3493]
loss: 0.017022  [  300/ 3493]
loss: 0.056822  [  400/ 3493]
loss: 0.020588  [  500/ 3493]
loss: 0.049062  [  600/ 3493]
loss: 0.156678  [  700/ 3493]
loss: 0.015186  [  800/ 3493]
loss: 0.010720  [  900/ 3493]
loss: 0.033092  [ 1000/ 3493]
loss: 0.035516  [ 1100/ 3493]
loss: 0.019373  [ 1200/ 3493]
loss: 0.014400  [ 1300/ 3493]
loss: 0.030333  [ 1400/ 3493]
loss: 0.051460  [ 1500/ 3493]
loss: 0.031566  [ 1600/ 3493]
loss: 0.015633  [ 1700/ 3493]
loss: 0.041845  [ 1800/ 3493]
loss: 0.011705  [ 1900/ 3493]
loss: 0.009971  [ 2000/ 3493]
loss: 0.018137  [ 2100/ 3493]
loss: 0.016021  [ 2200/ 3493]
loss: 0.022384  [ 2300/ 3493]
loss: 0.014766  [ 2400/ 3493]
loss: 0.083013  [ 2500/ 3493]
loss: 0.006865  [ 2600/ 3493]
loss: 0.009091  [ 2700/ 3493]
loss: 0.019753  [ 2800/ 3493]
loss: 0.007487  [ 2900/ 3493]
loss: 0.040737  [ 3000/ 3493]
loss: 0.014776  [ 3100/ 3493]
loss: 0.011040  [ 3200/ 3493]
loss: 0.021773  [ 3300/ 3493]
loss: 0.010333  [ 3400/ 3493]
Epoch 6
-------------------------------
loss: 0.047042  [    0/ 3493]
loss: 0.017428  [  100/ 3493]
loss: 0.022975  [  200/ 3493]
loss: 0.017283  [  300/ 3493]
loss: 0.056909  [  400/ 3493]
loss: 0.019990  [  500/ 3493]
loss: 0.048963  [  600/ 3493]
loss: 0.153669  [  700/ 3493]
loss: 0.014459  [  800/ 3493]
loss: 0.010126  [  900/ 3493]
loss: 0.032610  [ 1000/ 3493]
loss: 0.035397  [ 1100/ 3493]
loss: 0.019210  [ 1200/ 3493]
loss: 0.014355  [ 1300/ 3493]
loss: 0.030186  [ 1400/ 3493]
loss: 0.051156  [ 1500/ 3493]
loss: 0.028876  [ 1600/ 3493]
loss: 0.016083  [ 1700/ 3493]
loss: 0.037246  [ 1800/ 3493]
loss: 0.012441  [ 1900/ 3493]
loss: 0.010873  [ 2000/ 3493]
loss: 0.015662  [ 2100/ 3493]
loss: 0.016211  [ 2200/ 3493]
loss: 0.022370  [ 2300/ 3493]
loss: 0.014718  [ 2400/ 3493]
loss: 0.081767  [ 2500/ 3493]
loss: 0.008669  [ 2600/ 3493]
loss: 0.008981  [ 2700/ 3493]
loss: 0.019805  [ 2800/ 3493]
loss: 0.007621  [ 2900/ 3493]
loss: 0.039998  [ 3000/ 3493]
loss: 0.014829  [ 3100/ 3493]
loss: 0.008291  [ 3200/ 3493]
loss: 0.022300  [ 3300/ 3493]
loss: 0.010540  [ 3400/ 3493]
Epoch 7
-------------------------------
loss: 0.047444  [    0/ 3493]
loss: 0.017447  [  100/ 3493]
loss: 0.022158  [  200/ 3493]
loss: 0.017477  [  300/ 3493]
loss: 0.056093  [  400/ 3493]
loss: 0.020621  [  500/ 3493]
loss: 0.049190  [  600/ 3493]
loss: 0.153443  [  700/ 3493]
loss: 0.013493  [  800/ 3493]
loss: 0.009858  [  900/ 3493]
loss: 0.032210  [ 1000/ 3493]
loss: 0.034779  [ 1100/ 3493]
loss: 0.019084  [ 1200/ 3493]
loss: 0.013640  [ 1300/ 3493]
loss: 0.030228  [ 1400/ 3493]
loss: 0.051236  [ 1500/ 3493]
loss: 0.027871  [ 1600/ 3493]
loss: 0.016477  [ 1700/ 3493]
loss: 0.034600  [ 1800/ 3493]
loss: 0.012851  [ 1900/ 3493]
loss: 0.012165  [ 2000/ 3493]
loss: 0.013741  [ 2100/ 3493]
loss: 0.015348  [ 2200/ 3493]
loss: 0.022239  [ 2300/ 3493]
loss: 0.014621  [ 2400/ 3493]
loss: 0.079140  [ 2500/ 3493]
loss: 0.008617  [ 2600/ 3493]
loss: 0.009385  [ 2700/ 3493]
loss: 0.019707  [ 2800/ 3493]
loss: 0.007791  [ 2900/ 3493]
loss: 0.038783  [ 3000/ 3493]
loss: 0.014811  [ 3100/ 3493]
loss: 0.006900  [ 3200/ 3493]
loss: 0.023118  [ 3300/ 3493]
loss: 0.010617  [ 3400/ 3493]
Epoch 8
-------------------------------
loss: 0.047783  [    0/ 3493]
loss: 0.017673  [  100/ 3493]
loss: 0.021308  [  200/ 3493]
loss: 0.016964  [  300/ 3493]
loss: 0.055763  [  400/ 3493]
loss: 0.021488  [  500/ 3493]
loss: 0.049187  [  600/ 3493]
loss: 0.152353  [  700/ 3493]
loss: 0.012411  [  800/ 3493]
loss: 0.009783  [  900/ 3493]
loss: 0.030159  [ 1000/ 3493]
loss: 0.033827  [ 1100/ 3493]
loss: 0.018862  [ 1200/ 3493]
loss: 0.013048  [ 1300/ 3493]
loss: 0.030817  [ 1400/ 3493]
loss: 0.051678  [ 1500/ 3493]
loss: 0.027936  [ 1600/ 3493]
loss: 0.016848  [ 1700/ 3493]
loss: 0.032899  [ 1800/ 3493]
loss: 0.013292  [ 1900/ 3493]
loss: 0.013179  [ 2000/ 3493]
loss: 0.012227  [ 2100/ 3493]
loss: 0.014743  [ 2200/ 3493]
loss: 0.021761  [ 2300/ 3493]
loss: 0.014581  [ 2400/ 3493]
loss: 0.077529  [ 2500/ 3493]
loss: 0.007751  [ 2600/ 3493]
loss: 0.009942  [ 2700/ 3493]
loss: 0.019401  [ 2800/ 3493]
loss: 0.007845  [ 2900/ 3493]
loss: 0.038467  [ 3000/ 3493]
loss: 0.014886  [ 3100/ 3493]
loss: 0.006547  [ 3200/ 3493]
loss: 0.023799  [ 3300/ 3493]
loss: 0.010770  [ 3400/ 3493]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3493
First Spike after testing: [-2.5882328  -0.24960452]
[1 0 0 ... 1 0 2]
[1 2 2 ... 1 2 0]
Cluster 0 Occurrences: 1151; KMEANS: 1345
Cluster 1 Occurrences: 1195; KMEANS: 1188
Cluster 2 Occurrences: 1147; KMEANS: 960
Centroids: [[0.79866683, 0.8556337], [-1.5283228, -0.42584234], [0.25828108, 0.6842777]]
Centroids: [[0.27070072, 0.5993571], [-1.5471417, -0.43657616], [0.89904153, 1.013892]]
Contingency Matrix: 
[[ 343    2  806]
 [  10 1182    3]
 [ 992    4  151]]
[[343, -1, 806], [-1, -1, -1], [992, -1, 151]]
[[-1, -1, 806], [-1, -1, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[ 806    2  343]
 [   3 1182   10]
 [ 151    4  992]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [806, 1182, 992], Sum: 2980
All_Elements: [806, 2, 343, 3, 1182, 10, 151, 4, 992], Sum: 3493
Accuracy: 0.853134841110793
Done!
