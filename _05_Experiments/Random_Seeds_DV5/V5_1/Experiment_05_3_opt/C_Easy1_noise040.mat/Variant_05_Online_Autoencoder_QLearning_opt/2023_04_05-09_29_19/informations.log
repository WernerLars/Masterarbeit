Experiment_path: Random_Seeds_DV5//V5_1/Experiment_05_3_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise040.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise040.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: Random_Seeds_DV5//V5_1/Experiment_05_3_opt/C_Easy1_noise040.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_04_05-09_29_19
Punishment_Coefficient: 1.6
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000015F08465198>
Sampling rate: 24000.0
Raw: [ 0.09290294  0.20189621  0.33053674 ... -0.20677271 -0.1502611
 -0.11999569]
Times: [    239     439     824 ... 1439203 1439286 1439464]
Cluster: [1 3 1 ... 3 1 2]
Number of different clusters:  3
Number of Spikes: 3386
First aligned Spike Frame: [ 0.35272875  0.22782651  0.09804678  0.0330907   0.01763465 -0.00716808
 -0.07676483 -0.17393839 -0.25156268 -0.27611297 -0.26424986 -0.2572748
 -0.14070033  0.3091543   0.84618672  0.8657919   0.33633627 -0.19287563
 -0.36620283 -0.28464978 -0.16625656 -0.0936908  -0.03935964  0.02291416
  0.0699242   0.05739865 -0.02453232 -0.1465012  -0.24130255 -0.27021376
 -0.25598501 -0.22429694 -0.17333633 -0.09081612  0.03499496  0.1799287
  0.31672358  0.42401557  0.47266498  0.4416574   0.34587776  0.22594898
  0.10664098 -0.02537482 -0.17575272 -0.29850509 -0.34589514]
Cluster 0, Occurrences: 1079
Cluster 1, Occurrences: 1158
Cluster 2, Occurrences: 1149
Number of Clusters: 3
Online_Training [1/700]: mean_loss=0.12883407436311245
Online_Training [2/700]: mean_loss=0.32970982789993286
Online_Training [3/700]: mean_loss=0.2643314227461815
Online_Training [4/700]: mean_loss=0.22737973369657993
Online_Training [5/700]: mean_loss=0.14828474447131157
Online_Training [6/700]: mean_loss=0.7910403311252594
Online_Training [7/700]: mean_loss=0.27441344782710075
Online_Training [8/700]: mean_loss=0.11504939198493958
Online_Training [9/700]: mean_loss=0.4650845192372799
Online_Training [10/700]: mean_loss=0.37459244951605797
Online_Training [11/700]: mean_loss=0.3511110506951809
Online_Training [12/700]: mean_loss=0.2731182277202606
Online_Training [13/700]: mean_loss=0.23292400501668453
Online_Training [14/700]: mean_loss=0.45455581694841385
Online_Training [15/700]: mean_loss=0.6654925048351288
Online_Training [16/700]: mean_loss=0.2869323566555977
Online_Training [17/700]: mean_loss=0.21713764406740665
Online_Training [18/700]: mean_loss=0.19870816729962826
Online_Training [19/700]: mean_loss=0.1328684538602829
Online_Training [20/700]: mean_loss=0.7538693621754646
Online_Training [21/700]: mean_loss=0.20902574993669987
Online_Training [22/700]: mean_loss=0.21087776869535446
Online_Training [23/700]: mean_loss=0.11408109031617641
Online_Training [24/700]: mean_loss=0.3194211758673191
Online_Training [25/700]: mean_loss=0.4494561441242695
Online_Training [26/700]: mean_loss=0.199080690741539
Online_Training [27/700]: mean_loss=0.3039172440767288
Online_Training [28/700]: mean_loss=0.19161400198936462
Online_Training [29/700]: mean_loss=0.384494137018919
Online_Training [30/700]: mean_loss=0.25750743225216866
Online_Training [31/700]: mean_loss=0.058982040267437696
Online_Training [32/700]: mean_loss=0.3413093760609627
Online_Training [33/700]: mean_loss=0.19220251590013504
Online_Training [34/700]: mean_loss=0.06020728312432766
Online_Training [35/700]: mean_loss=0.12670132890343666
Online_Training [36/700]: mean_loss=0.22819864377379417
Online_Training [37/700]: mean_loss=0.17963102273643017
Online_Training [38/700]: mean_loss=0.06602472485974431
Online_Training [39/700]: mean_loss=0.17237208783626556
Online_Training [40/700]: mean_loss=0.10297563392668962
Online_Training [41/700]: mean_loss=0.12799171172082424
Online_Training [42/700]: mean_loss=0.45398687571287155
Online_Training [43/700]: mean_loss=0.09045161306858063
Online_Training [44/700]: mean_loss=0.29892755672335625
Online_Training [45/700]: mean_loss=0.18142595514655113
Online_Training [46/700]: mean_loss=0.19763104431331158
Online_Training [47/700]: mean_loss=0.23528489284217358
Online_Training [48/700]: mean_loss=0.08950299676507711
Online_Training [49/700]: mean_loss=0.10591680649667978
Online_Training [50/700]: mean_loss=0.11446956358850002
Online_Training [51/700]: mean_loss=0.14492282271385193
Online_Training [52/700]: mean_loss=0.09673136379569769
Online_Training [53/700]: mean_loss=0.0664210906252265
Online_Training [54/700]: mean_loss=0.10681346524506807
Online_Training [55/700]: mean_loss=0.06705786287784576
Online_Training [56/700]: mean_loss=0.0546198976226151
Online_Training [57/700]: mean_loss=0.16183067485690117
Online_Training [58/700]: mean_loss=0.22961418703198433
Online_Training [59/700]: mean_loss=0.0998671529814601
Online_Training [60/700]: mean_loss=0.11660503875464201
Online_Training [61/700]: mean_loss=0.3419211842119694
Online_Training [62/700]: mean_loss=0.17532442323863506
Online_Training [63/700]: mean_loss=0.2636475972831249
Online_Training [64/700]: mean_loss=0.15057112835347652
Online_Training [65/700]: mean_loss=0.063812674023211
Online_Training [66/700]: mean_loss=0.1446541678160429
Online_Training [67/700]: mean_loss=0.1554462080821395
Online_Training [68/700]: mean_loss=0.056892132852226496
Online_Training [69/700]: mean_loss=0.16486769914627075
Online_Training [70/700]: mean_loss=0.2287548966705799
Online_Training [71/700]: mean_loss=0.19263437017798424
Online_Training [72/700]: mean_loss=0.08187865279614925
Online_Training [73/700]: mean_loss=0.23682176880538464
Online_Training [74/700]: mean_loss=0.0692357961088419
Online_Training [75/700]: mean_loss=0.08095451630651951
Online_Training [76/700]: mean_loss=0.07280836254358292
Online_Training [77/700]: mean_loss=0.1813248060643673
Online_Training [78/700]: mean_loss=0.13981078751385212
Online_Training [79/700]: mean_loss=0.11906590219587088
Online_Training [80/700]: mean_loss=0.024853911250829697
Online_Training [81/700]: mean_loss=0.09671346098184586
Online_Training [82/700]: mean_loss=0.025415697367861867
Online_Training [83/700]: mean_loss=0.25488187186419964
Online_Training [84/700]: mean_loss=0.11571029759943485
Online_Training [85/700]: mean_loss=0.14764324203133583
Online_Training [86/700]: mean_loss=0.2359569761902094
Online_Training [87/700]: mean_loss=0.12584956921637058
Online_Training [88/700]: mean_loss=0.17007210105657578
Online_Training [89/700]: mean_loss=0.037942474940791726
Online_Training [90/700]: mean_loss=0.082907535135746
Online_Training [91/700]: mean_loss=0.3465095981955528
Online_Training [92/700]: mean_loss=0.05229326291009784
Online_Training [93/700]: mean_loss=0.12747155595570803
Online_Training [94/700]: mean_loss=0.11085877846926451
Online_Training [95/700]: mean_loss=0.15031755343079567
Online_Training [96/700]: mean_loss=0.0745392283424735
Online_Training [97/700]: mean_loss=0.021077685290947556
Online_Training [98/700]: mean_loss=0.0973455486819148
Online_Training [99/700]: mean_loss=0.08950898237526417
Online_Training [100/700]: mean_loss=0.27539400197565556
Online_Training [101/700]: mean_loss=0.19469671882689
Online_Training [102/700]: mean_loss=0.1342845819890499
Online_Training [103/700]: mean_loss=0.1904498152434826
Online_Training [104/700]: mean_loss=0.22707834094762802
Online_Training [105/700]: mean_loss=0.10678459517657757
Online_Training [106/700]: mean_loss=0.12458523828536272
Online_Training [107/700]: mean_loss=0.1405007392168045
Online_Training [108/700]: mean_loss=0.05975604755803943
Online_Training [109/700]: mean_loss=0.15482099913060665
Online_Training [110/700]: mean_loss=0.15008714236319065
Online_Training [111/700]: mean_loss=0.04047608794644475
Online_Training [112/700]: mean_loss=0.14279082976281643
Online_Training [113/700]: mean_loss=0.036877617705613375
Online_Training [114/700]: mean_loss=0.09044008981436491
Online_Training [115/700]: mean_loss=0.02695652865804732
Online_Training [116/700]: mean_loss=0.13772226870059967
Online_Training [117/700]: mean_loss=0.09771290887147188
Online_Training [118/700]: mean_loss=0.12355779018253088
Online_Training [119/700]: mean_loss=0.12130849994719028
Online_Training [120/700]: mean_loss=0.09118529502302408
Online_Training [121/700]: mean_loss=0.023639260791242123
Online_Training [122/700]: mean_loss=0.13132146187126637
Online_Training [123/700]: mean_loss=0.06675631087273359
Online_Training [124/700]: mean_loss=0.09126988518983126
Online_Training [125/700]: mean_loss=0.07139077596366405
Online_Training [126/700]: mean_loss=0.10565987601876259
Online_Training [127/700]: mean_loss=0.16076787747442722
Online_Training [128/700]: mean_loss=0.1460555586963892
Online_Training [129/700]: mean_loss=0.07498611696064472
Online_Training [130/700]: mean_loss=0.34510814398527145
Online_Training [131/700]: mean_loss=0.08948560990393162
Online_Training [132/700]: mean_loss=0.15578860137611628
Online_Training [133/700]: mean_loss=0.0692402608692646
Online_Training [134/700]: mean_loss=0.09621267300099134
Online_Training [135/700]: mean_loss=0.2060382068157196
Online_Training [136/700]: mean_loss=0.1148450393229723
Online_Training [137/700]: mean_loss=0.13875988498330116
Online_Training [138/700]: mean_loss=0.07663548085838556
Online_Training [139/700]: mean_loss=0.11325891595333815
Online_Training [140/700]: mean_loss=0.11639095563441515
Online_Training [141/700]: mean_loss=0.08701779320836067
Online_Training [142/700]: mean_loss=0.040029844269156456
Online_Training [143/700]: mean_loss=0.20701352134346962
Online_Training [144/700]: mean_loss=0.040342580527067184
Online_Training [145/700]: mean_loss=0.16801781579852104
Online_Training [146/700]: mean_loss=0.05268482258543372
Online_Training [147/700]: mean_loss=0.2859378792345524
Online_Training [148/700]: mean_loss=0.12176960706710815
Online_Training [149/700]: mean_loss=0.32613667100667953
Online_Training [150/700]: mean_loss=0.14831358939409256
Online_Training [151/700]: mean_loss=0.24546215869486332
Online_Training [152/700]: mean_loss=0.2492386195808649
Online_Training [153/700]: mean_loss=0.23413230292499065
Online_Training [154/700]: mean_loss=0.3514403998851776
Online_Training [155/700]: mean_loss=0.3151293657720089
Online_Training [156/700]: mean_loss=0.0628534615971148
Online_Training [157/700]: mean_loss=0.16556960716843605
Online_Training [158/700]: mean_loss=0.08145416621118784
Online_Training [159/700]: mean_loss=0.051172078121453524
Online_Training [160/700]: mean_loss=0.060707435477524996
Online_Training [161/700]: mean_loss=0.18084346130490303
Online_Training [162/700]: mean_loss=0.11352728214114904
Online_Training [163/700]: mean_loss=0.17785928025841713
Online_Training [164/700]: mean_loss=0.16480686888098717
Online_Training [165/700]: mean_loss=0.3285190649330616
Online_Training [166/700]: mean_loss=0.040875186678022146
Online_Training [167/700]: mean_loss=0.12981021218001842
Online_Training [168/700]: mean_loss=0.04498196253553033
Online_Training [169/700]: mean_loss=0.10725315753370523
Online_Training [170/700]: mean_loss=0.1849343702197075
Online_Training [171/700]: mean_loss=0.26518856175243855
Online_Training [172/700]: mean_loss=0.22761942446231842
Online_Training [173/700]: mean_loss=0.1550217978656292
Online_Training [174/700]: mean_loss=0.07218715269118547
Online_Training [175/700]: mean_loss=0.13947336189448833
Online_Training [176/700]: mean_loss=0.10774347372353077
Online_Training [177/700]: mean_loss=0.21864039450883865
Online_Training [178/700]: mean_loss=0.10591831803321838
Online_Training [179/700]: mean_loss=0.11748296488076448
Online_Training [180/700]: mean_loss=0.05120786465704441
Online_Training [181/700]: mean_loss=0.05982161778956652
Online_Training [182/700]: mean_loss=0.09483391791582108
Online_Training [183/700]: mean_loss=0.05594745697453618
Online_Training [184/700]: mean_loss=0.25730263255536556
Online_Training [185/700]: mean_loss=0.2447146773338318
Online_Training [186/700]: mean_loss=0.16649523936212063
Online_Training [187/700]: mean_loss=0.022774440702050924
Online_Training [188/700]: mean_loss=0.09835215844213963
Online_Training [189/700]: mean_loss=0.23480721935629845
Online_Training [190/700]: mean_loss=0.0432694386690855
Online_Training [191/700]: mean_loss=0.0695498501881957
Online_Training [192/700]: mean_loss=0.18246105313301086
Online_Training [193/700]: mean_loss=0.04413829557597637
Online_Training [194/700]: mean_loss=0.16990163922309875
Online_Training [195/700]: mean_loss=0.09760573785752058
Online_Training [196/700]: mean_loss=0.14458213932812214
Online_Training [197/700]: mean_loss=0.28318835981190205
Online_Training [198/700]: mean_loss=0.059749120846390724
Online_Training [199/700]: mean_loss=0.16698237601667643
Online_Training [200/700]: mean_loss=0.172255989164114
Online_Training [201/700]: mean_loss=0.10100273229181767
Online_Training [202/700]: mean_loss=0.09541683923453093
Online_Training [203/700]: mean_loss=0.16760185919702053
Online_Training [204/700]: mean_loss=0.07097792439162731
Online_Training [205/700]: mean_loss=0.24989377707242966
Online_Training [206/700]: mean_loss=0.08074940694496036
Online_Training [207/700]: mean_loss=0.23077572137117386
Online_Training [208/700]: mean_loss=0.08584630768746138
Online_Training [209/700]: mean_loss=0.38790612667798996
Online_Training [210/700]: mean_loss=0.33996114507317543
Online_Training [211/700]: mean_loss=0.09547255747020245
Online_Training [212/700]: mean_loss=0.2956903204321861
Online_Training [213/700]: mean_loss=0.02183601213619113
Online_Training [214/700]: mean_loss=0.08927315566688776
Online_Training [215/700]: mean_loss=0.09744023811072111
Online_Training [216/700]: mean_loss=0.20131315104663372
Online_Training [217/700]: mean_loss=0.09412801451981068
Online_Training [218/700]: mean_loss=0.060478180181235075
Online_Training [219/700]: mean_loss=0.1598556935787201
Online_Training [220/700]: mean_loss=0.19209418073296547
Online_Training [221/700]: mean_loss=0.12439953815191984
Online_Training [222/700]: mean_loss=0.22391338273882866
Online_Training [223/700]: mean_loss=0.08148851338773966
Online_Training [224/700]: mean_loss=0.2636030912399292
Online_Training [225/700]: mean_loss=0.05586113594472408
Online_Training [226/700]: mean_loss=0.06769023276865482
Online_Training [227/700]: mean_loss=0.11622815020382404
Online_Training [228/700]: mean_loss=0.30066078528761864
Online_Training [229/700]: mean_loss=0.2033121045678854
Online_Training [230/700]: mean_loss=0.18755674175918102
Online_Training [231/700]: mean_loss=0.20539951510727406
Online_Training [232/700]: mean_loss=0.6906740516424179
Online_Training [233/700]: mean_loss=0.14488802291452885
Online_Training [234/700]: mean_loss=0.2492045722901821
Online_Training [235/700]: mean_loss=0.16498225927352905
Online_Training [236/700]: mean_loss=0.1414613565430045
Online_Training [237/700]: mean_loss=0.21246114745736122
Online_Training [238/700]: mean_loss=0.1739362422376871
Online_Training [239/700]: mean_loss=0.22787872701883316
Online_Training [240/700]: mean_loss=0.13983719609677792
Online_Training [241/700]: mean_loss=0.05401231814175844
Online_Training [242/700]: mean_loss=0.06706978101283312
Online_Training [243/700]: mean_loss=0.14881039783358574
Online_Training [244/700]: mean_loss=0.1942244078963995
Online_Training [245/700]: mean_loss=0.06319486116990447
Online_Training [246/700]: mean_loss=0.19828998111188412
Online_Training [247/700]: mean_loss=0.1447428222745657
Online_Training [248/700]: mean_loss=0.06453159917145967
Online_Training [249/700]: mean_loss=0.11586015019565821
Online_Training [250/700]: mean_loss=0.1612779013812542
Online_Training [251/700]: mean_loss=0.09341255482286215
Online_Training [252/700]: mean_loss=0.06645933166146278
Online_Training [253/700]: mean_loss=0.08481027651578188
Online_Training [254/700]: mean_loss=0.1410982608795166
Online_Training [255/700]: mean_loss=0.4548926278948784
Online_Training [256/700]: mean_loss=0.38094091042876244
Online_Training [257/700]: mean_loss=0.07878049928694963
Online_Training [258/700]: mean_loss=0.31113947182893753
Online_Training [259/700]: mean_loss=0.13721382059156895
Online_Training [260/700]: mean_loss=0.1308033401146531
Online_Training [261/700]: mean_loss=0.17867009714245796
Online_Training [262/700]: mean_loss=0.21313421800732613
Online_Training [263/700]: mean_loss=0.061439502984285355
Online_Training [264/700]: mean_loss=0.2271581944078207
Online_Training [265/700]: mean_loss=0.1350519210100174
Online_Training [266/700]: mean_loss=0.14241783693432808
Online_Training [267/700]: mean_loss=0.09054138511419296
Online_Training [268/700]: mean_loss=0.08252826426178217
Online_Training [269/700]: mean_loss=0.05671765934675932
Online_Training [270/700]: mean_loss=0.11943083442747593
Online_Training [271/700]: mean_loss=0.08370039705187082
Online_Training [272/700]: mean_loss=0.06642413511872292
Online_Training [273/700]: mean_loss=0.18292640522122383
Online_Training [274/700]: mean_loss=0.05322089605033398
Online_Training [275/700]: mean_loss=0.054739159531891346
Online_Training [276/700]: mean_loss=0.03757755784317851
Online_Training [277/700]: mean_loss=0.10666813049465418
Online_Training [278/700]: mean_loss=0.09087887965142727
Online_Training [279/700]: mean_loss=0.04972332250326872
Online_Training [280/700]: mean_loss=0.16391851752996445
Online_Training [281/700]: mean_loss=0.13546240516006947
Online_Training [282/700]: mean_loss=0.3441308904439211
Online_Training [283/700]: mean_loss=0.10530893690884113
Online_Training [284/700]: mean_loss=0.08094879239797592
Online_Training [285/700]: mean_loss=0.13148834183812141
Online_Training [286/700]: mean_loss=0.18809696286916733
Online_Training [287/700]: mean_loss=0.059721173252910376
Online_Training [288/700]: mean_loss=0.02968649589456618
Online_Training [289/700]: mean_loss=0.18421580083668232
Online_Training [290/700]: mean_loss=0.09293410461395979
Online_Training [291/700]: mean_loss=0.15542870573699474
Online_Training [292/700]: mean_loss=0.05215497827157378
Online_Training [293/700]: mean_loss=0.29444873332977295
Online_Training [294/700]: mean_loss=0.021688606590032578
Online_Training [295/700]: mean_loss=0.162924874573946
Online_Training [296/700]: mean_loss=0.03313885140232742
Online_Training [297/700]: mean_loss=0.1362646222114563
Online_Training [298/700]: mean_loss=0.18152090534567833
Online_Training [299/700]: mean_loss=0.2525413762778044
Online_Training [300/700]: mean_loss=0.20668578892946243
Online_Training [301/700]: mean_loss=0.036210992839187384
Online_Training [302/700]: mean_loss=0.1024981839582324
Online_Training [303/700]: mean_loss=0.257356321439147
Online_Training [304/700]: mean_loss=0.07881215959787369
Online_Training [305/700]: mean_loss=0.022295594681054354
Online_Training [306/700]: mean_loss=0.03279755846597254
Online_Training [307/700]: mean_loss=0.20886785723268986
Online_Training [308/700]: mean_loss=0.06048275390639901
Online_Training [309/700]: mean_loss=0.10872910544276237
Online_Training [310/700]: mean_loss=0.18637548759579659
Online_Training [311/700]: mean_loss=0.08339264988899231
Online_Training [312/700]: mean_loss=0.3260653428733349
Online_Training [313/700]: mean_loss=0.019785062642768025
Online_Training [314/700]: mean_loss=0.09957901854068041
Online_Training [315/700]: mean_loss=0.1343827173113823
Online_Training [316/700]: mean_loss=0.1720012128353119
Online_Training [317/700]: mean_loss=0.030510443961247802
Online_Training [318/700]: mean_loss=0.22946138866245747
Online_Training [319/700]: mean_loss=0.16194921731948853
Online_Training [320/700]: mean_loss=0.09956671390682459
Online_Training [321/700]: mean_loss=0.09932999685406685
Online_Training [322/700]: mean_loss=0.10556002426892519
Online_Training [323/700]: mean_loss=0.09156396426260471
Online_Training [324/700]: mean_loss=0.03539856243878603
Online_Training [325/700]: mean_loss=0.043949391692876816
Online_Training [326/700]: mean_loss=0.12423579208552837
Online_Training [327/700]: mean_loss=0.08392583765089512
Online_Training [328/700]: mean_loss=0.06745622586458921
Online_Training [329/700]: mean_loss=0.06972542963922024
Online_Training [330/700]: mean_loss=0.08681998588144779
Online_Training [331/700]: mean_loss=0.14753756113350391
Online_Training [332/700]: mean_loss=0.11110608000308275
Online_Training [333/700]: mean_loss=0.07057404518127441
Online_Training [334/700]: mean_loss=0.021320446394383907
Online_Training [335/700]: mean_loss=0.12378502171486616
Online_Training [336/700]: mean_loss=0.1544637270271778
Online_Training [337/700]: mean_loss=0.15224496647715569
Online_Training [338/700]: mean_loss=0.4302527494728565
Online_Training [339/700]: mean_loss=0.03507045982405543
Online_Training [340/700]: mean_loss=0.12881183065474033
Online_Training [341/700]: mean_loss=0.05179873900488019
Online_Training [342/700]: mean_loss=0.034698647912591696
Online_Training [343/700]: mean_loss=0.08811869286000729
Online_Training [344/700]: mean_loss=0.05044128466397524
Online_Training [345/700]: mean_loss=0.0566691467538476
Online_Training [346/700]: mean_loss=0.34198688343167305
Online_Training [347/700]: mean_loss=0.3632573038339615
Online_Training [348/700]: mean_loss=0.03800621069967747
Online_Training [349/700]: mean_loss=0.18985844030976295
Online_Training [350/700]: mean_loss=0.17511963099241257
Online_Training [351/700]: mean_loss=0.09861463867127895
Online_Training [352/700]: mean_loss=0.08214758988469839
Online_Training [353/700]: mean_loss=0.09502909611910582
Online_Training [354/700]: mean_loss=0.055806090123951435
Online_Training [355/700]: mean_loss=0.027145572239533067
Online_Training [356/700]: mean_loss=0.0501143392175436
Online_Training [357/700]: mean_loss=0.21992269717156887
Online_Training [358/700]: mean_loss=0.09268420934677124
Online_Training [359/700]: mean_loss=0.07588907983154058
Online_Training [360/700]: mean_loss=0.21280717849731445
Online_Training [361/700]: mean_loss=0.06037304783239961
Online_Training [362/700]: mean_loss=0.09533806703984737
Online_Training [363/700]: mean_loss=0.05998363858088851
Online_Training [364/700]: mean_loss=0.054804244078695774
Online_Training [365/700]: mean_loss=0.09300206415355206
Online_Training [366/700]: mean_loss=0.05149072641506791
Online_Training [367/700]: mean_loss=0.08660168945789337
Online_Training [368/700]: mean_loss=0.06304607866331935
Online_Training [369/700]: mean_loss=0.22801298834383488
Online_Training [370/700]: mean_loss=0.09118487872183323
Online_Training [371/700]: mean_loss=0.13789202459156513
Online_Training [372/700]: mean_loss=0.2597971558570862
Online_Training [373/700]: mean_loss=0.3782069571316242
Online_Training [374/700]: mean_loss=0.05230739386752248
Online_Training [375/700]: mean_loss=0.08016868215054274
Online_Training [376/700]: mean_loss=0.072072793263942
Online_Training [377/700]: mean_loss=0.051781835965812206
Online_Training [378/700]: mean_loss=0.07163158431649208
Online_Training [379/700]: mean_loss=0.11512110382318497
Online_Training [380/700]: mean_loss=0.1429472453892231
Online_Training [381/700]: mean_loss=0.13082862924784422
Online_Training [382/700]: mean_loss=0.06997948791831732
Online_Training [383/700]: mean_loss=0.03077583876438439
Online_Training [384/700]: mean_loss=0.028190140379592776
Online_Training [385/700]: mean_loss=0.09330563992261887
Online_Training [386/700]: mean_loss=0.09155228268355131
Online_Training [387/700]: mean_loss=0.11450635734945536
Online_Training [388/700]: mean_loss=0.1250295229256153
Online_Training [389/700]: mean_loss=0.06319716852158308
Online_Training [390/700]: mean_loss=0.23501627519726753
Online_Training [391/700]: mean_loss=0.23472033068537712
Online_Training [392/700]: mean_loss=0.13592587038874626
Online_Training [393/700]: mean_loss=0.120122991502285
Online_Training [394/700]: mean_loss=0.07435917481780052
Online_Training [395/700]: mean_loss=0.036061807069927454
Online_Training [396/700]: mean_loss=0.055557649582624435
Online_Training [397/700]: mean_loss=0.11804464925080538
Online_Training [398/700]: mean_loss=0.328646969050169
Online_Training [399/700]: mean_loss=0.05892921844497323
Online_Training [400/700]: mean_loss=0.24323517084121704
Online_Training [401/700]: mean_loss=0.23592944256961346
Online_Training [402/700]: mean_loss=0.157028092071414
Online_Training [403/700]: mean_loss=0.06794954696670175
Online_Training [404/700]: mean_loss=0.29103275015950203
Online_Training [405/700]: mean_loss=0.09058755729347467
Online_Training [406/700]: mean_loss=0.14139761961996555
Online_Training [407/700]: mean_loss=0.31119874119758606
Online_Training [408/700]: mean_loss=0.023808862548321486
Online_Training [409/700]: mean_loss=0.24198612570762634
Online_Training [410/700]: mean_loss=0.29555442929267883
Online_Training [411/700]: mean_loss=0.10189471952617168
Online_Training [412/700]: mean_loss=0.14873212203383446
Online_Training [413/700]: mean_loss=0.030265825800597668
Online_Training [414/700]: mean_loss=0.09773571044206619
Online_Training [415/700]: mean_loss=0.2026991043239832
Online_Training [416/700]: mean_loss=0.06495749531313777
Online_Training [417/700]: mean_loss=0.33257295936346054
Online_Training [418/700]: mean_loss=0.33072895742952824
Online_Training [419/700]: mean_loss=0.47310784459114075
Online_Training [420/700]: mean_loss=0.20945331268012524
Online_Training [421/700]: mean_loss=0.1861496027559042
Online_Training [422/700]: mean_loss=0.253921402618289
Online_Training [423/700]: mean_loss=0.034126168582588434
Online_Training [424/700]: mean_loss=0.11830189451575279
Online_Training [425/700]: mean_loss=0.06010112352669239
Online_Training [426/700]: mean_loss=0.04443049570545554
Online_Training [427/700]: mean_loss=0.13697593566030264
Online_Training [428/700]: mean_loss=0.16515192575752735
Online_Training [429/700]: mean_loss=0.11042874027043581
Online_Training [430/700]: mean_loss=0.07629294414073229
Online_Training [431/700]: mean_loss=0.07554996758699417
Online_Training [432/700]: mean_loss=0.11069412343204021
Online_Training [433/700]: mean_loss=0.142436433583498
Online_Training [434/700]: mean_loss=0.17922957986593246
Online_Training [435/700]: mean_loss=0.10347828455269337
Online_Training [436/700]: mean_loss=0.08018770162016153
Online_Training [437/700]: mean_loss=0.08069414738565683
Online_Training [438/700]: mean_loss=0.04965517530217767
Online_Training [439/700]: mean_loss=0.12611548509448767
Online_Training [440/700]: mean_loss=0.18604428879916668
Online_Training [441/700]: mean_loss=0.1392714250832796
Online_Training [442/700]: mean_loss=0.06174518959596753
Online_Training [443/700]: mean_loss=0.12227683328092098
Online_Training [444/700]: mean_loss=0.2793775387108326
Online_Training [445/700]: mean_loss=0.12156012281775475
Online_Training [446/700]: mean_loss=0.13087988831102848
Online_Training [447/700]: mean_loss=0.16100732050836086
Online_Training [448/700]: mean_loss=0.10182052478194237
Online_Training [449/700]: mean_loss=0.06690259696915746
Online_Training [450/700]: mean_loss=0.1279958477243781
Online_Training [451/700]: mean_loss=0.08259270992130041
Online_Training [452/700]: mean_loss=0.07979900017380714
Online_Training [453/700]: mean_loss=0.22157912515103817
Online_Training [454/700]: mean_loss=0.2001271266490221
Online_Training [455/700]: mean_loss=0.15442896075546741
Online_Training [456/700]: mean_loss=0.22654986754059792
Online_Training [457/700]: mean_loss=0.04095738707110286
Online_Training [458/700]: mean_loss=0.1758519485592842
Online_Training [459/700]: mean_loss=0.1491110809147358
Online_Training [460/700]: mean_loss=0.07504280563443899
Online_Training [461/700]: mean_loss=0.07062521809712052
Online_Training [462/700]: mean_loss=0.04611451644450426
Online_Training [463/700]: mean_loss=0.04626577673479915
Online_Training [464/700]: mean_loss=0.03293459257110953
Online_Training [465/700]: mean_loss=0.08589306566864252
Online_Training [466/700]: mean_loss=0.09112929552793503
Online_Training [467/700]: mean_loss=0.03479386866092682
Online_Training [468/700]: mean_loss=0.07213067915290594
Online_Training [469/700]: mean_loss=0.14839453250169754
Online_Training [470/700]: mean_loss=0.07945622270926833
Online_Training [471/700]: mean_loss=0.12234443798661232
Online_Training [472/700]: mean_loss=0.032496587140485644
Online_Training [473/700]: mean_loss=0.13728982862085104
Online_Training [474/700]: mean_loss=0.0940528679639101
Online_Training [475/700]: mean_loss=0.1190739031881094
Online_Training [476/700]: mean_loss=0.11511528491973877
Online_Training [477/700]: mean_loss=0.10973390657454729
Online_Training [478/700]: mean_loss=0.01737211807630956
Online_Training [479/700]: mean_loss=0.2874903790652752
Online_Training [480/700]: mean_loss=0.18785742297768593
Online_Training [481/700]: mean_loss=0.09435493499040604
Online_Training [482/700]: mean_loss=0.11866631079465151
Online_Training [483/700]: mean_loss=0.2332216203212738
Online_Training [484/700]: mean_loss=0.06986230425536633
Online_Training [485/700]: mean_loss=0.07832013629376888
Online_Training [486/700]: mean_loss=0.1673161294311285
Online_Training [487/700]: mean_loss=0.06776280235499144
Online_Training [488/700]: mean_loss=0.05216976720839739
Online_Training [489/700]: mean_loss=0.0698170717805624
Online_Training [490/700]: mean_loss=0.10900967009365559
Online_Training [491/700]: mean_loss=0.06667924020439386
Online_Training [492/700]: mean_loss=0.04063185676932335
Online_Training [493/700]: mean_loss=0.24053450301289558
Online_Training [494/700]: mean_loss=0.2873211167752743
Online_Training [495/700]: mean_loss=0.0401365221478045
Online_Training [496/700]: mean_loss=0.1702189687639475
Online_Training [497/700]: mean_loss=0.36190129071474075
Online_Training [498/700]: mean_loss=0.09032062906771898
Online_Training [499/700]: mean_loss=0.0847581084817648
Online_Training [500/700]: mean_loss=0.048074740916490555
Online_Training [501/700]: mean_loss=0.08910136763006449
Online_Training [502/700]: mean_loss=0.1744058020412922
Online_Training [503/700]: mean_loss=0.1370988180860877
Online_Training [504/700]: mean_loss=0.03615661943331361
Online_Training [505/700]: mean_loss=0.0481932177208364
Online_Training [506/700]: mean_loss=0.21006238460540771
Online_Training [507/700]: mean_loss=0.1025925213471055
Online_Training [508/700]: mean_loss=0.049683846067637205
Online_Training [509/700]: mean_loss=0.04383605532348156
Online_Training [510/700]: mean_loss=0.2404664997011423
Online_Training [511/700]: mean_loss=0.0850618900731206
Online_Training [512/700]: mean_loss=0.08615265227854252
Online_Training [513/700]: mean_loss=0.12579910643398762
Online_Training [514/700]: mean_loss=0.18082431890070438
Online_Training [515/700]: mean_loss=0.11776556260883808
Online_Training [516/700]: mean_loss=0.17003031261265278
Online_Training [517/700]: mean_loss=0.21169462613761425
Online_Training [518/700]: mean_loss=0.05821602698415518
Online_Training [519/700]: mean_loss=0.06825643638148904
Online_Training [520/700]: mean_loss=0.06465035816654563
Online_Training [521/700]: mean_loss=0.10873133596032858
Online_Training [522/700]: mean_loss=0.11893733311444521
Online_Training [523/700]: mean_loss=0.0966293727979064
Online_Training [524/700]: mean_loss=0.0381324402987957
Online_Training [525/700]: mean_loss=0.09349843859672546
Online_Training [526/700]: mean_loss=0.1522247362881899
Online_Training [527/700]: mean_loss=0.0641174167394638
Online_Training [528/700]: mean_loss=0.13032212853431702
Online_Training [529/700]: mean_loss=0.044567505829036236
Online_Training [530/700]: mean_loss=0.26836368441581726
Online_Training [531/700]: mean_loss=0.15230803191661835
Online_Training [532/700]: mean_loss=0.0465226205997169
Online_Training [533/700]: mean_loss=0.09309414867311716
Online_Training [534/700]: mean_loss=0.08929666876792908
Online_Training [535/700]: mean_loss=0.11048565618693829
Online_Training [536/700]: mean_loss=0.142004257068038
Online_Training [537/700]: mean_loss=0.02422770904377103
Online_Training [538/700]: mean_loss=0.06540198624134064
Online_Training [539/700]: mean_loss=0.08488965034484863
Online_Training [540/700]: mean_loss=0.03906681202352047
Online_Training [541/700]: mean_loss=0.0829075900837779
Online_Training [542/700]: mean_loss=0.1640204656869173
Online_Training [543/700]: mean_loss=0.18887797184288502
Online_Training [544/700]: mean_loss=0.05142555106431246
Online_Training [545/700]: mean_loss=0.09302268829196692
Online_Training [546/700]: mean_loss=0.1064983457326889
Online_Training [547/700]: mean_loss=0.2214439082890749
Online_Training [548/700]: mean_loss=0.06608158117160201
Online_Training [549/700]: mean_loss=0.06590732373297215
Online_Training [550/700]: mean_loss=0.028203607071191072
Online_Training [551/700]: mean_loss=0.0515051600523293
Online_Training [552/700]: mean_loss=0.04163343133404851
Online_Training [553/700]: mean_loss=0.14491065312176943
Online_Training [554/700]: mean_loss=0.10647089313715696
Online_Training [555/700]: mean_loss=0.19851518981158733
Online_Training [556/700]: mean_loss=0.24043518863618374
Online_Training [557/700]: mean_loss=0.06853604456409812
Online_Training [558/700]: mean_loss=0.13522927463054657
Online_Training [559/700]: mean_loss=0.1291796648874879
Online_Training [560/700]: mean_loss=0.02399587887339294
Online_Training [561/700]: mean_loss=0.04910123301669955
Online_Training [562/700]: mean_loss=0.04761810414493084
Online_Training [563/700]: mean_loss=0.10730754025280476
Online_Training [564/700]: mean_loss=0.12976362649351358
Online_Training [565/700]: mean_loss=0.08331333287060261
Online_Training [566/700]: mean_loss=0.13558634836226702
Online_Training [567/700]: mean_loss=0.10312920063734055
Online_Training [568/700]: mean_loss=0.029229014879092574
Online_Training [569/700]: mean_loss=0.22606295347213745
Online_Training [570/700]: mean_loss=0.06159901386126876
Online_Training [571/700]: mean_loss=0.04459761315956712
Online_Training [572/700]: mean_loss=0.038174549117684364
Online_Training [573/700]: mean_loss=0.08878583833575249
Online_Training [574/700]: mean_loss=0.34014276415109634
Online_Training [575/700]: mean_loss=0.07482678070664406
Online_Training [576/700]: mean_loss=0.19505532458424568
Online_Training [577/700]: mean_loss=0.1983450185507536
Online_Training [578/700]: mean_loss=0.16551191359758377
Online_Training [579/700]: mean_loss=0.06889487523585558
Online_Training [580/700]: mean_loss=0.09601305332034826
Online_Training [581/700]: mean_loss=0.10195359401404858
Online_Training [582/700]: mean_loss=0.16160514764487743
Online_Training [583/700]: mean_loss=0.1675884947180748
Online_Training [584/700]: mean_loss=0.04857169697061181
Online_Training [585/700]: mean_loss=0.28859150409698486
Online_Training [586/700]: mean_loss=0.17341803573071957
Online_Training [587/700]: mean_loss=0.07993061654269695
Online_Training [588/700]: mean_loss=0.10211247764527798
Online_Training [589/700]: mean_loss=0.03440699586644769
Online_Training [590/700]: mean_loss=0.2641695886850357
Online_Training [591/700]: mean_loss=0.03812397085130215
Online_Training [592/700]: mean_loss=0.04266035952605307
Online_Training [593/700]: mean_loss=0.36284954473376274
Online_Training [594/700]: mean_loss=0.1635024957358837
Online_Training [595/700]: mean_loss=0.141939596273005
Online_Training [596/700]: mean_loss=0.07137078559026122
Online_Training [597/700]: mean_loss=0.03149137529544532
Online_Training [598/700]: mean_loss=0.065299351233989
Online_Training [599/700]: mean_loss=0.15387604385614395
Online_Training [600/700]: mean_loss=0.12099934834986925
Online_Training [601/700]: mean_loss=0.18550363555550575
Online_Training [602/700]: mean_loss=0.11418066639453173
Online_Training [603/700]: mean_loss=0.09693527035415173
Online_Training [604/700]: mean_loss=0.12332862708717585
Online_Training [605/700]: mean_loss=0.3246091306209564
Online_Training [606/700]: mean_loss=0.13527975510805845
Online_Training [607/700]: mean_loss=0.07989683002233505
Online_Training [608/700]: mean_loss=0.052993662655353546
Online_Training [609/700]: mean_loss=0.04797537997364998
Online_Training [610/700]: mean_loss=0.19304044358432293
Online_Training [611/700]: mean_loss=0.03531377296894789
Online_Training [612/700]: mean_loss=0.06893391627818346
Online_Training [613/700]: mean_loss=0.11988370679318905
Online_Training [614/700]: mean_loss=0.08086481597274542
Online_Training [615/700]: mean_loss=0.07657577283680439
Online_Training [616/700]: mean_loss=0.016885173390619457
Online_Training [617/700]: mean_loss=0.09544648882001638
Online_Training [618/700]: mean_loss=0.3383706584572792
Online_Training [619/700]: mean_loss=0.13377306424081326
Online_Training [620/700]: mean_loss=0.12327512539923191
Online_Training [621/700]: mean_loss=0.27438920736312866
Online_Training [622/700]: mean_loss=0.12991243600845337
Online_Training [623/700]: mean_loss=0.15018274262547493
Online_Training [624/700]: mean_loss=0.07313998695462942
Online_Training [625/700]: mean_loss=0.04846859676763415
Online_Training [626/700]: mean_loss=0.1425905954092741
Online_Training [627/700]: mean_loss=0.10453182086348534
Online_Training [628/700]: mean_loss=0.07911587413400412
Online_Training [629/700]: mean_loss=0.1531655304133892
Online_Training [630/700]: mean_loss=0.265105452388525
Online_Training [631/700]: mean_loss=0.07443977426737547
Online_Training [632/700]: mean_loss=0.06461732788011432
Online_Training [633/700]: mean_loss=0.3011244498193264
Online_Training [634/700]: mean_loss=0.05847884062677622
Online_Training [635/700]: mean_loss=0.39649636298418045
Online_Training [636/700]: mean_loss=0.12595626711845398
Online_Training [637/700]: mean_loss=0.09170350898057222
Online_Training [638/700]: mean_loss=0.06746091321110725
Online_Training [639/700]: mean_loss=0.07877469155937433
Online_Training [640/700]: mean_loss=0.04231088003143668
Online_Training [641/700]: mean_loss=0.0642378986813128
Online_Training [642/700]: mean_loss=0.03260072972625494
Online_Training [643/700]: mean_loss=0.20571929030120373
Online_Training [644/700]: mean_loss=0.057673134841024876
Online_Training [645/700]: mean_loss=0.1642572432756424
Online_Training [646/700]: mean_loss=0.06757267843931913
Online_Training [647/700]: mean_loss=0.12832188792526722
Online_Training [648/700]: mean_loss=0.027382267639040947
Online_Training [649/700]: mean_loss=0.12670673429965973
Online_Training [650/700]: mean_loss=0.1775819156318903
Online_Training [651/700]: mean_loss=0.37454821541905403
Online_Training [652/700]: mean_loss=0.1308619501069188
Online_Training [653/700]: mean_loss=0.066729879938066
Online_Training [654/700]: mean_loss=0.041758837178349495
Online_Training [655/700]: mean_loss=0.1480073630809784
Online_Training [656/700]: mean_loss=0.14979711081832647
Online_Training [657/700]: mean_loss=0.16999420523643494
Online_Training [658/700]: mean_loss=0.12277003470808268
Online_Training [659/700]: mean_loss=0.05189172783866525
Online_Training [660/700]: mean_loss=0.15753458626568317
Online_Training [661/700]: mean_loss=0.05657591996714473
Online_Training [662/700]: mean_loss=0.08169251773506403
Online_Training [663/700]: mean_loss=0.04747141199186444
Online_Training [664/700]: mean_loss=0.15674001164734364
Online_Training [665/700]: mean_loss=0.036437124479562044
Online_Training [666/700]: mean_loss=0.2258552499115467
Online_Training [667/700]: mean_loss=0.04980572825297713
Online_Training [668/700]: mean_loss=0.07082179095596075
Online_Training [669/700]: mean_loss=0.09896157868206501
Online_Training [670/700]: mean_loss=0.0982789983972907
Online_Training [671/700]: mean_loss=0.2031125072389841
Online_Training [672/700]: mean_loss=0.07834191247820854
Online_Training [673/700]: mean_loss=0.09309308975934982
Online_Training [674/700]: mean_loss=0.0753707131370902
Online_Training [675/700]: mean_loss=0.13896829448640347
Online_Training [676/700]: mean_loss=0.05268474668264389
Online_Training [677/700]: mean_loss=0.10387848038226366
Online_Training [678/700]: mean_loss=0.08360118232667446
Online_Training [679/700]: mean_loss=0.27650120109319687
Online_Training [680/700]: mean_loss=0.11908539943397045
Online_Training [681/700]: mean_loss=0.31097421422600746
Online_Training [682/700]: mean_loss=0.07666198629885912
Online_Training [683/700]: mean_loss=0.03682364150881767
Online_Training [684/700]: mean_loss=0.041769892908632755
Online_Training [685/700]: mean_loss=0.08660939149558544
Online_Training [686/700]: mean_loss=0.08989561069756746
Online_Training [687/700]: mean_loss=0.025650824885815382
Online_Training [688/700]: mean_loss=0.07235607132315636
Online_Training [689/700]: mean_loss=0.0751494662836194
Online_Training [690/700]: mean_loss=0.11594475619494915
Online_Training [691/700]: mean_loss=0.08102655410766602
Online_Training [692/700]: mean_loss=0.06327684596180916
Online_Training [693/700]: mean_loss=0.09704873990267515
Online_Training [694/700]: mean_loss=0.0416269744746387
Online_Training [695/700]: mean_loss=0.1117058489471674
Online_Training [696/700]: mean_loss=0.0741876196116209
Online_Training [697/700]: mean_loss=0.05174604244530201
Online_Training [698/700]: mean_loss=0.034799197455868125
Online_Training [699/700]: mean_loss=0.045785374473780394
Online_Training [700/700]: mean_loss=0.10799102764576674
Q_Learning [1/300]: mean_loss=0.12883407436311245
Q_Learning [2/300]: mean_loss=0.32970982789993286
Q_Learning [3/300]: mean_loss=0.2643314227461815
Q_Learning [4/300]: mean_loss=0.22737973369657993
Q_Learning [5/300]: mean_loss=0.14828474447131157
Q_Learning [6/300]: mean_loss=0.7910403311252594
Q_Learning [7/300]: mean_loss=0.27441344782710075
Q_Learning [8/300]: mean_loss=0.11504939198493958
Q_Learning [9/300]: mean_loss=0.4650845192372799
Q_Learning [10/300]: mean_loss=0.37459244951605797
Q_Learning [11/300]: mean_loss=0.3511110506951809
Q_Learning [12/300]: mean_loss=0.2731182277202606
Q_Learning [13/300]: mean_loss=0.23292400501668453
Q_Learning [14/300]: mean_loss=0.45455581694841385
Q_Learning [15/300]: mean_loss=0.6654925048351288
Q_Learning [16/300]: mean_loss=0.2869323566555977
Q_Learning [17/300]: mean_loss=0.21713764406740665
Q_Learning [18/300]: mean_loss=0.19870816729962826
Q_Learning [19/300]: mean_loss=0.1328684538602829
Q_Learning [20/300]: mean_loss=0.7538693621754646
Q_Learning [21/300]: mean_loss=0.20902574993669987
Q_Learning [22/300]: mean_loss=0.21087776869535446
Q_Learning [23/300]: mean_loss=0.11408109031617641
Q_Learning [24/300]: mean_loss=0.3194211758673191
Q_Learning [25/300]: mean_loss=0.4494561441242695
Q_Learning [26/300]: mean_loss=0.199080690741539
Q_Learning [27/300]: mean_loss=0.3039172440767288
Q_Learning [28/300]: mean_loss=0.19161400198936462
Q_Learning [29/300]: mean_loss=0.384494137018919
Q_Learning [30/300]: mean_loss=0.25750743225216866
Q_Learning [31/300]: mean_loss=0.058982040267437696
Q_Learning [32/300]: mean_loss=0.3413093760609627
Q_Learning [33/300]: mean_loss=0.19220251590013504
Q_Learning [34/300]: mean_loss=0.06020728312432766
Q_Learning [35/300]: mean_loss=0.12670132890343666
Q_Learning [36/300]: mean_loss=0.22819864377379417
Q_Learning [37/300]: mean_loss=0.17963102273643017
Q_Learning [38/300]: mean_loss=0.06602472485974431
Q_Learning [39/300]: mean_loss=0.17237208783626556
Q_Learning [40/300]: mean_loss=0.10297563392668962
Q_Learning [41/300]: mean_loss=0.12799171172082424
Q_Learning [42/300]: mean_loss=0.45398687571287155
Q_Learning [43/300]: mean_loss=0.09045161306858063
Q_Learning [44/300]: mean_loss=0.29892755672335625
Q_Learning [45/300]: mean_loss=0.18142595514655113
Q_Learning [46/300]: mean_loss=0.19763104431331158
Q_Learning [47/300]: mean_loss=0.23528489284217358
Q_Learning [48/300]: mean_loss=0.08950299676507711
Q_Learning [49/300]: mean_loss=0.10591680649667978
Q_Learning [50/300]: mean_loss=0.11446956358850002
Q_Learning [51/300]: mean_loss=0.14492282271385193
Q_Learning [52/300]: mean_loss=0.09673136379569769
Q_Learning [53/300]: mean_loss=0.0664210906252265
Q_Learning [54/300]: mean_loss=0.10681346524506807
Q_Learning [55/300]: mean_loss=0.06705786287784576
Q_Learning [56/300]: mean_loss=0.0546198976226151
Q_Learning [57/300]: mean_loss=0.16183067485690117
Q_Learning [58/300]: mean_loss=0.22961418703198433
Q_Learning [59/300]: mean_loss=0.0998671529814601
Q_Learning [60/300]: mean_loss=0.11660503875464201
Q_Learning [61/300]: mean_loss=0.3419211842119694
Q_Learning [62/300]: mean_loss=0.17532442323863506
Q_Learning [63/300]: mean_loss=0.2636475972831249
Q_Learning [64/300]: mean_loss=0.15057112835347652
Q_Learning [65/300]: mean_loss=0.063812674023211
Q_Learning [66/300]: mean_loss=0.1446541678160429
Q_Learning [67/300]: mean_loss=0.1554462080821395
Q_Learning [68/300]: mean_loss=0.056892132852226496
Q_Learning [69/300]: mean_loss=0.16486769914627075
Q_Learning [70/300]: mean_loss=0.2287548966705799
Q_Learning [71/300]: mean_loss=0.19263437017798424
Q_Learning [72/300]: mean_loss=0.08187865279614925
Q_Learning [73/300]: mean_loss=0.23682176880538464
Q_Learning [74/300]: mean_loss=0.0692357961088419
Q_Learning [75/300]: mean_loss=0.08095451630651951
Q_Learning [76/300]: mean_loss=0.07280836254358292
Q_Learning [77/300]: mean_loss=0.1813248060643673
Q_Learning [78/300]: mean_loss=0.13981078751385212
Q_Learning [79/300]: mean_loss=0.11906590219587088
Q_Learning [80/300]: mean_loss=0.024853911250829697
Q_Learning [81/300]: mean_loss=0.09671346098184586
Q_Learning [82/300]: mean_loss=0.025415697367861867
Q_Learning [83/300]: mean_loss=0.25488187186419964
Q_Learning [84/300]: mean_loss=0.11571029759943485
Q_Learning [85/300]: mean_loss=0.14764324203133583
Q_Learning [86/300]: mean_loss=0.2359569761902094
Q_Learning [87/300]: mean_loss=0.12584956921637058
Q_Learning [88/300]: mean_loss=0.17007210105657578
Q_Learning [89/300]: mean_loss=0.037942474940791726
Q_Learning [90/300]: mean_loss=0.082907535135746
Q_Learning [91/300]: mean_loss=0.3465095981955528
Q_Learning [92/300]: mean_loss=0.05229326291009784
Q_Learning [93/300]: mean_loss=0.12747155595570803
Q_Learning [94/300]: mean_loss=0.11085877846926451
Q_Learning [95/300]: mean_loss=0.15031755343079567
Q_Learning [96/300]: mean_loss=0.0745392283424735
Q_Learning [97/300]: mean_loss=0.021077685290947556
Q_Learning [98/300]: mean_loss=0.0973455486819148
Q_Learning [99/300]: mean_loss=0.08950898237526417
Q_Learning [100/300]: mean_loss=0.27539400197565556
Q_Learning [101/300]: mean_loss=0.19469671882689
Q_Learning [102/300]: mean_loss=0.1342845819890499
Q_Learning [103/300]: mean_loss=0.1904498152434826
Q_Learning [104/300]: mean_loss=0.22707834094762802
Q_Learning [105/300]: mean_loss=0.10678459517657757
Q_Learning [106/300]: mean_loss=0.12458523828536272
Q_Learning [107/300]: mean_loss=0.1405007392168045
Q_Learning [108/300]: mean_loss=0.05975604755803943
Q_Learning [109/300]: mean_loss=0.15482099913060665
Q_Learning [110/300]: mean_loss=0.15008714236319065
Q_Learning [111/300]: mean_loss=0.04047608794644475
Q_Learning [112/300]: mean_loss=0.14279082976281643
Q_Learning [113/300]: mean_loss=0.036877617705613375
Q_Learning [114/300]: mean_loss=0.09044008981436491
Q_Learning [115/300]: mean_loss=0.02695652865804732
Q_Learning [116/300]: mean_loss=0.13772226870059967
Q_Learning [117/300]: mean_loss=0.09771290887147188
Q_Learning [118/300]: mean_loss=0.12355779018253088
Q_Learning [119/300]: mean_loss=0.12130849994719028
Q_Learning [120/300]: mean_loss=0.09118529502302408
Q_Learning [121/300]: mean_loss=0.023639260791242123
Q_Learning [122/300]: mean_loss=0.13132146187126637
Q_Learning [123/300]: mean_loss=0.06675631087273359
Q_Learning [124/300]: mean_loss=0.09126988518983126
Q_Learning [125/300]: mean_loss=0.07139077596366405
Q_Learning [126/300]: mean_loss=0.10565987601876259
Q_Learning [127/300]: mean_loss=0.16076787747442722
Q_Learning [128/300]: mean_loss=0.1460555586963892
Q_Learning [129/300]: mean_loss=0.07498611696064472
Q_Learning [130/300]: mean_loss=0.34510814398527145
Q_Learning [131/300]: mean_loss=0.08948560990393162
Q_Learning [132/300]: mean_loss=0.15578860137611628
Q_Learning [133/300]: mean_loss=0.0692402608692646
Q_Learning [134/300]: mean_loss=0.09621267300099134
Q_Learning [135/300]: mean_loss=0.2060382068157196
Q_Learning [136/300]: mean_loss=0.1148450393229723
Q_Learning [137/300]: mean_loss=0.13875988498330116
Q_Learning [138/300]: mean_loss=0.07663548085838556
Q_Learning [139/300]: mean_loss=0.11325891595333815
Q_Learning [140/300]: mean_loss=0.11639095563441515
Q_Learning [141/300]: mean_loss=0.08701779320836067
Q_Learning [142/300]: mean_loss=0.040029844269156456
Q_Learning [143/300]: mean_loss=0.20701352134346962
Q_Learning [144/300]: mean_loss=0.040342580527067184
Q_Learning [145/300]: mean_loss=0.16801781579852104
Q_Learning [146/300]: mean_loss=0.05268482258543372
Q_Learning [147/300]: mean_loss=0.2859378792345524
Q_Learning [148/300]: mean_loss=0.12176960706710815
Q_Learning [149/300]: mean_loss=0.32613667100667953
Q_Learning [150/300]: mean_loss=0.14831358939409256
Q_Learning [151/300]: mean_loss=0.24546215869486332
Q_Learning [152/300]: mean_loss=0.2492386195808649
Q_Learning [153/300]: mean_loss=0.23413230292499065
Q_Learning [154/300]: mean_loss=0.3514403998851776
Q_Learning [155/300]: mean_loss=0.3151293657720089
Q_Learning [156/300]: mean_loss=0.0628534615971148
Q_Learning [157/300]: mean_loss=0.16556960716843605
Q_Learning [158/300]: mean_loss=0.08145416621118784
Q_Learning [159/300]: mean_loss=0.051172078121453524
Q_Learning [160/300]: mean_loss=0.060707435477524996
Q_Learning [161/300]: mean_loss=0.18084346130490303
Q_Learning [162/300]: mean_loss=0.11352728214114904
Q_Learning [163/300]: mean_loss=0.17785928025841713
Q_Learning [164/300]: mean_loss=0.16480686888098717
Q_Learning [165/300]: mean_loss=0.3285190649330616
Q_Learning [166/300]: mean_loss=0.040875186678022146
Q_Learning [167/300]: mean_loss=0.12981021218001842
Q_Learning [168/300]: mean_loss=0.04498196253553033
Q_Learning [169/300]: mean_loss=0.10725315753370523
Q_Learning [170/300]: mean_loss=0.1849343702197075
Q_Learning [171/300]: mean_loss=0.26518856175243855
Q_Learning [172/300]: mean_loss=0.22761942446231842
Q_Learning [173/300]: mean_loss=0.1550217978656292
Q_Learning [174/300]: mean_loss=0.07218715269118547
Q_Learning [175/300]: mean_loss=0.13947336189448833
Q_Learning [176/300]: mean_loss=0.10774347372353077
Q_Learning [177/300]: mean_loss=0.21864039450883865
Q_Learning [178/300]: mean_loss=0.10591831803321838
Q_Learning [179/300]: mean_loss=0.11748296488076448
Q_Learning [180/300]: mean_loss=0.05120786465704441
Q_Learning [181/300]: mean_loss=0.05982161778956652
Q_Learning [182/300]: mean_loss=0.09483391791582108
Q_Learning [183/300]: mean_loss=0.05594745697453618
Q_Learning [184/300]: mean_loss=0.25730263255536556
Q_Learning [185/300]: mean_loss=0.2447146773338318
Q_Learning [186/300]: mean_loss=0.16649523936212063
Q_Learning [187/300]: mean_loss=0.022774440702050924
Q_Learning [188/300]: mean_loss=0.09835215844213963
Q_Learning [189/300]: mean_loss=0.23480721935629845
Q_Learning [190/300]: mean_loss=0.0432694386690855
Q_Learning [191/300]: mean_loss=0.0695498501881957
Q_Learning [192/300]: mean_loss=0.18246105313301086
Q_Learning [193/300]: mean_loss=0.04413829557597637
Q_Learning [194/300]: mean_loss=0.16990163922309875
Q_Learning [195/300]: mean_loss=0.09760573785752058
Q_Learning [196/300]: mean_loss=0.14458213932812214
Q_Learning [197/300]: mean_loss=0.28318835981190205
Q_Learning [198/300]: mean_loss=0.059749120846390724
Q_Learning [199/300]: mean_loss=0.16698237601667643
Q_Learning [200/300]: mean_loss=0.172255989164114
Q_Learning [201/300]: mean_loss=0.10100273229181767
Q_Learning [202/300]: mean_loss=0.09541683923453093
Q_Learning [203/300]: mean_loss=0.16760185919702053
Q_Learning [204/300]: mean_loss=0.07097792439162731
Q_Learning [205/300]: mean_loss=0.24989377707242966
Q_Learning [206/300]: mean_loss=0.08074940694496036
Q_Learning [207/300]: mean_loss=0.23077572137117386
Q_Learning [208/300]: mean_loss=0.08584630768746138
Q_Learning [209/300]: mean_loss=0.38790612667798996
Q_Learning [210/300]: mean_loss=0.33996114507317543
Q_Learning [211/300]: mean_loss=0.09547255747020245
Q_Learning [212/300]: mean_loss=0.2956903204321861
Q_Learning [213/300]: mean_loss=0.02183601213619113
Q_Learning [214/300]: mean_loss=0.08927315566688776
Q_Learning [215/300]: mean_loss=0.09744023811072111
Q_Learning [216/300]: mean_loss=0.20131315104663372
Q_Learning [217/300]: mean_loss=0.09412801451981068
Q_Learning [218/300]: mean_loss=0.060478180181235075
Q_Learning [219/300]: mean_loss=0.1598556935787201
Q_Learning [220/300]: mean_loss=0.19209418073296547
Q_Learning [221/300]: mean_loss=0.12439953815191984
Q_Learning [222/300]: mean_loss=0.22391338273882866
Q_Learning [223/300]: mean_loss=0.08148851338773966
Q_Learning [224/300]: mean_loss=0.2636030912399292
Q_Learning [225/300]: mean_loss=0.05586113594472408
Q_Learning [226/300]: mean_loss=0.06769023276865482
Q_Learning [227/300]: mean_loss=0.11622815020382404
Q_Learning [228/300]: mean_loss=0.30066078528761864
Q_Learning [229/300]: mean_loss=0.2033121045678854
Q_Learning [230/300]: mean_loss=0.18755674175918102
Q_Learning [231/300]: mean_loss=0.20539951510727406
Q_Learning [232/300]: mean_loss=0.6906740516424179
Q_Learning [233/300]: mean_loss=0.14488802291452885
Q_Learning [234/300]: mean_loss=0.2492045722901821
Q_Learning [235/300]: mean_loss=0.16498225927352905
Q_Learning [236/300]: mean_loss=0.1414613565430045
Q_Learning [237/300]: mean_loss=0.21246114745736122
Q_Learning [238/300]: mean_loss=0.1739362422376871
Q_Learning [239/300]: mean_loss=0.22787872701883316
Q_Learning [240/300]: mean_loss=0.13983719609677792
Q_Learning [241/300]: mean_loss=0.05401231814175844
Q_Learning [242/300]: mean_loss=0.06706978101283312
Q_Learning [243/300]: mean_loss=0.14881039783358574
Q_Learning [244/300]: mean_loss=0.1942244078963995
Q_Learning [245/300]: mean_loss=0.06319486116990447
Q_Learning [246/300]: mean_loss=0.19828998111188412
Q_Learning [247/300]: mean_loss=0.1447428222745657
Q_Learning [248/300]: mean_loss=0.06453159917145967
Q_Learning [249/300]: mean_loss=0.11586015019565821
Q_Learning [250/300]: mean_loss=0.1612779013812542
Q_Learning [251/300]: mean_loss=0.09341255482286215
Q_Learning [252/300]: mean_loss=0.06645933166146278
Q_Learning [253/300]: mean_loss=0.08481027651578188
Q_Learning [254/300]: mean_loss=0.1410982608795166
Q_Learning [255/300]: mean_loss=0.4548926278948784
Q_Learning [256/300]: mean_loss=0.38094091042876244
Q_Learning [257/300]: mean_loss=0.07878049928694963
Q_Learning [258/300]: mean_loss=0.31113947182893753
Q_Learning [259/300]: mean_loss=0.13721382059156895
Q_Learning [260/300]: mean_loss=0.1308033401146531
Q_Learning [261/300]: mean_loss=0.17867009714245796
Q_Learning [262/300]: mean_loss=0.21313421800732613
Q_Learning [263/300]: mean_loss=0.061439502984285355
Q_Learning [264/300]: mean_loss=0.2271581944078207
Q_Learning [265/300]: mean_loss=0.1350519210100174
Q_Learning [266/300]: mean_loss=0.14241783693432808
Q_Learning [267/300]: mean_loss=0.09054138511419296
Q_Learning [268/300]: mean_loss=0.08252826426178217
Q_Learning [269/300]: mean_loss=0.05671765934675932
Q_Learning [270/300]: mean_loss=0.11943083442747593
Q_Learning [271/300]: mean_loss=0.08370039705187082
Q_Learning [272/300]: mean_loss=0.06642413511872292
Q_Learning [273/300]: mean_loss=0.18292640522122383
Q_Learning [274/300]: mean_loss=0.05322089605033398
Q_Learning [275/300]: mean_loss=0.054739159531891346
Q_Learning [276/300]: mean_loss=0.03757755784317851
Q_Learning [277/300]: mean_loss=0.10666813049465418
Q_Learning [278/300]: mean_loss=0.09087887965142727
Q_Learning [279/300]: mean_loss=0.04972332250326872
Q_Learning [280/300]: mean_loss=0.16391851752996445
Q_Learning [281/300]: mean_loss=0.13546240516006947
Q_Learning [282/300]: mean_loss=0.3441308904439211
Q_Learning [283/300]: mean_loss=0.10530893690884113
Q_Learning [284/300]: mean_loss=0.08094879239797592
Q_Learning [285/300]: mean_loss=0.13148834183812141
Q_Learning [286/300]: mean_loss=0.18809696286916733
Q_Learning [287/300]: mean_loss=0.059721173252910376
Q_Learning [288/300]: mean_loss=0.02968649589456618
Q_Learning [289/300]: mean_loss=0.18421580083668232
Q_Learning [290/300]: mean_loss=0.09293410461395979
Q_Learning [291/300]: mean_loss=0.15542870573699474
Q_Learning [292/300]: mean_loss=0.05215497827157378
Q_Learning [293/300]: mean_loss=0.29444873332977295
Q_Learning [294/300]: mean_loss=0.021688606590032578
Q_Learning [295/300]: mean_loss=0.162924874573946
Q_Learning [296/300]: mean_loss=0.03313885140232742
Q_Learning [297/300]: mean_loss=0.1362646222114563
Q_Learning [298/300]: mean_loss=0.18152090534567833
Q_Learning [299/300]: mean_loss=0.2525413762778044
Q_Learning [300/300]: mean_loss=0.20668578892946243
Number of Samples after Autoencoder testing: 300
First Spike after testing: [ 0.50202405 -1.9323211 ]
[0, 1, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 2, 2, 1, 1, 1, 0, 2, 0, 1, 2, 1, 0, 0, 2, 0, 2, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 2, 2, 1, 0, 2, 1, 0, 1, 2, 2, 2, 0, 1, 1, 2, 2, 0, 2, 1, 1, 2, 2, 1, 0, 1, 2, 0, 1, 1, 0, 1, 2, 1, 2, 1, 1, 2, 0, 2, 1, 0, 1, 2, 1, 2, 0, 1, 2, 2, 1, 0, 2, 1, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0, 0, 0, 1, 2, 2, 2, 2, 0, 1, 0, 2, 0, 0, 0, 2, 2, 0, 1, 1, 0, 0, 1, 0, 2, 2, 1, 0, 0, 2, 0, 2, 1, 1, 2, 1, 2, 2, 0, 2, 0, 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 2, 1, 0, 2, 2, 0, 2, 0, 0, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 0, 2, 2, 2, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 0, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 1, 2, 0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 2, 0, 1, 0, 0, 0, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 1, 2, 1, 2, 2, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2]
[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 2, 1, 2, 2, 0, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2, 0, 1, 1, 2, 0, 0, 2, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 3, 0, 2, 0, 1, 4, 0, 2, 0, 2, 2, 1, 0, 1, 2, 2, 0, 2, 0, 1, 1, 0, 2, 3, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 1, 4, 0, 1, 0, 0, 3, 2, 0, 0, 1, 0, 1, 1, 4, 2, 0, 0, 1, 4, 0, 2, 1, 0, 0, 4, 2, 1, 1, 1, 4, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 1, 4, 2, 4, 2, 4, 1, 0, 1, 0, 2, 2, 1, 3, 1, 0, 0, 1, 4, 1, 1, 4, 2, 0, 1, 1, 4, 1, 3, 4, 4, 4, 0, 0, 4, 1, 0, 2, 2, 0, 2, 2, 0, 1, 0, 4, 4, 1, 1, 0, 1, 1, 0, 4, 4, 4, 0, 0, 1, 0, 2, 2, 4, 0, 0, 4, 4, 1, 1, 0, 1, 0, 0, 0, 4, 0, 1, 0, 2, 1, 2, 0, 4, 4, 4, 2, 0, 0, 4, 1, 1, 0, 4, 0, 1, 2]
Centroids: [[-0.13663357, -0.79077405], [0.07652808, 1.8965385], [1.7387241, -0.86878604]]
Centroids: [[0.68339944, -0.7359722], [-0.15770678, 2.1494298], [2.1938114, -0.32820255], [2.0840678, 2.6561275], [-1.0538003, -1.4375359]]
Contingency Matrix: 
[[67  1  6  0 30]
 [14 78  2  5  1]
 [57  1 38  0  0]]
[[67, -1, 6, 0, 30], [-1, -1, -1, -1, -1], [57, -1, 38, 0, 0]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, 38, 0, 0]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {1: 1, 0: 0, 2: 2}
New Contingency Matrix: 
[[67  1  6  0 30]
 [14 78  2  5  1]
 [57  1 38  0  0]]
New Clustered Label Sequence: [0, 1, 2, 3, 4]
Diagonal_Elements: [67, 78, 38], Sum: 183
All_Elements: [67, 1, 6, 0, 30, 14, 78, 2, 5, 1, 57, 1, 38, 0, 0], Sum: 300
Accuracy: 0.61
