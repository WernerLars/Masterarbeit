Seed: 0
Experiment_path: AE_Model_2/Base_Line_W_PC//0.4/Experiment_05
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Drift_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Drift_Easy2_noise015.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning
Visualisation_Path: AE_Model_2/Base_Line_W_PC//0.4/Experiment_05/C_Drift_Easy2_noise015.mat/Variant_05_Online_Autoencoder_QLearning/2023_04_23-18_58_48
Normalisation: False
Template Matching: False
Optimising Autoencoder: False
Update Factor: 1
Noisy Batches: False
Noisy Factor: 0.1
Epochs: 8
Batch Size: 1
maximal Spikes for Autoencoder Training : 700
maximal Spikes for Training: 1000
Input Size: 47
Chosen Model: Convolutional Autoencoder
ConvolutionalAutoencoder(
  (encoder): Sequential(
    (0): Conv1d(1, 6, kernel_size=(6,), stride=(1,))
    (1): LeakyReLU(negative_slope=0.01)
    (2): Conv1d(6, 1, kernel_size=(6,), stride=(1,))
    (3): Flatten(start_dim=1, end_dim=-1)
    (4): Linear(in_features=37, out_features=2, bias=True)
  )
  (decoder): Sequential(
    (0): ConvTranspose1d(1, 6, kernel_size=(6,), stride=(1,))
    (1): LeakyReLU(negative_slope=0.01)
    (2): ConvTranspose1d(6, 1, kernel_size=(6,), stride=(1,))
    (3): Flatten(start_dim=1, end_dim=-1)
    (4): Linear(in_features=12, out_features=47, bias=True)
  )
)
MSELoss()
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.001
    maximize: False
    weight_decay: 0
)
---Q Learning Parameters---
Normalisation: False
Punishment Coefficient: 0.4
Alpha: 0.8
Epsilon: 0.01
Gamma: 0.97
Initial Episode Number: 0
Episode Number Coefficient: 1.4
Number of Random Features: 20
Planning Number: 20
Max Random Features: 60
New Episode Number: 72
New Episode Number: 143
New Episode Number: 215
New Episode Number: 286
New Episode Number: 358
New Episode Number: 429
New Episode Number: 500
New Episode Number: 572
New Episode Number: 643
New Episode Number: 715
New Episode Number: 786
New Episode Number: 858
New Episode Number: 929
New Episode Number: 1000
New Episode Number: 1072
               0     1      2     3      4   ...     11    12     13     14    15
new_cluster -6.67 -6.07 -14.23 -6.58 -11.58  ... -34.05 -8.34 -20.32 -10.21 -7.52
c1          -6.55 -6.06 -14.17 -6.68 -11.60  ... -34.43 -8.80 -19.54 -10.30 -7.46
c2          -6.59 -6.27 -14.15 -6.70 -11.60  ... -34.07 -8.62 -20.08 -10.40 -7.48
c3          -6.69 -6.09 -14.43 -6.54 -11.84  ... -34.16 -8.73 -19.11 -10.33 -7.55
c4          -6.53 -6.11 -14.54 -6.57 -11.63  ... -34.20 -8.71 -19.32 -10.00 -7.67
c5          -6.59 -6.29 -14.52 -6.46 -11.76  ... -34.07 -8.68 -19.97 -10.18 -7.41
c6          -6.55 -6.02 -14.41 -6.53 -11.47  ... -34.12 -8.62 -19.51  -9.83 -7.60
c7          -6.66 -6.08 -14.19 -6.45 -11.73  ... -34.12 -8.60 -20.95 -10.30 -7.54
c8          -6.62 -6.01 -14.21 -6.57 -11.59  ... -34.07 -8.82 -19.92  -9.58 -7.96
c9          -6.56 -6.10 -14.41 -6.76 -11.25  ... -34.20 -8.52 -18.59 -10.24 -7.60
c10         -6.60 -6.09 -14.56 -6.38 -11.58  ... -34.42 -8.82 -20.20 -10.63 -7.66
c11         -6.55 -6.07 -14.14 -6.54 -12.00  ... -34.20 -8.57 -18.73  -9.81 -7.55
c12         -6.55 -5.93 -14.61 -6.47 -11.55  ... -34.07 -8.47 -17.25  -9.87 -7.67
c13         -6.53 -6.12 -14.23 -6.68 -11.38  ... -34.15 -8.50 -20.23 -10.11 -7.57
c14         -6.53 -6.18 -14.41 -6.43 -12.09  ... -34.15 -8.77 -19.33 -10.26 -7.38
c15         -6.62 -6.16 -14.55 -6.50 -11.77  ... -34.39 -8.65 -21.30 -10.13 -7.53

[16 rows x 16 columns]
\begin{tabular}{lrrrrrrrrrrrrrrrr}
\toprule
{} &    0  &    1  &     2  &    3  &     4  &    5  &    6  &    7  &    8  &    9  &     10 &     11 &    12 &     13 &     14 &    15 \\
\midrule
new\_cluster & -6.67 & -6.07 & -14.23 & -6.58 & -11.58 & -6.71 & -9.74 & -9.30 & -6.89 & -7.62 & -18.45 & -34.05 & -8.34 & -20.32 & -10.21 & -7.52 \\
c1          & -6.55 & -6.06 & -14.17 & -6.68 & -11.60 & -6.68 & -9.76 & -9.17 & -6.91 & -7.54 & -18.05 & -34.43 & -8.80 & -19.54 & -10.30 & -7.46 \\
c2          & -6.59 & -6.27 & -14.15 & -6.70 & -11.60 & -6.58 & -9.05 & -9.94 & -6.76 & -7.49 & -18.41 & -34.07 & -8.62 & -20.08 & -10.40 & -7.48 \\
c3          & -6.69 & -6.09 & -14.43 & -6.54 & -11.84 & -6.65 & -9.68 & -9.39 & -7.15 & -7.65 & -18.32 & -34.16 & -8.73 & -19.11 & -10.33 & -7.55 \\
c4          & -6.53 & -6.11 & -14.54 & -6.57 & -11.63 & -6.63 & -9.42 & -9.48 & -7.08 & -7.53 & -18.35 & -34.20 & -8.71 & -19.32 & -10.00 & -7.67 \\
c5          & -6.59 & -6.29 & -14.52 & -6.46 & -11.76 & -6.54 & -9.07 & -9.56 & -7.15 & -7.76 & -18.76 & -34.07 & -8.68 & -19.97 & -10.18 & -7.41 \\
c6          & -6.55 & -6.02 & -14.41 & -6.53 & -11.47 & -6.58 & -9.04 & -9.44 & -7.06 & -7.63 & -18.89 & -34.12 & -8.62 & -19.51 &  -9.83 & -7.60 \\
c7          & -6.66 & -6.08 & -14.19 & -6.45 & -11.73 & -6.55 & -9.15 & -9.47 & -7.08 & -7.72 & -18.50 & -34.12 & -8.60 & -20.95 & -10.30 & -7.54 \\
c8          & -6.62 & -6.01 & -14.21 & -6.57 & -11.59 & -6.54 & -9.27 & -9.43 & -6.78 & -7.71 & -18.24 & -34.07 & -8.82 & -19.92 &  -9.58 & -7.96 \\
c9          & -6.56 & -6.10 & -14.41 & -6.76 & -11.25 & -6.76 & -9.17 & -9.50 & -7.10 & -7.56 & -17.99 & -34.20 & -8.52 & -18.59 & -10.24 & -7.60 \\
c10         & -6.60 & -6.09 & -14.56 & -6.38 & -11.58 & -6.73 & -9.16 & -9.40 & -6.69 & -7.61 & -18.29 & -34.42 & -8.82 & -20.20 & -10.63 & -7.66 \\
c11         & -6.55 & -6.07 & -14.14 & -6.54 & -12.00 & -6.55 & -8.94 & -9.10 & -7.12 & -7.98 & -18.47 & -34.20 & -8.57 & -18.73 &  -9.81 & -7.55 \\
c12         & -6.55 & -5.93 & -14.61 & -6.47 & -11.55 & -6.53 & -9.19 & -9.35 & -6.74 & -7.58 & -18.76 & -34.07 & -8.47 & -17.25 &  -9.87 & -7.67 \\
c13         & -6.53 & -6.12 & -14.23 & -6.68 & -11.38 & -6.57 & -9.24 & -9.40 & -6.73 & -8.06 & -19.18 & -34.15 & -8.50 & -20.23 & -10.11 & -7.57 \\
c14         & -6.53 & -6.18 & -14.41 & -6.43 & -12.09 & -6.72 & -9.88 & -9.58 & -6.82 & -7.70 & -18.38 & -34.15 & -8.77 & -19.33 & -10.26 & -7.38 \\
c15         & -6.62 & -6.16 & -14.55 & -6.50 & -11.77 & -6.56 & -9.03 & -9.62 & -6.76 & -7.48 & -18.47 & -34.39 & -8.65 & -21.30 & -10.13 & -7.53 \\
\bottomrule
\end{tabular}

                               0            1   ...            14            15
new_cluster  [-0.64, new_cluster]  [-0.17, c1]  ...  [-4.18, c14]  [-1.49, c15]
c1           [-0.64, new_cluster]   [-0.2, c1]  ...   [-4.3, c14]  [-1.44, c15]
c2           [-0.64, new_cluster]  [-0.16, c1]  ...  [-4.37, c14]  [-1.35, c15]
c3           [-0.64, new_cluster]  [-0.23, c1]  ...  [-4.32, c14]  [-1.54, c15]
c4           [-0.64, new_cluster]  [-0.16, c1]  ...   [-4.0, c14]  [-1.63, c15]
c5           [-0.64, new_cluster]  [-0.19, c1]  ...  [-4.18, c14]  [-1.35, c15]
c6           [-0.64, new_cluster]  [-0.15, c1]  ...  [-3.84, c14]  [-1.54, c15]
c7           [-0.64, new_cluster]  [-0.22, c1]  ...  [-4.29, c14]  [-1.54, c15]
c8           [-0.64, new_cluster]  [-0.21, c1]  ...  [-3.58, c14]  [-1.68, c15]
c9           [-0.64, new_cluster]  [-0.22, c1]  ...  [-4.03, c14]  [-1.54, c15]
c10          [-0.64, new_cluster]  [-0.14, c1]  ...  [-4.63, c14]  [-1.53, c15]
c11          [-0.64, new_cluster]  [-0.18, c1]  ...  [-3.82, c14]  [-1.53, c15]
c12          [-0.64, new_cluster]  [-0.14, c1]  ...  [-3.87, c14]  [-1.53, c15]
c13          [-0.64, new_cluster]  [-0.22, c1]  ...   [-4.1, c14]  [-1.31, c15]
c14          [-0.64, new_cluster]  [-0.13, c1]  ...  [-4.23, c14]   [-1.4, c15]
c15          [-0.64, new_cluster]  [-0.27, c1]  ...  [-4.13, c14]  [-1.27, c15]

[16 rows x 16 columns]
\begin{tabular}{lllllllllllllllll}
\toprule
{} &                    0  &           1  &           2  &           3  &           4  &           5  &           6  &           7  &           8  &           9  &             10 &             11 &            12 &             13 &            14 &            15 \\
\midrule
new\_cluster &  [-0.64, new\_cluster] &  [-0.17, c1] &  [-8.15, c2] &   [-0.6, c3] &   [-5.5, c4] &  [-0.61, c5] &  [-3.83, c6] &  [-3.37, c7] &  [-0.91, c8] &  [-1.45, c9] &   [-12.5, c10] &  [-28.16, c11] &  [-2.55, c12] &  [-14.21, c13] &  [-4.18, c14] &  [-1.49, c15] \\
c1          &  [-0.64, new\_cluster] &   [-0.2, c1] &  [-8.09, c2] &   [-0.7, c3] &  [-5.49, c4] &  [-0.58, c5] &  [-3.58, c6] &  [-3.26, c7] &  [-1.06, c8] &  [-1.37, c9] &  [-12.11, c10] &  [-28.16, c11] &  [-2.77, c12] &  [-13.57, c13] &   [-4.3, c14] &  [-1.44, c15] \\
c2          &  [-0.64, new\_cluster] &  [-0.16, c1] &  [-8.07, c2] &  [-0.57, c3] &  [-5.68, c4] &  [-0.48, c5] &  [-3.14, c6] &  [-3.89, c7] &  [-0.82, c8] &  [-1.52, c9] &   [-12.5, c10] &  [-28.16, c11] &  [-2.86, c12] &  [-13.94, c13] &  [-4.37, c14] &  [-1.35, c15] \\
c3          &  [-0.64, new\_cluster] &  [-0.23, c1] &  [-8.34, c2] &  [-0.63, c3] &  [-5.75, c4] &  [-0.55, c5] &  [-3.49, c6] &  [-3.46, c7] &  [-0.96, c8] &  [-1.68, c9] &  [-12.34, c10] &  [-28.16, c11] &  [-2.69, c12] &  [-13.14, c13] &  [-4.32, c14] &  [-1.54, c15] \\
c4          &  [-0.64, new\_cluster] &  [-0.16, c1] &  [-8.45, c2] &  [-0.65, c3] &  [-5.52, c4] &  [-0.53, c5] &  [-3.24, c6] &  [-3.42, c7] &  [-0.89, c8] &  [-1.36, c9] &  [-12.43, c10] &  [-28.16, c11] &  [-2.84, c12] &  [-13.35, c13] &   [-4.0, c14] &  [-1.63, c15] \\
c5          &  [-0.64, new\_cluster] &  [-0.19, c1] &  [-8.42, c2] &  [-0.54, c3] &  [-5.83, c4] &  [-0.44, c5] &  [-3.22, c6] &   [-3.5, c7] &  [-0.98, c8] &   [-1.6, c9] &  [-12.84, c10] &  [-28.16, c11] &  [-2.65, c12] &  [-13.74, c13] &  [-4.18, c14] &  [-1.35, c15] \\
c6          &  [-0.64, new\_cluster] &  [-0.15, c1] &   [-8.3, c2] &  [-0.62, c3] &  [-5.54, c4] &  [-0.46, c5] &  [-3.18, c6] &  [-3.51, c7] &  [-0.88, c8] &  [-1.47, c9] &  [-12.94, c10] &  [-28.16, c11] &  [-2.68, c12] &  [-13.57, c13] &  [-3.84, c14] &  [-1.54, c15] \\
c7          &  [-0.64, new\_cluster] &  [-0.22, c1] &  [-8.02, c2] &   [-0.5, c3] &  [-5.63, c4] &  [-0.45, c5] &  [-3.29, c6] &  [-3.26, c7] &  [-0.85, c8] &  [-1.75, c9] &  [-12.56, c10] &  [-28.16, c11] &  [-2.71, c12] &  [-14.82, c13] &  [-4.29, c14] &  [-1.54, c15] \\
c8          &  [-0.64, new\_cluster] &  [-0.21, c1] &  [-8.11, c2] &  [-0.66, c3] &   [-5.5, c4] &  [-0.44, c5] &   [-3.4, c6] &  [-3.46, c7] &   [-0.8, c8] &   [-1.6, c9] &  [-12.14, c10] &  [-28.16, c11] &  [-2.79, c12] &  [-13.98, c13] &  [-3.58, c14] &  [-1.68, c15] \\
c9          &  [-0.64, new\_cluster] &  [-0.22, c1] &  [-8.33, c2] &  [-0.51, c3] &  [-5.29, c4] &  [-0.67, c5] &  [-3.31, c6] &  [-3.57, c7] &  [-0.91, c8] &   [-1.6, c9] &  [-12.07, c10] &  [-28.16, c11] &  [-2.68, c12] &  [-12.62, c13] &  [-4.03, c14] &  [-1.54, c15] \\
c10         &  [-0.64, new\_cluster] &  [-0.14, c1] &  [-8.47, c2] &  [-0.46, c3] &  [-5.48, c4] &  [-0.62, c5] &   [-3.3, c6] &  [-3.44, c7] &  [-0.86, c8] &  [-1.65, c9] &  [-12.34, c10] &  [-28.16, c11] &  [-2.64, c12] &  [-14.09, c13] &  [-4.63, c14] &  [-1.53, c15] \\
c11         &  [-0.64, new\_cluster] &  [-0.18, c1] &  [-8.06, c2] &  [-0.56, c3] &  [-6.05, c4] &  [-0.45, c5] &  [-3.07, c6] &  [-3.18, c7] &  [-0.93, c8] &  [-1.82, c9] &  [-12.53, c10] &  [-28.16, c11] &  [-2.78, c12] &  [-12.51, c13] &  [-3.82, c14] &  [-1.53, c15] \\
c12         &  [-0.64, new\_cluster] &  [-0.14, c1] &  [-8.53, c2] &  [-0.56, c3] &  [-5.62, c4] &  [-0.44, c5] &  [-3.28, c6] &  [-3.42, c7] &  [-0.84, c8] &  [-1.41, c9] &  [-12.81, c10] &  [-28.16, c11] &  [-2.53, c12] &  [-11.15, c13] &  [-3.87, c14] &  [-1.53, c15] \\
c13         &  [-0.64, new\_cluster] &  [-0.22, c1] &  [-8.16, c2] &  [-0.55, c3] &  [-5.45, c4] &  [-0.46, c5] &  [-3.38, c6] &  [-3.44, c7] &  [-0.88, c8] &  [-1.89, c9] &  [-13.23, c10] &  [-28.16, c11] &  [-2.59, c12] &  [-14.29, c13] &   [-4.1, c14] &  [-1.31, c15] \\
c14         &  [-0.64, new\_cluster] &  [-0.13, c1] &  [-8.33, c2] &   [-0.5, c3] &  [-6.16, c4] &   [-0.6, c5] &   [-3.7, c6] &  [-3.55, c7] &  [-0.85, c8] &  [-1.78, c9] &  [-12.44, c10] &  [-28.16, c11] &  [-2.83, c12] &  [-13.21, c13] &  [-4.23, c14] &   [-1.4, c15] \\
c15         &  [-0.64, new\_cluster] &  [-0.27, c1] &  [-8.46, c2] &  [-0.59, c3] &  [-5.81, c4] &  [-0.43, c5] &  [-3.12, c6] &  [-3.64, c7] &  [-0.79, c8] &  [-1.56, c9] &   [-12.5, c10] &  [-28.16, c11] &  [-2.81, c12] &  [-15.33, c13] &  [-4.13, c14] &  [-1.27, c15] \\
\bottomrule
\end{tabular}

