Experiment_path: AE_Model_2/Reduce_Training//V5_200/Experiment_05
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise005.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise005.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning
Visualisation_Path: AE_Model_2/Reduce_Training//V5_200/Experiment_05/C_Difficult2_noise005.mat/Variant_05_Online_Autoencoder_QLearning/2023_05_25-10_18_18
Punishment_Coefficient: 0.4
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000023587CBA908>
Sampling rate: 24000.0
Raw: [ 0.02085333  0.02043967  0.02052644 ... -0.02218732 -0.02150573
 -0.01811243]
Times: [   1583    1934    2430 ... 1439313 1439656 1439854]
Cluster: [3 3 3 ... 2 2 1]
Number of different clusters:  3
Number of Spikes: 3364
First aligned Spike Frame: [-0.05170878 -0.0548761  -0.06029554 -0.06053219 -0.04807119 -0.02780025
 -0.01550543 -0.01702494 -0.02945104 -0.04493807 -0.07056858 -0.07003585
  0.07629654  0.43081562  0.80470191  0.96319627  0.89198123  0.73643948
  0.58987232  0.46714337  0.36345495  0.2828462   0.22743292  0.182731
  0.13931053  0.09524506  0.05136602  0.01367166 -0.01393093 -0.03985679
 -0.07387102 -0.11218435 -0.1444455  -0.16672578 -0.17809238 -0.18020802
 -0.17953732 -0.18246903 -0.18617363 -0.18205375 -0.17299738 -0.16958427
 -0.17248955 -0.17516876 -0.1727246  -0.16696514 -0.15993314]
Cluster 0, Occurrences: 1120
Cluster 1, Occurrences: 1109
Cluster 2, Occurrences: 1135
Number of Clusters: 3
Online_Training [1/200]: mean_loss=0.10365145560353994
Online_Training [2/200]: mean_loss=0.11032601073384285
Online_Training [3/200]: mean_loss=0.12324978224933147
Online_Training [4/200]: mean_loss=0.08281749207526445
Online_Training [5/200]: mean_loss=0.17593178898096085
Online_Training [6/200]: mean_loss=0.13678775168955326
Online_Training [7/200]: mean_loss=0.10475646797567606
Online_Training [8/200]: mean_loss=0.07844597660005093
Online_Training [9/200]: mean_loss=0.08166727051138878
Online_Training [10/200]: mean_loss=0.05881204130128026
Online_Training [11/200]: mean_loss=0.03367684339173138
Online_Training [12/200]: mean_loss=0.04206027090549469
Online_Training [13/200]: mean_loss=0.045394167304039
Online_Training [14/200]: mean_loss=0.03462403756566346
Online_Training [15/200]: mean_loss=0.018880605115555227
Online_Training [16/200]: mean_loss=0.016226745792664587
Online_Training [17/200]: mean_loss=0.007401854498311877
Online_Training [18/200]: mean_loss=0.004871895886026323
Online_Training [19/200]: mean_loss=0.17688319832086563
Online_Training [20/200]: mean_loss=0.037656186148524284
Online_Training [21/200]: mean_loss=0.03248878428712487
Online_Training [22/200]: mean_loss=0.14228772185742855
Online_Training [23/200]: mean_loss=0.021359647158533335
Online_Training [24/200]: mean_loss=0.20841757394373417
Online_Training [25/200]: mean_loss=0.15897620283067226
Online_Training [26/200]: mean_loss=0.10443187225610018
Online_Training [27/200]: mean_loss=0.10757176112383604
Online_Training [28/200]: mean_loss=0.04720088513568044
Online_Training [29/200]: mean_loss=0.03677733661606908
Online_Training [30/200]: mean_loss=0.13761113584041595
Online_Training [31/200]: mean_loss=0.1431687157601118
Online_Training [32/200]: mean_loss=0.019256820203736424
Online_Training [33/200]: mean_loss=0.043071835301816463
Online_Training [34/200]: mean_loss=0.030112592270597816
Online_Training [35/200]: mean_loss=0.10551659017801285
Online_Training [36/200]: mean_loss=0.08616775833070278
Online_Training [37/200]: mean_loss=0.020726023241877556
Online_Training [38/200]: mean_loss=0.05800630198791623
Online_Training [39/200]: mean_loss=0.007180728716775775
Online_Training [40/200]: mean_loss=0.02774784038774669
Online_Training [41/200]: mean_loss=0.02626761025749147
Online_Training [42/200]: mean_loss=0.01978986756876111
Online_Training [43/200]: mean_loss=0.008411378774326295
Online_Training [44/200]: mean_loss=0.011373386951163411
Online_Training [45/200]: mean_loss=0.00821247830754146
Online_Training [46/200]: mean_loss=0.004238194291247055
Online_Training [47/200]: mean_loss=0.02949133631773293
Online_Training [48/200]: mean_loss=0.004260493937181309
Online_Training [49/200]: mean_loss=0.018429998075589538
Online_Training [50/200]: mean_loss=0.01711435813922435
Online_Training [51/200]: mean_loss=0.1360895950347185
Online_Training [52/200]: mean_loss=0.031166138825938106
Online_Training [53/200]: mean_loss=0.09338579513132572
Online_Training [54/200]: mean_loss=0.008337036415468901
Online_Training [55/200]: mean_loss=0.024897169321775436
Online_Training [56/200]: mean_loss=0.0036039277038071305
Online_Training [57/200]: mean_loss=0.003303736710222438
Online_Training [58/200]: mean_loss=0.11129074450582266
Online_Training [59/200]: mean_loss=0.03596072643995285
Online_Training [60/200]: mean_loss=0.10391237586736679
Online_Training [61/200]: mean_loss=0.021732816705480218
Online_Training [62/200]: mean_loss=0.006713446055073291
Online_Training [63/200]: mean_loss=0.011146091623231769
Online_Training [64/200]: mean_loss=0.10149426758289337
Online_Training [65/200]: mean_loss=0.09804860409349203
Online_Training [66/200]: mean_loss=0.07573483418673277
Online_Training [67/200]: mean_loss=0.07075796462595463
Online_Training [68/200]: mean_loss=0.07257424015551805
Online_Training [69/200]: mean_loss=0.06798689439892769
Online_Training [70/200]: mean_loss=0.05906947935000062
Online_Training [71/200]: mean_loss=0.06344622373580933
Online_Training [72/200]: mean_loss=0.04424005653709173
Online_Training [73/200]: mean_loss=0.027873980114236474
Online_Training [74/200]: mean_loss=0.09777424670755863
Online_Training [75/200]: mean_loss=0.06724763289093971
Online_Training [76/200]: mean_loss=0.06754542235285044
Online_Training [77/200]: mean_loss=0.032720517832785845
Online_Training [78/200]: mean_loss=0.055330121889710426
Online_Training [79/200]: mean_loss=0.013495855149812996
Online_Training [80/200]: mean_loss=0.08054616767913103
Online_Training [81/200]: mean_loss=0.0437273234128952
Online_Training [82/200]: mean_loss=0.08499826025217772
Online_Training [83/200]: mean_loss=0.02056444832123816
Online_Training [84/200]: mean_loss=0.017491289065219462
Online_Training [85/200]: mean_loss=0.007829079520888627
Online_Training [86/200]: mean_loss=0.0074065360822714865
Online_Training [87/200]: mean_loss=0.06273051491007209
Online_Training [88/200]: mean_loss=0.009714392595924437
Online_Training [89/200]: mean_loss=0.06111495988443494
Online_Training [90/200]: mean_loss=0.060299737844616175
Online_Training [91/200]: mean_loss=0.008792552165687084
Online_Training [92/200]: mean_loss=0.05466541228815913
Online_Training [93/200]: mean_loss=0.05721602030098438
Online_Training [94/200]: mean_loss=0.13045566715300083
Online_Training [95/200]: mean_loss=0.05291889188811183
Online_Training [96/200]: mean_loss=0.04926346708089113
Online_Training [97/200]: mean_loss=0.024598735850304365
Online_Training [98/200]: mean_loss=0.014928906108252704
Online_Training [99/200]: mean_loss=0.01985082437749952
Online_Training [100/200]: mean_loss=0.017860141466371715
Online_Training [101/200]: mean_loss=0.060042348224669695
Online_Training [102/200]: mean_loss=0.009252469288185239
Online_Training [103/200]: mean_loss=0.010778892901726067
Online_Training [104/200]: mean_loss=0.047829851508140564
Online_Training [105/200]: mean_loss=0.01840776391327381
Online_Training [106/200]: mean_loss=0.04645727574825287
Online_Training [107/200]: mean_loss=0.006798359216190875
Online_Training [108/200]: mean_loss=0.00610763841541484
Online_Training [109/200]: mean_loss=0.028709819307550788
Online_Training [110/200]: mean_loss=0.019684824161231518
Online_Training [111/200]: mean_loss=0.010498521500267088
Online_Training [112/200]: mean_loss=0.006751081964466721
Online_Training [113/200]: mean_loss=0.01034175290260464
Online_Training [114/200]: mean_loss=0.011149410158395767
Online_Training [115/200]: mean_loss=0.014709096169099212
Online_Training [116/200]: mean_loss=0.0065655403304845095
Online_Training [117/200]: mean_loss=0.006951473944354802
Online_Training [118/200]: mean_loss=0.001988915479159914
Online_Training [119/200]: mean_loss=0.002013455901760608
Online_Training [120/200]: mean_loss=0.006597104948014021
Online_Training [121/200]: mean_loss=0.013850798597559333
Online_Training [122/200]: mean_loss=0.014681951492093503
Online_Training [123/200]: mean_loss=0.009678838367108256
Online_Training [124/200]: mean_loss=0.009763802867382765
Online_Training [125/200]: mean_loss=0.008301526599097997
Online_Training [126/200]: mean_loss=0.007543921121396124
Online_Training [127/200]: mean_loss=0.11539273522794247
Online_Training [128/200]: mean_loss=0.09463217575103045
Online_Training [129/200]: mean_loss=0.016270413645543158
Online_Training [130/200]: mean_loss=0.01073003769852221
Online_Training [131/200]: mean_loss=0.00872373318998143
Online_Training [132/200]: mean_loss=0.005926633486524224
Online_Training [133/200]: mean_loss=0.011852476745843887
Online_Training [134/200]: mean_loss=0.0052040740847587585
Online_Training [135/200]: mean_loss=0.014633383019827306
Online_Training [136/200]: mean_loss=0.01216001552529633
Online_Training [137/200]: mean_loss=0.018349175923503935
Online_Training [138/200]: mean_loss=0.008509076025802642
Online_Training [139/200]: mean_loss=0.011497713392600417
Online_Training [140/200]: mean_loss=0.0122202483471483
Online_Training [141/200]: mean_loss=0.006040609732735902
Online_Training [142/200]: mean_loss=0.00642919767415151
Online_Training [143/200]: mean_loss=0.0028363063174765557
Online_Training [144/200]: mean_loss=0.003309556719614193
Online_Training [145/200]: mean_loss=0.01326039934065193
Online_Training [146/200]: mean_loss=0.00506099930498749
Online_Training [147/200]: mean_loss=0.004484147269977257
Online_Training [148/200]: mean_loss=0.005810578470118344
Online_Training [149/200]: mean_loss=0.002339016427868046
Online_Training [150/200]: mean_loss=0.004586454626405612
Online_Training [151/200]: mean_loss=0.0022628989536315203
Online_Training [152/200]: mean_loss=0.004293730511562899
Online_Training [153/200]: mean_loss=0.014796476578339934
Online_Training [154/200]: mean_loss=0.002298004168551415
Online_Training [155/200]: mean_loss=0.003514375421218574
Online_Training [156/200]: mean_loss=0.007737517007626593
Online_Training [157/200]: mean_loss=0.005237550649326295
Online_Training [158/200]: mean_loss=0.015046882443130016
Online_Training [159/200]: mean_loss=0.00804614118533209
Online_Training [160/200]: mean_loss=0.01736472500488162
Online_Training [161/200]: mean_loss=0.006737415213137865
Online_Training [162/200]: mean_loss=0.002214852356701158
Online_Training [163/200]: mean_loss=0.018473158706910908
Online_Training [164/200]: mean_loss=0.004557408974505961
Online_Training [165/200]: mean_loss=0.014908914570696652
Online_Training [166/200]: mean_loss=0.008018000342417508
Online_Training [167/200]: mean_loss=0.001991684199310839
Online_Training [168/200]: mean_loss=0.003039173170691356
Online_Training [169/200]: mean_loss=0.09553625620901585
Online_Training [170/200]: mean_loss=0.12663145549595356
Online_Training [171/200]: mean_loss=0.01808978710323572
Online_Training [172/200]: mean_loss=0.013899345183745027
Online_Training [173/200]: mean_loss=0.025777155300602317
Online_Training [174/200]: mean_loss=0.011906710686162114
Online_Training [175/200]: mean_loss=0.011823006789200008
Online_Training [176/200]: mean_loss=0.007099157897755504
Online_Training [177/200]: mean_loss=0.015644605737179518
Online_Training [178/200]: mean_loss=0.004297217528801411
Online_Training [179/200]: mean_loss=0.1097798952832818
Online_Training [180/200]: mean_loss=0.12669316213577986
Online_Training [181/200]: mean_loss=0.012994521879591048
Online_Training [182/200]: mean_loss=0.006929464638233185
Online_Training [183/200]: mean_loss=0.0027011947240680456
Online_Training [184/200]: mean_loss=0.01584337605163455
Online_Training [185/200]: mean_loss=0.0066597722470760345
Online_Training [186/200]: mean_loss=0.004428373620612547
Online_Training [187/200]: mean_loss=0.012718723737634718
Online_Training [188/200]: mean_loss=0.013073867303319275
Online_Training [189/200]: mean_loss=0.00467903510434553
Online_Training [190/200]: mean_loss=0.004519809037446976
Online_Training [191/200]: mean_loss=0.0035439689236227423
Online_Training [192/200]: mean_loss=0.005060015362687409
Online_Training [193/200]: mean_loss=0.0025454145506955683
Online_Training [194/200]: mean_loss=0.006818775727879256
Online_Training [195/200]: mean_loss=0.008517082780599594
Online_Training [196/200]: mean_loss=0.01383148122113198
Online_Training [197/200]: mean_loss=0.0026859635254368186
Online_Training [198/200]: mean_loss=0.006848382938187569
Online_Training [199/200]: mean_loss=0.006694566924124956
Online_Training [200/200]: mean_loss=0.007047178572975099
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-0.18763866 -2.0743434 ]
[0, 0, 1, 2, 0, 1, 2, 2, 1, 1, 1, 0, 2, 0, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 0, 2, 1, 2, 2, 1, 2, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 2, 1, 0, 2, 1, 0, 1, 2, 2, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 2, 1, 2, 2, 1, 1, 1, 2, 0, 1, 2, 2, 0, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 2, 1, 0, 0, 2, 1, 2, 2, 1, 2, 1, 1, 0, 0, 1, 2, 1, 0, 1, 0, 2, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 1, 2, 2, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 2, 1, 2, 1, 1, 2, 0, 2, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 1, 2, 1, 0, 1, 0, 2, 2, 0]
[0, 0, 1, 0, 0, 1, 2, 2, 1, 1, 1, 3, 4, 0, 2, 1, 2, 1, 0, 3, 2, 1, 0, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 0, 2, 1, 2, 2, 1, 2, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 0, 5, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 2, 1, 2, 2, 0, 1, 2, 1, 2, 0, 1, 2, 1, 0, 1, 1, 0, 2, 0, 6, 7, 0, 2, 1, 0, 2, 1, 0, 1, 2, 2, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 2, 1, 2, 2, 1, 1, 1, 2, 0, 1, 2, 2, 0, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 2, 1, 0, 0, 2, 1, 2, 2, 1, 2, 1, 1, 0, 4, 8, 4, 1, 0, 1, 0, 0, 1, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 2, 1, 2, 1, 0, 1, 9, 1, 2, 2, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 2, 1, 2, 1, 6, 7, 0, 2, 2, 2, 1, 1, 0, 2, 1, 1, 8, 1, 1, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 1, 2, 1, 0, 1, 2, 2, 2, 0]
Centroids: [[-0.11536619, -2.3148057], [-0.10355831, 1.8362693], [-0.6054229, -1.4044913]]
Centroids: [[-0.11929866, -2.3516207], [-0.11664927, 1.8289539], [-0.6561912, -1.3516942], [0.006460078, 0.18738669], [0.5956558, -2.0748653], [1.2543007, 0.22767124], [-0.48278546, 3.5250316], [-0.5657786, -4.210169], [0.029045552, -0.6840502], [2.0821893, -2.5583427]]
Standard Derivations: [0.46437216, 0.3007843, 0.42659414]
Cluster Distances: [3.385935, 0.14287508, 3.385935, 2.5520115, 0.14287508, 2.5520113]
Minimal Cluster Distance: 0.142875075340271
Contingency Matrix: 
[[ 80   1   2   1   1   0   0   1   1   0]
 [  0 111   0   1   0   1   2   0   1   0]
 [  2   0  91   0   2   0   0   1   0   1]]
[[80, 1, 2, 1, 1, 0, 0, 1, 1, 0], [0, 111, 0, 1, 0, 1, 2, 0, 1, 0], [2, 0, 91, 0, 2, 0, 0, 1, 0, 1]]
[[80, 1, 2, 1, 1, 0, 0, 1, 1, 0], [0, 111, 0, 1, 0, 1, 2, 0, 1, 0], [2, 0, 91, 0, 2, 0, 0, 1, 0, 1]]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[[80, -1, 2, 1, 1, 0, 0, 1, 1, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [2, -1, 91, 0, 2, 0, 0, 1, 0, 1]]
[[80, -1, -1, 1, 1, 0, 0, 1, 1, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {1: 1, 2: 2, 0: 0}
New Contingency Matrix: 
[[ 80   1   2   1   1   0   0   1   1   0]
 [  0 111   0   1   0   1   2   0   1   0]
 [  2   0  91   0   2   0   0   1   0   1]]
New Clustered Label Sequence: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Diagonal_Elements: [80, 111, 91], Sum: 282
All_Elements: [80, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 111, 0, 1, 0, 1, 2, 0, 1, 0, 2, 0, 91, 0, 2, 0, 0, 1, 0, 1], Sum: 300
Accuracy: 0.94
