Experiment_path: AE_Model_2/Reduce_Training//V5_200/Experiment_05
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise025.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise025.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning
Visualisation_Path: AE_Model_2/Reduce_Training//V5_200/Experiment_05/C_Easy1_noise025.mat/Variant_05_Online_Autoencoder_QLearning/2023_05_25-10_28_24
Punishment_Coefficient: 1.2
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000023583DFD198>
Sampling rate: 24000.0
Raw: [-0.1861928  -0.15538047 -0.11159897 ... -0.04566289 -0.07495693
 -0.11387027]
Times: [    288     764     962 ... 1439565 1439599 1439750]
Cluster: [2 1 1 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3298
First aligned Spike Frame: [ 0.30343498  0.30504401  0.30003499  0.28306832  0.25612953  0.20234245
  0.11026158  0.00607927 -0.07206812 -0.11511366 -0.12845949 -0.13294027
 -0.18390234 -0.33132976 -0.53531084 -0.64122966 -0.43321471  0.14319913
  0.78508862  1.13178271  1.12964756  0.95557126  0.768731    0.62108183
  0.50039946  0.39401216  0.30447426  0.22854935  0.15922545  0.09984913
  0.06405489  0.05593058  0.05062423  0.00682243 -0.07060307 -0.1367616
 -0.15929316 -0.15555753 -0.15669153 -0.16914157 -0.17192467 -0.15578403
 -0.14071413 -0.14785593 -0.17738608 -0.22110055 -0.28163013]
Cluster 0, Occurrences: 1094
Cluster 1, Occurrences: 1089
Cluster 2, Occurrences: 1115
Number of Clusters: 3
Online_Training [1/200]: mean_loss=0.20480766706168652
Online_Training [2/200]: mean_loss=0.20315546728670597
Online_Training [3/200]: mean_loss=0.158534063026309
Online_Training [4/200]: mean_loss=0.27716802433133125
Online_Training [5/200]: mean_loss=0.15220120176672935
Online_Training [6/200]: mean_loss=0.14351083897054195
Online_Training [7/200]: mean_loss=0.20233283750712872
Online_Training [8/200]: mean_loss=0.10319474618881941
Online_Training [9/200]: mean_loss=0.14161592163145542
Online_Training [10/200]: mean_loss=0.1205058516934514
Online_Training [11/200]: mean_loss=0.3995361365377903
Online_Training [12/200]: mean_loss=0.11976262927055359
Online_Training [13/200]: mean_loss=0.2026494052261114
Online_Training [14/200]: mean_loss=0.12010273523628712
Online_Training [15/200]: mean_loss=0.12420045584440231
Online_Training [16/200]: mean_loss=0.19336942583322525
Online_Training [17/200]: mean_loss=0.08880605641752481
Online_Training [18/200]: mean_loss=0.31800566613674164
Online_Training [19/200]: mean_loss=0.16296997107565403
Online_Training [20/200]: mean_loss=0.39402754232287407
Online_Training [21/200]: mean_loss=0.17322039231657982
Online_Training [22/200]: mean_loss=0.21242069825530052
Online_Training [23/200]: mean_loss=0.27962059527635574
Online_Training [24/200]: mean_loss=0.30607539042830467
Online_Training [25/200]: mean_loss=0.14556518010795116
Online_Training [26/200]: mean_loss=0.08650119416415691
Online_Training [27/200]: mean_loss=0.23559877276420593
Online_Training [28/200]: mean_loss=0.18866843543946743
Online_Training [29/200]: mean_loss=0.23826197534799576
Online_Training [30/200]: mean_loss=0.34390540793538094
Online_Training [31/200]: mean_loss=0.24928001500666142
Online_Training [32/200]: mean_loss=0.2776941265910864
Online_Training [33/200]: mean_loss=0.15565422549843788
Online_Training [34/200]: mean_loss=0.16478683426976204
Online_Training [35/200]: mean_loss=0.2755562327802181
Online_Training [36/200]: mean_loss=0.0711630005389452
Online_Training [37/200]: mean_loss=0.07509797252714634
Online_Training [38/200]: mean_loss=0.04330667154863477
Online_Training [39/200]: mean_loss=0.05390985868871212
Online_Training [40/200]: mean_loss=0.12220894452184439
Online_Training [41/200]: mean_loss=0.11515799071639776
Online_Training [42/200]: mean_loss=0.18637263029813766
Online_Training [43/200]: mean_loss=0.12877441942691803
Online_Training [44/200]: mean_loss=0.13981701992452145
Online_Training [45/200]: mean_loss=0.12670368235558271
Online_Training [46/200]: mean_loss=0.06026314804330468
Online_Training [47/200]: mean_loss=0.10288288444280624
Online_Training [48/200]: mean_loss=0.18194785341620445
Online_Training [49/200]: mean_loss=0.06625298410654068
Online_Training [50/200]: mean_loss=0.18511773832142353
Online_Training [51/200]: mean_loss=0.11378316581249237
Online_Training [52/200]: mean_loss=0.08117584604769945
Online_Training [53/200]: mean_loss=0.12823764700442553
Online_Training [54/200]: mean_loss=0.0388538958504796
Online_Training [55/200]: mean_loss=0.12764748372137547
Online_Training [56/200]: mean_loss=0.2628277502954006
Online_Training [57/200]: mean_loss=0.04903462575748563
Online_Training [58/200]: mean_loss=0.023367759305983782
Online_Training [59/200]: mean_loss=0.21041109785437584
Online_Training [60/200]: mean_loss=0.05721445009112358
Online_Training [61/200]: mean_loss=0.028945568250492215
Online_Training [62/200]: mean_loss=0.14644723199307919
Online_Training [63/200]: mean_loss=0.2509224805980921
Online_Training [64/200]: mean_loss=0.09699266776442528
Online_Training [65/200]: mean_loss=0.09259756933897734
Online_Training [66/200]: mean_loss=0.15223255194723606
Online_Training [67/200]: mean_loss=0.203843729570508
Online_Training [68/200]: mean_loss=0.5726235508918762
Online_Training [69/200]: mean_loss=0.16288230009377003
Online_Training [70/200]: mean_loss=0.06663124682381749
Online_Training [71/200]: mean_loss=0.08788519911468029
Online_Training [72/200]: mean_loss=0.1737420242279768
Online_Training [73/200]: mean_loss=0.0718071898445487
Online_Training [74/200]: mean_loss=0.17856134474277496
Online_Training [75/200]: mean_loss=0.07265439815819263
Online_Training [76/200]: mean_loss=0.18006866425275803
Online_Training [77/200]: mean_loss=0.05995275266468525
Online_Training [78/200]: mean_loss=0.10922655370086432
Online_Training [79/200]: mean_loss=0.10062772873789072
Online_Training [80/200]: mean_loss=0.12114390172064304
Online_Training [81/200]: mean_loss=0.10498842690140009
Online_Training [82/200]: mean_loss=0.07083941996097565
Online_Training [83/200]: mean_loss=0.1333235427737236
Online_Training [84/200]: mean_loss=0.22367401979863644
Online_Training [85/200]: mean_loss=0.3668130114674568
Online_Training [86/200]: mean_loss=0.21795736253261566
Online_Training [87/200]: mean_loss=0.09772313479334116
Online_Training [88/200]: mean_loss=0.07473686710000038
Online_Training [89/200]: mean_loss=0.14288767240941525
Online_Training [90/200]: mean_loss=0.0697036781348288
Online_Training [91/200]: mean_loss=0.055737821850925684
Online_Training [92/200]: mean_loss=0.1936558298766613
Online_Training [93/200]: mean_loss=0.08018306735903025
Online_Training [94/200]: mean_loss=0.29472244158387184
Online_Training [95/200]: mean_loss=0.12820311076939106
Online_Training [96/200]: mean_loss=0.10920947976410389
Online_Training [97/200]: mean_loss=0.1586223989725113
Online_Training [98/200]: mean_loss=0.09897653479129076
Online_Training [99/200]: mean_loss=0.0854813614860177
Online_Training [100/200]: mean_loss=0.08763685543090105
Online_Training [101/200]: mean_loss=0.07156789535656571
Online_Training [102/200]: mean_loss=0.0756576769053936
Online_Training [103/200]: mean_loss=0.0670969532802701
Online_Training [104/200]: mean_loss=0.061805041041225195
Online_Training [105/200]: mean_loss=0.1152772894129157
Online_Training [106/200]: mean_loss=0.29324788227677345
Online_Training [107/200]: mean_loss=0.15262028388679028
Online_Training [108/200]: mean_loss=0.0713021270930767
Online_Training [109/200]: mean_loss=0.20475557446479797
Online_Training [110/200]: mean_loss=0.15525763854384422
Online_Training [111/200]: mean_loss=0.11346877831965685
Online_Training [112/200]: mean_loss=0.07084855157881975
Online_Training [113/200]: mean_loss=0.019772968837060034
Online_Training [114/200]: mean_loss=0.025407358072698116
Online_Training [115/200]: mean_loss=0.24908187985420227
Online_Training [116/200]: mean_loss=0.024961068760603666
Online_Training [117/200]: mean_loss=0.09278540778905153
Online_Training [118/200]: mean_loss=0.14158117398619652
Online_Training [119/200]: mean_loss=0.23958437889814377
Online_Training [120/200]: mean_loss=0.14644179679453373
Online_Training [121/200]: mean_loss=0.06142519321292639
Online_Training [122/200]: mean_loss=0.051235444843769073
Online_Training [123/200]: mean_loss=0.13014283683151007
Online_Training [124/200]: mean_loss=0.08094079419970512
Online_Training [125/200]: mean_loss=0.0981630701571703
Online_Training [126/200]: mean_loss=0.039170683827251196
Online_Training [127/200]: mean_loss=0.03669271618127823
Online_Training [128/200]: mean_loss=0.12982824724167585
Online_Training [129/200]: mean_loss=0.17663941718637943
Online_Training [130/200]: mean_loss=0.07170287240296602
Online_Training [131/200]: mean_loss=0.06756744487211108
Online_Training [132/200]: mean_loss=0.03411810286343098
Online_Training [133/200]: mean_loss=0.06620135810226202
Online_Training [134/200]: mean_loss=0.07103709410876036
Online_Training [135/200]: mean_loss=0.04231124138459563
Online_Training [136/200]: mean_loss=0.06892487592995167
Online_Training [137/200]: mean_loss=0.06688506714999676
Online_Training [138/200]: mean_loss=0.05602722987532616
Online_Training [139/200]: mean_loss=0.03637077193707228
Online_Training [140/200]: mean_loss=0.04210531199350953
Online_Training [141/200]: mean_loss=0.05699911527335644
Online_Training [142/200]: mean_loss=0.035708528477698565
Online_Training [143/200]: mean_loss=0.14389765076339245
Online_Training [144/200]: mean_loss=0.04048648150637746
Online_Training [145/200]: mean_loss=0.05035217246040702
Online_Training [146/200]: mean_loss=0.14118401613086462
Online_Training [147/200]: mean_loss=0.07631357852369547
Online_Training [148/200]: mean_loss=0.06754141766577959
Online_Training [149/200]: mean_loss=0.016335354070179164
Online_Training [150/200]: mean_loss=0.02231546025723219
Online_Training [151/200]: mean_loss=0.017115867929533124
Online_Training [152/200]: mean_loss=0.026484667556360364
Online_Training [153/200]: mean_loss=0.016641817870549858
Online_Training [154/200]: mean_loss=0.012439746176823974
Online_Training [155/200]: mean_loss=0.03589897649362683
Online_Training [156/200]: mean_loss=0.08191493060439825
Online_Training [157/200]: mean_loss=0.04012616677209735
Online_Training [158/200]: mean_loss=0.06092047831043601
Online_Training [159/200]: mean_loss=0.06818835437297821
Online_Training [160/200]: mean_loss=0.01927181612700224
Online_Training [161/200]: mean_loss=0.1813481952995062
Online_Training [162/200]: mean_loss=0.10671621188521385
Online_Training [163/200]: mean_loss=0.026209868025034666
Online_Training [164/200]: mean_loss=0.06264957645907998
Online_Training [165/200]: mean_loss=0.09147041477262974
Online_Training [166/200]: mean_loss=0.05054127424955368
Online_Training [167/200]: mean_loss=0.059338309802114964
Online_Training [168/200]: mean_loss=0.1022528251633048
Online_Training [169/200]: mean_loss=0.1400438966229558
Online_Training [170/200]: mean_loss=0.05853155581280589
Online_Training [171/200]: mean_loss=0.06856924947351217
Online_Training [172/200]: mean_loss=0.030929364264011383
Online_Training [173/200]: mean_loss=0.05554580641910434
Online_Training [174/200]: mean_loss=0.0649967985227704
Online_Training [175/200]: mean_loss=0.14791975915431976
Online_Training [176/200]: mean_loss=0.02348227333277464
Online_Training [177/200]: mean_loss=0.06317949295043945
Online_Training [178/200]: mean_loss=0.06252975948154926
Online_Training [179/200]: mean_loss=0.0414168699644506
Online_Training [180/200]: mean_loss=0.0893960427492857
Online_Training [181/200]: mean_loss=0.05424180068075657
Online_Training [182/200]: mean_loss=0.07337941695004702
Online_Training [183/200]: mean_loss=0.058359170332551
Online_Training [184/200]: mean_loss=0.033611899707466364
Online_Training [185/200]: mean_loss=0.03403594112023711
Online_Training [186/200]: mean_loss=0.026107469806447625
Online_Training [187/200]: mean_loss=0.07955818716436625
Online_Training [188/200]: mean_loss=0.030655173351988196
Online_Training [189/200]: mean_loss=0.02900268230587244
Online_Training [190/200]: mean_loss=0.07432563230395317
Online_Training [191/200]: mean_loss=0.020177793689072132
Online_Training [192/200]: mean_loss=0.03435654076747596
Online_Training [193/200]: mean_loss=0.05438935803249478
Online_Training [194/200]: mean_loss=0.07606216426938772
Online_Training [195/200]: mean_loss=0.041020996402949095
Online_Training [196/200]: mean_loss=0.032850079238414764
Online_Training [197/200]: mean_loss=0.023358148522675037
Online_Training [198/200]: mean_loss=0.05082095926627517
Online_Training [199/200]: mean_loss=0.06998913828283548
Online_Training [200/200]: mean_loss=0.024735383689403534
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.8574328  2.2126617]
[2, 1, 0, 0, 2, 1, 2, 1, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 2, 2, 2, 2, 0, 2, 1, 0, 1, 1, 2, 1, 2, 2, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 2, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 2, 1, 0, 1, 2, 2, 0, 2, 1, 2, 0, 1, 2, 1, 1, 1, 0, 0, 1, 2, 2, 1, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 1, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 2, 0, 1, 1, 0, 2, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 0, 2, 2, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 1, 1, 2, 0, 0, 1, 2, 2, 1, 1, 1, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 1, 1, 1, 0, 2, 0, 1, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 1, 2, 2, 2, 0, 1, 1, 1, 0, 2, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 2, 0, 2, 0, 0, 1, 2, 0]
[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 0, 0, 2, 0, 1, 2, 1, 1, 0, 1, 0, 0, 2, 2, 2, 1, 2, 1, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 2, 1, 2, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 0, 0, 2, 0, 1, 0, 2, 1, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 2, 2, 0, 0, 2, 0, 2, 1, 0, 1, 1, 1, 4, 0, 0, 0, 1, 3, 2, 2, 1, 1, 2, 1, 1, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 2, 3, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 2, 0, 0, 1, 3, 2, 1, 0, 2, 1, 3, 0, 2, 0, 0, 0, 2, 1, 2, 3, 3, 2, 2, 1, 4, 4, 1, 1, 1, 2, 2, 0, 2, 1, 1, 0, 2, 2, 1, 0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 2, 2, 1, 2, 0, 2, 0, 3, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 2, 1, 1, 1, 0, 0, 3, 1, 1, 0, 0, 0, 0, 3, 4, 3, 3, 0, 2, 2, 1, 1, 2, 2, 0, 3, 2, 0, 0, 0, 3, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 1, 2, 1, 0, 2, 3, 0, 1, 2, 0, 0, 2, 4, 2, 3, 1, 0, 2]
Centroids: [[-1.5255244, -0.0039028348], [1.1487298, -0.89179754], [-1.307921, 2.2261734]]
Centroids: [[-1.492606, 2.1950583], [1.1115062, -0.8797582], [-1.6016061, 0.19465217], [-1.2503998, -1.1627748], [1.6034887, 1.1572474]]
Standard Derivations: [0.48363143, 0.4767355, 0.73908764]
Cluster Distances: [1.8574325, 1.0179486, 1.8574324, 2.75367, 1.0179486, 2.75367]
Minimal Cluster Distance: 1.0179486274719238
Contingency Matrix: 
[[ 8  3 85 15  0]
 [ 0 89  0  1  2]
 [89  3  2  0  3]]
[[8, 3, 85, 15, 0], [0, 89, 0, 1, 2], [89, 3, 2, 0, 3]]
[[8, 3, 85, 15, 0], [0, 89, 0, 1, 2], [89, 3, 2, 0, 3]]
[0, 1, 2, 3, 4]
[[8, -1, 85, 15, 0], [-1, -1, -1, -1, -1], [89, -1, 2, 0, 3]]
[[-1, -1, 85, 15, 0], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {1: 1, 2: 0, 0: 2}
New Contingency Matrix: 
[[85  3  8 15  0]
 [ 0 89  0  1  2]
 [ 2  3 89  0  3]]
New Clustered Label Sequence: [2, 1, 0, 3, 4]
Diagonal_Elements: [85, 89, 89], Sum: 263
All_Elements: [85, 3, 8, 15, 0, 0, 89, 0, 1, 2, 2, 3, 89, 0, 3], Sum: 300
Accuracy: 0.8766666666666667
