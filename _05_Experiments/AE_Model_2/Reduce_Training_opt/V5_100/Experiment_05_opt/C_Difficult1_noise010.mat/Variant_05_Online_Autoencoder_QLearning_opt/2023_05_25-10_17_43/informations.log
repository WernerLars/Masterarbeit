Experiment_path: AE_Model_2/Reduce_Training_opt//V5_100/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise010.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_100/Experiment_05_opt/C_Difficult1_noise010.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-10_17_43
Punishment_Coefficient: 0.5
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001C7FFDFB518>
Sampling rate: 24000.0
Raw: [-0.08093593 -0.05766541 -0.01986255 ... -0.05525448 -0.03568967
 -0.02908211]
Times: [   1054    1166    1249 ... 1439165 1439229 1439456]
Cluster: [3 3 3 ... 2 1 3]
Number of different clusters:  3
Number of Spikes: 3448
First aligned Spike Frame: [-4.79707202e-02 -8.83141277e-02 -1.14477415e-01 -1.20070620e-01
 -1.07771810e-01 -8.38316152e-02 -5.31944576e-02 -2.65798100e-02
 -6.71143520e-03  2.42590968e-02  6.29914810e-02  1.33638321e-01
  3.34948892e-01  7.06515114e-01  1.07966210e+00  1.10962398e+00
  6.55362247e-01 -7.19265760e-04 -4.64310305e-01 -6.23655437e-01
 -6.27154873e-01 -6.07058236e-01 -5.79298390e-01 -5.10136794e-01
 -4.12813198e-01 -3.36153681e-01 -2.86889156e-01 -2.38391657e-01
 -1.98571332e-01 -1.83172365e-01 -1.85707124e-01 -1.94134424e-01
 -2.08249204e-01 -2.33590971e-01 -2.49276546e-01 -2.46246400e-01
 -2.44939654e-01 -2.59799331e-01 -2.74706204e-01 -2.68658955e-01
 -2.53059716e-01 -2.46742300e-01 -2.46057086e-01 -2.38944634e-01
 -2.31254619e-01 -2.20748865e-01 -1.79535377e-01]
Cluster 0, Occurrences: 1164
Cluster 1, Occurrences: 1155
Cluster 2, Occurrences: 1129
Number of Clusters: 3
Online_Training [1/100]: mean_loss=0.14315092004835606
Online_Training [2/100]: mean_loss=0.20914656482636929
Online_Training [3/100]: mean_loss=0.17308266088366508
Online_Training [4/100]: mean_loss=0.09879879746586084
Online_Training [5/100]: mean_loss=0.10151166282594204
Online_Training [6/100]: mean_loss=0.08501005172729492
Online_Training [7/100]: mean_loss=0.11326076555997133
Online_Training [8/100]: mean_loss=0.17461254261434078
Online_Training [9/100]: mean_loss=0.13524658977985382
Online_Training [10/100]: mean_loss=0.06922707846388221
Online_Training [11/100]: mean_loss=0.060163058806210756
Online_Training [12/100]: mean_loss=0.044691684655845165
Online_Training [13/100]: mean_loss=0.06201318698003888
Online_Training [14/100]: mean_loss=0.036019091960042715
Online_Training [15/100]: mean_loss=0.030802769120782614
Online_Training [16/100]: mean_loss=0.03125676745548844
Online_Training [17/100]: mean_loss=0.035673695150762796
Online_Training [18/100]: mean_loss=0.03470006724819541
Online_Training [19/100]: mean_loss=0.11907844617962837
Online_Training [20/100]: mean_loss=0.037526112981140614
Online_Training [21/100]: mean_loss=0.01938587659969926
Online_Training [22/100]: mean_loss=0.043309700675308704
Online_Training [23/100]: mean_loss=0.02531195874325931
Online_Training [24/100]: mean_loss=0.013316514901816845
Online_Training [25/100]: mean_loss=0.03564142296090722
Online_Training [26/100]: mean_loss=0.04005439253523946
Online_Training [27/100]: mean_loss=0.07089022733271122
Online_Training [28/100]: mean_loss=0.03753557545132935
Online_Training [29/100]: mean_loss=0.01773003046400845
Online_Training [30/100]: mean_loss=0.021055573131889105
Online_Training [31/100]: mean_loss=0.014807893196120858
Online_Training [32/100]: mean_loss=0.05260462546721101
Online_Training [33/100]: mean_loss=0.027097886661067605
Online_Training [34/100]: mean_loss=0.019066870911046863
Online_Training [35/100]: mean_loss=0.028852663934230804
Online_Training [36/100]: mean_loss=0.033745494205504656
Online_Training [37/100]: mean_loss=0.006381165236234665
Online_Training [38/100]: mean_loss=0.020631111692637205
Online_Training [39/100]: mean_loss=0.022027290891855955
Online_Training [40/100]: mean_loss=0.00949448358733207
Online_Training [41/100]: mean_loss=0.03681726008653641
Online_Training [42/100]: mean_loss=0.057523755356669426
Online_Training [43/100]: mean_loss=0.038235798478126526
Online_Training [44/100]: mean_loss=0.010587608441710472
Online_Training [45/100]: mean_loss=0.021911055548116565
Online_Training [46/100]: mean_loss=0.008038896485231817
Online_Training [47/100]: mean_loss=0.02738478872925043
Online_Training [48/100]: mean_loss=0.0382571667432785
Online_Training [49/100]: mean_loss=0.0161933577619493
Online_Training [50/100]: mean_loss=0.006113033858127892
Online_Training [51/100]: mean_loss=0.011348131345584989
Online_Training [52/100]: mean_loss=0.024903234094381332
Online_Training [53/100]: mean_loss=0.03424553689546883
Online_Training [54/100]: mean_loss=0.019247791031375527
Online_Training [55/100]: mean_loss=0.02219703048467636
Online_Training [56/100]: mean_loss=0.016741087310947478
Online_Training [57/100]: mean_loss=0.013330450863577425
Online_Training [58/100]: mean_loss=0.021212700521573424
Online_Training [59/100]: mean_loss=0.13769053108990192
Online_Training [60/100]: mean_loss=0.03800333244726062
Online_Training [61/100]: mean_loss=0.06716535007581115
Online_Training [62/100]: mean_loss=0.007939038681797683
Online_Training [63/100]: mean_loss=0.0166391609236598
Online_Training [64/100]: mean_loss=0.019104980630800128
Online_Training [65/100]: mean_loss=0.029602124821394682
Online_Training [66/100]: mean_loss=0.06268368754535913
Online_Training [67/100]: mean_loss=0.015050516463816166
Online_Training [68/100]: mean_loss=0.03688577702268958
Online_Training [69/100]: mean_loss=0.02567170560359955
Online_Training [70/100]: mean_loss=0.014064809074625373
Online_Training [71/100]: mean_loss=0.013518512016162276
Online_Training [72/100]: mean_loss=0.035616465378552675
Online_Training [73/100]: mean_loss=0.0167205665493384
Online_Training [74/100]: mean_loss=0.025184019235894084
Online_Training [75/100]: mean_loss=0.018291188986040652
Online_Training [76/100]: mean_loss=0.024630652740597725
Online_Training [77/100]: mean_loss=0.04570246860384941
Online_Training [78/100]: mean_loss=0.008764234371483326
Online_Training [79/100]: mean_loss=0.0048358263447880745
Online_Training [80/100]: mean_loss=0.017472760286182165
Online_Training [81/100]: mean_loss=0.025201597483828664
Online_Training [82/100]: mean_loss=0.04402362369000912
Online_Training [83/100]: mean_loss=0.006785459583625197
Online_Training [84/100]: mean_loss=0.025994653813540936
Online_Training [85/100]: mean_loss=0.023830415681004524
Online_Training [86/100]: mean_loss=0.07844273000955582
Online_Training [87/100]: mean_loss=0.05145524209365249
Online_Training [88/100]: mean_loss=0.014942133566364646
Online_Training [89/100]: mean_loss=0.016373183229006827
Online_Training [90/100]: mean_loss=0.036238199565559626
Online_Training [91/100]: mean_loss=0.016232625348493457
Online_Training [92/100]: mean_loss=0.0077439649030566216
Online_Training [93/100]: mean_loss=0.022286569233983755
Online_Training [94/100]: mean_loss=0.02403730875812471
Online_Training [95/100]: mean_loss=0.014308225945569575
Online_Training [96/100]: mean_loss=0.016073044040240347
Online_Training [97/100]: mean_loss=0.018275921465829015
Online_Training [98/100]: mean_loss=0.010000530513934791
Online_Training [99/100]: mean_loss=0.015672062058001757
Online_Training [100/100]: mean_loss=0.006980990699958056
Q_Learning [1/300]: mean_loss=0.14315092004835606
Q_Learning [2/300]: mean_loss=0.20914656482636929
Q_Learning [3/300]: mean_loss=0.17308266088366508
Q_Learning [4/300]: mean_loss=0.09879879746586084
Q_Learning [5/300]: mean_loss=0.10151166282594204
Q_Learning [6/300]: mean_loss=0.08501005172729492
Q_Learning [7/300]: mean_loss=0.11326076555997133
Q_Learning [8/300]: mean_loss=0.17461254261434078
Q_Learning [9/300]: mean_loss=0.13524658977985382
Q_Learning [10/300]: mean_loss=0.06922707846388221
Q_Learning [11/300]: mean_loss=0.060163058806210756
Q_Learning [12/300]: mean_loss=0.044691684655845165
Q_Learning [13/300]: mean_loss=0.06201318698003888
Q_Learning [14/300]: mean_loss=0.036019091960042715
Q_Learning [15/300]: mean_loss=0.030802769120782614
Q_Learning [16/300]: mean_loss=0.03125676745548844
Q_Learning [17/300]: mean_loss=0.035673695150762796
Q_Learning [18/300]: mean_loss=0.03470006724819541
Q_Learning [19/300]: mean_loss=0.11907844617962837
Q_Learning [20/300]: mean_loss=0.037526112981140614
Q_Learning [21/300]: mean_loss=0.01938587659969926
Q_Learning [22/300]: mean_loss=0.043309700675308704
Q_Learning [23/300]: mean_loss=0.02531195874325931
Q_Learning [24/300]: mean_loss=0.013316514901816845
Q_Learning [25/300]: mean_loss=0.03564142296090722
Q_Learning [26/300]: mean_loss=0.04005439253523946
Q_Learning [27/300]: mean_loss=0.07089022733271122
Q_Learning [28/300]: mean_loss=0.03753557545132935
Q_Learning [29/300]: mean_loss=0.01773003046400845
Q_Learning [30/300]: mean_loss=0.021055573131889105
Q_Learning [31/300]: mean_loss=0.014807893196120858
Q_Learning [32/300]: mean_loss=0.05260462546721101
Q_Learning [33/300]: mean_loss=0.027097886661067605
Q_Learning [34/300]: mean_loss=0.019066870911046863
Q_Learning [35/300]: mean_loss=0.028852663934230804
Q_Learning [36/300]: mean_loss=0.033745494205504656
Q_Learning [37/300]: mean_loss=0.006381165236234665
Q_Learning [38/300]: mean_loss=0.020631111692637205
Q_Learning [39/300]: mean_loss=0.022027290891855955
Q_Learning [40/300]: mean_loss=0.00949448358733207
Q_Learning [41/300]: mean_loss=0.03681726008653641
Q_Learning [42/300]: mean_loss=0.057523755356669426
Q_Learning [43/300]: mean_loss=0.038235798478126526
Q_Learning [44/300]: mean_loss=0.010587608441710472
Q_Learning [45/300]: mean_loss=0.021911055548116565
Q_Learning [46/300]: mean_loss=0.008038896485231817
Q_Learning [47/300]: mean_loss=0.02738478872925043
Q_Learning [48/300]: mean_loss=0.0382571667432785
Q_Learning [49/300]: mean_loss=0.0161933577619493
Q_Learning [50/300]: mean_loss=0.006113033858127892
Q_Learning [51/300]: mean_loss=0.011348131345584989
Q_Learning [52/300]: mean_loss=0.024903234094381332
Q_Learning [53/300]: mean_loss=0.03424553689546883
Q_Learning [54/300]: mean_loss=0.019247791031375527
Q_Learning [55/300]: mean_loss=0.02219703048467636
Q_Learning [56/300]: mean_loss=0.016741087310947478
Q_Learning [57/300]: mean_loss=0.013330450863577425
Q_Learning [58/300]: mean_loss=0.021212700521573424
Q_Learning [59/300]: mean_loss=0.13769053108990192
Q_Learning [60/300]: mean_loss=0.03800333244726062
Q_Learning [61/300]: mean_loss=0.06716535007581115
Q_Learning [62/300]: mean_loss=0.007939038681797683
Q_Learning [63/300]: mean_loss=0.0166391609236598
Q_Learning [64/300]: mean_loss=0.019104980630800128
Q_Learning [65/300]: mean_loss=0.029602124821394682
Q_Learning [66/300]: mean_loss=0.06268368754535913
Q_Learning [67/300]: mean_loss=0.015050516463816166
Q_Learning [68/300]: mean_loss=0.03688577702268958
Q_Learning [69/300]: mean_loss=0.02567170560359955
Q_Learning [70/300]: mean_loss=0.014064809074625373
Q_Learning [71/300]: mean_loss=0.013518512016162276
Q_Learning [72/300]: mean_loss=0.035616465378552675
Q_Learning [73/300]: mean_loss=0.0167205665493384
Q_Learning [74/300]: mean_loss=0.025184019235894084
Q_Learning [75/300]: mean_loss=0.018291188986040652
Q_Learning [76/300]: mean_loss=0.024630652740597725
Q_Learning [77/300]: mean_loss=0.04570246860384941
Q_Learning [78/300]: mean_loss=0.008764234371483326
Q_Learning [79/300]: mean_loss=0.0048358263447880745
Q_Learning [80/300]: mean_loss=0.017472760286182165
Q_Learning [81/300]: mean_loss=0.025201597483828664
Q_Learning [82/300]: mean_loss=0.04402362369000912
Q_Learning [83/300]: mean_loss=0.006785459583625197
Q_Learning [84/300]: mean_loss=0.025994653813540936
Q_Learning [85/300]: mean_loss=0.023830415681004524
Q_Learning [86/300]: mean_loss=0.07844273000955582
Q_Learning [87/300]: mean_loss=0.05145524209365249
Q_Learning [88/300]: mean_loss=0.014942133566364646
Q_Learning [89/300]: mean_loss=0.016373183229006827
Q_Learning [90/300]: mean_loss=0.036238199565559626
Q_Learning [91/300]: mean_loss=0.016232625348493457
Q_Learning [92/300]: mean_loss=0.0077439649030566216
Q_Learning [93/300]: mean_loss=0.022286569233983755
Q_Learning [94/300]: mean_loss=0.02403730875812471
Q_Learning [95/300]: mean_loss=0.014308225945569575
Q_Learning [96/300]: mean_loss=0.016073044040240347
Q_Learning [97/300]: mean_loss=0.018275921465829015
Q_Learning [98/300]: mean_loss=0.010000530513934791
Q_Learning [99/300]: mean_loss=0.015672062058001757
Q_Learning [100/300]: mean_loss=0.006980990699958056
Q_Learning [101/300]: mean_loss=0.014660446322523057
Q_Learning [102/300]: mean_loss=0.009589446592144668
Q_Learning [103/300]: mean_loss=0.03504427056759596
Q_Learning [104/300]: mean_loss=0.009392360458150506
Q_Learning [105/300]: mean_loss=0.013194173690862954
Q_Learning [106/300]: mean_loss=0.01879364240448922
Q_Learning [107/300]: mean_loss=0.03211385919712484
Q_Learning [108/300]: mean_loss=0.01194500201381743
Q_Learning [109/300]: mean_loss=0.016982187516987324
Q_Learning [110/300]: mean_loss=0.02200519386678934
Q_Learning [111/300]: mean_loss=0.013830268173478544
Q_Learning [112/300]: mean_loss=0.014766536885872483
Q_Learning [113/300]: mean_loss=0.021795740583911538
Q_Learning [114/300]: mean_loss=0.01364173088222742
Q_Learning [115/300]: mean_loss=0.02544472413137555
Q_Learning [116/300]: mean_loss=0.017092313850298524
Q_Learning [117/300]: mean_loss=0.010542428819462657
Q_Learning [118/300]: mean_loss=0.012864308548159897
Q_Learning [119/300]: mean_loss=0.00970970164053142
Q_Learning [120/300]: mean_loss=0.011931655928492546
Q_Learning [121/300]: mean_loss=0.05127476388588548
Q_Learning [122/300]: mean_loss=0.02890847367234528
Q_Learning [123/300]: mean_loss=0.018533670576289296
Q_Learning [124/300]: mean_loss=0.007013510796241462
Q_Learning [125/300]: mean_loss=0.02876485767774284
Q_Learning [126/300]: mean_loss=0.031530093401670456
Q_Learning [127/300]: mean_loss=0.010204310296103358
Q_Learning [128/300]: mean_loss=0.09107198473066092
Q_Learning [129/300]: mean_loss=0.08173981215804815
Q_Learning [130/300]: mean_loss=0.017496039625257254
Q_Learning [131/300]: mean_loss=0.03894313191995025
Q_Learning [132/300]: mean_loss=0.044551624450832605
Q_Learning [133/300]: mean_loss=0.02426726114936173
Q_Learning [134/300]: mean_loss=0.014659543405286968
Q_Learning [135/300]: mean_loss=0.019069273956120014
Q_Learning [136/300]: mean_loss=0.004585742542985827
Q_Learning [137/300]: mean_loss=0.013511632452718914
Q_Learning [138/300]: mean_loss=0.02264313935302198
Q_Learning [139/300]: mean_loss=0.023550430312752724
Q_Learning [140/300]: mean_loss=0.1258283443748951
Q_Learning [141/300]: mean_loss=0.03519213851541281
Q_Learning [142/300]: mean_loss=0.03505589114502072
Q_Learning [143/300]: mean_loss=0.017297193175181746
Q_Learning [144/300]: mean_loss=0.009173861588351429
Q_Learning [145/300]: mean_loss=0.04072408890351653
Q_Learning [146/300]: mean_loss=0.017291578697040677
Q_Learning [147/300]: mean_loss=0.07315016072243452
Q_Learning [148/300]: mean_loss=0.0473119867965579
Q_Learning [149/300]: mean_loss=0.016422871383838356
Q_Learning [150/300]: mean_loss=0.0475414739921689
Q_Learning [151/300]: mean_loss=0.021879527252167463
Q_Learning [152/300]: mean_loss=0.019949016626924276
Q_Learning [153/300]: mean_loss=0.01139135705307126
Q_Learning [154/300]: mean_loss=0.0065688161994330585
Q_Learning [155/300]: mean_loss=0.012115825549699366
Q_Learning [156/300]: mean_loss=0.023820877773687243
Q_Learning [157/300]: mean_loss=0.02690466120839119
Q_Learning [158/300]: mean_loss=0.01784113235771656
Q_Learning [159/300]: mean_loss=0.029464905615895987
Q_Learning [160/300]: mean_loss=0.012948975199833512
Q_Learning [161/300]: mean_loss=0.011437266133725643
Q_Learning [162/300]: mean_loss=0.014945643255487084
Q_Learning [163/300]: mean_loss=0.014410062110982835
Q_Learning [164/300]: mean_loss=0.01425510027911514
Q_Learning [165/300]: mean_loss=0.022797654615715146
Q_Learning [166/300]: mean_loss=0.01742741232737899
Q_Learning [167/300]: mean_loss=0.014963643276132643
Q_Learning [168/300]: mean_loss=0.00984584039542824
Q_Learning [169/300]: mean_loss=0.018191055627539754
Q_Learning [170/300]: mean_loss=0.027199710486456752
Q_Learning [171/300]: mean_loss=0.05131404614076018
Q_Learning [172/300]: mean_loss=0.016133843804709613
Q_Learning [173/300]: mean_loss=0.00557049730559811
Q_Learning [174/300]: mean_loss=0.0065320820431225
Q_Learning [175/300]: mean_loss=0.029426461085677147
Q_Learning [176/300]: mean_loss=0.013902642880566418
Q_Learning [177/300]: mean_loss=0.03194832568988204
Q_Learning [178/300]: mean_loss=0.016671399935148656
Q_Learning [179/300]: mean_loss=0.09271093178540468
Q_Learning [180/300]: mean_loss=0.0666606561280787
Q_Learning [181/300]: mean_loss=0.02690525120124221
Q_Learning [182/300]: mean_loss=0.012904043542221189
Q_Learning [183/300]: mean_loss=0.01506911322940141
Q_Learning [184/300]: mean_loss=0.013889565248973668
Q_Learning [185/300]: mean_loss=0.013454490690492094
Q_Learning [186/300]: mean_loss=0.02465630159713328
Q_Learning [187/300]: mean_loss=0.02012805570848286
Q_Learning [188/300]: mean_loss=0.015156214940361679
Q_Learning [189/300]: mean_loss=0.019189647398889065
Q_Learning [190/300]: mean_loss=0.015051402966491878
Q_Learning [191/300]: mean_loss=0.011245437432080507
Q_Learning [192/300]: mean_loss=0.009944697027094662
Q_Learning [193/300]: mean_loss=0.014467414701357484
Q_Learning [194/300]: mean_loss=0.02008758974261582
Q_Learning [195/300]: mean_loss=0.012165270745754242
Q_Learning [196/300]: mean_loss=0.010069995187222958
Q_Learning [197/300]: mean_loss=0.004293073434382677
Q_Learning [198/300]: mean_loss=0.008629930787719786
Q_Learning [199/300]: mean_loss=0.002979103708639741
Q_Learning [200/300]: mean_loss=0.01578597619663924
Q_Learning [201/300]: mean_loss=0.005651405022945255
Q_Learning [202/300]: mean_loss=0.11474632937461138
Q_Learning [203/300]: mean_loss=0.02716190740466118
Q_Learning [204/300]: mean_loss=0.018279005773365498
Q_Learning [205/300]: mean_loss=0.014854329638183117
Q_Learning [206/300]: mean_loss=0.013458492001518607
Q_Learning [207/300]: mean_loss=0.008575050393119454
Q_Learning [208/300]: mean_loss=0.030775896506384015
Q_Learning [209/300]: mean_loss=0.00945649784989655
Q_Learning [210/300]: mean_loss=0.01256065967027098
Q_Learning [211/300]: mean_loss=0.022958218352869153
Q_Learning [212/300]: mean_loss=0.01051871245726943
Q_Learning [213/300]: mean_loss=0.015270616975612938
Q_Learning [214/300]: mean_loss=0.007423827075399458
Q_Learning [215/300]: mean_loss=0.025990396738052368
Q_Learning [216/300]: mean_loss=0.015930007677525282
Q_Learning [217/300]: mean_loss=0.008402555482462049
Q_Learning [218/300]: mean_loss=0.006961781356949359
Q_Learning [219/300]: mean_loss=0.014783187536522746
Q_Learning [220/300]: mean_loss=0.012560296920128167
Q_Learning [221/300]: mean_loss=0.0077361384173855186
Q_Learning [222/300]: mean_loss=0.006655101431533694
Q_Learning [223/300]: mean_loss=0.008550542988814414
Q_Learning [224/300]: mean_loss=0.006211054744198918
Q_Learning [225/300]: mean_loss=0.0051304947410244495
Q_Learning [226/300]: mean_loss=0.02103913272731006
Q_Learning [227/300]: mean_loss=0.00979610439389944
Q_Learning [228/300]: mean_loss=0.004392062954138964
Q_Learning [229/300]: mean_loss=0.01796136610209942
Q_Learning [230/300]: mean_loss=0.0073365030111745
Q_Learning [231/300]: mean_loss=0.039645150769501925
Q_Learning [232/300]: mean_loss=0.011818776722066104
Q_Learning [233/300]: mean_loss=0.0049163964577019215
Q_Learning [234/300]: mean_loss=0.06897859368473291
Q_Learning [235/300]: mean_loss=0.09994420036673546
Q_Learning [236/300]: mean_loss=0.011178569053299725
Q_Learning [237/300]: mean_loss=0.02743848436512053
Q_Learning [238/300]: mean_loss=0.009169154276605695
Q_Learning [239/300]: mean_loss=0.01172438613139093
Q_Learning [240/300]: mean_loss=0.01278805104084313
Q_Learning [241/300]: mean_loss=0.016187555971555412
Q_Learning [242/300]: mean_loss=0.012539921910502017
Q_Learning [243/300]: mean_loss=0.018041353207081556
Q_Learning [244/300]: mean_loss=0.009761269320733845
Q_Learning [245/300]: mean_loss=0.015067083295434713
Q_Learning [246/300]: mean_loss=0.020248693879693747
Q_Learning [247/300]: mean_loss=0.012742933235131204
Q_Learning [248/300]: mean_loss=0.019819549983367324
Q_Learning [249/300]: mean_loss=0.03662543464452028
Q_Learning [250/300]: mean_loss=0.0065973601886071265
Q_Learning [251/300]: mean_loss=0.00481082484475337
Q_Learning [252/300]: mean_loss=0.020464808447286487
Q_Learning [253/300]: mean_loss=0.009562042192555964
Q_Learning [254/300]: mean_loss=0.003333800195832737
Q_Learning [255/300]: mean_loss=0.005344657169189304
Q_Learning [256/300]: mean_loss=0.011604009428992867
Q_Learning [257/300]: mean_loss=0.007127550547011197
Q_Learning [258/300]: mean_loss=0.00895280868280679
Q_Learning [259/300]: mean_loss=0.005007828352972865
Q_Learning [260/300]: mean_loss=0.004257375141605735
Q_Learning [261/300]: mean_loss=0.009940821677446365
Q_Learning [262/300]: mean_loss=0.006013803475070745
Q_Learning [263/300]: mean_loss=0.00651500589447096
Q_Learning [264/300]: mean_loss=0.004566975345369428
Q_Learning [265/300]: mean_loss=0.10138597525656223
Q_Learning [266/300]: mean_loss=0.08111023157835007
Q_Learning [267/300]: mean_loss=0.01323423138819635
Q_Learning [268/300]: mean_loss=0.010944321635179222
Q_Learning [269/300]: mean_loss=0.01847262098453939
Q_Learning [270/300]: mean_loss=0.10123134963214397
Q_Learning [271/300]: mean_loss=0.03169226320460439
Q_Learning [272/300]: mean_loss=0.013453858671709895
Q_Learning [273/300]: mean_loss=0.01323487504851073
Q_Learning [274/300]: mean_loss=0.00950542592909187
Q_Learning [275/300]: mean_loss=0.017090650158934295
Q_Learning [276/300]: mean_loss=0.0155469486489892
Q_Learning [277/300]: mean_loss=0.00973694073036313
Q_Learning [278/300]: mean_loss=0.007710681005846709
Q_Learning [279/300]: mean_loss=0.010526522411964834
Q_Learning [280/300]: mean_loss=0.007682044757530093
Q_Learning [281/300]: mean_loss=0.007461672823410481
Q_Learning [282/300]: mean_loss=0.013285567867569625
Q_Learning [283/300]: mean_loss=0.02264365879818797
Q_Learning [284/300]: mean_loss=0.011245191912166774
Q_Learning [285/300]: mean_loss=0.023960632737725973
Q_Learning [286/300]: mean_loss=0.005379533162340522
Q_Learning [287/300]: mean_loss=0.0035198232217226177
Q_Learning [288/300]: mean_loss=0.015829994226805866
Q_Learning [289/300]: mean_loss=0.0046166810498107225
Q_Learning [290/300]: mean_loss=0.004735371330752969
Q_Learning [291/300]: mean_loss=0.012651312630623579
Q_Learning [292/300]: mean_loss=0.008758290554396808
Q_Learning [293/300]: mean_loss=0.08045392576605082
Q_Learning [294/300]: mean_loss=0.027117159916087985
Q_Learning [295/300]: mean_loss=0.009712261031381786
Q_Learning [296/300]: mean_loss=0.009991534403525293
Q_Learning [297/300]: mean_loss=0.011471380479633808
Q_Learning [298/300]: mean_loss=0.007317279640119523
Q_Learning [299/300]: mean_loss=0.01773377088829875
Q_Learning [300/300]: mean_loss=0.011475021135993302
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.5122975   0.23632924]
[0, 1, 0, 1, 0, 2, 2, 2, 1, 1, 2, 0, 0, 1, 2, 0, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 0, 1, 2, 1, 1, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 1, 0, 1, 2, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2, 1, 1, 2, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 2, 2, 1, 1, 1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 2, 1, 0, 0, 2, 2, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1, 1, 2, 0, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 1, 2, 1, 2, 1, 0, 2, 2, 0, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 2, 0, 2, 0, 2, 1, 1, 2, 0, 2, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 0, 0, 1, 0, 0, 2, 0, 2, 2, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 2, 0, 1, 2, 0, 0, 1, 0, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 2, 1, 3, 3, 1, 1, 3, 1, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 1, 0, 1, 0, 0, 4, 1, 2, 4, 0, 2, 2, 0, 2, 0, 1, 2, 0, 4, 0, 1, 2, 1, 2, 2, 0, 4, 1, 3, 0, 0, 1, 2, 1, 3, 2, 1, 1, 4, 3, 1, 3, 2, 5, 4, 6, 2, 1, 2, 6, 4, 0, 4, 2, 1, 4, 2, 2, 2, 3, 4, 2, 4, 6, 1, 1, 6, 3, 3, 0, 2, 3, 3, 1, 0, 3, 4, 7, 4, 5, 2, 4, 3, 8, 2, 3, 2, 4, 4, 0, 2, 4, 2, 2, 2, 6, 9, 6, 5, 3, 1, 1, 4, 2, 4, 3, 8, 2, 2, 2, 4, 2, 2, 3, 5, 8, 4, 2, 4, 8, 4, 3, 2, 4, 4, 6, 1, 3, 0, 2, 0, 0, 5, 2, 0, 3, 0, 1, 1, 2, 5, 1, 8, 5, 3, 0, 5, 4, 4, 5, 2, 8, 5, 2, 2, 2, 2, 4, 4, 3, 4, 3, 7, 6, 2, 2, 2, 2, 5, 2, 5, 2, 5, 2, 0, 8, 0, 2, 4, 5, 2, 2, 2, 0, 2, 5, 8, 2, 2, 5, 2, 0, 8, 3, 3, 1, 6, 1, 5, 0, 0, 0, 5, 0, 0, 3, 0, 0, 5, 0, 5, 0, 2]
Centroids: [[-1.1929221, -0.325403], [-1.4160985, 0.42448014], [-2.0346384, -0.12310548]]
Centroids: [[-1.592983, 0.060121574], [-2.2124872, 0.20082852], [-0.9685621, -0.36264217], [-1.4475181, 0.71552086], [-2.0108752, -0.4788594], [-0.86695194, 0.51678675], [-2.7277975, -0.5679395], [-2.4792295, 1.1803955], [-1.406507, -0.8650117], [-3.0783424, -1.7055873]]
Standard Derivations: [0.31995928, 0.2627325, 0.26520392]
Cluster Distances: [0.19969714, 0.28052184, 0.19969714, 0.2981638, 0.28052187, 0.29816377]
Minimal Cluster Distance: 0.19969713687896729
Contingency Matrix: 
[[32  2 60  0  5  2  0  0  5  1]
 [31  7  6 26  0 18  0  2  1  0]
 [36 26  1  1 26  0  9  0  3  0]]
[[32, 2, 60, 0, 5, 2, 0, 0, 5, 1], [31, 7, 6, 26, 0, 18, 0, 2, 1, 0], [36, 26, 1, 1, 26, 0, 9, 0, 3, 0]]
[[32, 2, 60, 0, 5, 2, 0, 0, 5, 1], [31, 7, 6, 26, 0, 18, 0, 2, 1, 0], [36, 26, 1, 1, 26, 0, 9, 0, 3, 0]]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [31, 7, -1, 26, 0, 18, 0, 2, 1, 0], [36, 26, -1, 1, 26, 0, 9, 0, 3, 0]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, 7, -1, 26, 0, 18, 0, 2, 1, 0], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {0: 2, 2: 0, 1: 3}
New Contingency Matrix: 
[[60  0 32  2  5  2  0  0  5  1]
 [ 6 26 31  7  0 18  0  2  1  0]
 [ 1  1 36 26 26  0  9  0  3  0]]
New Clustered Label Sequence: [2, 3, 0, 1, 4, 5, 6, 7, 8, 9]
Diagonal_Elements: [60, 26, 36], Sum: 122
All_Elements: [60, 0, 32, 2, 5, 2, 0, 0, 5, 1, 6, 26, 31, 7, 0, 18, 0, 2, 1, 0, 1, 1, 36, 26, 26, 0, 9, 0, 3, 0], Sum: 300
Accuracy: 0.4066666666666667
