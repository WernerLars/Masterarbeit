Experiment_path: AE_Model_2/Reduce_Training_opt//V5_050/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise035.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise035.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_050/Experiment_05_opt/C_Easy1_noise035.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-15_04_07
Punishment_Coefficient: 1.5
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002ACF0EE5E10>
Sampling rate: 24000.0
Raw: [-0.01748803 -0.01945498 -0.02011069 ... -0.20744344 -0.24709427
 -0.25077586]
Times: [    662    1043    2861 ... 1439172 1439620 1439793]
Cluster: [1 2 3 ... 3 3 2]
Number of different clusters:  3
Number of Spikes: 3534
First aligned Spike Frame: [ 0.43999329  0.4839933   0.52909327  0.52642944  0.43496308  0.26335103
  0.0652557  -0.09376199 -0.19786698 -0.28302287 -0.39101775 -0.51215993
 -0.44771361  0.07217119  0.76700554  0.91966677  0.38465989 -0.27458603
 -0.59813837 -0.63307973 -0.5997719  -0.60009658 -0.61792931 -0.61010846
 -0.56778745 -0.50195254 -0.4233035  -0.35404397 -0.29120082 -0.20969116
 -0.09592158  0.02951377  0.1302449   0.18393993  0.21396859  0.24968719
  0.25635801  0.17294061 -0.01474948 -0.24084414 -0.43698551 -0.59191978
 -0.72153644 -0.80995398 -0.82451785 -0.75320979 -0.64145157]
Cluster 0, Occurrences: 1208
Cluster 1, Occurrences: 1137
Cluster 2, Occurrences: 1189
Number of Clusters: 3
Online_Training [1/50]: mean_loss=0.24250935018062592
Online_Training [2/50]: mean_loss=0.30755768343806267
Online_Training [3/50]: mean_loss=0.8155114129185677
Online_Training [4/50]: mean_loss=0.3472812548279762
Online_Training [5/50]: mean_loss=0.3812336251139641
Online_Training [6/50]: mean_loss=0.10381869878619909
Online_Training [7/50]: mean_loss=0.20923802442848682
Online_Training [8/50]: mean_loss=0.19944708421826363
Online_Training [9/50]: mean_loss=0.12449843715876341
Online_Training [10/50]: mean_loss=0.737969383597374
Online_Training [11/50]: mean_loss=0.7861589342355728
Online_Training [12/50]: mean_loss=0.300646536052227
Online_Training [13/50]: mean_loss=0.3811856023967266
Online_Training [14/50]: mean_loss=0.17256972193717957
Online_Training [15/50]: mean_loss=0.29496733844280243
Online_Training [16/50]: mean_loss=0.08104617055505514
Online_Training [17/50]: mean_loss=0.2980463467538357
Online_Training [18/50]: mean_loss=0.17852269113063812
Online_Training [19/50]: mean_loss=0.10265733767300844
Online_Training [20/50]: mean_loss=0.3406606800854206
Online_Training [21/50]: mean_loss=0.17328646779060364
Online_Training [22/50]: mean_loss=0.3214990794658661
Online_Training [23/50]: mean_loss=0.10498737450689077
Online_Training [24/50]: mean_loss=0.22472275607287884
Online_Training [25/50]: mean_loss=0.1814324539154768
Online_Training [26/50]: mean_loss=0.11394559871405363
Online_Training [27/50]: mean_loss=0.08435830846428871
Online_Training [28/50]: mean_loss=0.17846021614968777
Online_Training [29/50]: mean_loss=0.42736342921853065
Online_Training [30/50]: mean_loss=0.3842437379062176
Online_Training [31/50]: mean_loss=0.127707258798182
Online_Training [32/50]: mean_loss=0.4381319731473923
Online_Training [33/50]: mean_loss=0.1560197789222002
Online_Training [34/50]: mean_loss=0.21332196332514286
Online_Training [35/50]: mean_loss=0.13425369560718536
Online_Training [36/50]: mean_loss=0.06914911139756441
Online_Training [37/50]: mean_loss=0.04237658157944679
Online_Training [38/50]: mean_loss=0.11574092041701078
Online_Training [39/50]: mean_loss=0.1709567829966545
Online_Training [40/50]: mean_loss=0.1310995826497674
Online_Training [41/50]: mean_loss=0.2507405485957861
Online_Training [42/50]: mean_loss=0.19832063652575016
Online_Training [43/50]: mean_loss=0.06710442528128624
Online_Training [44/50]: mean_loss=0.1915582101792097
Online_Training [45/50]: mean_loss=0.47513772919774055
Online_Training [46/50]: mean_loss=0.3133830167353153
Online_Training [47/50]: mean_loss=0.11611533630639315
Online_Training [48/50]: mean_loss=0.1863153837621212
Online_Training [49/50]: mean_loss=0.6050011813640594
Online_Training [50/50]: mean_loss=0.19875557720661163
Q_Learning [1/300]: mean_loss=0.24250935018062592
Q_Learning [2/300]: mean_loss=0.30755768343806267
Q_Learning [3/300]: mean_loss=0.8155114129185677
Q_Learning [4/300]: mean_loss=0.3472812548279762
Q_Learning [5/300]: mean_loss=0.3812336251139641
Q_Learning [6/300]: mean_loss=0.10381869878619909
Q_Learning [7/300]: mean_loss=0.20923802442848682
Q_Learning [8/300]: mean_loss=0.19944708421826363
Q_Learning [9/300]: mean_loss=0.12449843715876341
Q_Learning [10/300]: mean_loss=0.737969383597374
Q_Learning [11/300]: mean_loss=0.7861589342355728
Q_Learning [12/300]: mean_loss=0.300646536052227
Q_Learning [13/300]: mean_loss=0.3811856023967266
Q_Learning [14/300]: mean_loss=0.17256972193717957
Q_Learning [15/300]: mean_loss=0.29496733844280243
Q_Learning [16/300]: mean_loss=0.08104617055505514
Q_Learning [17/300]: mean_loss=0.2980463467538357
Q_Learning [18/300]: mean_loss=0.17852269113063812
Q_Learning [19/300]: mean_loss=0.10265733767300844
Q_Learning [20/300]: mean_loss=0.3406606800854206
Q_Learning [21/300]: mean_loss=0.17328646779060364
Q_Learning [22/300]: mean_loss=0.3214990794658661
Q_Learning [23/300]: mean_loss=0.10498737450689077
Q_Learning [24/300]: mean_loss=0.22472275607287884
Q_Learning [25/300]: mean_loss=0.1814324539154768
Q_Learning [26/300]: mean_loss=0.11394559871405363
Q_Learning [27/300]: mean_loss=0.08435830846428871
Q_Learning [28/300]: mean_loss=0.17846021614968777
Q_Learning [29/300]: mean_loss=0.42736342921853065
Q_Learning [30/300]: mean_loss=0.3842437379062176
Q_Learning [31/300]: mean_loss=0.127707258798182
Q_Learning [32/300]: mean_loss=0.4381319731473923
Q_Learning [33/300]: mean_loss=0.1560197789222002
Q_Learning [34/300]: mean_loss=0.21332196332514286
Q_Learning [35/300]: mean_loss=0.13425369560718536
Q_Learning [36/300]: mean_loss=0.06914911139756441
Q_Learning [37/300]: mean_loss=0.04237658157944679
Q_Learning [38/300]: mean_loss=0.11574092041701078
Q_Learning [39/300]: mean_loss=0.1709567829966545
Q_Learning [40/300]: mean_loss=0.1310995826497674
Q_Learning [41/300]: mean_loss=0.2507405485957861
Q_Learning [42/300]: mean_loss=0.19832063652575016
Q_Learning [43/300]: mean_loss=0.06710442528128624
Q_Learning [44/300]: mean_loss=0.1915582101792097
Q_Learning [45/300]: mean_loss=0.47513772919774055
Q_Learning [46/300]: mean_loss=0.3133830167353153
Q_Learning [47/300]: mean_loss=0.11611533630639315
Q_Learning [48/300]: mean_loss=0.1863153837621212
Q_Learning [49/300]: mean_loss=0.6050011813640594
Q_Learning [50/300]: mean_loss=0.19875557720661163
Q_Learning [51/300]: mean_loss=0.10160743072628975
Q_Learning [52/300]: mean_loss=0.15908648818731308
Q_Learning [53/300]: mean_loss=0.17773114517331123
Q_Learning [54/300]: mean_loss=0.055809634272009134
Q_Learning [55/300]: mean_loss=0.42927859351038933
Q_Learning [56/300]: mean_loss=0.20681174844503403
Q_Learning [57/300]: mean_loss=0.06268236972391605
Q_Learning [58/300]: mean_loss=0.2005415242165327
Q_Learning [59/300]: mean_loss=0.16286531649529934
Q_Learning [60/300]: mean_loss=0.15645011328160763
Q_Learning [61/300]: mean_loss=0.1182139040902257
Q_Learning [62/300]: mean_loss=0.19859509356319904
Q_Learning [63/300]: mean_loss=0.122573122382164
Q_Learning [64/300]: mean_loss=0.08348744828253984
Q_Learning [65/300]: mean_loss=0.23944700323045254
Q_Learning [66/300]: mean_loss=0.6602336093783379
Q_Learning [67/300]: mean_loss=0.10061898361891508
Q_Learning [68/300]: mean_loss=0.13263578619807959
Q_Learning [69/300]: mean_loss=0.1275299098342657
Q_Learning [70/300]: mean_loss=0.04305675672367215
Q_Learning [71/300]: mean_loss=0.07642880640923977
Q_Learning [72/300]: mean_loss=0.1661215927451849
Q_Learning [73/300]: mean_loss=0.04790127510204911
Q_Learning [74/300]: mean_loss=0.1852447371929884
Q_Learning [75/300]: mean_loss=0.258096843957901
Q_Learning [76/300]: mean_loss=0.08542698482051492
Q_Learning [77/300]: mean_loss=0.12374807707965374
Q_Learning [78/300]: mean_loss=0.1198617396876216
Q_Learning [79/300]: mean_loss=0.18945278227329254
Q_Learning [80/300]: mean_loss=0.08891020249575377
Q_Learning [81/300]: mean_loss=0.09193497058004141
Q_Learning [82/300]: mean_loss=0.10045152436941862
Q_Learning [83/300]: mean_loss=0.13301651179790497
Q_Learning [84/300]: mean_loss=0.2922940328717232
Q_Learning [85/300]: mean_loss=0.07180473674088717
Q_Learning [86/300]: mean_loss=0.05662336386740208
Q_Learning [87/300]: mean_loss=0.20660694036632776
Q_Learning [88/300]: mean_loss=0.1383423414081335
Q_Learning [89/300]: mean_loss=0.09715983597561717
Q_Learning [90/300]: mean_loss=0.1463199406862259
Q_Learning [91/300]: mean_loss=0.06103878701105714
Q_Learning [92/300]: mean_loss=0.10491787921637297
Q_Learning [93/300]: mean_loss=0.08011893648654222
Q_Learning [94/300]: mean_loss=0.023564114468172193
Q_Learning [95/300]: mean_loss=0.05492329690605402
Q_Learning [96/300]: mean_loss=0.23988508433103561
Q_Learning [97/300]: mean_loss=0.20026703737676144
Q_Learning [98/300]: mean_loss=0.1017896980047226
Q_Learning [99/300]: mean_loss=0.060731221456080675
Q_Learning [100/300]: mean_loss=0.2419031821191311
Q_Learning [101/300]: mean_loss=0.14067973662167788
Q_Learning [102/300]: mean_loss=0.0352805033326149
Q_Learning [103/300]: mean_loss=0.038433101028203964
Q_Learning [104/300]: mean_loss=0.07724225986748934
Q_Learning [105/300]: mean_loss=0.16074813343584538
Q_Learning [106/300]: mean_loss=0.14452527090907097
Q_Learning [107/300]: mean_loss=0.14532572217285633
Q_Learning [108/300]: mean_loss=0.12912968918681145
Q_Learning [109/300]: mean_loss=0.1325846565887332
Q_Learning [110/300]: mean_loss=0.05234034080058336
Q_Learning [111/300]: mean_loss=0.07238240167498589
Q_Learning [112/300]: mean_loss=0.06397231668233871
Q_Learning [113/300]: mean_loss=0.10007474198937416
Q_Learning [114/300]: mean_loss=0.06791913136839867
Q_Learning [115/300]: mean_loss=0.029526062309741974
Q_Learning [116/300]: mean_loss=0.09446341078728437
Q_Learning [117/300]: mean_loss=0.12422912009060383
Q_Learning [118/300]: mean_loss=0.08024326991289854
Q_Learning [119/300]: mean_loss=0.10689735319465399
Q_Learning [120/300]: mean_loss=0.15325911715626717
Q_Learning [121/300]: mean_loss=0.11754661612212658
Q_Learning [122/300]: mean_loss=0.04771604994311929
Q_Learning [123/300]: mean_loss=0.12499864399433136
Q_Learning [124/300]: mean_loss=0.07599836029112339
Q_Learning [125/300]: mean_loss=0.07417750637978315
Q_Learning [126/300]: mean_loss=0.28704629838466644
Q_Learning [127/300]: mean_loss=0.1214808989316225
Q_Learning [128/300]: mean_loss=0.125187736004591
Q_Learning [129/300]: mean_loss=0.0863256873562932
Q_Learning [130/300]: mean_loss=0.05281607806682587
Q_Learning [131/300]: mean_loss=0.0658148992806673
Q_Learning [132/300]: mean_loss=0.46009741723537445
Q_Learning [133/300]: mean_loss=0.24751939438283443
Q_Learning [134/300]: mean_loss=0.022879372583702207
Q_Learning [135/300]: mean_loss=0.16305183619260788
Q_Learning [136/300]: mean_loss=0.07668575830757618
Q_Learning [137/300]: mean_loss=0.14315451122820377
Q_Learning [138/300]: mean_loss=0.15228118188679218
Q_Learning [139/300]: mean_loss=0.07103690039366484
Q_Learning [140/300]: mean_loss=0.07455620542168617
Q_Learning [141/300]: mean_loss=0.09258237201720476
Q_Learning [142/300]: mean_loss=0.08150458429008722
Q_Learning [143/300]: mean_loss=0.08454775717109442
Q_Learning [144/300]: mean_loss=0.05278218677267432
Q_Learning [145/300]: mean_loss=0.3266316428780556
Q_Learning [146/300]: mean_loss=0.1245387876406312
Q_Learning [147/300]: mean_loss=0.1691582389175892
Q_Learning [148/300]: mean_loss=0.16313246823847294
Q_Learning [149/300]: mean_loss=0.037971016485244036
Q_Learning [150/300]: mean_loss=0.11134015209972858
Q_Learning [151/300]: mean_loss=0.1043484527617693
Q_Learning [152/300]: mean_loss=0.04727176995947957
Q_Learning [153/300]: mean_loss=0.03672132035717368
Q_Learning [154/300]: mean_loss=0.10135461762547493
Q_Learning [155/300]: mean_loss=0.0802213829010725
Q_Learning [156/300]: mean_loss=0.1408283580094576
Q_Learning [157/300]: mean_loss=0.09957364574074745
Q_Learning [158/300]: mean_loss=0.07741416059434414
Q_Learning [159/300]: mean_loss=0.17431870102882385
Q_Learning [160/300]: mean_loss=0.10915015824139118
Q_Learning [161/300]: mean_loss=0.20269149914383888
Q_Learning [162/300]: mean_loss=0.12230292055755854
Q_Learning [163/300]: mean_loss=0.22579293884336948
Q_Learning [164/300]: mean_loss=0.03826220566406846
Q_Learning [165/300]: mean_loss=0.07383451983332634
Q_Learning [166/300]: mean_loss=0.04902695817872882
Q_Learning [167/300]: mean_loss=0.07968959677964449
Q_Learning [168/300]: mean_loss=0.16223281808197498
Q_Learning [169/300]: mean_loss=0.18138074688613415
Q_Learning [170/300]: mean_loss=0.035262497840449214
Q_Learning [171/300]: mean_loss=0.12709719873964787
Q_Learning [172/300]: mean_loss=0.20257867127656937
Q_Learning [173/300]: mean_loss=0.06495276559144258
Q_Learning [174/300]: mean_loss=0.08840353600680828
Q_Learning [175/300]: mean_loss=0.17347864992916584
Q_Learning [176/300]: mean_loss=0.4543381370604038
Q_Learning [177/300]: mean_loss=0.3605448119342327
Q_Learning [178/300]: mean_loss=0.18070901930332184
Q_Learning [179/300]: mean_loss=0.04181547975167632
Q_Learning [180/300]: mean_loss=0.08787964098155499
Q_Learning [181/300]: mean_loss=0.19835362397134304
Q_Learning [182/300]: mean_loss=0.11136624030768871
Q_Learning [183/300]: mean_loss=0.19728489965200424
Q_Learning [184/300]: mean_loss=0.04515914851799607
Q_Learning [185/300]: mean_loss=0.1656715888530016
Q_Learning [186/300]: mean_loss=0.1908507701009512
Q_Learning [187/300]: mean_loss=0.04321915237233043
Q_Learning [188/300]: mean_loss=0.14003807120025158
Q_Learning [189/300]: mean_loss=0.058621213771402836
Q_Learning [190/300]: mean_loss=0.02502544689923525
Q_Learning [191/300]: mean_loss=0.11813674960285425
Q_Learning [192/300]: mean_loss=0.14489353075623512
Q_Learning [193/300]: mean_loss=0.03658434143289924
Q_Learning [194/300]: mean_loss=0.1750874798744917
Q_Learning [195/300]: mean_loss=0.1935331765562296
Q_Learning [196/300]: mean_loss=0.06891756411641836
Q_Learning [197/300]: mean_loss=0.19649743288755417
Q_Learning [198/300]: mean_loss=0.09946313686668873
Q_Learning [199/300]: mean_loss=0.19139152020215988
Q_Learning [200/300]: mean_loss=0.06578314863145351
Q_Learning [201/300]: mean_loss=0.1239958293735981
Q_Learning [202/300]: mean_loss=0.1059333709999919
Q_Learning [203/300]: mean_loss=0.06363044586032629
Q_Learning [204/300]: mean_loss=0.07964455336332321
Q_Learning [205/300]: mean_loss=0.13250710256397724
Q_Learning [206/300]: mean_loss=0.06187670864164829
Q_Learning [207/300]: mean_loss=0.11682626977562904
Q_Learning [208/300]: mean_loss=0.27957461401820183
Q_Learning [209/300]: mean_loss=0.16192479617893696
Q_Learning [210/300]: mean_loss=0.14752011559903622
Q_Learning [211/300]: mean_loss=0.13740903325378895
Q_Learning [212/300]: mean_loss=0.0318283278029412
Q_Learning [213/300]: mean_loss=0.08965455368161201
Q_Learning [214/300]: mean_loss=0.059487422462552786
Q_Learning [215/300]: mean_loss=0.027224275982007384
Q_Learning [216/300]: mean_loss=0.08504845853894949
Q_Learning [217/300]: mean_loss=0.056192693300545216
Q_Learning [218/300]: mean_loss=0.08397188875824213
Q_Learning [219/300]: mean_loss=0.07970098685473204
Q_Learning [220/300]: mean_loss=0.04480257770046592
Q_Learning [221/300]: mean_loss=0.1676137112081051
Q_Learning [222/300]: mean_loss=0.027067797491326928
Q_Learning [223/300]: mean_loss=0.08327465876936913
Q_Learning [224/300]: mean_loss=0.11118718702346087
Q_Learning [225/300]: mean_loss=0.07685354724526405
Q_Learning [226/300]: mean_loss=0.10220346506685019
Q_Learning [227/300]: mean_loss=0.06304747937247157
Q_Learning [228/300]: mean_loss=0.23987248726189137
Q_Learning [229/300]: mean_loss=0.05056968377903104
Q_Learning [230/300]: mean_loss=0.10394535958766937
Q_Learning [231/300]: mean_loss=0.3134946674108505
Q_Learning [232/300]: mean_loss=0.04007716616615653
Q_Learning [233/300]: mean_loss=0.08517653029412031
Q_Learning [234/300]: mean_loss=0.04042492667213082
Q_Learning [235/300]: mean_loss=0.06535065080970526
Q_Learning [236/300]: mean_loss=0.0618058773688972
Q_Learning [237/300]: mean_loss=0.0313058546744287
Q_Learning [238/300]: mean_loss=0.12574420403689146
Q_Learning [239/300]: mean_loss=0.15743068419396877
Q_Learning [240/300]: mean_loss=0.0750636188313365
Q_Learning [241/300]: mean_loss=0.03280753688886762
Q_Learning [242/300]: mean_loss=0.1061063976958394
Q_Learning [243/300]: mean_loss=0.08437160216271877
Q_Learning [244/300]: mean_loss=0.08216528687626123
Q_Learning [245/300]: mean_loss=0.06300400663167238
Q_Learning [246/300]: mean_loss=0.05600649770349264
Q_Learning [247/300]: mean_loss=0.07530688028782606
Q_Learning [248/300]: mean_loss=0.10937829501926899
Q_Learning [249/300]: mean_loss=0.10064984671771526
Q_Learning [250/300]: mean_loss=0.06973376777023077
Q_Learning [251/300]: mean_loss=0.1147475354373455
Q_Learning [252/300]: mean_loss=0.023945528315380216
Q_Learning [253/300]: mean_loss=0.05047008115798235
Q_Learning [254/300]: mean_loss=0.1653603222221136
Q_Learning [255/300]: mean_loss=0.47221871837973595
Q_Learning [256/300]: mean_loss=0.4727412164211273
Q_Learning [257/300]: mean_loss=0.07156430371105671
Q_Learning [258/300]: mean_loss=0.07514078635722399
Q_Learning [259/300]: mean_loss=0.07437524478882551
Q_Learning [260/300]: mean_loss=0.07467643357813358
Q_Learning [261/300]: mean_loss=0.08697565831243992
Q_Learning [262/300]: mean_loss=0.07018262706696987
Q_Learning [263/300]: mean_loss=0.05799147207289934
Q_Learning [264/300]: mean_loss=0.030667877988889813
Q_Learning [265/300]: mean_loss=0.015081426477991045
Q_Learning [266/300]: mean_loss=0.10190844908356667
Q_Learning [267/300]: mean_loss=0.08769737370312214
Q_Learning [268/300]: mean_loss=0.17118646949529648
Q_Learning [269/300]: mean_loss=0.034208932192996144
Q_Learning [270/300]: mean_loss=0.12196778226643801
Q_Learning [271/300]: mean_loss=0.180718706920743
Q_Learning [272/300]: mean_loss=0.10483650863170624
Q_Learning [273/300]: mean_loss=0.5645790621638298
Q_Learning [274/300]: mean_loss=0.0851153451949358
Q_Learning [275/300]: mean_loss=0.14635768346488476
Q_Learning [276/300]: mean_loss=0.14375005848705769
Q_Learning [277/300]: mean_loss=0.3272121101617813
Q_Learning [278/300]: mean_loss=0.2612367309629917
Q_Learning [279/300]: mean_loss=0.0524013196118176
Q_Learning [280/300]: mean_loss=0.09200422093272209
Q_Learning [281/300]: mean_loss=0.03854616405442357
Q_Learning [282/300]: mean_loss=0.07503760419785976
Q_Learning [283/300]: mean_loss=0.10366138257086277
Q_Learning [284/300]: mean_loss=0.14257538877427578
Q_Learning [285/300]: mean_loss=0.10142201744019985
Q_Learning [286/300]: mean_loss=0.22792471013963223
Q_Learning [287/300]: mean_loss=0.13264225050807
Q_Learning [288/300]: mean_loss=0.1256211083382368
Q_Learning [289/300]: mean_loss=0.10410357639193535
Q_Learning [290/300]: mean_loss=0.04633736051619053
Q_Learning [291/300]: mean_loss=0.08452002331614494
Q_Learning [292/300]: mean_loss=0.07310173567384481
Q_Learning [293/300]: mean_loss=0.1680894624441862
Q_Learning [294/300]: mean_loss=0.03155056806281209
Q_Learning [295/300]: mean_loss=0.044904000125825405
Q_Learning [296/300]: mean_loss=0.07460199110209942
Q_Learning [297/300]: mean_loss=0.052231688518077135
Q_Learning [298/300]: mean_loss=0.35975179448723793
Q_Learning [299/300]: mean_loss=0.2758052721619606
Q_Learning [300/300]: mean_loss=0.05709068011492491
Number of Samples after Autoencoder testing: 300
First Spike after testing: [0.5487466 2.2375684]
[2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 1, 0, 0, 1, 2, 0, 2, 2, 2, 0, 0, 0, 2, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 2, 1, 1, 2, 1, 2, 1, 2, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 0, 2, 1, 2, 0, 2, 1, 0, 0, 1, 2, 2, 0, 1, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0, 1, 2, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 2, 0, 2, 2, 0, 0, 1, 1, 0, 1, 2, 1, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 0, 0, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 2, 0, 2, 2, 2, 1, 0, 0, 1, 2, 1, 1, 0, 1, 2, 0, 1, 2, 2, 0, 1, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 0, 1, 0, 2, 1, 2, 1, 0, 1, 2, 1, 0, 2, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2, 1, 1, 0, 0, 2, 0, 0, 1, 2, 2, 1, 2, 0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 1, 0, 0, 1, 1, 2, 2, 2, 0, 1, 0, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1]
[0, 0, 1, 2, 0, 2, 0, 0, 0, 2, 0, 1, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 1, 2, 1, 0, 0, 2, 2, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 1, 3, 0, 1, 3, 1, 1, 0, 2, 0, 0, 4, 2, 0, 0, 3, 0, 0, 2, 1, 2, 0, 3, 1, 1, 0, 1, 0, 0, 1, 3, 2, 1, 0, 2, 0, 2, 3, 0, 2, 0, 0, 0, 3, 0, 0, 3, 3, 0, 3, 0, 0, 1, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 3, 1, 0, 1, 3, 0, 1, 0, 0, 2, 3, 2, 0, 3, 1, 0, 1, 2, 3, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 2, 0, 3, 2, 0, 2, 2, 2, 0, 3, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 0, 3, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 3, 0, 0, 0, 3, 1, 2, 0, 3, 3, 2, 3, 3, 0, 1, 0, 1, 0, 0, 1, 2, 3, 0, 3, 2, 1, 0, 2, 0, 2, 3, 0, 2, 2, 0, 0, 2, 2, 1, 3, 2, 0, 2, 2, 0, 3, 2, 2, 1, 3, 1, 0, 0, 2, 2, 2, 0, 3, 2, 2, 1, 0, 0, 1, 1, 2, 2, 2, 0, 1, 0, 2, 3, 0, 0, 2, 2, 2, 3, 2, 2, 1, 1, 0, 2, 3, 3, 2, 1, 3, 1, 3]
Centroids: [[-0.8228283, 0.9604263], [0.38402972, -1.3058928], [1.0086812, 1.7968829]]
Centroids: [[-0.4289685, 1.0872254], [-0.17781328, -1.3666037], [1.096148, 2.197956], [1.2867239, -0.69515026], [0.59623045, -3.861936]]
Standard Derivations: [0.49782968, 0.5387204, 0.63334763]
Cluster Distances: [1.5310769, 0.88229907, 1.5310769, 1.9929606, 0.88229907, 1.9929607]
Minimal Cluster Distance: 0.8822990655899048
Contingency Matrix: 
[[82  8  5  0  0]
 [ 2 59  0 31  1]
 [33  1 71  7  0]]
[[82, 8, 5, 0, 0], [2, 59, 0, 31, 1], [33, 1, 71, 7, 0]]
[[82, 8, 5, 0, 0], [2, 59, 0, 31, 1], [33, 1, 71, 7, 0]]
[0, 1, 2, 3, 4]
[[-1, -1, -1, -1, -1], [-1, 59, 0, 31, 1], [-1, 1, 71, 7, 0]]
[[-1, -1, -1, -1, -1], [-1, 59, -1, 31, 1], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {0: 0, 2: 2, 1: 1}
New Contingency Matrix: 
[[82  8  5  0  0]
 [ 2 59  0 31  1]
 [33  1 71  7  0]]
New Clustered Label Sequence: [0, 1, 2, 3, 4]
Diagonal_Elements: [82, 59, 71], Sum: 212
All_Elements: [82, 8, 5, 0, 0, 2, 59, 0, 31, 1, 33, 1, 71, 7, 0], Sum: 300
Accuracy: 0.7066666666666667
