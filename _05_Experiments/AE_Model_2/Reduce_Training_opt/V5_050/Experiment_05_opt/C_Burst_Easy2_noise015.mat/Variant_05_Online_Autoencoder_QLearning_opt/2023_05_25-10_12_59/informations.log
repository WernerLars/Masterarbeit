Experiment_path: AE_Model_2/Reduce_Training_opt//V5_050/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Burst_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Burst_Easy2_noise015.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_050/Experiment_05_opt/C_Burst_Easy2_noise015.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-10_12_59
Punishment_Coefficient: 1.2
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x0000021E7A8FC8D0>
Sampling rate: 24000.0
Raw: [ 0.24336953  0.26920333  0.26782334 ... -0.02629827 -0.02223585
 -0.02239043]
Times: [    195     430     737 ... 1439108 1439373 1439782]
Cluster: [2 1 1 ... 2 3 1]
Number of different clusters:  3
Number of Spikes: 3442
First aligned Spike Frame: [-0.01689698 -0.02635498 -0.01562648  0.02897143  0.09419909  0.16126917
  0.22354469  0.27941475  0.32258352  0.34699582  0.35463705  0.34576274
  0.299707    0.15447276 -0.11443537 -0.29135945 -0.02374047  0.60144887
  1.08218794  1.17595279  1.04108946  0.87905736  0.74420278  0.62460764
  0.52287424  0.43951429  0.36219288  0.28506818  0.21680111  0.17041962
  0.1410207   0.12802623  0.13803385  0.16548243  0.19507167  0.22209636
  0.24476727  0.25441697  0.2415664   0.21156445  0.18433246  0.16716092
  0.15280507  0.14158827  0.14947965  0.19464084  0.26501024]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1156
Cluster 2, Occurrences: 1127
Number of Clusters: 3
Online_Training [1/50]: mean_loss=0.2006339505314827
Online_Training [2/50]: mean_loss=0.2430319171398878
Online_Training [3/50]: mean_loss=0.22050213254988194
Online_Training [4/50]: mean_loss=0.08446377236396074
Online_Training [5/50]: mean_loss=0.25137187726795673
Online_Training [6/50]: mean_loss=0.10796177014708519
Online_Training [7/50]: mean_loss=0.11403135862201452
Online_Training [8/50]: mean_loss=0.07530721183866262
Online_Training [9/50]: mean_loss=0.12204575445502996
Online_Training [10/50]: mean_loss=0.15029273927211761
Online_Training [11/50]: mean_loss=0.06494785845279694
Online_Training [12/50]: mean_loss=0.07111447770148516
Online_Training [13/50]: mean_loss=0.08543363772332668
Online_Training [14/50]: mean_loss=0.0352630908600986
Online_Training [15/50]: mean_loss=0.04451932059600949
Online_Training [16/50]: mean_loss=0.12520800530910492
Online_Training [17/50]: mean_loss=0.09295208565890789
Online_Training [18/50]: mean_loss=0.0564671796746552
Online_Training [19/50]: mean_loss=0.026302488055080175
Online_Training [20/50]: mean_loss=0.14758675545454025
Online_Training [21/50]: mean_loss=0.12977823242545128
Online_Training [22/50]: mean_loss=0.019443512661382556
Online_Training [23/50]: mean_loss=0.018300584517419338
Online_Training [24/50]: mean_loss=0.07178271654993296
Online_Training [25/50]: mean_loss=0.03866708232089877
Online_Training [26/50]: mean_loss=0.04723857855424285
Online_Training [27/50]: mean_loss=0.10611386317759752
Online_Training [28/50]: mean_loss=0.035896089393645525
Online_Training [29/50]: mean_loss=0.04460737854242325
Online_Training [30/50]: mean_loss=0.03173204930499196
Online_Training [31/50]: mean_loss=0.028062039287760854
Online_Training [32/50]: mean_loss=0.04436734225600958
Online_Training [33/50]: mean_loss=0.023949507158249617
Online_Training [34/50]: mean_loss=0.14377667382359505
Online_Training [35/50]: mean_loss=0.05411825329065323
Online_Training [36/50]: mean_loss=0.08588493801653385
Online_Training [37/50]: mean_loss=0.05232199653983116
Online_Training [38/50]: mean_loss=0.040948871057480574
Online_Training [39/50]: mean_loss=0.11715704295784235
Online_Training [40/50]: mean_loss=0.033325752476230264
Online_Training [41/50]: mean_loss=0.031153156189247966
Online_Training [42/50]: mean_loss=0.0577699551358819
Online_Training [43/50]: mean_loss=0.11939004436135292
Online_Training [44/50]: mean_loss=0.15081367641687393
Online_Training [45/50]: mean_loss=0.06321040214970708
Online_Training [46/50]: mean_loss=0.03800788428634405
Online_Training [47/50]: mean_loss=0.08822763618081808
Online_Training [48/50]: mean_loss=0.07015220634639263
Online_Training [49/50]: mean_loss=0.09023109171539545
Online_Training [50/50]: mean_loss=0.1199053218588233
Q_Learning [1/300]: mean_loss=0.2006339505314827
Q_Learning [2/300]: mean_loss=0.2430319171398878
Q_Learning [3/300]: mean_loss=0.22050213254988194
Q_Learning [4/300]: mean_loss=0.08446377236396074
Q_Learning [5/300]: mean_loss=0.25137187726795673
Q_Learning [6/300]: mean_loss=0.10796177014708519
Q_Learning [7/300]: mean_loss=0.11403135862201452
Q_Learning [8/300]: mean_loss=0.07530721183866262
Q_Learning [9/300]: mean_loss=0.12204575445502996
Q_Learning [10/300]: mean_loss=0.15029273927211761
Q_Learning [11/300]: mean_loss=0.06494785845279694
Q_Learning [12/300]: mean_loss=0.07111447770148516
Q_Learning [13/300]: mean_loss=0.08543363772332668
Q_Learning [14/300]: mean_loss=0.0352630908600986
Q_Learning [15/300]: mean_loss=0.04451932059600949
Q_Learning [16/300]: mean_loss=0.12520800530910492
Q_Learning [17/300]: mean_loss=0.09295208565890789
Q_Learning [18/300]: mean_loss=0.0564671796746552
Q_Learning [19/300]: mean_loss=0.026302488055080175
Q_Learning [20/300]: mean_loss=0.14758675545454025
Q_Learning [21/300]: mean_loss=0.12977823242545128
Q_Learning [22/300]: mean_loss=0.019443512661382556
Q_Learning [23/300]: mean_loss=0.018300584517419338
Q_Learning [24/300]: mean_loss=0.07178271654993296
Q_Learning [25/300]: mean_loss=0.03866708232089877
Q_Learning [26/300]: mean_loss=0.04723857855424285
Q_Learning [27/300]: mean_loss=0.10611386317759752
Q_Learning [28/300]: mean_loss=0.035896089393645525
Q_Learning [29/300]: mean_loss=0.04460737854242325
Q_Learning [30/300]: mean_loss=0.03173204930499196
Q_Learning [31/300]: mean_loss=0.028062039287760854
Q_Learning [32/300]: mean_loss=0.04436734225600958
Q_Learning [33/300]: mean_loss=0.023949507158249617
Q_Learning [34/300]: mean_loss=0.14377667382359505
Q_Learning [35/300]: mean_loss=0.05411825329065323
Q_Learning [36/300]: mean_loss=0.08588493801653385
Q_Learning [37/300]: mean_loss=0.05232199653983116
Q_Learning [38/300]: mean_loss=0.040948871057480574
Q_Learning [39/300]: mean_loss=0.11715704295784235
Q_Learning [40/300]: mean_loss=0.033325752476230264
Q_Learning [41/300]: mean_loss=0.031153156189247966
Q_Learning [42/300]: mean_loss=0.0577699551358819
Q_Learning [43/300]: mean_loss=0.11939004436135292
Q_Learning [44/300]: mean_loss=0.15081367641687393
Q_Learning [45/300]: mean_loss=0.06321040214970708
Q_Learning [46/300]: mean_loss=0.03800788428634405
Q_Learning [47/300]: mean_loss=0.08822763618081808
Q_Learning [48/300]: mean_loss=0.07015220634639263
Q_Learning [49/300]: mean_loss=0.09023109171539545
Q_Learning [50/300]: mean_loss=0.1199053218588233
Q_Learning [51/300]: mean_loss=0.026291385293006897
Q_Learning [52/300]: mean_loss=0.16657978668808937
Q_Learning [53/300]: mean_loss=0.02095453324727714
Q_Learning [54/300]: mean_loss=0.08665870502591133
Q_Learning [55/300]: mean_loss=0.03816726943477988
Q_Learning [56/300]: mean_loss=0.04069593036547303
Q_Learning [57/300]: mean_loss=0.06534491712227464
Q_Learning [58/300]: mean_loss=0.3484125882387161
Q_Learning [59/300]: mean_loss=0.07024162588641047
Q_Learning [60/300]: mean_loss=0.06856631208211184
Q_Learning [61/300]: mean_loss=0.05523296119645238
Q_Learning [62/300]: mean_loss=0.08747024228796363
Q_Learning [63/300]: mean_loss=0.06703915633261204
Q_Learning [64/300]: mean_loss=0.00816499680513516
Q_Learning [65/300]: mean_loss=0.06413964787498116
Q_Learning [66/300]: mean_loss=0.04327953141182661
Q_Learning [67/300]: mean_loss=0.061305887065827847
Q_Learning [68/300]: mean_loss=0.0730448691174388
Q_Learning [69/300]: mean_loss=0.17818692326545715
Q_Learning [70/300]: mean_loss=0.03714572498574853
Q_Learning [71/300]: mean_loss=0.07513544801622629
Q_Learning [72/300]: mean_loss=0.07723300904035568
Q_Learning [73/300]: mean_loss=0.04133720276877284
Q_Learning [74/300]: mean_loss=0.04172035586088896
Q_Learning [75/300]: mean_loss=0.030572673538699746
Q_Learning [76/300]: mean_loss=0.04238187009468675
Q_Learning [77/300]: mean_loss=0.04193710698746145
Q_Learning [78/300]: mean_loss=0.06769988685846329
Q_Learning [79/300]: mean_loss=0.04319629538804293
Q_Learning [80/300]: mean_loss=0.035978137981146574
Q_Learning [81/300]: mean_loss=0.014137008460238576
Q_Learning [82/300]: mean_loss=0.029106634203344584
Q_Learning [83/300]: mean_loss=0.10961493477225304
Q_Learning [84/300]: mean_loss=0.03134161210618913
Q_Learning [85/300]: mean_loss=0.023666636552661657
Q_Learning [86/300]: mean_loss=0.05738385906443
Q_Learning [87/300]: mean_loss=0.013945995247922838
Q_Learning [88/300]: mean_loss=0.03137581027112901
Q_Learning [89/300]: mean_loss=0.04651913931593299
Q_Learning [90/300]: mean_loss=0.02241769223473966
Q_Learning [91/300]: mean_loss=0.03718185983598232
Q_Learning [92/300]: mean_loss=0.04715253412723541
Q_Learning [93/300]: mean_loss=0.058585465885698795
Q_Learning [94/300]: mean_loss=0.056631291285157204
Q_Learning [95/300]: mean_loss=0.06617959029972553
Q_Learning [96/300]: mean_loss=0.020651123370043933
Q_Learning [97/300]: mean_loss=0.0560779832303524
Q_Learning [98/300]: mean_loss=0.04428124940022826
Q_Learning [99/300]: mean_loss=0.01881395443342626
Q_Learning [100/300]: mean_loss=0.04375685704872012
Q_Learning [101/300]: mean_loss=0.031521196477115154
Q_Learning [102/300]: mean_loss=0.058606951497495174
Q_Learning [103/300]: mean_loss=0.04588177427649498
Q_Learning [104/300]: mean_loss=0.05606377124786377
Q_Learning [105/300]: mean_loss=0.03824298968538642
Q_Learning [106/300]: mean_loss=0.01496502012014389
Q_Learning [107/300]: mean_loss=0.017027721041813493
Q_Learning [108/300]: mean_loss=0.031226832885295153
Q_Learning [109/300]: mean_loss=0.03681512945331633
Q_Learning [110/300]: mean_loss=0.03113732673227787
Q_Learning [111/300]: mean_loss=0.029751094989478588
Q_Learning [112/300]: mean_loss=0.15565561316907406
Q_Learning [113/300]: mean_loss=0.026922553777694702
Q_Learning [114/300]: mean_loss=0.05732702696695924
Q_Learning [115/300]: mean_loss=0.04295912757515907
Q_Learning [116/300]: mean_loss=0.028974503744393587
Q_Learning [117/300]: mean_loss=0.05432074470445514
Q_Learning [118/300]: mean_loss=0.056795123498886824
Q_Learning [119/300]: mean_loss=0.12788630090653896
Q_Learning [120/300]: mean_loss=0.06395474309101701
Q_Learning [121/300]: mean_loss=0.06745206750929356
Q_Learning [122/300]: mean_loss=0.07385201565921307
Q_Learning [123/300]: mean_loss=0.035073064267635345
Q_Learning [124/300]: mean_loss=0.013005473301745951
Q_Learning [125/300]: mean_loss=0.08400718914344907
Q_Learning [126/300]: mean_loss=0.039744215086102486
Q_Learning [127/300]: mean_loss=0.0693319933488965
Q_Learning [128/300]: mean_loss=0.02797252801246941
Q_Learning [129/300]: mean_loss=0.03271903400309384
Q_Learning [130/300]: mean_loss=0.020516570657491684
Q_Learning [131/300]: mean_loss=0.027882908936589956
Q_Learning [132/300]: mean_loss=0.018296118592843413
Q_Learning [133/300]: mean_loss=0.02529704663902521
Q_Learning [134/300]: mean_loss=0.025064500281587243
Q_Learning [135/300]: mean_loss=0.10897853318601847
Q_Learning [136/300]: mean_loss=0.022181940963491797
Q_Learning [137/300]: mean_loss=0.02677396940998733
Q_Learning [138/300]: mean_loss=0.011712275561876595
Q_Learning [139/300]: mean_loss=0.020239447010681033
Q_Learning [140/300]: mean_loss=0.03317991178482771
Q_Learning [141/300]: mean_loss=0.030773463426157832
Q_Learning [142/300]: mean_loss=0.013160407543182373
Q_Learning [143/300]: mean_loss=0.028075278969481587
Q_Learning [144/300]: mean_loss=0.014512298861518502
Q_Learning [145/300]: mean_loss=0.06288888864219189
Q_Learning [146/300]: mean_loss=0.021489011589437723
Q_Learning [147/300]: mean_loss=0.016469066846184433
Q_Learning [148/300]: mean_loss=0.051110494416207075
Q_Learning [149/300]: mean_loss=0.09652089048177004
Q_Learning [150/300]: mean_loss=0.11653646174818277
Q_Learning [151/300]: mean_loss=0.022119836416095495
Q_Learning [152/300]: mean_loss=0.019971338333562016
Q_Learning [153/300]: mean_loss=0.05183590669184923
Q_Learning [154/300]: mean_loss=0.038561627734452486
Q_Learning [155/300]: mean_loss=0.013934488641098142
Q_Learning [156/300]: mean_loss=0.020498201483860612
Q_Learning [157/300]: mean_loss=0.14889876265078783
Q_Learning [158/300]: mean_loss=0.013430431310553104
Q_Learning [159/300]: mean_loss=0.018320608651265502
Q_Learning [160/300]: mean_loss=0.036105782724916935
Q_Learning [161/300]: mean_loss=0.033204405568540096
Q_Learning [162/300]: mean_loss=0.015688073355704546
Q_Learning [163/300]: mean_loss=0.04933762131258845
Q_Learning [164/300]: mean_loss=0.014591657323762774
Q_Learning [165/300]: mean_loss=0.010413410607725382
Q_Learning [166/300]: mean_loss=0.02153017884120345
Q_Learning [167/300]: mean_loss=0.03955838456749916
Q_Learning [168/300]: mean_loss=0.04579492565244436
Q_Learning [169/300]: mean_loss=0.05121366959065199
Q_Learning [170/300]: mean_loss=0.18074725568294525
Q_Learning [171/300]: mean_loss=0.020672898273915052
Q_Learning [172/300]: mean_loss=0.016595662687905133
Q_Learning [173/300]: mean_loss=0.02635489613749087
Q_Learning [174/300]: mean_loss=0.013711231877095997
Q_Learning [175/300]: mean_loss=0.012496649869717658
Q_Learning [176/300]: mean_loss=0.0062939790659584105
Q_Learning [177/300]: mean_loss=0.036527652060613036
Q_Learning [178/300]: mean_loss=0.03487574029713869
Q_Learning [179/300]: mean_loss=0.024289379362016916
Q_Learning [180/300]: mean_loss=0.03565683378838003
Q_Learning [181/300]: mean_loss=0.03773608151823282
Q_Learning [182/300]: mean_loss=0.021283756708726287
Q_Learning [183/300]: mean_loss=0.01409360091201961
Q_Learning [184/300]: mean_loss=0.008574437350034714
Q_Learning [185/300]: mean_loss=0.036075526382774115
Q_Learning [186/300]: mean_loss=0.0344013930298388
Q_Learning [187/300]: mean_loss=0.026932145236060023
Q_Learning [188/300]: mean_loss=0.025777998380362988
Q_Learning [189/300]: mean_loss=0.05469953361898661
Q_Learning [190/300]: mean_loss=0.023901014123111963
Q_Learning [191/300]: mean_loss=0.15187152661383152
Q_Learning [192/300]: mean_loss=0.021969560300931334
Q_Learning [193/300]: mean_loss=0.0242291996255517
Q_Learning [194/300]: mean_loss=0.021388118620961905
Q_Learning [195/300]: mean_loss=0.0234365938231349
Q_Learning [196/300]: mean_loss=0.014048067037947476
Q_Learning [197/300]: mean_loss=0.04716943670064211
Q_Learning [198/300]: mean_loss=0.015883205691352487
Q_Learning [199/300]: mean_loss=0.08217114489525557
Q_Learning [200/300]: mean_loss=0.018270117812789977
Q_Learning [201/300]: mean_loss=0.03445654036477208
Q_Learning [202/300]: mean_loss=0.019443725934252143
Q_Learning [203/300]: mean_loss=0.010627388255670667
Q_Learning [204/300]: mean_loss=0.01923739700578153
Q_Learning [205/300]: mean_loss=0.0710673313587904
Q_Learning [206/300]: mean_loss=0.018716043676249683
Q_Learning [207/300]: mean_loss=0.049809171352535486
Q_Learning [208/300]: mean_loss=0.045632341876626015
Q_Learning [209/300]: mean_loss=0.04798362636938691
Q_Learning [210/300]: mean_loss=0.02611483889631927
Q_Learning [211/300]: mean_loss=0.018470288021489978
Q_Learning [212/300]: mean_loss=0.09271610248833895
Q_Learning [213/300]: mean_loss=0.08247792162001133
Q_Learning [214/300]: mean_loss=0.021624871995300055
Q_Learning [215/300]: mean_loss=0.02961090300232172
Q_Learning [216/300]: mean_loss=0.054740657564252615
Q_Learning [217/300]: mean_loss=0.013616817421279848
Q_Learning [218/300]: mean_loss=0.008721047779545188
Q_Learning [219/300]: mean_loss=0.053996832109987736
Q_Learning [220/300]: mean_loss=0.012692464515566826
Q_Learning [221/300]: mean_loss=0.031368950149044394
Q_Learning [222/300]: mean_loss=0.023417797638103366
Q_Learning [223/300]: mean_loss=0.031245161779224873
Q_Learning [224/300]: mean_loss=0.006692674709483981
Q_Learning [225/300]: mean_loss=0.02119963150471449
Q_Learning [226/300]: mean_loss=0.018405392300337553
Q_Learning [227/300]: mean_loss=0.009404577082023025
Q_Learning [228/300]: mean_loss=0.037183700827881694
Q_Learning [229/300]: mean_loss=0.05196100287139416
Q_Learning [230/300]: mean_loss=0.01645104947965592
Q_Learning [231/300]: mean_loss=0.025091601070016623
Q_Learning [232/300]: mean_loss=0.020277239615097642
Q_Learning [233/300]: mean_loss=0.02735265577211976
Q_Learning [234/300]: mean_loss=0.025038850959390402
Q_Learning [235/300]: mean_loss=0.06882391963154078
Q_Learning [236/300]: mean_loss=0.04993595788255334
Q_Learning [237/300]: mean_loss=0.03349197958596051
Q_Learning [238/300]: mean_loss=0.09738982748240232
Q_Learning [239/300]: mean_loss=0.22969739511609077
Q_Learning [240/300]: mean_loss=0.07791665568947792
Q_Learning [241/300]: mean_loss=0.042448027059435844
Q_Learning [242/300]: mean_loss=0.03546882001683116
Q_Learning [243/300]: mean_loss=0.04043419286608696
Q_Learning [244/300]: mean_loss=0.041086018551141024
Q_Learning [245/300]: mean_loss=0.05826086364686489
Q_Learning [246/300]: mean_loss=0.026851337403059006
Q_Learning [247/300]: mean_loss=0.011403331998735666
Q_Learning [248/300]: mean_loss=0.05863264761865139
Q_Learning [249/300]: mean_loss=0.05208519147709012
Q_Learning [250/300]: mean_loss=0.015628131572157145
Q_Learning [251/300]: mean_loss=0.020789144560694695
Q_Learning [252/300]: mean_loss=0.015322895022109151
Q_Learning [253/300]: mean_loss=0.0130675412947312
Q_Learning [254/300]: mean_loss=0.02053826511837542
Q_Learning [255/300]: mean_loss=0.0455809086561203
Q_Learning [256/300]: mean_loss=0.019824121613055468
Q_Learning [257/300]: mean_loss=0.029654108453541994
Q_Learning [258/300]: mean_loss=0.01252309198025614
Q_Learning [259/300]: mean_loss=0.00988991349004209
Q_Learning [260/300]: mean_loss=0.019256274681538343
Q_Learning [261/300]: mean_loss=0.021794989705085754
Q_Learning [262/300]: mean_loss=0.03076478629373014
Q_Learning [263/300]: mean_loss=0.01656468107830733
Q_Learning [264/300]: mean_loss=0.04489274136722088
Q_Learning [265/300]: mean_loss=0.019230927107855678
Q_Learning [266/300]: mean_loss=0.011998748290352523
Q_Learning [267/300]: mean_loss=0.0136406309902668
Q_Learning [268/300]: mean_loss=0.008618883788585663
Q_Learning [269/300]: mean_loss=0.031586474273353815
Q_Learning [270/300]: mean_loss=0.01703294867184013
Q_Learning [271/300]: mean_loss=0.019151558401063085
Q_Learning [272/300]: mean_loss=0.007192717806901783
Q_Learning [273/300]: mean_loss=0.01441699406132102
Q_Learning [274/300]: mean_loss=0.029931313591077924
Q_Learning [275/300]: mean_loss=0.010413801181130111
Q_Learning [276/300]: mean_loss=0.018217624397948384
Q_Learning [277/300]: mean_loss=0.07867702934890985
Q_Learning [278/300]: mean_loss=0.025452018715441227
Q_Learning [279/300]: mean_loss=0.024804947432130575
Q_Learning [280/300]: mean_loss=0.03425543592311442
Q_Learning [281/300]: mean_loss=0.03581477305851877
Q_Learning [282/300]: mean_loss=0.024642087053507566
Q_Learning [283/300]: mean_loss=0.019410347566008568
Q_Learning [284/300]: mean_loss=0.033821118995547295
Q_Learning [285/300]: mean_loss=0.027920314809307456
Q_Learning [286/300]: mean_loss=0.00828343944158405
Q_Learning [287/300]: mean_loss=0.010976861929520965
Q_Learning [288/300]: mean_loss=0.014739537262357771
Q_Learning [289/300]: mean_loss=0.03626002324745059
Q_Learning [290/300]: mean_loss=0.008597983280196786
Q_Learning [291/300]: mean_loss=0.013560950988903642
Q_Learning [292/300]: mean_loss=0.006929626455530524
Q_Learning [293/300]: mean_loss=0.021183595759794116
Q_Learning [294/300]: mean_loss=0.031478629913181067
Q_Learning [295/300]: mean_loss=0.03454837156459689
Q_Learning [296/300]: mean_loss=0.038725089048966765
Q_Learning [297/300]: mean_loss=0.019843329908326268
Q_Learning [298/300]: mean_loss=0.030470542376860976
Q_Learning [299/300]: mean_loss=0.034540430177003145
Q_Learning [300/300]: mean_loss=0.04836683114990592
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.241205  -1.5824136]
[0, 2, 1, 2, 0, 1, 0, 2, 0, 2, 2, 0, 0, 0, 1, 1, 0, 2, 2, 1, 0, 1, 1, 1, 0, 2, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 0, 2, 0, 0, 2, 2, 1, 2, 0, 1, 1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 2, 1, 2, 0, 1, 1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 1, 0, 1, 2, 0, 0, 2, 2, 2, 1, 1, 0, 1, 1, 2, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 0, 1, 2, 1, 2, 2, 1, 1, 1, 0, 2, 1, 1, 2, 0, 2, 2, 2, 0, 2, 2, 1, 0, 0, 0, 2, 2, 2, 1, 2, 2, 1, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 0, 1, 2, 2, 2, 0, 2, 2, 1, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2, 2, 0, 1, 0, 0, 1, 1, 2, 0, 2, 0]
[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 1, 1, 2, 0, 0, 2, 2, 2, 1, 0, 1, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 2, 0, 2, 1, 0, 0, 3, 3, 0, 0, 1, 3, 2, 0, 0, 0, 3, 1, 2, 0, 1, 0, 2, 0, 2, 2, 0, 1, 2, 0, 1, 1, 1, 0, 0, 4, 3, 1, 1, 2, 0, 2, 0, 0, 3, 2, 0, 2, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 3, 2, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 3, 0, 1, 2, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 0, 3, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 2, 3, 0, 1, 2, 0, 0, 1, 0, 3, 1, 4, 2, 3, 1, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 3, 0, 2, 2, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 3, 2, 1, 2, 2, 0, 1, 0, 0, 2, 2, 3, 0, 2, 2, 0, 0, 2, 2, 0, 1, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 3, 2, 3, 1, 2, 2, 0, 0, 0, 2, 2, 1, 2, 1, 1, 0, 1, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0]
Centroids: [[-1.5404905, -2.189947], [-0.33682305, -1.7696563], [1.9924048, -0.24304189]]
Centroids: [[-0.4071991, -1.5898242], [-1.7928271, -2.510667], [1.8095374, -0.21336626], [3.3843148, -0.3624549], [-3.8275557, -3.2117171]]
Standard Derivations: [0.52305466, 0.49998274, 0.6211652]
Cluster Distances: [0.25189772, 2.8896103, 0.25189775, 1.6637852, 2.8896103, 1.6637853]
Minimal Cluster Distance: 0.2518977224826813
Contingency Matrix: 
[[34 58  0  0  2]
 [90  7  2  0  0]
 [10  0 75 22  0]]
[[34, 58, 0, 0, 2], [90, 7, 2, 0, 0], [10, 0, 75, 22, 0]]
[[34, 58, 0, 0, 2], [90, 7, 2, 0, 0], [10, 0, 75, 22, 0]]
[0, 1, 2, 3, 4]
[[-1, 58, 0, 0, 2], [-1, -1, -1, -1, -1], [-1, 0, 75, 22, 0]]
[[-1, 58, -1, 0, 2], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {1: 0, 2: 2, 0: 1}
New Contingency Matrix: 
[[58 34  0  0  2]
 [ 7 90  2  0  0]
 [ 0 10 75 22  0]]
New Clustered Label Sequence: [1, 0, 2, 3, 4]
Diagonal_Elements: [58, 90, 75], Sum: 223
All_Elements: [58, 34, 0, 0, 2, 7, 90, 2, 0, 0, 0, 10, 75, 22, 0], Sum: 300
Accuracy: 0.7433333333333333
