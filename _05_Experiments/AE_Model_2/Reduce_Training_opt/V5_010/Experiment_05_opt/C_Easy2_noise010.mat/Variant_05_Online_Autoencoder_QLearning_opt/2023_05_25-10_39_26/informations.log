Experiment_path: AE_Model_2/Reduce_Training_opt//V5_010/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy2_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy2_noise010.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_010/Experiment_05_opt/C_Easy2_noise010.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-10_39_26
Punishment_Coefficient: 0.9
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E34D2A2358>
Sampling rate: 24000.0
Raw: [-0.04397287 -0.05368168 -0.05753576 ... -0.17707654 -0.14968225
 -0.12084286]
Times: [   1077    1809    2216 ... 1439324 1439736 1439818]
Cluster: [1 2 3 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3520
First aligned Spike Frame: [-5.66507481e-02 -6.59320228e-02 -6.70701971e-02 -7.19520617e-02
 -7.89243788e-02 -8.44863120e-02 -9.23204981e-02 -9.75387283e-02
 -7.89589716e-02 -3.66949571e-02  2.34965171e-04 -2.60677777e-03
 -8.36059782e-02 -2.16751250e-01 -3.29544857e-01 -3.35165947e-01
 -2.03449552e-01  7.47840458e-02  4.22419255e-01  7.09409540e-01
  8.78002642e-01  9.55364309e-01  9.77809330e-01  9.55005143e-01
  8.85120577e-01  8.00574977e-01  7.20670596e-01  6.49598354e-01
  5.48520603e-01  4.27922886e-01  3.27637830e-01  2.50259973e-01
  1.79725440e-01  1.08182425e-01  5.15669298e-02  1.18971249e-02
 -1.33865595e-02 -3.45955406e-02 -6.81150537e-02 -1.12799097e-01
 -1.58924383e-01 -1.84417551e-01 -2.01640893e-01 -2.18864546e-01
 -2.16773696e-01 -2.09095391e-01 -1.81456244e-01]
Cluster 0, Occurrences: 1160
Cluster 1, Occurrences: 1146
Cluster 2, Occurrences: 1214
Number of Clusters: 3
Online_Training [1/10]: mean_loss=0.19527359120547771
Online_Training [2/10]: mean_loss=0.11446700990200043
Online_Training [3/10]: mean_loss=0.16366861388087273
Online_Training [4/10]: mean_loss=0.1733571607619524
Online_Training [5/10]: mean_loss=0.08714599534869194
Online_Training [6/10]: mean_loss=0.13379769399762154
Online_Training [7/10]: mean_loss=0.09787097945809364
Online_Training [8/10]: mean_loss=0.13407660089433193
Online_Training [9/10]: mean_loss=0.10947480797767639
Online_Training [10/10]: mean_loss=0.11561282724142075
Q_Learning [1/300]: mean_loss=0.19527359120547771
Q_Learning [2/300]: mean_loss=0.11446700990200043
Q_Learning [3/300]: mean_loss=0.16366861388087273
Q_Learning [4/300]: mean_loss=0.1733571607619524
Q_Learning [5/300]: mean_loss=0.08714599534869194
Q_Learning [6/300]: mean_loss=0.13379769399762154
Q_Learning [7/300]: mean_loss=0.09787097945809364
Q_Learning [8/300]: mean_loss=0.13407660089433193
Q_Learning [9/300]: mean_loss=0.10947480797767639
Q_Learning [10/300]: mean_loss=0.11561282724142075
Q_Learning [11/300]: mean_loss=0.06538240751251578
Q_Learning [12/300]: mean_loss=0.08640924096107483
Q_Learning [13/300]: mean_loss=0.07320245634764433
Q_Learning [14/300]: mean_loss=0.09504516236484051
Q_Learning [15/300]: mean_loss=0.08115658350288868
Q_Learning [16/300]: mean_loss=0.06505122128874063
Q_Learning [17/300]: mean_loss=0.11493020225316286
Q_Learning [18/300]: mean_loss=0.0509871244430542
Q_Learning [19/300]: mean_loss=0.06094505824148655
Q_Learning [20/300]: mean_loss=0.059403580613434315
Q_Learning [21/300]: mean_loss=0.05584857054054737
Q_Learning [22/300]: mean_loss=0.12371100578457117
Q_Learning [23/300]: mean_loss=0.15975791588425636
Q_Learning [24/300]: mean_loss=0.029981949366629124
Q_Learning [25/300]: mean_loss=0.04484987910836935
Q_Learning [26/300]: mean_loss=0.12507416121661663
Q_Learning [27/300]: mean_loss=0.07743923645466566
Q_Learning [28/300]: mean_loss=0.20391037873923779
Q_Learning [29/300]: mean_loss=0.03698872588574886
Q_Learning [30/300]: mean_loss=0.033091042190790176
Q_Learning [31/300]: mean_loss=0.09566742274910212
Q_Learning [32/300]: mean_loss=0.06903451401740313
Q_Learning [33/300]: mean_loss=0.019223765702918172
Q_Learning [34/300]: mean_loss=0.0879120733588934
Q_Learning [35/300]: mean_loss=0.036722722463309765
Q_Learning [36/300]: mean_loss=0.09677619952708483
Q_Learning [37/300]: mean_loss=0.030961033888161182
Q_Learning [38/300]: mean_loss=0.029225562466308475
Q_Learning [39/300]: mean_loss=0.02812477294355631
Q_Learning [40/300]: mean_loss=0.04040983645245433
Q_Learning [41/300]: mean_loss=0.10235720407217741
Q_Learning [42/300]: mean_loss=0.07441815827041864
Q_Learning [43/300]: mean_loss=0.02087478176690638
Q_Learning [44/300]: mean_loss=0.05016411887481809
Q_Learning [45/300]: mean_loss=0.019408483058214188
Q_Learning [46/300]: mean_loss=0.09579417388886213
Q_Learning [47/300]: mean_loss=0.029722961829975247
Q_Learning [48/300]: mean_loss=0.06998549588024616
Q_Learning [49/300]: mean_loss=0.015696620917879045
Q_Learning [50/300]: mean_loss=0.01902663055807352
Q_Learning [51/300]: mean_loss=0.024280880810692906
Q_Learning [52/300]: mean_loss=0.02939424803480506
Q_Learning [53/300]: mean_loss=0.019077429547905922
Q_Learning [54/300]: mean_loss=0.031719275284558535
Q_Learning [55/300]: mean_loss=0.07692536246031523
Q_Learning [56/300]: mean_loss=0.08759458549320698
Q_Learning [57/300]: mean_loss=0.012239724630489945
Q_Learning [58/300]: mean_loss=0.027468736516311765
Q_Learning [59/300]: mean_loss=0.1112309331074357
Q_Learning [60/300]: mean_loss=0.08849829435348511
Q_Learning [61/300]: mean_loss=0.02198309963569045
Q_Learning [62/300]: mean_loss=0.035596434492617846
Q_Learning [63/300]: mean_loss=0.0199763469863683
Q_Learning [64/300]: mean_loss=0.04074232466518879
Q_Learning [65/300]: mean_loss=0.016682534012943506
Q_Learning [66/300]: mean_loss=0.021244085393846035
Q_Learning [67/300]: mean_loss=0.025544110918417573
Q_Learning [68/300]: mean_loss=0.07252408936619759
Q_Learning [69/300]: mean_loss=0.02549512544646859
Q_Learning [70/300]: mean_loss=0.02089731115847826
Q_Learning [71/300]: mean_loss=0.034845079528167844
Q_Learning [72/300]: mean_loss=0.07144157961010933
Q_Learning [73/300]: mean_loss=0.024095118744298816
Q_Learning [74/300]: mean_loss=0.0726832440122962
Q_Learning [75/300]: mean_loss=0.021917622536420822
Q_Learning [76/300]: mean_loss=0.05306348716840148
Q_Learning [77/300]: mean_loss=0.07760914508253336
Q_Learning [78/300]: mean_loss=0.008981736726127565
Q_Learning [79/300]: mean_loss=0.005534355412237346
Q_Learning [80/300]: mean_loss=0.02930263220332563
Q_Learning [81/300]: mean_loss=0.023262592731043696
Q_Learning [82/300]: mean_loss=0.01468038116581738
Q_Learning [83/300]: mean_loss=0.029634277569130063
Q_Learning [84/300]: mean_loss=0.03893078677356243
Q_Learning [85/300]: mean_loss=0.018433382734656334
Q_Learning [86/300]: mean_loss=0.010425892774946988
Q_Learning [87/300]: mean_loss=0.12806371692568064
Q_Learning [88/300]: mean_loss=0.17346026748418808
Q_Learning [89/300]: mean_loss=0.028525551315397024
Q_Learning [90/300]: mean_loss=0.020407208940014243
Q_Learning [91/300]: mean_loss=0.01816408894956112
Q_Learning [92/300]: mean_loss=0.041559658478945494
Q_Learning [93/300]: mean_loss=0.016798792988993227
Q_Learning [94/300]: mean_loss=0.02100001717917621
Q_Learning [95/300]: mean_loss=0.09431785065680742
Q_Learning [96/300]: mean_loss=0.12210843991488218
Q_Learning [97/300]: mean_loss=0.015543868532404304
Q_Learning [98/300]: mean_loss=0.019635755103081465
Q_Learning [99/300]: mean_loss=0.07230424741283059
Q_Learning [100/300]: mean_loss=0.15730352140963078
Q_Learning [101/300]: mean_loss=0.03709954069927335
Q_Learning [102/300]: mean_loss=0.038866606540977955
Q_Learning [103/300]: mean_loss=0.023016556166112423
Q_Learning [104/300]: mean_loss=0.04158212011680007
Q_Learning [105/300]: mean_loss=0.03201673389412463
Q_Learning [106/300]: mean_loss=0.021745643112808466
Q_Learning [107/300]: mean_loss=0.024917750153690577
Q_Learning [108/300]: mean_loss=0.014390037627890706
Q_Learning [109/300]: mean_loss=0.021701748250052333
Q_Learning [110/300]: mean_loss=0.007478536979760975
Q_Learning [111/300]: mean_loss=0.06709624826908112
Q_Learning [112/300]: mean_loss=0.017456501140259206
Q_Learning [113/300]: mean_loss=0.02681725611910224
Q_Learning [114/300]: mean_loss=0.03276955243200064
Q_Learning [115/300]: mean_loss=0.02214046404697001
Q_Learning [116/300]: mean_loss=0.005813588679302484
Q_Learning [117/300]: mean_loss=0.017172526568174362
Q_Learning [118/300]: mean_loss=0.016483849147334695
Q_Learning [119/300]: mean_loss=0.011840079445391893
Q_Learning [120/300]: mean_loss=0.0295218366663903
Q_Learning [121/300]: mean_loss=0.023735111113637686
Q_Learning [122/300]: mean_loss=0.01800773316062987
Q_Learning [123/300]: mean_loss=0.007288116961717606
Q_Learning [124/300]: mean_loss=0.01874045771546662
Q_Learning [125/300]: mean_loss=0.010627082199789584
Q_Learning [126/300]: mean_loss=0.023455174872651696
Q_Learning [127/300]: mean_loss=0.017541951849125326
Q_Learning [128/300]: mean_loss=0.01394196052569896
Q_Learning [129/300]: mean_loss=0.003988998621935025
Q_Learning [130/300]: mean_loss=0.10846057534217834
Q_Learning [131/300]: mean_loss=0.12878432590514421
Q_Learning [132/300]: mean_loss=0.029935269383713603
Q_Learning [133/300]: mean_loss=0.0539262811653316
Q_Learning [134/300]: mean_loss=0.016819456359371543
Q_Learning [135/300]: mean_loss=0.014519502641633153
Q_Learning [136/300]: mean_loss=0.014498102362267673
Q_Learning [137/300]: mean_loss=0.052906802855432034
Q_Learning [138/300]: mean_loss=0.049249804113060236
Q_Learning [139/300]: mean_loss=0.02762623247690499
Q_Learning [140/300]: mean_loss=0.019717842806130648
Q_Learning [141/300]: mean_loss=0.016920364112593234
Q_Learning [142/300]: mean_loss=0.018650482641533017
Q_Learning [143/300]: mean_loss=0.017382829915732145
Q_Learning [144/300]: mean_loss=0.013665513368323445
Q_Learning [145/300]: mean_loss=0.07597412634640932
Q_Learning [146/300]: mean_loss=0.16891146823763847
Q_Learning [147/300]: mean_loss=0.038938724901527166
Q_Learning [148/300]: mean_loss=0.013516428763978183
Q_Learning [149/300]: mean_loss=0.005400515801738948
Q_Learning [150/300]: mean_loss=0.026397461304441094
Q_Learning [151/300]: mean_loss=0.008142908103764057
Q_Learning [152/300]: mean_loss=0.012779478216543794
Q_Learning [153/300]: mean_loss=0.01811222976539284
Q_Learning [154/300]: mean_loss=0.015189801692031324
Q_Learning [155/300]: mean_loss=0.01196044438984245
Q_Learning [156/300]: mean_loss=0.08338429592549801
Q_Learning [157/300]: mean_loss=0.05224109627306461
Q_Learning [158/300]: mean_loss=0.07551061641424894
Q_Learning [159/300]: mean_loss=0.02336372109130025
Q_Learning [160/300]: mean_loss=0.02753660688176751
Q_Learning [161/300]: mean_loss=0.009577556513249874
Q_Learning [162/300]: mean_loss=0.01177413621917367
Q_Learning [163/300]: mean_loss=0.011330417240969837
Q_Learning [164/300]: mean_loss=0.015432500396855175
Q_Learning [165/300]: mean_loss=0.009105528413783759
Q_Learning [166/300]: mean_loss=0.007038532581645995
Q_Learning [167/300]: mean_loss=0.004354350618086755
Q_Learning [168/300]: mean_loss=0.007245599990710616
Q_Learning [169/300]: mean_loss=0.010753262089565396
Q_Learning [170/300]: mean_loss=0.03618819359689951
Q_Learning [171/300]: mean_loss=0.02178517822176218
Q_Learning [172/300]: mean_loss=0.044738391414284706
Q_Learning [173/300]: mean_loss=0.012859390350058675
Q_Learning [174/300]: mean_loss=0.012842045514844358
Q_Learning [175/300]: mean_loss=0.010501592070795596
Q_Learning [176/300]: mean_loss=0.009080551972147077
Q_Learning [177/300]: mean_loss=0.08032247005030513
Q_Learning [178/300]: mean_loss=0.1316953981295228
Q_Learning [179/300]: mean_loss=0.03210668172687292
Q_Learning [180/300]: mean_loss=0.012768550426699221
Q_Learning [181/300]: mean_loss=0.006529655249323696
Q_Learning [182/300]: mean_loss=0.019091822439804673
Q_Learning [183/300]: mean_loss=0.008385872701182961
Q_Learning [184/300]: mean_loss=0.017489496734924614
Q_Learning [185/300]: mean_loss=0.028014904586598277
Q_Learning [186/300]: mean_loss=0.009657113929279149
Q_Learning [187/300]: mean_loss=0.02343054604716599
Q_Learning [188/300]: mean_loss=0.025772143620997667
Q_Learning [189/300]: mean_loss=0.01354282209649682
Q_Learning [190/300]: mean_loss=0.05926804430782795
Q_Learning [191/300]: mean_loss=0.07421013433486223
Q_Learning [192/300]: mean_loss=0.025509421480819583
Q_Learning [193/300]: mean_loss=0.019918741891160607
Q_Learning [194/300]: mean_loss=0.02078678086400032
Q_Learning [195/300]: mean_loss=0.010919114924035966
Q_Learning [196/300]: mean_loss=0.2063507791608572
Q_Learning [197/300]: mean_loss=0.08865915331989527
Q_Learning [198/300]: mean_loss=0.02603830909356475
Q_Learning [199/300]: mean_loss=0.019249822944402695
Q_Learning [200/300]: mean_loss=0.10563306417316198
Q_Learning [201/300]: mean_loss=0.1486168522387743
Q_Learning [202/300]: mean_loss=0.013343244441784918
Q_Learning [203/300]: mean_loss=0.04894294077530503
Q_Learning [204/300]: mean_loss=0.020102061796933413
Q_Learning [205/300]: mean_loss=0.00726067932555452
Q_Learning [206/300]: mean_loss=0.02355966717004776
Q_Learning [207/300]: mean_loss=0.02142420131713152
Q_Learning [208/300]: mean_loss=0.015242807916365564
Q_Learning [209/300]: mean_loss=0.008670070790685713
Q_Learning [210/300]: mean_loss=0.03444792749360204
Q_Learning [211/300]: mean_loss=0.009500957909040153
Q_Learning [212/300]: mean_loss=0.00790464848978445
Q_Learning [213/300]: mean_loss=0.007806235225871205
Q_Learning [214/300]: mean_loss=0.10708323400467634
Q_Learning [215/300]: mean_loss=0.06157493870705366
Q_Learning [216/300]: mean_loss=0.018369763740338385
Q_Learning [217/300]: mean_loss=0.005673205305356532
Q_Learning [218/300]: mean_loss=0.012152998126111925
Q_Learning [219/300]: mean_loss=0.012483519385568798
Q_Learning [220/300]: mean_loss=0.014306922210380435
Q_Learning [221/300]: mean_loss=0.010048470867332071
Q_Learning [222/300]: mean_loss=0.01123971096239984
Q_Learning [223/300]: mean_loss=0.003766574984183535
Q_Learning [224/300]: mean_loss=0.010962187079712749
Q_Learning [225/300]: mean_loss=0.006719941913615912
Q_Learning [226/300]: mean_loss=0.0038963120605330914
Q_Learning [227/300]: mean_loss=0.014007393270730972
Q_Learning [228/300]: mean_loss=0.01341367547865957
Q_Learning [229/300]: mean_loss=0.004015867714770138
Q_Learning [230/300]: mean_loss=0.005721864465158433
Q_Learning [231/300]: mean_loss=0.010363334440626204
Q_Learning [232/300]: mean_loss=0.00464971864130348
Q_Learning [233/300]: mean_loss=0.006832172046415508
Q_Learning [234/300]: mean_loss=0.0019369307556189597
Q_Learning [235/300]: mean_loss=0.016607524477876723
Q_Learning [236/300]: mean_loss=0.011590812122449279
Q_Learning [237/300]: mean_loss=0.012796013499610126
Q_Learning [238/300]: mean_loss=0.011569699388928711
Q_Learning [239/300]: mean_loss=0.09256675373762846
Q_Learning [240/300]: mean_loss=0.01608226483222097
Q_Learning [241/300]: mean_loss=0.010305756703019142
Q_Learning [242/300]: mean_loss=0.002217197441495955
Q_Learning [243/300]: mean_loss=0.015743045136332512
Q_Learning [244/300]: mean_loss=0.09139376599341631
Q_Learning [245/300]: mean_loss=0.12449627183377743
Q_Learning [246/300]: mean_loss=0.015006552101112902
Q_Learning [247/300]: mean_loss=0.013365112827159464
Q_Learning [248/300]: mean_loss=0.04093198059126735
Q_Learning [249/300]: mean_loss=0.009301267098635435
Q_Learning [250/300]: mean_loss=0.011497428291477263
Q_Learning [251/300]: mean_loss=0.008952567586675286
Q_Learning [252/300]: mean_loss=0.01059891830664128
Q_Learning [253/300]: mean_loss=0.005003575992304832
Q_Learning [254/300]: mean_loss=0.006727193249389529
Q_Learning [255/300]: mean_loss=0.007626688166055828
Q_Learning [256/300]: mean_loss=0.011947013670578599
Q_Learning [257/300]: mean_loss=0.01623186527285725
Q_Learning [258/300]: mean_loss=0.014305746648460627
Q_Learning [259/300]: mean_loss=0.008981807972304523
Q_Learning [260/300]: mean_loss=0.01008277281653136
Q_Learning [261/300]: mean_loss=0.00924563582520932
Q_Learning [262/300]: mean_loss=0.016007429803721607
Q_Learning [263/300]: mean_loss=0.01706796756479889
Q_Learning [264/300]: mean_loss=0.003547512402292341
Q_Learning [265/300]: mean_loss=0.008953356184065342
Q_Learning [266/300]: mean_loss=0.0055251867161132395
Q_Learning [267/300]: mean_loss=0.0029363718058448285
Q_Learning [268/300]: mean_loss=0.00990072323475033
Q_Learning [269/300]: mean_loss=0.016553194844163954
Q_Learning [270/300]: mean_loss=0.009226424561347812
Q_Learning [271/300]: mean_loss=0.013256255420856178
Q_Learning [272/300]: mean_loss=0.011028860055375844
Q_Learning [273/300]: mean_loss=0.012641743407584727
Q_Learning [274/300]: mean_loss=0.013848639442585409
Q_Learning [275/300]: mean_loss=0.02689333283342421
Q_Learning [276/300]: mean_loss=0.02657445869408548
Q_Learning [277/300]: mean_loss=0.007975336804520339
Q_Learning [278/300]: mean_loss=0.009803424356505275
Q_Learning [279/300]: mean_loss=0.015458677662536502
Q_Learning [280/300]: mean_loss=0.005360855488106608
Q_Learning [281/300]: mean_loss=0.03211008571088314
Q_Learning [282/300]: mean_loss=0.005866173771210015
Q_Learning [283/300]: mean_loss=0.005734911886975169
Q_Learning [284/300]: mean_loss=0.025171104818582535
Q_Learning [285/300]: mean_loss=0.006849106342997402
Q_Learning [286/300]: mean_loss=0.009601005585864186
Q_Learning [287/300]: mean_loss=0.01858339924365282
Q_Learning [288/300]: mean_loss=0.00816149276215583
Q_Learning [289/300]: mean_loss=0.012153359246440232
Q_Learning [290/300]: mean_loss=0.006466192833613604
Q_Learning [291/300]: mean_loss=0.006939967395737767
Q_Learning [292/300]: mean_loss=0.007847502070944756
Q_Learning [293/300]: mean_loss=0.013363511301577091
Q_Learning [294/300]: mean_loss=0.00647701375419274
Q_Learning [295/300]: mean_loss=0.007149820157792419
Q_Learning [296/300]: mean_loss=0.012630427721887827
Q_Learning [297/300]: mean_loss=0.016104959417134523
Q_Learning [298/300]: mean_loss=0.030581357656046748
Q_Learning [299/300]: mean_loss=0.013822041684761643
Q_Learning [300/300]: mean_loss=0.007342672033701092
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-0.8299028 -1.0197086]
[2, 2, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 2, 1, 0, 2, 2, 0, 2, 1, 2, 0, 1, 1, 0, 2, 2, 0, 0, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 2, 1, 2, 1, 2, 2, 0, 0, 2, 1, 1, 2, 2, 2, 0, 0, 1, 1, 0, 2, 1, 2, 1, 1, 0, 2, 2, 1, 2, 0, 1, 0, 1, 0, 2, 2, 2, 0, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0, 2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 0, 1, 1, 0, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 0, 2, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 1, 1, 1, 1, 0, 2, 1, 0, 1, 1, 2, 1, 2, 2, 2, 0, 2, 1, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 2, 0, 0, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2, 2, 1, 1, 2, 2, 1, 0, 1, 2, 0, 1, 1, 0, 0, 2, 0, 0, 1, 2, 1, 0, 2, 0, 0, 2, 2, 1, 1, 0, 1, 0, 0, 1, 1, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 2, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 0, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 0, 3, 1, 1, 3, 2, 0, 0, 3, 2, 0, 2, 1, 2, 2, 3, 1, 2, 0, 1, 2, 2, 2, 1, 4, 0, 0, 1, 2, 1, 2, 1, 3, 0, 2, 2, 3, 5, 0, 0, 1, 0, 1, 2, 2, 2, 1, 1, 0, 1, 0, 0, 3, 2, 0, 1, 1, 2, 1, 1, 2, 5, 5, 2, 1, 5, 5, 3, 0, 0, 0, 0, 0, 5, 3, 0, 1, 0, 2, 1, 0, 0, 4, 5, 0, 5, 5, 5, 2, 5, 0, 5, 0, 1, 5, 1, 5, 0, 5, 0, 1, 1, 1, 5, 1, 1, 5, 0, 0, 0, 0, 1, 5, 0, 1, 0, 0, 5, 0, 5, 5, 5, 1, 5, 0, 1, 0, 2, 0, 1, 0, 0, 0, 6, 0, 1, 0, 1, 0, 5, 0, 0, 5, 0, 0, 0, 5, 1, 1, 0, 1, 5, 0, 5, 0, 5, 0, 1, 1, 5, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 5, 5, 0, 1, 0, 5, 1, 0, 0, 1, 1, 5, 1, 1, 0, 5, 0, 1, 5, 1, 0, 5, 5, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 5, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 5, 0, 1, 1, 0, 1, 5, 0, 5, 1, 5, 1, 5, 0, 1, 1, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 1, 0]
Centroids: [[-2.060195, -2.1190174], [-0.655331, -1.7060283], [-0.53663516, 1.0934418]]
Centroids: [[-0.6325751, -1.486833], [-2.0208151, -2.0583503], [-0.4694199, 0.81956863], [-2.604106, -3.5341306], [-2.8948236, -4.9786177], [-0.51697534, 1.8574789], [0.012501419, -3.2494245]]
Standard Derivations: [0.47268564, 0.4857334, 0.5897457]
Cluster Distances: [0.50589067, 2.493005, 0.50589067, 1.726506, 2.493005, 1.7265061]
Minimal Cluster Distance: 0.5058906674385071
Contingency Matrix: 
[[16 75  0  6  2  0  0]
 [92 14  0  3  0  0  1]
 [15  0 27  0  0 49  0]]
[[16, 75, 0, 6, 2, 0, 0], [92, 14, 0, 3, 0, 0, 1], [15, 0, 27, 0, 0, 49, 0]]
[[16, 75, 0, 6, 2, 0, 0], [92, 14, 0, 3, 0, 0, 1], [15, 0, 27, 0, 0, 49, 0]]
[0, 1, 2, 3, 4, 5, 6]
[[-1, 75, 0, 6, 2, 0, 0], [-1, -1, -1, -1, -1, -1, -1], [-1, 0, 27, 0, 0, 49, 0]]
[[-1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1], [-1, -1, 27, 0, 0, 49, 0]]
[[-1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {1: 0, 0: 1, 2: 5}
New Contingency Matrix: 
[[75 16  0  0  6  2  0]
 [14 92  0  0  3  0  1]
 [ 0 15 49 27  0  0  0]]
New Clustered Label Sequence: [1, 0, 5, 2, 3, 4, 6]
Diagonal_Elements: [75, 92, 49], Sum: 216
All_Elements: [75, 16, 0, 0, 6, 2, 0, 14, 92, 0, 0, 3, 0, 1, 0, 15, 49, 27, 0, 0, 0], Sum: 300
Accuracy: 0.72
