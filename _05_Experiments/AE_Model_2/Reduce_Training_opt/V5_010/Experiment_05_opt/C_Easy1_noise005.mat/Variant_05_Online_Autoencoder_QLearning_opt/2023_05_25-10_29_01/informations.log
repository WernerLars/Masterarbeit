Experiment_path: AE_Model_2/Reduce_Training_opt//V5_010/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise005.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise005.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_010/Experiment_05_opt/C_Easy1_noise005.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-10_29_01
Punishment_Coefficient: 1.0
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E349770978>
Sampling rate: 24000.0
Raw: [-0.05265172 -0.03124187 -0.00282162 ...  0.01798155  0.01678863
  0.0119459 ]
Times: [    283     469    1484 ... 1438285 1438773 1439067]
Cluster: [2 1 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3514
First aligned Spike Frame: [-2.11647214e-02 -2.00144278e-02 -2.48166304e-02 -2.70972753e-02
 -1.11241704e-02  1.86904987e-02  3.99716833e-02  4.40400999e-02
  4.38833221e-02  5.06364129e-02  6.02243042e-02  3.59622148e-02
 -9.64451652e-02 -3.71359573e-01 -6.92987060e-01 -8.74449953e-01
 -7.13363902e-01 -1.84182190e-01  4.08997970e-01  7.26119515e-01
  7.19977210e-01  5.61000789e-01  4.04007238e-01  2.96025242e-01
  2.22861462e-01  1.69209408e-01  1.33269005e-01  1.11481721e-01
  9.67043158e-02  8.35988040e-02  6.87571423e-02  5.74871826e-02
  5.26722178e-02  4.53956038e-02  3.31356602e-02  2.21250606e-02
  1.35048482e-02 -4.41592673e-04 -2.31921908e-02 -4.69576347e-02
 -6.03503288e-02 -6.27551095e-02 -6.19812766e-02 -6.37499251e-02
 -6.42747873e-02 -5.93586264e-02 -5.06150772e-02]
Cluster 0, Occurrences: 1165
Cluster 1, Occurrences: 1157
Cluster 2, Occurrences: 1192
Number of Clusters: 3
Online_Training [1/10]: mean_loss=0.12203092779964209
Online_Training [2/10]: mean_loss=0.0931862648576498
Online_Training [3/10]: mean_loss=0.31805216521024704
Online_Training [4/10]: mean_loss=0.297378022223711
Online_Training [5/10]: mean_loss=0.34334487840533257
Online_Training [6/10]: mean_loss=0.1183405527845025
Online_Training [7/10]: mean_loss=0.08711644075810909
Online_Training [8/10]: mean_loss=0.329997181892395
Online_Training [9/10]: mean_loss=0.09089411050081253
Online_Training [10/10]: mean_loss=0.08787775412201881
Q_Learning [1/300]: mean_loss=0.12203092779964209
Q_Learning [2/300]: mean_loss=0.0931862648576498
Q_Learning [3/300]: mean_loss=0.31805216521024704
Q_Learning [4/300]: mean_loss=0.297378022223711
Q_Learning [5/300]: mean_loss=0.34334487840533257
Q_Learning [6/300]: mean_loss=0.1183405527845025
Q_Learning [7/300]: mean_loss=0.08711644075810909
Q_Learning [8/300]: mean_loss=0.329997181892395
Q_Learning [9/300]: mean_loss=0.09089411050081253
Q_Learning [10/300]: mean_loss=0.08787775412201881
Q_Learning [11/300]: mean_loss=0.07912520878016949
Q_Learning [12/300]: mean_loss=0.07762576546519995
Q_Learning [13/300]: mean_loss=0.0622808369807899
Q_Learning [14/300]: mean_loss=0.13065756671130657
Q_Learning [15/300]: mean_loss=0.26100360229611397
Q_Learning [16/300]: mean_loss=0.12043617386370897
Q_Learning [17/300]: mean_loss=0.2286447063088417
Q_Learning [18/300]: mean_loss=0.1264693532139063
Q_Learning [19/300]: mean_loss=0.07507376559078693
Q_Learning [20/300]: mean_loss=0.05810216115787625
Q_Learning [21/300]: mean_loss=0.08189860358834267
Q_Learning [22/300]: mean_loss=0.1031818613409996
Q_Learning [23/300]: mean_loss=0.0755965025164187
Q_Learning [24/300]: mean_loss=0.027393420226871967
Q_Learning [25/300]: mean_loss=0.09952157828956842
Q_Learning [26/300]: mean_loss=0.07595342304557562
Q_Learning [27/300]: mean_loss=0.06672346545383334
Q_Learning [28/300]: mean_loss=0.0490247905254364
Q_Learning [29/300]: mean_loss=0.048591998871415854
Q_Learning [30/300]: mean_loss=0.015938426135107875
Q_Learning [31/300]: mean_loss=0.06965378625318408
Q_Learning [32/300]: mean_loss=0.012540768249891698
Q_Learning [33/300]: mean_loss=0.008130267320666462
Q_Learning [34/300]: mean_loss=0.04670473374426365
Q_Learning [35/300]: mean_loss=0.006543702271301299
Q_Learning [36/300]: mean_loss=0.09134126733988523
Q_Learning [37/300]: mean_loss=0.021507256431505084
Q_Learning [38/300]: mean_loss=0.026870762929320335
Q_Learning [39/300]: mean_loss=0.0063714138232171535
Q_Learning [40/300]: mean_loss=0.014921598485670984
Q_Learning [41/300]: mean_loss=0.01079490757547319
Q_Learning [42/300]: mean_loss=0.012578738620504737
Q_Learning [43/300]: mean_loss=0.012706472771242261
Q_Learning [44/300]: mean_loss=0.0028880933386972174
Q_Learning [45/300]: mean_loss=0.01758958538994193
Q_Learning [46/300]: mean_loss=0.007737124047707766
Q_Learning [47/300]: mean_loss=0.07674396829679608
Q_Learning [48/300]: mean_loss=0.07023495528846979
Q_Learning [49/300]: mean_loss=0.007919178431620821
Q_Learning [50/300]: mean_loss=0.0209225055295974
Q_Learning [51/300]: mean_loss=0.007683334319153801
Q_Learning [52/300]: mean_loss=0.0030892193026375026
Q_Learning [53/300]: mean_loss=0.06921925395727158
Q_Learning [54/300]: mean_loss=0.2015184499323368
Q_Learning [55/300]: mean_loss=0.03383738617412746
Q_Learning [56/300]: mean_loss=0.025460192700847983
Q_Learning [57/300]: mean_loss=0.020284791011363268
Q_Learning [58/300]: mean_loss=0.007941041025333107
Q_Learning [59/300]: mean_loss=0.11421282775700092
Q_Learning [60/300]: mean_loss=0.015304755070246756
Q_Learning [61/300]: mean_loss=0.015768938581459224
Q_Learning [62/300]: mean_loss=0.011719986447133124
Q_Learning [63/300]: mean_loss=0.0018264470272697508
Q_Learning [64/300]: mean_loss=0.0017685739730950445
Q_Learning [65/300]: mean_loss=0.011415616958402097
Q_Learning [66/300]: mean_loss=0.003790350310737267
Q_Learning [67/300]: mean_loss=0.0025938300241250545
Q_Learning [68/300]: mean_loss=0.07997014047577977
Q_Learning [69/300]: mean_loss=0.005387542594689876
Q_Learning [70/300]: mean_loss=0.0068326121545396745
Q_Learning [71/300]: mean_loss=0.0019226814183639362
Q_Learning [72/300]: mean_loss=0.007583118393085897
Q_Learning [73/300]: mean_loss=0.002294769656145945
Q_Learning [74/300]: mean_loss=0.0021047447517048568
Q_Learning [75/300]: mean_loss=0.006164929596707225
Q_Learning [76/300]: mean_loss=0.000674370807246305
Q_Learning [77/300]: mean_loss=0.0017623755556996912
Q_Learning [78/300]: mean_loss=0.0059223073767498136
Q_Learning [79/300]: mean_loss=0.02058813744224608
Q_Learning [80/300]: mean_loss=0.009670862113125622
Q_Learning [81/300]: mean_loss=0.004684887069743127
Q_Learning [82/300]: mean_loss=0.008785084704868495
Q_Learning [83/300]: mean_loss=0.006259198184125125
Q_Learning [84/300]: mean_loss=0.0022994791361270472
Q_Learning [85/300]: mean_loss=0.004838208551518619
Q_Learning [86/300]: mean_loss=0.010193676280323416
Q_Learning [87/300]: mean_loss=0.003285627521108836
Q_Learning [88/300]: mean_loss=0.013416142552159727
Q_Learning [89/300]: mean_loss=0.003752735850866884
Q_Learning [90/300]: mean_loss=0.006647070404142141
Q_Learning [91/300]: mean_loss=0.0056008470710366964
Q_Learning [92/300]: mean_loss=0.005320414202287793
Q_Learning [93/300]: mean_loss=0.0014485380379483104
Q_Learning [94/300]: mean_loss=0.003323216369608417
Q_Learning [95/300]: mean_loss=0.001634342726902105
Q_Learning [96/300]: mean_loss=0.005306373466737568
Q_Learning [97/300]: mean_loss=0.005369988328311592
Q_Learning [98/300]: mean_loss=0.0028850214439444244
Q_Learning [99/300]: mean_loss=0.00393942859955132
Q_Learning [100/300]: mean_loss=0.0008798450726317242
Q_Learning [101/300]: mean_loss=0.0029317894659470767
Q_Learning [102/300]: mean_loss=0.0033757791097741574
Q_Learning [103/300]: mean_loss=0.008463712118100375
Q_Learning [104/300]: mean_loss=0.004253039252944291
Q_Learning [105/300]: mean_loss=0.019237485015764832
Q_Learning [106/300]: mean_loss=0.016753169475123286
Q_Learning [107/300]: mean_loss=0.005273351867799647
Q_Learning [108/300]: mean_loss=0.009485549759119749
Q_Learning [109/300]: mean_loss=0.010705141176003963
Q_Learning [110/300]: mean_loss=0.00683646573452279
Q_Learning [111/300]: mean_loss=0.0048384110268671066
Q_Learning [112/300]: mean_loss=0.0038492492749355733
Q_Learning [113/300]: mean_loss=0.006306650000624359
Q_Learning [114/300]: mean_loss=0.0030338859069161117
Q_Learning [115/300]: mean_loss=0.012874627485871315
Q_Learning [116/300]: mean_loss=0.0018075316183967516
Q_Learning [117/300]: mean_loss=0.003615634923335165
Q_Learning [118/300]: mean_loss=0.002869071759050712
Q_Learning [119/300]: mean_loss=0.003200821054633707
Q_Learning [120/300]: mean_loss=0.006353940698318183
Q_Learning [121/300]: mean_loss=0.0020865823898930103
Q_Learning [122/300]: mean_loss=0.00941713631618768
Q_Learning [123/300]: mean_loss=0.0005165708062122576
Q_Learning [124/300]: mean_loss=0.009375505120260641
Q_Learning [125/300]: mean_loss=0.0040382247971137986
Q_Learning [126/300]: mean_loss=0.0019983357342425734
Q_Learning [127/300]: mean_loss=0.005924240511376411
Q_Learning [128/300]: mean_loss=0.00942013319581747
Q_Learning [129/300]: mean_loss=0.0038585983274970204
Q_Learning [130/300]: mean_loss=0.004480482719372958
Q_Learning [131/300]: mean_loss=0.003806793421972543
Q_Learning [132/300]: mean_loss=0.010431374539621174
Q_Learning [133/300]: mean_loss=0.0038689736102242023
Q_Learning [134/300]: mean_loss=0.01543070503976196
Q_Learning [135/300]: mean_loss=0.007429225865053013
Q_Learning [136/300]: mean_loss=0.001400843873852864
Q_Learning [137/300]: mean_loss=0.005691270693205297
Q_Learning [138/300]: mean_loss=0.0016625457647023723
Q_Learning [139/300]: mean_loss=0.001800680547603406
Q_Learning [140/300]: mean_loss=0.0016551887965761125
Q_Learning [141/300]: mean_loss=0.0022657115187030286
Q_Learning [142/300]: mean_loss=0.005571186076849699
Q_Learning [143/300]: mean_loss=0.06558709568344057
Q_Learning [144/300]: mean_loss=0.01989270420745015
Q_Learning [145/300]: mean_loss=0.06539452588185668
Q_Learning [146/300]: mean_loss=0.21638780646026134
Q_Learning [147/300]: mean_loss=0.020259996177628636
Q_Learning [148/300]: mean_loss=0.010850432212464511
Q_Learning [149/300]: mean_loss=0.004788517020642757
Q_Learning [150/300]: mean_loss=0.0856213141232729
Q_Learning [151/300]: mean_loss=0.2837612181901932
Q_Learning [152/300]: mean_loss=0.014596345485188067
Q_Learning [153/300]: mean_loss=0.012506394064985216
Q_Learning [154/300]: mean_loss=0.006519272166769952
Q_Learning [155/300]: mean_loss=0.040842898190021515
Q_Learning [156/300]: mean_loss=0.02216844609938562
Q_Learning [157/300]: mean_loss=0.005725189461372793
Q_Learning [158/300]: mean_loss=0.021348209818825126
Q_Learning [159/300]: mean_loss=0.006050312687875703
Q_Learning [160/300]: mean_loss=0.013716899673454463
Q_Learning [161/300]: mean_loss=0.0042391135066282
Q_Learning [162/300]: mean_loss=0.05173009913414717
Q_Learning [163/300]: mean_loss=0.14234752021729946
Q_Learning [164/300]: mean_loss=0.01568647613748908
Q_Learning [165/300]: mean_loss=0.009312510723248124
Q_Learning [166/300]: mean_loss=0.025287078460678458
Q_Learning [167/300]: mean_loss=0.00825723831076175
Q_Learning [168/300]: mean_loss=0.007017112919129431
Q_Learning [169/300]: mean_loss=0.0017409864958608523
Q_Learning [170/300]: mean_loss=0.004224601056193933
Q_Learning [171/300]: mean_loss=0.012251887237653136
Q_Learning [172/300]: mean_loss=0.02090983302332461
Q_Learning [173/300]: mean_loss=0.02633884083479643
Q_Learning [174/300]: mean_loss=0.008236137160565704
Q_Learning [175/300]: mean_loss=0.017936445539817214
Q_Learning [176/300]: mean_loss=0.0015452778025064617
Q_Learning [177/300]: mean_loss=0.006525565142510459
Q_Learning [178/300]: mean_loss=0.004786849371157587
Q_Learning [179/300]: mean_loss=0.007598863099701703
Q_Learning [180/300]: mean_loss=0.008565956028178334
Q_Learning [181/300]: mean_loss=0.003398795408429578
Q_Learning [182/300]: mean_loss=0.002598536972072907
Q_Learning [183/300]: mean_loss=0.004764192213770002
Q_Learning [184/300]: mean_loss=0.0036002065462525934
Q_Learning [185/300]: mean_loss=0.004514160216785967
Q_Learning [186/300]: mean_loss=0.002921095583587885
Q_Learning [187/300]: mean_loss=0.0018637716566445306
Q_Learning [188/300]: mean_loss=0.08752061426639557
Q_Learning [189/300]: mean_loss=0.10405462514609098
Q_Learning [190/300]: mean_loss=0.01561755279544741
Q_Learning [191/300]: mean_loss=0.06012956704944372
Q_Learning [192/300]: mean_loss=0.025026530027389526
Q_Learning [193/300]: mean_loss=0.01271014113444835
Q_Learning [194/300]: mean_loss=0.02263563498854637
Q_Learning [195/300]: mean_loss=0.009625272417906672
Q_Learning [196/300]: mean_loss=0.005698388442397118
Q_Learning [197/300]: mean_loss=0.0051528908661566675
Q_Learning [198/300]: mean_loss=0.007615544425789267
Q_Learning [199/300]: mean_loss=0.010382022242993116
Q_Learning [200/300]: mean_loss=0.0068147198762744665
Q_Learning [201/300]: mean_loss=0.00901526096276939
Q_Learning [202/300]: mean_loss=0.0058457006816752255
Q_Learning [203/300]: mean_loss=0.0017013284959830344
Q_Learning [204/300]: mean_loss=0.005387103476095945
Q_Learning [205/300]: mean_loss=0.002703225356526673
Q_Learning [206/300]: mean_loss=0.007356073881965131
Q_Learning [207/300]: mean_loss=0.005690714227966964
Q_Learning [208/300]: mean_loss=0.005054375185864046
Q_Learning [209/300]: mean_loss=0.007793817261699587
Q_Learning [210/300]: mean_loss=0.006673894065897912
Q_Learning [211/300]: mean_loss=0.004511180246481672
Q_Learning [212/300]: mean_loss=0.003650967526482418
Q_Learning [213/300]: mean_loss=0.004867560521233827
Q_Learning [214/300]: mean_loss=0.0028028701490256935
Q_Learning [215/300]: mean_loss=0.004019675892777741
Q_Learning [216/300]: mean_loss=0.0014196583942975849
Q_Learning [217/300]: mean_loss=0.0035922270617447793
Q_Learning [218/300]: mean_loss=0.003219928272301331
Q_Learning [219/300]: mean_loss=0.0066756505984812975
Q_Learning [220/300]: mean_loss=0.00723431701771915
Q_Learning [221/300]: mean_loss=0.0050072294543497264
Q_Learning [222/300]: mean_loss=0.002277702064020559
Q_Learning [223/300]: mean_loss=0.0006810334598412737
Q_Learning [224/300]: mean_loss=0.0037265804421622306
Q_Learning [225/300]: mean_loss=0.0009690688020782545
Q_Learning [226/300]: mean_loss=0.0014672653342131525
Q_Learning [227/300]: mean_loss=0.009675916167907417
Q_Learning [228/300]: mean_loss=0.006480827578343451
Q_Learning [229/300]: mean_loss=0.004300316039007157
Q_Learning [230/300]: mean_loss=0.003184377943398431
Q_Learning [231/300]: mean_loss=0.0018310522136744112
Q_Learning [232/300]: mean_loss=0.003672758030006662
Q_Learning [233/300]: mean_loss=0.00417269766330719
Q_Learning [234/300]: mean_loss=0.005136691266670823
Q_Learning [235/300]: mean_loss=0.005953774554654956
Q_Learning [236/300]: mean_loss=0.010189655877184123
Q_Learning [237/300]: mean_loss=0.0022357595153152943
Q_Learning [238/300]: mean_loss=0.004381875129183754
Q_Learning [239/300]: mean_loss=0.0019839970773318782
Q_Learning [240/300]: mean_loss=0.007361670781392604
Q_Learning [241/300]: mean_loss=0.007925592188257724
Q_Learning [242/300]: mean_loss=0.004229033540468663
Q_Learning [243/300]: mean_loss=0.006932191434316337
Q_Learning [244/300]: mean_loss=0.002701925885048695
Q_Learning [245/300]: mean_loss=0.002032397373113781
Q_Learning [246/300]: mean_loss=0.0042529993515927345
Q_Learning [247/300]: mean_loss=0.001705976261291653
Q_Learning [248/300]: mean_loss=0.016113142832182348
Q_Learning [249/300]: mean_loss=0.01110516581684351
Q_Learning [250/300]: mean_loss=0.004169903142610565
Q_Learning [251/300]: mean_loss=0.011198149179108441
Q_Learning [252/300]: mean_loss=0.005566150997765362
Q_Learning [253/300]: mean_loss=0.0019610641174949706
Q_Learning [254/300]: mean_loss=0.003877342795021832
Q_Learning [255/300]: mean_loss=0.0038295231643132865
Q_Learning [256/300]: mean_loss=0.06662419438362122
Q_Learning [257/300]: mean_loss=0.08023672644048929
Q_Learning [258/300]: mean_loss=0.007322171586565673
Q_Learning [259/300]: mean_loss=0.0015967696672305465
Q_Learning [260/300]: mean_loss=0.002749120816588402
Q_Learning [261/300]: mean_loss=0.021832307800650597
Q_Learning [262/300]: mean_loss=0.014443418651353568
Q_Learning [263/300]: mean_loss=0.004702536418335512
Q_Learning [264/300]: mean_loss=0.00894522073213011
Q_Learning [265/300]: mean_loss=0.0012106324575142935
Q_Learning [266/300]: mean_loss=0.002748727274592966
Q_Learning [267/300]: mean_loss=0.0027440586854936555
Q_Learning [268/300]: mean_loss=0.009959949529729784
Q_Learning [269/300]: mean_loss=0.007955175067763776
Q_Learning [270/300]: mean_loss=0.0028820766310673207
Q_Learning [271/300]: mean_loss=0.0076492580701597035
Q_Learning [272/300]: mean_loss=0.002538663800805807
Q_Learning [273/300]: mean_loss=0.00429621065268293
Q_Learning [274/300]: mean_loss=0.0019597744685597718
Q_Learning [275/300]: mean_loss=0.20986565202474594
Q_Learning [276/300]: mean_loss=0.052997519727796316
Q_Learning [277/300]: mean_loss=0.005077735346276313
Q_Learning [278/300]: mean_loss=0.02076619490981102
Q_Learning [279/300]: mean_loss=0.00426145238452591
Q_Learning [280/300]: mean_loss=0.00830988201778382
Q_Learning [281/300]: mean_loss=0.004190547566395253
Q_Learning [282/300]: mean_loss=0.008937050588428974
Q_Learning [283/300]: mean_loss=0.011931190616451204
Q_Learning [284/300]: mean_loss=0.005935954686719924
Q_Learning [285/300]: mean_loss=0.01436538330744952
Q_Learning [286/300]: mean_loss=0.007789253315422684
Q_Learning [287/300]: mean_loss=0.0031260397227015346
Q_Learning [288/300]: mean_loss=0.0019993060996057466
Q_Learning [289/300]: mean_loss=0.005363457778003067
Q_Learning [290/300]: mean_loss=0.0030419074173551053
Q_Learning [291/300]: mean_loss=0.0060566983302123845
Q_Learning [292/300]: mean_loss=0.001809805806260556
Q_Learning [293/300]: mean_loss=0.004548508382868022
Q_Learning [294/300]: mean_loss=0.0020088089077034965
Q_Learning [295/300]: mean_loss=0.0025022312183864415
Q_Learning [296/300]: mean_loss=0.1933486144989729
Q_Learning [297/300]: mean_loss=0.1016862653195858
Q_Learning [298/300]: mean_loss=0.01487797952722758
Q_Learning [299/300]: mean_loss=0.014766920474357903
Q_Learning [300/300]: mean_loss=0.007801751664374024
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-0.08435661  0.289659  ]
[0, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 0, 0, 0, 0, 2, 2, 1, 2, 2, 1, 2, 0, 1, 0, 1, 0, 0, 2, 0, 2, 1, 0, 1, 0, 2, 0, 2, 2, 2, 0, 0, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 1, 1, 0, 0, 1, 0, 2, 2, 1, 1, 0, 0, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2, 0, 2, 2, 0, 1, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 0, 0, 1, 2, 2, 1, 0, 1, 0, 1, 1, 2, 1, 2, 2, 0, 1, 2, 2, 0, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 0, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 0, 1, 2, 2, 0, 0, 2, 1, 0, 0, 1, 2, 2, 1, 0, 1, 0, 2, 0, 2, 2, 0, 2, 1, 2, 0, 1, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 1, 2, 1, 2, 1, 0, 2, 2, 2, 0, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 0, 2, 2, 1, 0, 2, 2, 2, 0, 1, 2, 0, 2, 0, 2, 0, 0, 2, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 2, 1, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 2, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 3, 2, 2, 3, 2, 4, 3, 4, 3, 4, 4, 2, 4, 2, 3, 4, 3, 4, 2, 4, 2, 2, 2, 0, 4, 2, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 5, 2, 3, 3, 3, 2, 3, 3, 2, 2, 3, 4, 3, 3, 4, 4, 3, 4, 2, 2, 3, 3, 4, 4, 2, 4, 4, 2, 2, 4, 3, 2, 2, 2, 4, 2, 2, 0, 0, 2, 4, 2, 2, 2, 2, 4, 3, 2, 2, 3, 3, 3, 2, 4, 4, 3, 2, 2, 3, 4, 3, 4, 3, 3, 2, 3, 2, 2, 4, 3, 2, 2, 4, 2, 3, 6, 6, 2, 3, 3, 3, 4, 2, 0, 4, 3, 3, 2, 2, 3, 4, 2, 4, 3, 2, 0, 4, 3, 4, 3, 3, 3, 3, 1, 0, 0, 3, 4, 3, 2, 2, 4, 4, 2, 3, 4, 4, 3, 2, 2, 3, 4, 3, 7, 2, 4, 2, 2, 4, 2, 3, 2, 4, 3, 4, 4, 4, 2, 4, 1, 1, 4, 1, 1, 4, 4, 3, 2, 3, 2, 3, 4, 2, 1, 1, 4, 3, 4, 2, 4, 4, 3, 3, 4, 3, 4, 4, 3, 1, 4, 4, 4, 3, 2, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 4, 1, 1, 3, 4, 1, 1, 1, 4, 3, 1, 4, 1, 4, 1, 4, 4, 1, 3, 3, 3, 1, 5, 1, 1, 3, 3, 3, 3, 1, 3, 2, 4, 4, 2, 2, 4, 4, 4, 2, 1, 4, 4, 4, 4, 1, 4, 4, 4, 4, 3, 3, 4, 1, 4, 3, 1, 1]
Centroids: [[-0.9700051, 1.5589015], [0.32417405, -2.0247583], [1.9301816, 3.2597616]]
Centroids: [[-0.21878605, -0.06284939], [2.2499995, 2.907367], [1.817149, 3.574174], [0.32863963, -2.1349251], [-0.9569998, 1.6986445], [-2.5304956, 0.90063655], [3.3324738, -1.3418134], [-0.3502649, 4.756559]]
Standard Derivations: [0.5964987, 0.49008238, 0.61181104]
Cluster Distances: [2.7236052, 2.1538339, 2.723605, 4.421277, 2.1538336, 4.421277]
Minimal Cluster Distance: 2.1538336277008057
Contingency Matrix: 
[[12  0  0  0 79  2  0  1]
 [ 8  0  0 83  1  0  1  0]
 [ 2 36 72  0  2  0  1  0]]
[[12, 0, 0, 0, 79, 2, 0, 1], [8, 0, 0, 83, 1, 0, 1, 0], [2, 36, 72, 0, 2, 0, 1, 0]]
[[12, 0, 0, 0, 79, 2, 0, 1], [8, 0, 0, 83, 1, 0, 1, 0], [2, 36, 72, 0, 2, 0, 1, 0]]
[0, 1, 2, 3, 4, 5, 6, 7]
[[12, 0, 0, -1, 79, 2, 0, 1], [-1, -1, -1, -1, -1, -1, -1, -1], [2, 36, 72, -1, 2, 0, 1, 0]]
[[-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [2, 36, 72, -1, -1, 0, 1, 0]]
[[-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {1: 3, 0: 4, 2: 2}
New Contingency Matrix: 
[[79  0  0 12  0  2  0  1]
 [ 1 83  0  8  0  0  1  0]
 [ 2  0 72  2 36  0  1  0]]
New Clustered Label Sequence: [4, 3, 2, 0, 1, 5, 6, 7]
Diagonal_Elements: [79, 83, 72], Sum: 234
All_Elements: [79, 0, 0, 12, 0, 2, 0, 1, 1, 83, 0, 8, 0, 0, 1, 0, 2, 0, 72, 2, 36, 0, 1, 0], Sum: 300
Accuracy: 0.78
