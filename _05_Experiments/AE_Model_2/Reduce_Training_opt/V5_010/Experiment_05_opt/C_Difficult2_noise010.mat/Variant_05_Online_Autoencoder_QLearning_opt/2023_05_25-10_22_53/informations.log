Experiment_path: AE_Model_2/Reduce_Training_opt//V5_010/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult2_noise010.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult2_noise010.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_010/Experiment_05_opt/C_Difficult2_noise010.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-10_22_53
Punishment_Coefficient: 0.6
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001E34979AF98>
Sampling rate: 24000.0
Raw: [ 0.15602285  0.13816666  0.12280393 ... -0.08081559 -0.08529616
 -0.09321123]
Times: [    182     667     748 ... 1438018 1438700 1439563]
Cluster: [1 3 3 ... 1 2 3]
Number of different clusters:  3
Number of Spikes: 3462
First aligned Spike Frame: [ 0.0569593   0.06304523  0.0540705   0.04226901  0.04435466  0.07367561
  0.11842591  0.15581396  0.18051202  0.20464622  0.25110595  0.34905547
  0.52973433  0.78604807  1.00019855  1.02993402  0.87276972  0.64136808
  0.42542707  0.24213728  0.08732396 -0.0251061  -0.08440505 -0.1076534
 -0.12386236 -0.14599821 -0.16968468 -0.19109174 -0.20831529 -0.21879359
 -0.21563414 -0.19606358 -0.16928275 -0.14859233 -0.13954347 -0.13618571
 -0.12902379 -0.12127763 -0.12365015 -0.13615822 -0.14611472 -0.13936073
 -0.11885552 -0.10582878 -0.11163038 -0.12511067 -0.12700369]
Cluster 0, Occurrences: 1187
Cluster 1, Occurrences: 1136
Cluster 2, Occurrences: 1139
Number of Clusters: 3
Online_Training [1/10]: mean_loss=0.11722156964242458
Online_Training [2/10]: mean_loss=0.11699849925935268
Online_Training [3/10]: mean_loss=0.10471382830291986
Online_Training [4/10]: mean_loss=0.12288398295640945
Online_Training [5/10]: mean_loss=0.14371967688202858
Online_Training [6/10]: mean_loss=0.17981652542948723
Online_Training [7/10]: mean_loss=0.16735242865979671
Online_Training [8/10]: mean_loss=0.12671642191708088
Online_Training [9/10]: mean_loss=0.0925007201731205
Online_Training [10/10]: mean_loss=0.10432971175760031
Q_Learning [1/300]: mean_loss=0.11722156964242458
Q_Learning [2/300]: mean_loss=0.11699849925935268
Q_Learning [3/300]: mean_loss=0.10471382830291986
Q_Learning [4/300]: mean_loss=0.12288398295640945
Q_Learning [5/300]: mean_loss=0.14371967688202858
Q_Learning [6/300]: mean_loss=0.17981652542948723
Q_Learning [7/300]: mean_loss=0.16735242865979671
Q_Learning [8/300]: mean_loss=0.12671642191708088
Q_Learning [9/300]: mean_loss=0.0925007201731205
Q_Learning [10/300]: mean_loss=0.10432971175760031
Q_Learning [11/300]: mean_loss=0.03536963229998946
Q_Learning [12/300]: mean_loss=0.06283683562651277
Q_Learning [13/300]: mean_loss=0.03444395214319229
Q_Learning [14/300]: mean_loss=0.02936534327454865
Q_Learning [15/300]: mean_loss=0.04048964008688927
Q_Learning [16/300]: mean_loss=0.037943147122859955
Q_Learning [17/300]: mean_loss=0.10190457943826914
Q_Learning [18/300]: mean_loss=0.04602328268811107
Q_Learning [19/300]: mean_loss=0.1519092544913292
Q_Learning [20/300]: mean_loss=0.07584355864673853
Q_Learning [21/300]: mean_loss=0.06752020306885242
Q_Learning [22/300]: mean_loss=0.05265439534559846
Q_Learning [23/300]: mean_loss=0.08817483857274055
Q_Learning [24/300]: mean_loss=0.04760352848097682
Q_Learning [25/300]: mean_loss=0.09576492570340633
Q_Learning [26/300]: mean_loss=0.0833059186115861
Q_Learning [27/300]: mean_loss=0.0735719557851553
Q_Learning [28/300]: mean_loss=0.04346939641982317
Q_Learning [29/300]: mean_loss=0.07067489251494408
Q_Learning [30/300]: mean_loss=0.07749759405851364
Q_Learning [31/300]: mean_loss=0.09092468861490488
Q_Learning [32/300]: mean_loss=0.07747476547956467
Q_Learning [33/300]: mean_loss=0.024439156986773014
Q_Learning [34/300]: mean_loss=0.020308365812525153
Q_Learning [35/300]: mean_loss=0.026845893589779735
Q_Learning [36/300]: mean_loss=0.09475414641201496
Q_Learning [37/300]: mean_loss=0.06895906012505293
Q_Learning [38/300]: mean_loss=0.071961160749197
Q_Learning [39/300]: mean_loss=0.01971877017058432
Q_Learning [40/300]: mean_loss=0.07303245086222887
Q_Learning [41/300]: mean_loss=0.03338199481368065
Q_Learning [42/300]: mean_loss=0.06299551390111446
Q_Learning [43/300]: mean_loss=0.027177121257409453
Q_Learning [44/300]: mean_loss=0.016195404226891696
Q_Learning [45/300]: mean_loss=0.013274322380311787
Q_Learning [46/300]: mean_loss=0.0933961346745491
Q_Learning [47/300]: mean_loss=0.01121176965534687
Q_Learning [48/300]: mean_loss=0.008123491774313152
Q_Learning [49/300]: mean_loss=0.010203945101238787
Q_Learning [50/300]: mean_loss=0.047611086163669825
Q_Learning [51/300]: mean_loss=0.08067153673619032
Q_Learning [52/300]: mean_loss=0.008694525749888271
Q_Learning [53/300]: mean_loss=0.029562306124716997
Q_Learning [54/300]: mean_loss=0.007278412871528417
Q_Learning [55/300]: mean_loss=0.05299413623288274
Q_Learning [56/300]: mean_loss=0.07853266689926386
Q_Learning [57/300]: mean_loss=0.022344274446368217
Q_Learning [58/300]: mean_loss=0.08011057507246733
Q_Learning [59/300]: mean_loss=0.04096960090100765
Q_Learning [60/300]: mean_loss=0.03424601280130446
Q_Learning [61/300]: mean_loss=0.06132074072957039
Q_Learning [62/300]: mean_loss=0.044185373932123184
Q_Learning [63/300]: mean_loss=0.02981736813671887
Q_Learning [64/300]: mean_loss=0.08654615469276905
Q_Learning [65/300]: mean_loss=0.13306544534862041
Q_Learning [66/300]: mean_loss=0.029563805554062128
Q_Learning [67/300]: mean_loss=0.11713354289531708
Q_Learning [68/300]: mean_loss=0.10961729381233454
Q_Learning [69/300]: mean_loss=0.0243050295393914
Q_Learning [70/300]: mean_loss=0.017164025572128594
Q_Learning [71/300]: mean_loss=0.018562266137450933
Q_Learning [72/300]: mean_loss=0.023408632026985288
Q_Learning [73/300]: mean_loss=0.021967162610962987
Q_Learning [74/300]: mean_loss=0.021171083208173513
Q_Learning [75/300]: mean_loss=0.13315331563353539
Q_Learning [76/300]: mean_loss=0.12281114235520363
Q_Learning [77/300]: mean_loss=0.03523086244240403
Q_Learning [78/300]: mean_loss=0.00820322916842997
Q_Learning [79/300]: mean_loss=0.020318592665717006
Q_Learning [80/300]: mean_loss=0.007135819585528225
Q_Learning [81/300]: mean_loss=0.011543985223397613
Q_Learning [82/300]: mean_loss=0.035964726470410824
Q_Learning [83/300]: mean_loss=0.012862410861998796
Q_Learning [84/300]: mean_loss=0.054103761445730925
Q_Learning [85/300]: mean_loss=0.01177781296428293
Q_Learning [86/300]: mean_loss=0.005028924730140716
Q_Learning [87/300]: mean_loss=0.05091803567484021
Q_Learning [88/300]: mean_loss=0.02231546863913536
Q_Learning [89/300]: mean_loss=0.01832326699513942
Q_Learning [90/300]: mean_loss=0.009993927320465446
Q_Learning [91/300]: mean_loss=0.007316842034924775
Q_Learning [92/300]: mean_loss=0.004961073165759444
Q_Learning [93/300]: mean_loss=0.013432212406769395
Q_Learning [94/300]: mean_loss=0.012036321451887488
Q_Learning [95/300]: mean_loss=0.0825457014143467
Q_Learning [96/300]: mean_loss=0.03959034150466323
Q_Learning [97/300]: mean_loss=0.010072324774228036
Q_Learning [98/300]: mean_loss=0.019525728188455105
Q_Learning [99/300]: mean_loss=0.018588809994980693
Q_Learning [100/300]: mean_loss=0.026455561630427837
Q_Learning [101/300]: mean_loss=0.01537565269973129
Q_Learning [102/300]: mean_loss=0.007685850316192955
Q_Learning [103/300]: mean_loss=0.028570764465257525
Q_Learning [104/300]: mean_loss=0.03419617121107876
Q_Learning [105/300]: mean_loss=0.03706944826990366
Q_Learning [106/300]: mean_loss=0.025682761217467487
Q_Learning [107/300]: mean_loss=0.02287559164687991
Q_Learning [108/300]: mean_loss=0.009056559880264103
Q_Learning [109/300]: mean_loss=0.015220734174363315
Q_Learning [110/300]: mean_loss=0.0191265206085518
Q_Learning [111/300]: mean_loss=0.01914560270961374
Q_Learning [112/300]: mean_loss=0.00968033669050783
Q_Learning [113/300]: mean_loss=0.007872741436585784
Q_Learning [114/300]: mean_loss=0.009064575308002532
Q_Learning [115/300]: mean_loss=0.005790185765363276
Q_Learning [116/300]: mean_loss=0.0034481135371606797
Q_Learning [117/300]: mean_loss=0.013282448169775307
Q_Learning [118/300]: mean_loss=0.04553188430145383
Q_Learning [119/300]: mean_loss=0.01579440280329436
Q_Learning [120/300]: mean_loss=0.014819426811300218
Q_Learning [121/300]: mean_loss=0.015299714053981006
Q_Learning [122/300]: mean_loss=0.005248092522379011
Q_Learning [123/300]: mean_loss=0.027419218560680747
Q_Learning [124/300]: mean_loss=0.015045766136609018
Q_Learning [125/300]: mean_loss=0.007359019014984369
Q_Learning [126/300]: mean_loss=0.015398191288113594
Q_Learning [127/300]: mean_loss=0.01954052154906094
Q_Learning [128/300]: mean_loss=0.012169407098554075
Q_Learning [129/300]: mean_loss=0.008272710372693837
Q_Learning [130/300]: mean_loss=0.010967099689878523
Q_Learning [131/300]: mean_loss=0.0199167801765725
Q_Learning [132/300]: mean_loss=0.016392419347539544
Q_Learning [133/300]: mean_loss=0.09999249130487442
Q_Learning [134/300]: mean_loss=0.03036757535301149
Q_Learning [135/300]: mean_loss=0.02693841024301946
Q_Learning [136/300]: mean_loss=0.03217420889995992
Q_Learning [137/300]: mean_loss=0.04704276379197836
Q_Learning [138/300]: mean_loss=0.014351351070217788
Q_Learning [139/300]: mean_loss=0.0073536637355573475
Q_Learning [140/300]: mean_loss=0.018596966518089175
Q_Learning [141/300]: mean_loss=0.01300631184130907
Q_Learning [142/300]: mean_loss=0.00640169793041423
Q_Learning [143/300]: mean_loss=0.005724095390178263
Q_Learning [144/300]: mean_loss=0.025985841872170568
Q_Learning [145/300]: mean_loss=0.006730775989126414
Q_Learning [146/300]: mean_loss=0.009564472595229745
Q_Learning [147/300]: mean_loss=0.01275551866274327
Q_Learning [148/300]: mean_loss=0.02439113543368876
Q_Learning [149/300]: mean_loss=0.011555882054381073
Q_Learning [150/300]: mean_loss=0.014466968947090209
Q_Learning [151/300]: mean_loss=0.004214870888972655
Q_Learning [152/300]: mean_loss=0.009916272014379501
Q_Learning [153/300]: mean_loss=0.026345015969127417
Q_Learning [154/300]: mean_loss=0.054682453628629446
Q_Learning [155/300]: mean_loss=0.007277042837813497
Q_Learning [156/300]: mean_loss=0.02289861300960183
Q_Learning [157/300]: mean_loss=0.0039535980322398245
Q_Learning [158/300]: mean_loss=0.010545144556090236
Q_Learning [159/300]: mean_loss=0.002095424148137681
Q_Learning [160/300]: mean_loss=0.012087395996786654
Q_Learning [161/300]: mean_loss=0.0045759815839119256
Q_Learning [162/300]: mean_loss=0.006723249680362642
Q_Learning [163/300]: mean_loss=0.010896905674599111
Q_Learning [164/300]: mean_loss=0.0060425918200053275
Q_Learning [165/300]: mean_loss=0.018922632560133934
Q_Learning [166/300]: mean_loss=0.17780493386089802
Q_Learning [167/300]: mean_loss=0.10490525513887405
Q_Learning [168/300]: mean_loss=0.006131100351922214
Q_Learning [169/300]: mean_loss=0.011404969380237162
Q_Learning [170/300]: mean_loss=0.06508786510676146
Q_Learning [171/300]: mean_loss=0.06475098570808768
Q_Learning [172/300]: mean_loss=0.016260116710327566
Q_Learning [173/300]: mean_loss=0.04153050621971488
Q_Learning [174/300]: mean_loss=0.008163742371834815
Q_Learning [175/300]: mean_loss=0.00878635555272922
Q_Learning [176/300]: mean_loss=0.011437034700065851
Q_Learning [177/300]: mean_loss=0.009897193522192538
Q_Learning [178/300]: mean_loss=0.09960694331675768
Q_Learning [179/300]: mean_loss=0.026122115785256028
Q_Learning [180/300]: mean_loss=0.01285580697003752
Q_Learning [181/300]: mean_loss=0.02229622146114707
Q_Learning [182/300]: mean_loss=0.008486814040225
Q_Learning [183/300]: mean_loss=0.01308254397008568
Q_Learning [184/300]: mean_loss=0.007809595263097435
Q_Learning [185/300]: mean_loss=0.02378364442847669
Q_Learning [186/300]: mean_loss=0.004597309336531907
Q_Learning [187/300]: mean_loss=0.004463239834876731
Q_Learning [188/300]: mean_loss=0.012298841262236238
Q_Learning [189/300]: mean_loss=0.018057724926620722
Q_Learning [190/300]: mean_loss=0.014097113395109773
Q_Learning [191/300]: mean_loss=0.03219007235020399
Q_Learning [192/300]: mean_loss=0.06690456392243505
Q_Learning [193/300]: mean_loss=0.059872528072446585
Q_Learning [194/300]: mean_loss=0.0708975251764059
Q_Learning [195/300]: mean_loss=0.030349808977916837
Q_Learning [196/300]: mean_loss=0.01559554401319474
Q_Learning [197/300]: mean_loss=0.010656269267201424
Q_Learning [198/300]: mean_loss=0.012789433123543859
Q_Learning [199/300]: mean_loss=0.02036871504969895
Q_Learning [200/300]: mean_loss=0.011856982950121164
Q_Learning [201/300]: mean_loss=0.0194870897103101
Q_Learning [202/300]: mean_loss=0.021701027173548937
Q_Learning [203/300]: mean_loss=0.050032536033540964
Q_Learning [204/300]: mean_loss=0.043465795926749706
Q_Learning [205/300]: mean_loss=0.01658441184554249
Q_Learning [206/300]: mean_loss=0.01082156179472804
Q_Learning [207/300]: mean_loss=0.025892570847645402
Q_Learning [208/300]: mean_loss=0.014523505931720138
Q_Learning [209/300]: mean_loss=0.009516019898001105
Q_Learning [210/300]: mean_loss=0.00553829938871786
Q_Learning [211/300]: mean_loss=0.015212501515634358
Q_Learning [212/300]: mean_loss=0.01632692920975387
Q_Learning [213/300]: mean_loss=0.014093235251493752
Q_Learning [214/300]: mean_loss=0.004087676090421155
Q_Learning [215/300]: mean_loss=0.015112199122086167
Q_Learning [216/300]: mean_loss=0.012059418833814561
Q_Learning [217/300]: mean_loss=0.020416580606251955
Q_Learning [218/300]: mean_loss=0.02020650962367654
Q_Learning [219/300]: mean_loss=0.009651106665842235
Q_Learning [220/300]: mean_loss=0.009773456607945263
Q_Learning [221/300]: mean_loss=0.07565762102603912
Q_Learning [222/300]: mean_loss=0.013399106566794217
Q_Learning [223/300]: mean_loss=0.011108225327916443
Q_Learning [224/300]: mean_loss=0.010278046713210642
Q_Learning [225/300]: mean_loss=0.00764376693405211
Q_Learning [226/300]: mean_loss=0.008338143641594797
Q_Learning [227/300]: mean_loss=0.022214704426005483
Q_Learning [228/300]: mean_loss=0.018035772838629782
Q_Learning [229/300]: mean_loss=0.016906372387893498
Q_Learning [230/300]: mean_loss=0.0089960036566481
Q_Learning [231/300]: mean_loss=0.013050504378043115
Q_Learning [232/300]: mean_loss=0.01851774868555367
Q_Learning [233/300]: mean_loss=0.013846999616362154
Q_Learning [234/300]: mean_loss=0.0119202914647758
Q_Learning [235/300]: mean_loss=0.008247346500866115
Q_Learning [236/300]: mean_loss=0.019101319136098027
Q_Learning [237/300]: mean_loss=0.007608831860125065
Q_Learning [238/300]: mean_loss=0.016116537852212787
Q_Learning [239/300]: mean_loss=0.007503217610064894
Q_Learning [240/300]: mean_loss=0.008827494108118117
Q_Learning [241/300]: mean_loss=0.0066663300967775285
Q_Learning [242/300]: mean_loss=0.008078742655925453
Q_Learning [243/300]: mean_loss=0.00559961301041767
Q_Learning [244/300]: mean_loss=0.008192915003746748
Q_Learning [245/300]: mean_loss=0.0108364662155509
Q_Learning [246/300]: mean_loss=0.005382910720072687
Q_Learning [247/300]: mean_loss=0.01532201049849391
Q_Learning [248/300]: mean_loss=0.10099036246538162
Q_Learning [249/300]: mean_loss=0.16426121816039085
Q_Learning [250/300]: mean_loss=0.016779329860582948
Q_Learning [251/300]: mean_loss=0.012055465835146606
Q_Learning [252/300]: mean_loss=0.04261089954525232
Q_Learning [253/300]: mean_loss=0.0063539729453623295
Q_Learning [254/300]: mean_loss=0.014101384789682925
Q_Learning [255/300]: mean_loss=0.01382364600431174
Q_Learning [256/300]: mean_loss=0.03951974352821708
Q_Learning [257/300]: mean_loss=0.05838640732690692
Q_Learning [258/300]: mean_loss=0.03234655666165054
Q_Learning [259/300]: mean_loss=0.013621924095787108
Q_Learning [260/300]: mean_loss=0.010508474195376039
Q_Learning [261/300]: mean_loss=0.027305014431476593
Q_Learning [262/300]: mean_loss=0.026438258588314056
Q_Learning [263/300]: mean_loss=0.01920052175410092
Q_Learning [264/300]: mean_loss=0.006608953815884888
Q_Learning [265/300]: mean_loss=0.026844118954613805
Q_Learning [266/300]: mean_loss=0.020930834813043475
Q_Learning [267/300]: mean_loss=0.0030516548722516745
Q_Learning [268/300]: mean_loss=0.03179419809021056
Q_Learning [269/300]: mean_loss=0.02232709201052785
Q_Learning [270/300]: mean_loss=0.0061068624490872025
Q_Learning [271/300]: mean_loss=0.010821215226314962
Q_Learning [272/300]: mean_loss=0.01258770830463618
Q_Learning [273/300]: mean_loss=0.008801545249298215
Q_Learning [274/300]: mean_loss=0.0034493357234168798
Q_Learning [275/300]: mean_loss=0.010400082916021347
Q_Learning [276/300]: mean_loss=0.00827796250814572
Q_Learning [277/300]: mean_loss=0.0078288828372024
Q_Learning [278/300]: mean_loss=0.010588184231892228
Q_Learning [279/300]: mean_loss=0.002908760739956051
Q_Learning [280/300]: mean_loss=0.006199101568199694
Q_Learning [281/300]: mean_loss=0.004056781472172588
Q_Learning [282/300]: mean_loss=0.08359710033982992
Q_Learning [283/300]: mean_loss=0.08584569953382015
Q_Learning [284/300]: mean_loss=0.021127786953002214
Q_Learning [285/300]: mean_loss=0.014751641545444727
Q_Learning [286/300]: mean_loss=0.014603487448766828
Q_Learning [287/300]: mean_loss=0.006398983648978174
Q_Learning [288/300]: mean_loss=0.010894747916609049
Q_Learning [289/300]: mean_loss=0.019386404426768422
Q_Learning [290/300]: mean_loss=0.00819764711195603
Q_Learning [291/300]: mean_loss=0.008127419394440949
Q_Learning [292/300]: mean_loss=0.008325951814185828
Q_Learning [293/300]: mean_loss=0.008794442750513554
Q_Learning [294/300]: mean_loss=0.009974624263122678
Q_Learning [295/300]: mean_loss=0.30226605758070946
Q_Learning [296/300]: mean_loss=0.08074318338185549
Q_Learning [297/300]: mean_loss=0.04677760507911444
Q_Learning [298/300]: mean_loss=0.017662366619333625
Q_Learning [299/300]: mean_loss=0.012075431877747178
Q_Learning [300/300]: mean_loss=0.03458285122178495
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.5182168 -0.5949039]
[2, 0, 2, 2, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 2, 2, 1, 0, 1, 2, 0, 0, 0, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 2, 2, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 2, 0, 2, 2, 0, 1, 0, 1, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 0, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 1, 1, 1, 1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 2, 2, 1, 2, 0, 2, 0, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 1, 2, 2, 2, 1, 2, 1, 0, 1, 2, 2, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1, 2, 2, 1, 2, 1, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 0, 1, 2, 2, 0, 0, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1, 0, 2, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2, 1, 2, 0, 1, 1, 1, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 2, 0, 1, 3, 1, 2, 2, 2, 0, 0, 3, 4, 1, 2, 0, 0, 3, 0, 3, 2, 3, 2, 3, 0, 0, 2, 0, 2, 2, 2, 0, 2, 0, 1, 0, 3, 3, 3, 3, 3, 0, 3, 2, 0, 3, 3, 2, 0, 3, 2, 2, 2, 2, 2, 0, 0, 3, 2, 2, 0, 2, 3, 0, 2, 0, 3, 0, 2, 2, 5, 6, 0, 3, 3, 0, 7, 3, 1, 7, 7, 2, 0, 0, 0, 7, 3, 0, 7, 3, 6, 1, 3, 2, 3, 3, 3, 2, 3, 3, 0, 2, 6, 7, 1, 2, 2, 8, 6, 3, 2, 3, 3, 0, 0, 1, 0, 0, 2, 2, 7, 0, 2, 7, 3, 7, 3, 2, 0, 0, 8, 0, 7, 2, 0, 3, 0, 3, 3, 2, 2, 0, 0, 3, 0, 3, 2, 3, 2, 0, 7, 0, 2, 1, 1, 3, 2, 7, 7, 3, 3, 3, 3, 2, 3, 2, 7, 3, 2, 2, 2, 0, 0, 1, 7, 2, 0, 3, 3, 7, 7, 3, 3, 7, 6, 4, 9, 0, 3, 1, 7, 0, 3, 7, 6, 3, 2, 7, 0, 2, 2, 2, 3, 2, 3, 7, 2, 7, 3, 2, 3, 3, 2, 7, 3, 3, 3, 3, 3, 2, 0, 2, 2, 2, 3, 7, 3, 3, 2, 3, 6, 3, 2, 6, 6, 3, 7, 2, 6, 3, 7, 2, 7, 3, 2, 2, 2, 3]
Centroids: [[-1.2451366, -1.475547], [-2.19524, 1.6835699], [-1.8832235, -0.73576534]]
Centroids: [[-1.5681609, -0.8198], [-1.8442618, 0.8456212], [-2.2918587, 2.0644724], [-1.1841136, -1.6120279], [-4.044264, 1.2392019], [-3.5481966, 2.658295], [-1.5900071, -2.3768158], [-2.2780828, -0.38344488], [-3.8974516, 4.53075], [-0.1087772, -2.1104772]]
Standard Derivations: [0.33545983, 0.6739981, 0.48458576]
Cluster Distances: [2.289438, 0.15690464, 2.2894382, 1.2807887, 0.15690464, 1.2807887]
Minimal Cluster Distance: 0.15690463781356812
Contingency Matrix: 
[[28  0  0 70  0  0  7  1  0  1]
 [12 14 69  0  0  1  0  0  2  0]
 [57  3  0  3  2  0  3 27  0  0]]
[[28, 0, 0, 70, 0, 0, 7, 1, 0, 1], [12, 14, 69, 0, 0, 1, 0, 0, 2, 0], [57, 3, 0, 3, 2, 0, 3, 27, 0, 0]]
[[28, 0, 0, 70, 0, 0, 7, 1, 0, 1], [12, 14, 69, 0, 0, 1, 0, 0, 2, 0], [57, 3, 0, 3, 2, 0, 3, 27, 0, 0]]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [12, 14, 69, -1, 0, 1, 0, 0, 2, 0], [57, 3, 0, -1, 2, 0, 3, 27, 0, 0]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [57, 3, -1, -1, 2, 0, 3, 27, 0, 0]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {0: 3, 1: 2, 2: 0}
New Contingency Matrix: 
[[70  0 28  0  0  0  7  1  0  1]
 [ 0 69 12 14  0  1  0  0  2  0]
 [ 3  0 57  3  2  0  3 27  0  0]]
New Clustered Label Sequence: [3, 2, 0, 1, 4, 5, 6, 7, 8, 9]
Diagonal_Elements: [70, 69, 57], Sum: 196
All_Elements: [70, 0, 28, 0, 0, 0, 7, 1, 0, 1, 0, 69, 12, 14, 0, 1, 0, 0, 2, 0, 3, 0, 57, 3, 2, 0, 3, 27, 0, 0], Sum: 300
Accuracy: 0.6533333333333333
