Experiment_path: AE_Model_2/Reduce_Training_opt//V5_200/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise015.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_200/Experiment_05_opt/C_Difficult1_noise015.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-10_20_59
Punishment_Coefficient: 0.7
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002253AB1EE10>
Sampling rate: 24000.0
Raw: [ 0.04887081  0.02693095 -0.0154249  ... -0.09301659 -0.11629005
 -0.14613101]
Times: [    340     491     641 ... 1439047 1439065 1439816]
Cluster: [1 1 1 ... 3 2 2]
Number of different clusters:  3
Number of Spikes: 3472
First aligned Spike Frame: [ 0.12751554  0.12305882  0.10482977  0.09479529  0.10214978  0.11675932
  0.11777927  0.09307299  0.04670706 -0.00574343 -0.06143573 -0.14637617
 -0.20942665 -0.00208103  0.52241508  0.81651544  0.46446121 -0.19226425
 -0.60927882 -0.6713583  -0.57871227 -0.49011309 -0.4269388  -0.3668903
 -0.30523219 -0.24747124 -0.19738203 -0.15189972 -0.10449507 -0.05533325
 -0.01452429  0.01008816  0.02570853  0.04365027  0.06334113  0.07980397
  0.08484457  0.07688513  0.06142919  0.04320028  0.02240626  0.00477291
 -0.00393242 -0.00135684  0.00575182  0.0026944  -0.01541647]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1172
Cluster 2, Occurrences: 1141
Number of Clusters: 3
Online_Training [1/200]: mean_loss=0.09397314582020044
Online_Training [2/200]: mean_loss=0.10217858664691448
Online_Training [3/200]: mean_loss=0.09752676729112864
Online_Training [4/200]: mean_loss=0.07433859445154667
Online_Training [5/200]: mean_loss=0.08873854018747807
Online_Training [6/200]: mean_loss=0.07786494679749012
Online_Training [7/200]: mean_loss=0.06350833689793944
Online_Training [8/200]: mean_loss=0.1179585661739111
Online_Training [9/200]: mean_loss=0.0712103908881545
Online_Training [10/200]: mean_loss=0.0705952225252986
Online_Training [11/200]: mean_loss=0.019701057579368353
Online_Training [12/200]: mean_loss=0.059673980344086885
Online_Training [13/200]: mean_loss=0.08085877448320389
Online_Training [14/200]: mean_loss=0.08632215019315481
Online_Training [15/200]: mean_loss=0.10559828951954842
Online_Training [16/200]: mean_loss=0.012644559144973755
Online_Training [17/200]: mean_loss=0.019465958001092076
Online_Training [18/200]: mean_loss=0.02917453832924366
Online_Training [19/200]: mean_loss=0.027949462179094553
Online_Training [20/200]: mean_loss=0.04780314210802317
Online_Training [21/200]: mean_loss=0.07443212158977985
Online_Training [22/200]: mean_loss=0.025448864325881004
Online_Training [23/200]: mean_loss=0.06842098059132695
Online_Training [24/200]: mean_loss=0.04349863715469837
Online_Training [25/200]: mean_loss=0.02611721050925553
Online_Training [26/200]: mean_loss=0.03701772494241595
Online_Training [27/200]: mean_loss=0.016134780133143067
Online_Training [28/200]: mean_loss=0.020377293461933732
Online_Training [29/200]: mean_loss=0.05835900641977787
Online_Training [30/200]: mean_loss=0.04126113187521696
Online_Training [31/200]: mean_loss=0.12738885171711445
Online_Training [32/200]: mean_loss=0.05684929946437478
Online_Training [33/200]: mean_loss=0.03933723317459226
Online_Training [34/200]: mean_loss=0.026335870381444693
Online_Training [35/200]: mean_loss=0.016278193099424243
Online_Training [36/200]: mean_loss=0.09356239438056946
Online_Training [37/200]: mean_loss=0.030475642532110214
Online_Training [38/200]: mean_loss=0.027286204043775797
Online_Training [39/200]: mean_loss=0.06328495685011148
Online_Training [40/200]: mean_loss=0.022326350677758455
Online_Training [41/200]: mean_loss=0.06296553974971175
Online_Training [42/200]: mean_loss=0.033823168370872736
Online_Training [43/200]: mean_loss=0.03569540334865451
Online_Training [44/200]: mean_loss=0.013359864242374897
Online_Training [45/200]: mean_loss=0.009938690811395645
Online_Training [46/200]: mean_loss=0.03053813916631043
Online_Training [47/200]: mean_loss=0.02080248761922121
Online_Training [48/200]: mean_loss=0.03534778719767928
Online_Training [49/200]: mean_loss=0.0054955981322564185
Online_Training [50/200]: mean_loss=0.03950990829616785
Online_Training [51/200]: mean_loss=0.04611998936161399
Online_Training [52/200]: mean_loss=0.1001456631347537
Online_Training [53/200]: mean_loss=0.01324718608520925
Online_Training [54/200]: mean_loss=0.010672280797734857
Online_Training [55/200]: mean_loss=0.02713759895414114
Online_Training [56/200]: mean_loss=0.0710312258452177
Online_Training [57/200]: mean_loss=0.07827310357242823
Online_Training [58/200]: mean_loss=0.05543779814615846
Online_Training [59/200]: mean_loss=0.042630007956176996
Online_Training [60/200]: mean_loss=0.07564797904342413
Online_Training [61/200]: mean_loss=0.043874428141862154
Online_Training [62/200]: mean_loss=0.09498889185488224
Online_Training [63/200]: mean_loss=0.04066305793821812
Online_Training [64/200]: mean_loss=0.026261977152898908
Online_Training [65/200]: mean_loss=0.028736030450090766
Online_Training [66/200]: mean_loss=0.028695827117189765
Online_Training [67/200]: mean_loss=0.04316624440252781
Online_Training [68/200]: mean_loss=0.015685392543673515
Online_Training [69/200]: mean_loss=0.023089387454092503
Online_Training [70/200]: mean_loss=0.05171227641403675
Online_Training [71/200]: mean_loss=0.07235103193670511
Online_Training [72/200]: mean_loss=0.023318362655118108
Online_Training [73/200]: mean_loss=0.039083731826394796
Online_Training [74/200]: mean_loss=0.03934704652056098
Online_Training [75/200]: mean_loss=0.03735246090218425
Online_Training [76/200]: mean_loss=0.027906429022550583
Online_Training [77/200]: mean_loss=0.04067141516134143
Online_Training [78/200]: mean_loss=0.041262330021709204
Online_Training [79/200]: mean_loss=0.014524680911563337
Online_Training [80/200]: mean_loss=0.022375137777999043
Online_Training [81/200]: mean_loss=0.08236963674426079
Online_Training [82/200]: mean_loss=0.0484643611125648
Online_Training [83/200]: mean_loss=0.03397712600417435
Online_Training [84/200]: mean_loss=0.033073111437261105
Online_Training [85/200]: mean_loss=0.016106265946291387
Online_Training [86/200]: mean_loss=0.03134338138625026
Online_Training [87/200]: mean_loss=0.1350499028339982
Online_Training [88/200]: mean_loss=0.09327582083642483
Online_Training [89/200]: mean_loss=0.03720394195988774
Online_Training [90/200]: mean_loss=0.023106751730665565
Online_Training [91/200]: mean_loss=0.016610271646641195
Online_Training [92/200]: mean_loss=0.08572633937001228
Online_Training [93/200]: mean_loss=0.031827476574108005
Online_Training [94/200]: mean_loss=0.031834934605285525
Online_Training [95/200]: mean_loss=0.0370706464163959
Online_Training [96/200]: mean_loss=0.02884936472401023
Online_Training [97/200]: mean_loss=0.03638620022684336
Online_Training [98/200]: mean_loss=0.0274060838855803
Online_Training [99/200]: mean_loss=0.05045072780922055
Online_Training [100/200]: mean_loss=0.022511999122798443
Online_Training [101/200]: mean_loss=0.027459373231977224
Online_Training [102/200]: mean_loss=0.03588941274210811
Online_Training [103/200]: mean_loss=0.008566202828660607
Online_Training [104/200]: mean_loss=0.06370636355131865
Online_Training [105/200]: mean_loss=0.045989800710231066
Online_Training [106/200]: mean_loss=0.05291416822001338
Online_Training [107/200]: mean_loss=0.03281687665730715
Online_Training [108/200]: mean_loss=0.015390274347737432
Online_Training [109/200]: mean_loss=0.017698266077786684
Online_Training [110/200]: mean_loss=0.017393815563991666
Online_Training [111/200]: mean_loss=0.024480399442836642
Online_Training [112/200]: mean_loss=0.03447575448080897
Online_Training [113/200]: mean_loss=0.1141852829605341
Online_Training [114/200]: mean_loss=0.07740298192948103
Online_Training [115/200]: mean_loss=0.02167863165959716
Online_Training [116/200]: mean_loss=0.043376816902309656
Online_Training [117/200]: mean_loss=0.024550122441723943
Online_Training [118/200]: mean_loss=0.029914291575551033
Online_Training [119/200]: mean_loss=0.03807640029117465
Online_Training [120/200]: mean_loss=0.012340189306996763
Online_Training [121/200]: mean_loss=0.0658495887182653
Online_Training [122/200]: mean_loss=0.06413992121815681
Online_Training [123/200]: mean_loss=0.02264370396733284
Online_Training [124/200]: mean_loss=0.015557818464003503
Online_Training [125/200]: mean_loss=0.0444643278606236
Online_Training [126/200]: mean_loss=0.1722047422081232
Online_Training [127/200]: mean_loss=0.054837801959365606
Online_Training [128/200]: mean_loss=0.045412583742290735
Online_Training [129/200]: mean_loss=0.035752585623413324
Online_Training [130/200]: mean_loss=0.042617976665496826
Online_Training [131/200]: mean_loss=0.12274495139718056
Online_Training [132/200]: mean_loss=0.13972090929746628
Online_Training [133/200]: mean_loss=0.019420343916863203
Online_Training [134/200]: mean_loss=0.02611474576406181
Online_Training [135/200]: mean_loss=0.06707540899515152
Online_Training [136/200]: mean_loss=0.16187377460300922
Online_Training [137/200]: mean_loss=0.01447483862284571
Online_Training [138/200]: mean_loss=0.048877769615501165
Online_Training [139/200]: mean_loss=0.02879344904795289
Online_Training [140/200]: mean_loss=0.024614609545096755
Online_Training [141/200]: mean_loss=0.033526604529470205
Online_Training [142/200]: mean_loss=0.05382490996271372
Online_Training [143/200]: mean_loss=0.017568274633958936
Online_Training [144/200]: mean_loss=0.01601964107248932
Online_Training [145/200]: mean_loss=0.02220876794308424
Online_Training [146/200]: mean_loss=0.02250588219612837
Online_Training [147/200]: mean_loss=0.016666294308379292
Online_Training [148/200]: mean_loss=0.022457221522927284
Online_Training [149/200]: mean_loss=0.02227833867073059
Online_Training [150/200]: mean_loss=0.025906966999173164
Online_Training [151/200]: mean_loss=0.012243580538779497
Online_Training [152/200]: mean_loss=0.026664480566978455
Online_Training [153/200]: mean_loss=0.04557977104559541
Online_Training [154/200]: mean_loss=0.013166398974135518
Online_Training [155/200]: mean_loss=0.041152570862323046
Online_Training [156/200]: mean_loss=0.03768535004928708
Online_Training [157/200]: mean_loss=0.03376830229535699
Online_Training [158/200]: mean_loss=0.03420779062435031
Online_Training [159/200]: mean_loss=0.01920084306038916
Online_Training [160/200]: mean_loss=0.037072671577334404
Online_Training [161/200]: mean_loss=0.0285677220672369
Online_Training [162/200]: mean_loss=0.042938674334436655
Online_Training [163/200]: mean_loss=0.011470193625427783
Online_Training [164/200]: mean_loss=0.028237066930159926
Online_Training [165/200]: mean_loss=0.02728505409322679
Online_Training [166/200]: mean_loss=0.027827756013721228
Online_Training [167/200]: mean_loss=0.031173710245639086
Online_Training [168/200]: mean_loss=0.03432101057842374
Online_Training [169/200]: mean_loss=0.03259240370243788
Online_Training [170/200]: mean_loss=0.019811106380075216
Online_Training [171/200]: mean_loss=0.030858374200761318
Online_Training [172/200]: mean_loss=0.017103155376389623
Online_Training [173/200]: mean_loss=0.020178163889795542
Online_Training [174/200]: mean_loss=0.026233994401991367
Online_Training [175/200]: mean_loss=0.03772301506251097
Online_Training [176/200]: mean_loss=0.0680127302184701
Online_Training [177/200]: mean_loss=0.012052952544763684
Online_Training [178/200]: mean_loss=0.020153285702690482
Online_Training [179/200]: mean_loss=0.01659165881574154
Online_Training [180/200]: mean_loss=0.014596333843655884
Online_Training [181/200]: mean_loss=0.01057110889814794
Online_Training [182/200]: mean_loss=0.07313288375735283
Online_Training [183/200]: mean_loss=0.013270320021547377
Online_Training [184/200]: mean_loss=0.017606857465580106
Online_Training [185/200]: mean_loss=0.03043968160636723
Online_Training [186/200]: mean_loss=0.024993411730974913
Online_Training [187/200]: mean_loss=0.019019316416233778
Online_Training [188/200]: mean_loss=0.02836164180189371
Online_Training [189/200]: mean_loss=0.030597115634009242
Online_Training [190/200]: mean_loss=0.013986588804982603
Online_Training [191/200]: mean_loss=0.02418771805241704
Online_Training [192/200]: mean_loss=0.013962609227746725
Online_Training [193/200]: mean_loss=0.029120062245056033
Online_Training [194/200]: mean_loss=0.0353299742564559
Online_Training [195/200]: mean_loss=0.0291827074252069
Online_Training [196/200]: mean_loss=0.0666397474706173
Online_Training [197/200]: mean_loss=0.060694434214383364
Online_Training [198/200]: mean_loss=0.029024755116552114
Online_Training [199/200]: mean_loss=0.02287673531100154
Online_Training [200/200]: mean_loss=0.028045291546732187
Q_Learning [1/300]: mean_loss=0.09397314582020044
Q_Learning [2/300]: mean_loss=0.10217858664691448
Q_Learning [3/300]: mean_loss=0.09752676729112864
Q_Learning [4/300]: mean_loss=0.07433859445154667
Q_Learning [5/300]: mean_loss=0.08873854018747807
Q_Learning [6/300]: mean_loss=0.07786494679749012
Q_Learning [7/300]: mean_loss=0.06350833689793944
Q_Learning [8/300]: mean_loss=0.1179585661739111
Q_Learning [9/300]: mean_loss=0.0712103908881545
Q_Learning [10/300]: mean_loss=0.0705952225252986
Q_Learning [11/300]: mean_loss=0.019701057579368353
Q_Learning [12/300]: mean_loss=0.059673980344086885
Q_Learning [13/300]: mean_loss=0.08085877448320389
Q_Learning [14/300]: mean_loss=0.08632215019315481
Q_Learning [15/300]: mean_loss=0.10559828951954842
Q_Learning [16/300]: mean_loss=0.012644559144973755
Q_Learning [17/300]: mean_loss=0.019465958001092076
Q_Learning [18/300]: mean_loss=0.02917453832924366
Q_Learning [19/300]: mean_loss=0.027949462179094553
Q_Learning [20/300]: mean_loss=0.04780314210802317
Q_Learning [21/300]: mean_loss=0.07443212158977985
Q_Learning [22/300]: mean_loss=0.025448864325881004
Q_Learning [23/300]: mean_loss=0.06842098059132695
Q_Learning [24/300]: mean_loss=0.04349863715469837
Q_Learning [25/300]: mean_loss=0.02611721050925553
Q_Learning [26/300]: mean_loss=0.03701772494241595
Q_Learning [27/300]: mean_loss=0.016134780133143067
Q_Learning [28/300]: mean_loss=0.020377293461933732
Q_Learning [29/300]: mean_loss=0.05835900641977787
Q_Learning [30/300]: mean_loss=0.04126113187521696
Q_Learning [31/300]: mean_loss=0.12738885171711445
Q_Learning [32/300]: mean_loss=0.05684929946437478
Q_Learning [33/300]: mean_loss=0.03933723317459226
Q_Learning [34/300]: mean_loss=0.026335870381444693
Q_Learning [35/300]: mean_loss=0.016278193099424243
Q_Learning [36/300]: mean_loss=0.09356239438056946
Q_Learning [37/300]: mean_loss=0.030475642532110214
Q_Learning [38/300]: mean_loss=0.027286204043775797
Q_Learning [39/300]: mean_loss=0.06328495685011148
Q_Learning [40/300]: mean_loss=0.022326350677758455
Q_Learning [41/300]: mean_loss=0.06296553974971175
Q_Learning [42/300]: mean_loss=0.033823168370872736
Q_Learning [43/300]: mean_loss=0.03569540334865451
Q_Learning [44/300]: mean_loss=0.013359864242374897
Q_Learning [45/300]: mean_loss=0.009938690811395645
Q_Learning [46/300]: mean_loss=0.03053813916631043
Q_Learning [47/300]: mean_loss=0.02080248761922121
Q_Learning [48/300]: mean_loss=0.03534778719767928
Q_Learning [49/300]: mean_loss=0.0054955981322564185
Q_Learning [50/300]: mean_loss=0.03950990829616785
Q_Learning [51/300]: mean_loss=0.04611998936161399
Q_Learning [52/300]: mean_loss=0.1001456631347537
Q_Learning [53/300]: mean_loss=0.01324718608520925
Q_Learning [54/300]: mean_loss=0.010672280797734857
Q_Learning [55/300]: mean_loss=0.02713759895414114
Q_Learning [56/300]: mean_loss=0.0710312258452177
Q_Learning [57/300]: mean_loss=0.07827310357242823
Q_Learning [58/300]: mean_loss=0.05543779814615846
Q_Learning [59/300]: mean_loss=0.042630007956176996
Q_Learning [60/300]: mean_loss=0.07564797904342413
Q_Learning [61/300]: mean_loss=0.043874428141862154
Q_Learning [62/300]: mean_loss=0.09498889185488224
Q_Learning [63/300]: mean_loss=0.04066305793821812
Q_Learning [64/300]: mean_loss=0.026261977152898908
Q_Learning [65/300]: mean_loss=0.028736030450090766
Q_Learning [66/300]: mean_loss=0.028695827117189765
Q_Learning [67/300]: mean_loss=0.04316624440252781
Q_Learning [68/300]: mean_loss=0.015685392543673515
Q_Learning [69/300]: mean_loss=0.023089387454092503
Q_Learning [70/300]: mean_loss=0.05171227641403675
Q_Learning [71/300]: mean_loss=0.07235103193670511
Q_Learning [72/300]: mean_loss=0.023318362655118108
Q_Learning [73/300]: mean_loss=0.039083731826394796
Q_Learning [74/300]: mean_loss=0.03934704652056098
Q_Learning [75/300]: mean_loss=0.03735246090218425
Q_Learning [76/300]: mean_loss=0.027906429022550583
Q_Learning [77/300]: mean_loss=0.04067141516134143
Q_Learning [78/300]: mean_loss=0.041262330021709204
Q_Learning [79/300]: mean_loss=0.014524680911563337
Q_Learning [80/300]: mean_loss=0.022375137777999043
Q_Learning [81/300]: mean_loss=0.08236963674426079
Q_Learning [82/300]: mean_loss=0.0484643611125648
Q_Learning [83/300]: mean_loss=0.03397712600417435
Q_Learning [84/300]: mean_loss=0.033073111437261105
Q_Learning [85/300]: mean_loss=0.016106265946291387
Q_Learning [86/300]: mean_loss=0.03134338138625026
Q_Learning [87/300]: mean_loss=0.1350499028339982
Q_Learning [88/300]: mean_loss=0.09327582083642483
Q_Learning [89/300]: mean_loss=0.03720394195988774
Q_Learning [90/300]: mean_loss=0.023106751730665565
Q_Learning [91/300]: mean_loss=0.016610271646641195
Q_Learning [92/300]: mean_loss=0.08572633937001228
Q_Learning [93/300]: mean_loss=0.031827476574108005
Q_Learning [94/300]: mean_loss=0.031834934605285525
Q_Learning [95/300]: mean_loss=0.0370706464163959
Q_Learning [96/300]: mean_loss=0.02884936472401023
Q_Learning [97/300]: mean_loss=0.03638620022684336
Q_Learning [98/300]: mean_loss=0.0274060838855803
Q_Learning [99/300]: mean_loss=0.05045072780922055
Q_Learning [100/300]: mean_loss=0.022511999122798443
Q_Learning [101/300]: mean_loss=0.027459373231977224
Q_Learning [102/300]: mean_loss=0.03588941274210811
Q_Learning [103/300]: mean_loss=0.008566202828660607
Q_Learning [104/300]: mean_loss=0.06370636355131865
Q_Learning [105/300]: mean_loss=0.045989800710231066
Q_Learning [106/300]: mean_loss=0.05291416822001338
Q_Learning [107/300]: mean_loss=0.03281687665730715
Q_Learning [108/300]: mean_loss=0.015390274347737432
Q_Learning [109/300]: mean_loss=0.017698266077786684
Q_Learning [110/300]: mean_loss=0.017393815563991666
Q_Learning [111/300]: mean_loss=0.024480399442836642
Q_Learning [112/300]: mean_loss=0.03447575448080897
Q_Learning [113/300]: mean_loss=0.1141852829605341
Q_Learning [114/300]: mean_loss=0.07740298192948103
Q_Learning [115/300]: mean_loss=0.02167863165959716
Q_Learning [116/300]: mean_loss=0.043376816902309656
Q_Learning [117/300]: mean_loss=0.024550122441723943
Q_Learning [118/300]: mean_loss=0.029914291575551033
Q_Learning [119/300]: mean_loss=0.03807640029117465
Q_Learning [120/300]: mean_loss=0.012340189306996763
Q_Learning [121/300]: mean_loss=0.0658495887182653
Q_Learning [122/300]: mean_loss=0.06413992121815681
Q_Learning [123/300]: mean_loss=0.02264370396733284
Q_Learning [124/300]: mean_loss=0.015557818464003503
Q_Learning [125/300]: mean_loss=0.0444643278606236
Q_Learning [126/300]: mean_loss=0.1722047422081232
Q_Learning [127/300]: mean_loss=0.054837801959365606
Q_Learning [128/300]: mean_loss=0.045412583742290735
Q_Learning [129/300]: mean_loss=0.035752585623413324
Q_Learning [130/300]: mean_loss=0.042617976665496826
Q_Learning [131/300]: mean_loss=0.12274495139718056
Q_Learning [132/300]: mean_loss=0.13972090929746628
Q_Learning [133/300]: mean_loss=0.019420343916863203
Q_Learning [134/300]: mean_loss=0.02611474576406181
Q_Learning [135/300]: mean_loss=0.06707540899515152
Q_Learning [136/300]: mean_loss=0.16187377460300922
Q_Learning [137/300]: mean_loss=0.01447483862284571
Q_Learning [138/300]: mean_loss=0.048877769615501165
Q_Learning [139/300]: mean_loss=0.02879344904795289
Q_Learning [140/300]: mean_loss=0.024614609545096755
Q_Learning [141/300]: mean_loss=0.033526604529470205
Q_Learning [142/300]: mean_loss=0.05382490996271372
Q_Learning [143/300]: mean_loss=0.017568274633958936
Q_Learning [144/300]: mean_loss=0.01601964107248932
Q_Learning [145/300]: mean_loss=0.02220876794308424
Q_Learning [146/300]: mean_loss=0.02250588219612837
Q_Learning [147/300]: mean_loss=0.016666294308379292
Q_Learning [148/300]: mean_loss=0.022457221522927284
Q_Learning [149/300]: mean_loss=0.02227833867073059
Q_Learning [150/300]: mean_loss=0.025906966999173164
Q_Learning [151/300]: mean_loss=0.012243580538779497
Q_Learning [152/300]: mean_loss=0.026664480566978455
Q_Learning [153/300]: mean_loss=0.04557977104559541
Q_Learning [154/300]: mean_loss=0.013166398974135518
Q_Learning [155/300]: mean_loss=0.041152570862323046
Q_Learning [156/300]: mean_loss=0.03768535004928708
Q_Learning [157/300]: mean_loss=0.03376830229535699
Q_Learning [158/300]: mean_loss=0.03420779062435031
Q_Learning [159/300]: mean_loss=0.01920084306038916
Q_Learning [160/300]: mean_loss=0.037072671577334404
Q_Learning [161/300]: mean_loss=0.0285677220672369
Q_Learning [162/300]: mean_loss=0.042938674334436655
Q_Learning [163/300]: mean_loss=0.011470193625427783
Q_Learning [164/300]: mean_loss=0.028237066930159926
Q_Learning [165/300]: mean_loss=0.02728505409322679
Q_Learning [166/300]: mean_loss=0.027827756013721228
Q_Learning [167/300]: mean_loss=0.031173710245639086
Q_Learning [168/300]: mean_loss=0.03432101057842374
Q_Learning [169/300]: mean_loss=0.03259240370243788
Q_Learning [170/300]: mean_loss=0.019811106380075216
Q_Learning [171/300]: mean_loss=0.030858374200761318
Q_Learning [172/300]: mean_loss=0.017103155376389623
Q_Learning [173/300]: mean_loss=0.020178163889795542
Q_Learning [174/300]: mean_loss=0.026233994401991367
Q_Learning [175/300]: mean_loss=0.03772301506251097
Q_Learning [176/300]: mean_loss=0.0680127302184701
Q_Learning [177/300]: mean_loss=0.012052952544763684
Q_Learning [178/300]: mean_loss=0.020153285702690482
Q_Learning [179/300]: mean_loss=0.01659165881574154
Q_Learning [180/300]: mean_loss=0.014596333843655884
Q_Learning [181/300]: mean_loss=0.01057110889814794
Q_Learning [182/300]: mean_loss=0.07313288375735283
Q_Learning [183/300]: mean_loss=0.013270320021547377
Q_Learning [184/300]: mean_loss=0.017606857465580106
Q_Learning [185/300]: mean_loss=0.03043968160636723
Q_Learning [186/300]: mean_loss=0.024993411730974913
Q_Learning [187/300]: mean_loss=0.019019316416233778
Q_Learning [188/300]: mean_loss=0.02836164180189371
Q_Learning [189/300]: mean_loss=0.030597115634009242
Q_Learning [190/300]: mean_loss=0.013986588804982603
Q_Learning [191/300]: mean_loss=0.02418771805241704
Q_Learning [192/300]: mean_loss=0.013962609227746725
Q_Learning [193/300]: mean_loss=0.029120062245056033
Q_Learning [194/300]: mean_loss=0.0353299742564559
Q_Learning [195/300]: mean_loss=0.0291827074252069
Q_Learning [196/300]: mean_loss=0.0666397474706173
Q_Learning [197/300]: mean_loss=0.060694434214383364
Q_Learning [198/300]: mean_loss=0.029024755116552114
Q_Learning [199/300]: mean_loss=0.02287673531100154
Q_Learning [200/300]: mean_loss=0.028045291546732187
Q_Learning [201/300]: mean_loss=0.017577962251380086
Q_Learning [202/300]: mean_loss=0.029097864171490073
Q_Learning [203/300]: mean_loss=0.011693244217894971
Q_Learning [204/300]: mean_loss=0.015478612040169537
Q_Learning [205/300]: mean_loss=0.01224723202176392
Q_Learning [206/300]: mean_loss=0.03963551390916109
Q_Learning [207/300]: mean_loss=0.01813094923272729
Q_Learning [208/300]: mean_loss=0.029645745642483234
Q_Learning [209/300]: mean_loss=0.028271488845348358
Q_Learning [210/300]: mean_loss=0.03906992031261325
Q_Learning [211/300]: mean_loss=0.01490464003290981
Q_Learning [212/300]: mean_loss=0.031180500984191895
Q_Learning [213/300]: mean_loss=0.016830351669341326
Q_Learning [214/300]: mean_loss=0.024224175605922937
Q_Learning [215/300]: mean_loss=0.02240510331466794
Q_Learning [216/300]: mean_loss=0.03648871881887317
Q_Learning [217/300]: mean_loss=0.011529344134032726
Q_Learning [218/300]: mean_loss=0.01506362296640873
Q_Learning [219/300]: mean_loss=0.012493350077420473
Q_Learning [220/300]: mean_loss=0.023812714498490095
Q_Learning [221/300]: mean_loss=0.016238417592830956
Q_Learning [222/300]: mean_loss=0.01953015197068453
Q_Learning [223/300]: mean_loss=0.07780478149652481
Q_Learning [224/300]: mean_loss=0.0325621603988111
Q_Learning [225/300]: mean_loss=0.014352764817886055
Q_Learning [226/300]: mean_loss=0.010026139905676246
Q_Learning [227/300]: mean_loss=0.02701192256063223
Q_Learning [228/300]: mean_loss=0.10387047939002514
Q_Learning [229/300]: mean_loss=0.09035975951701403
Q_Learning [230/300]: mean_loss=0.012902536313049495
Q_Learning [231/300]: mean_loss=0.023530657636001706
Q_Learning [232/300]: mean_loss=0.014185465406626463
Q_Learning [233/300]: mean_loss=0.013198726112022996
Q_Learning [234/300]: mean_loss=0.06742752902209759
Q_Learning [235/300]: mean_loss=0.02032006182707846
Q_Learning [236/300]: mean_loss=0.01887163962237537
Q_Learning [237/300]: mean_loss=0.02026451611891389
Q_Learning [238/300]: mean_loss=0.03129729232750833
Q_Learning [239/300]: mean_loss=0.02585566695779562
Q_Learning [240/300]: mean_loss=0.01962413382716477
Q_Learning [241/300]: mean_loss=0.031041596550494432
Q_Learning [242/300]: mean_loss=0.01656209328211844
Q_Learning [243/300]: mean_loss=0.019862559624016285
Q_Learning [244/300]: mean_loss=0.01587703009136021
Q_Learning [245/300]: mean_loss=0.014826732454821467
Q_Learning [246/300]: mean_loss=0.022496439050883055
Q_Learning [247/300]: mean_loss=0.03343129949644208
Q_Learning [248/300]: mean_loss=0.02188418060541153
Q_Learning [249/300]: mean_loss=0.024735886370763183
Q_Learning [250/300]: mean_loss=0.01594859967008233
Q_Learning [251/300]: mean_loss=0.007885186118073761
Q_Learning [252/300]: mean_loss=0.026145205600187182
Q_Learning [253/300]: mean_loss=0.0075667822966352105
Q_Learning [254/300]: mean_loss=0.003777644509682432
Q_Learning [255/300]: mean_loss=0.017903226427733898
Q_Learning [256/300]: mean_loss=0.033164181280881166
Q_Learning [257/300]: mean_loss=0.06357297208160162
Q_Learning [258/300]: mean_loss=0.10680619720369577
Q_Learning [259/300]: mean_loss=0.08898567128926516
Q_Learning [260/300]: mean_loss=0.04664819082245231
Q_Learning [261/300]: mean_loss=0.012561394483782351
Q_Learning [262/300]: mean_loss=0.011959340539760888
Q_Learning [263/300]: mean_loss=0.12833973858505487
Q_Learning [264/300]: mean_loss=0.06618683785200119
Q_Learning [265/300]: mean_loss=0.04446292435750365
Q_Learning [266/300]: mean_loss=0.027588059660047293
Q_Learning [267/300]: mean_loss=0.040557550732046366
Q_Learning [268/300]: mean_loss=0.027573154773563147
Q_Learning [269/300]: mean_loss=0.028074705507606268
Q_Learning [270/300]: mean_loss=0.016120340209454298
Q_Learning [271/300]: mean_loss=0.02446353156119585
Q_Learning [272/300]: mean_loss=0.010927775467280298
Q_Learning [273/300]: mean_loss=0.010713114868849516
Q_Learning [274/300]: mean_loss=0.08870079554617405
Q_Learning [275/300]: mean_loss=0.05799455940723419
Q_Learning [276/300]: mean_loss=0.03945754235610366
Q_Learning [277/300]: mean_loss=0.022396337473765016
Q_Learning [278/300]: mean_loss=0.04023824259638786
Q_Learning [279/300]: mean_loss=0.04334188811480999
Q_Learning [280/300]: mean_loss=0.02670092135667801
Q_Learning [281/300]: mean_loss=0.18430104851722717
Q_Learning [282/300]: mean_loss=0.158487344160676
Q_Learning [283/300]: mean_loss=0.11283648107200861
Q_Learning [284/300]: mean_loss=0.033504603896290064
Q_Learning [285/300]: mean_loss=0.035308219492435455
Q_Learning [286/300]: mean_loss=0.042984812054783106
Q_Learning [287/300]: mean_loss=0.04561829101294279
Q_Learning [288/300]: mean_loss=0.01478592783678323
Q_Learning [289/300]: mean_loss=0.029510913882404566
Q_Learning [290/300]: mean_loss=0.012135219760239124
Q_Learning [291/300]: mean_loss=0.10414674319326878
Q_Learning [292/300]: mean_loss=0.025229043094441295
Q_Learning [293/300]: mean_loss=0.010042436653748155
Q_Learning [294/300]: mean_loss=0.005194538040086627
Q_Learning [295/300]: mean_loss=0.03344368562102318
Q_Learning [296/300]: mean_loss=0.01020329212769866
Q_Learning [297/300]: mean_loss=0.026950020343065262
Q_Learning [298/300]: mean_loss=0.00985189329367131
Q_Learning [299/300]: mean_loss=0.01416964246891439
Q_Learning [300/300]: mean_loss=0.015938651165924966
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.8094006  -0.85844696]
[2, 0, 2, 0, 0, 1, 2, 2, 1, 0, 2, 2, 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 2, 0, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 2, 2, 2, 0, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 0, 1, 2, 2, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 2, 2, 2, 1, 0, 1, 0, 0, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 1, 1, 1, 1, 1, 0, 1, 2, 2, 1, 1, 2, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 1, 2, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 2, 0, 2, 1, 2, 2, 1, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 2, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1, 2, 2, 0, 2, 1, 1, 1, 0, 2, 0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 2, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 1, 2, 0, 2, 2, 1, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 2, 0, 1, 0, 1, 1, 0, 1, 1, 0, 2, 2, 1, 2, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 1, 0, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 1, 2, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 3, 1, 3, 0, 1, 1, 0, 2, 1, 3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 3, 1, 2, 0, 1, 0, 0, 0, 3, 3, 3, 0, 1, 0, 2, 3, 0, 3, 1, 3, 0, 0, 2, 1, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 1, 3, 1, 3, 1, 0, 1]
Centroids: [[-0.9560051, -0.70474833], [-1.1118987, -0.54155487], [-1.6535589, -1.003085]]
Centroids: [[-1.4585176, -0.7185751], [-0.63894224, -0.72884226], [-2.3138657, -0.7171673], [-0.98922336, -1.5001107], [0.70735013, -0.9855947]]
Standard Derivations: [0.34714466, 0.32717124, 0.30682528]
Cluster Distances: [-0.44862825, 0.1047039, -0.44862825, 0.07762548, 0.1047039, 0.07762548]
Minimal Cluster Distance: -0.4486282467842102
Contingency Matrix: 
[[52 51  2  0  1]
 [45 41  4  0  0]
 [57  8 25 14  0]]
[[52, 51, 2, 0, 1], [45, 41, 4, 0, 0], [57, 8, 25, 14, 0]]
[[52, 51, 2, 0, 1], [45, 41, 4, 0, 0], [57, 8, 25, 14, 0]]
[0, 1, 2, 3, 4]
[[-1, 51, 2, 0, 1], [-1, 41, 4, 0, 0], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, -1, 4, 0, 0], [-1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1], [-1, -1, -1, -1, -1]]
Match_Labels: {2: 0, 0: 1, 1: 2}
New Contingency Matrix: 
[[51  2 52  0  1]
 [41  4 45  0  0]
 [ 8 25 57 14  0]]
New Clustered Label Sequence: [1, 2, 0, 3, 4]
Diagonal_Elements: [51, 4, 57], Sum: 112
All_Elements: [51, 2, 52, 0, 1, 41, 4, 45, 0, 0, 8, 25, 57, 14, 0], Sum: 300
Accuracy: 0.37333333333333335
