Experiment_path: AE_Model_2/Reduce_Training_opt//V5_200/Experiment_05_opt
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Drift_Easy2_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Drift_Easy2_noise015.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt
Visualisation_Path: AE_Model_2/Reduce_Training_opt//V5_200/Experiment_05_opt/C_Drift_Easy2_noise015.mat/Variant_05_Online_Autoencoder_QLearning_opt/2023_05_25-14_58_16
Punishment_Coefficient: 0.7
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000002CA5739F668>
Sampling rate: 24000.0
Raw: [-0.11406566 -0.12673582 -0.13859424 ... -0.1533925  -0.11314303
 -0.07599672]
Times: [    141    1662    1690 ... 1437394 1438167 1439221]
Cluster: [3 3 1 ... 1 3 1]
Number of different clusters:  3
Number of Spikes: 3444
First aligned Spike Frame: [-1.36998177e-01 -1.49794115e-01 -1.51139147e-01 -1.34027918e-01
 -1.09988960e-01 -9.86934846e-02 -1.08483729e-01 -1.27522960e-01
 -1.35591044e-01 -1.26517001e-01 -9.48742956e-02 -8.16393331e-04
  2.25765217e-01  5.72256463e-01  8.98736621e-01  1.04373325e+00
  9.77396764e-01  8.07455467e-01  6.41295597e-01  5.04504644e-01
  3.89667525e-01  2.93991016e-01  2.08446734e-01  1.08695180e-01
 -1.90255699e-02 -1.51076860e-01 -2.47294168e-01 -3.00867038e-01
 -3.38922213e-01 -3.74759690e-01 -3.88805853e-01 -3.48577503e-01
 -2.56264435e-01 -1.52199911e-01 -7.91585816e-02 -5.05132281e-02
 -5.44251469e-02 -6.88811373e-02 -7.02917794e-02 -5.09609752e-02
 -2.91934475e-02 -2.32878628e-02 -2.62245500e-02 -1.24323704e-02
  2.48287815e-02  6.36178972e-02  8.45690766e-02]
Cluster 0, Occurrences: 1142
Cluster 1, Occurrences: 1180
Cluster 2, Occurrences: 1122
Number of Clusters: 3
Online_Training [1/200]: mean_loss=0.1386891845613718
Online_Training [2/200]: mean_loss=0.1497445497661829
Online_Training [3/200]: mean_loss=0.2779655270278454
Online_Training [4/200]: mean_loss=0.1383659914135933
Online_Training [5/200]: mean_loss=0.14062516763806343
Online_Training [6/200]: mean_loss=0.11520449724048376
Online_Training [7/200]: mean_loss=0.03626192221418023
Online_Training [8/200]: mean_loss=0.0901311868801713
Online_Training [9/200]: mean_loss=0.1226275758817792
Online_Training [10/200]: mean_loss=0.08810629043728113
Online_Training [11/200]: mean_loss=0.05878388276323676
Online_Training [12/200]: mean_loss=0.03977879602462053
Online_Training [13/200]: mean_loss=0.09639801923185587
Online_Training [14/200]: mean_loss=0.14839691296219826
Online_Training [15/200]: mean_loss=0.0852551069110632
Online_Training [16/200]: mean_loss=0.076807064935565
Online_Training [17/200]: mean_loss=0.058581402990967035
Online_Training [18/200]: mean_loss=0.09216954093426466
Online_Training [19/200]: mean_loss=0.0379923521541059
Online_Training [20/200]: mean_loss=0.06016507092863321
Online_Training [21/200]: mean_loss=0.033609868958592415
Online_Training [22/200]: mean_loss=0.10138948913663626
Online_Training [23/200]: mean_loss=0.03482343070209026
Online_Training [24/200]: mean_loss=0.05260655423626304
Online_Training [25/200]: mean_loss=0.06338182743638754
Online_Training [26/200]: mean_loss=0.1542759370058775
Online_Training [27/200]: mean_loss=0.10424620658159256
Online_Training [28/200]: mean_loss=0.1026335870847106
Online_Training [29/200]: mean_loss=0.04172274889424443
Online_Training [30/200]: mean_loss=0.0990932947024703
Online_Training [31/200]: mean_loss=0.1135997911915183
Online_Training [32/200]: mean_loss=0.05867856368422508
Online_Training [33/200]: mean_loss=0.023510202765464783
Online_Training [34/200]: mean_loss=0.04083793517202139
Online_Training [35/200]: mean_loss=0.048160846810787916
Online_Training [36/200]: mean_loss=0.03938200371339917
Online_Training [37/200]: mean_loss=0.08847056329250336
Online_Training [38/200]: mean_loss=0.1424392145127058
Online_Training [39/200]: mean_loss=0.1286744438111782
Online_Training [40/200]: mean_loss=0.04801238002255559
Online_Training [41/200]: mean_loss=0.048298731446266174
Online_Training [42/200]: mean_loss=0.013975822017528117
Online_Training [43/200]: mean_loss=0.11654714122414589
Online_Training [44/200]: mean_loss=0.03709075786173344
Online_Training [45/200]: mean_loss=0.022186013171449304
Online_Training [46/200]: mean_loss=0.04031105153262615
Online_Training [47/200]: mean_loss=0.02182466513477266
Online_Training [48/200]: mean_loss=0.046980857849121094
Online_Training [49/200]: mean_loss=0.049511362332850695
Online_Training [50/200]: mean_loss=0.04064651299268007
Online_Training [51/200]: mean_loss=0.07195530366152525
Online_Training [52/200]: mean_loss=0.02545077702961862
Online_Training [53/200]: mean_loss=0.10789504088461399
Online_Training [54/200]: mean_loss=0.020545597886666656
Online_Training [55/200]: mean_loss=0.0438411277718842
Online_Training [56/200]: mean_loss=0.016564278746955097
Online_Training [57/200]: mean_loss=0.03564262203872204
Online_Training [58/200]: mean_loss=0.045573145151138306
Online_Training [59/200]: mean_loss=0.08358924090862274
Online_Training [60/200]: mean_loss=0.09910212643444538
Online_Training [61/200]: mean_loss=0.0496137673035264
Online_Training [62/200]: mean_loss=0.13169045001268387
Online_Training [63/200]: mean_loss=0.057230036705732346
Online_Training [64/200]: mean_loss=0.033875648165121675
Online_Training [65/200]: mean_loss=0.027812812011688948
Online_Training [66/200]: mean_loss=0.06358882831409574
Online_Training [67/200]: mean_loss=0.04362253472208977
Online_Training [68/200]: mean_loss=0.0531936502084136
Online_Training [69/200]: mean_loss=0.04999755695462227
Online_Training [70/200]: mean_loss=0.019571253331378102
Online_Training [71/200]: mean_loss=0.009841717663221061
Online_Training [72/200]: mean_loss=0.04190941248089075
Online_Training [73/200]: mean_loss=0.034238401567563415
Online_Training [74/200]: mean_loss=0.06379625434055924
Online_Training [75/200]: mean_loss=0.014792058616876602
Online_Training [76/200]: mean_loss=0.05527726840227842
Online_Training [77/200]: mean_loss=0.02331985766068101
Online_Training [78/200]: mean_loss=0.049761611968278885
Online_Training [79/200]: mean_loss=0.10654414631426334
Online_Training [80/200]: mean_loss=0.07881140895187855
Online_Training [81/200]: mean_loss=0.01623241521883756
Online_Training [82/200]: mean_loss=0.04849237110465765
Online_Training [83/200]: mean_loss=0.036528876051306725
Online_Training [84/200]: mean_loss=0.020957248052582145
Online_Training [85/200]: mean_loss=0.05747340666130185
Online_Training [86/200]: mean_loss=0.016773576266132295
Online_Training [87/200]: mean_loss=0.12566499132663012
Online_Training [88/200]: mean_loss=0.038180158007889986
Online_Training [89/200]: mean_loss=0.030290013877674937
Online_Training [90/200]: mean_loss=0.06181915896013379
Online_Training [91/200]: mean_loss=0.01872201939113438
Online_Training [92/200]: mean_loss=0.017286726739257574
Online_Training [93/200]: mean_loss=0.056962916161864996
Online_Training [94/200]: mean_loss=0.018101069377735257
Online_Training [95/200]: mean_loss=0.010587218566797674
Online_Training [96/200]: mean_loss=0.05456191347911954
Online_Training [97/200]: mean_loss=0.021622053114697337
Online_Training [98/200]: mean_loss=0.00993971707066521
Online_Training [99/200]: mean_loss=0.11166264954954386
Online_Training [100/200]: mean_loss=0.04974474245682359
Online_Training [101/200]: mean_loss=0.029735549120232463
Online_Training [102/200]: mean_loss=0.015328453271649778
Online_Training [103/200]: mean_loss=0.027933738427236676
Online_Training [104/200]: mean_loss=0.016806813539005816
Online_Training [105/200]: mean_loss=0.00818157836329192
Online_Training [106/200]: mean_loss=0.013953607762232423
Online_Training [107/200]: mean_loss=0.00979380082571879
Online_Training [108/200]: mean_loss=0.022784008644521236
Online_Training [109/200]: mean_loss=0.01001479709520936
Online_Training [110/200]: mean_loss=0.03147562243975699
Online_Training [111/200]: mean_loss=0.03575822548009455
Online_Training [112/200]: mean_loss=0.00517306721303612
Online_Training [113/200]: mean_loss=0.018297819420695305
Online_Training [114/200]: mean_loss=0.02662260178476572
Online_Training [115/200]: mean_loss=0.02882001013495028
Online_Training [116/200]: mean_loss=0.021098067285493016
Online_Training [117/200]: mean_loss=0.035807478008791804
Online_Training [118/200]: mean_loss=0.013046544627286494
Online_Training [119/200]: mean_loss=0.14509966038167477
Online_Training [120/200]: mean_loss=0.12482766155153513
Online_Training [121/200]: mean_loss=0.10644058417528868
Online_Training [122/200]: mean_loss=0.1576990783214569
Online_Training [123/200]: mean_loss=0.04603720991872251
Online_Training [124/200]: mean_loss=0.06572446506470442
Online_Training [125/200]: mean_loss=0.05696090729907155
Online_Training [126/200]: mean_loss=0.01865440676920116
Online_Training [127/200]: mean_loss=0.05164021113887429
Online_Training [128/200]: mean_loss=0.03745573200285435
Online_Training [129/200]: mean_loss=0.07867278158664703
Online_Training [130/200]: mean_loss=0.03604480158537626
Online_Training [131/200]: mean_loss=0.0996980108320713
Online_Training [132/200]: mean_loss=0.016423375462181866
Online_Training [133/200]: mean_loss=0.04018240957520902
Online_Training [134/200]: mean_loss=0.015793175320141017
Online_Training [135/200]: mean_loss=0.035141681553795934
Online_Training [136/200]: mean_loss=0.013029851368628442
Online_Training [137/200]: mean_loss=0.02318241586908698
Online_Training [138/200]: mean_loss=0.028753849677741528
Online_Training [139/200]: mean_loss=0.008333133708219975
Online_Training [140/200]: mean_loss=0.016547851730138063
Online_Training [141/200]: mean_loss=0.009036323754116893
Online_Training [142/200]: mean_loss=0.021532091544941068
Online_Training [143/200]: mean_loss=0.06175365811213851
Online_Training [144/200]: mean_loss=0.050393758341670036
Online_Training [145/200]: mean_loss=0.033674301113933325
Online_Training [146/200]: mean_loss=0.06076376512646675
Online_Training [147/200]: mean_loss=0.0398036097176373
Online_Training [148/200]: mean_loss=0.015778350760228932
Online_Training [149/200]: mean_loss=0.039403229020535946
Online_Training [150/200]: mean_loss=0.02458388963714242
Online_Training [151/200]: mean_loss=0.013176409876905382
Online_Training [152/200]: mean_loss=0.015859418781474233
Online_Training [153/200]: mean_loss=0.013367806212045252
Online_Training [154/200]: mean_loss=0.08448120672255754
Online_Training [155/200]: mean_loss=0.02171203913167119
Online_Training [156/200]: mean_loss=0.05749441357329488
Online_Training [157/200]: mean_loss=0.04519463609904051
Online_Training [158/200]: mean_loss=0.018660938600078225
Online_Training [159/200]: mean_loss=0.004743648634757847
Online_Training [160/200]: mean_loss=0.024772884557023644
Online_Training [161/200]: mean_loss=0.02676260587759316
Online_Training [162/200]: mean_loss=0.015987329534254968
Online_Training [163/200]: mean_loss=0.02849179646000266
Online_Training [164/200]: mean_loss=0.019784142379648983
Online_Training [165/200]: mean_loss=0.014111656579189003
Online_Training [166/200]: mean_loss=0.03800779627636075
Online_Training [167/200]: mean_loss=0.035923253279179335
Online_Training [168/200]: mean_loss=0.01830149325542152
Online_Training [169/200]: mean_loss=0.018970091361552477
Online_Training [170/200]: mean_loss=0.034882755018770695
Online_Training [171/200]: mean_loss=0.015426412457600236
Online_Training [172/200]: mean_loss=0.027884915005415678
Online_Training [173/200]: mean_loss=0.010213752626441419
Online_Training [174/200]: mean_loss=0.01859465939924121
Online_Training [175/200]: mean_loss=0.01843697566073388
Online_Training [176/200]: mean_loss=0.020177611149847507
Online_Training [177/200]: mean_loss=0.03721404494717717
Online_Training [178/200]: mean_loss=0.021728168707340956
Online_Training [179/200]: mean_loss=0.03867259109392762
Online_Training [180/200]: mean_loss=0.014111812692135572
Online_Training [181/200]: mean_loss=0.017574308905750513
Online_Training [182/200]: mean_loss=0.014115531463176012
Online_Training [183/200]: mean_loss=0.01442002400290221
Online_Training [184/200]: mean_loss=0.015493680257350206
Online_Training [185/200]: mean_loss=0.018929218058474362
Online_Training [186/200]: mean_loss=0.04988692980259657
Online_Training [187/200]: mean_loss=0.015777758439071476
Online_Training [188/200]: mean_loss=0.031781579833477736
Online_Training [189/200]: mean_loss=0.049793666228652
Online_Training [190/200]: mean_loss=0.011383047443814576
Online_Training [191/200]: mean_loss=0.024103828938677907
Online_Training [192/200]: mean_loss=0.03693155851215124
Online_Training [193/200]: mean_loss=0.04365531960502267
Online_Training [194/200]: mean_loss=0.028421754948794842
Online_Training [195/200]: mean_loss=0.03043316025286913
Online_Training [196/200]: mean_loss=0.022138883359730244
Online_Training [197/200]: mean_loss=0.02656226302497089
Online_Training [198/200]: mean_loss=0.008416979340836406
Online_Training [199/200]: mean_loss=0.01186742028221488
Online_Training [200/200]: mean_loss=0.014396533952094615
Q_Learning [1/300]: mean_loss=0.1386891845613718
Q_Learning [2/300]: mean_loss=0.1497445497661829
Q_Learning [3/300]: mean_loss=0.2779655270278454
Q_Learning [4/300]: mean_loss=0.1383659914135933
Q_Learning [5/300]: mean_loss=0.14062516763806343
Q_Learning [6/300]: mean_loss=0.11520449724048376
Q_Learning [7/300]: mean_loss=0.03626192221418023
Q_Learning [8/300]: mean_loss=0.0901311868801713
Q_Learning [9/300]: mean_loss=0.1226275758817792
Q_Learning [10/300]: mean_loss=0.08810629043728113
Q_Learning [11/300]: mean_loss=0.05878388276323676
Q_Learning [12/300]: mean_loss=0.03977879602462053
Q_Learning [13/300]: mean_loss=0.09639801923185587
Q_Learning [14/300]: mean_loss=0.14839691296219826
Q_Learning [15/300]: mean_loss=0.0852551069110632
Q_Learning [16/300]: mean_loss=0.076807064935565
Q_Learning [17/300]: mean_loss=0.058581402990967035
Q_Learning [18/300]: mean_loss=0.09216954093426466
Q_Learning [19/300]: mean_loss=0.0379923521541059
Q_Learning [20/300]: mean_loss=0.06016507092863321
Q_Learning [21/300]: mean_loss=0.033609868958592415
Q_Learning [22/300]: mean_loss=0.10138948913663626
Q_Learning [23/300]: mean_loss=0.03482343070209026
Q_Learning [24/300]: mean_loss=0.05260655423626304
Q_Learning [25/300]: mean_loss=0.06338182743638754
Q_Learning [26/300]: mean_loss=0.1542759370058775
Q_Learning [27/300]: mean_loss=0.10424620658159256
Q_Learning [28/300]: mean_loss=0.1026335870847106
Q_Learning [29/300]: mean_loss=0.04172274889424443
Q_Learning [30/300]: mean_loss=0.0990932947024703
Q_Learning [31/300]: mean_loss=0.1135997911915183
Q_Learning [32/300]: mean_loss=0.05867856368422508
Q_Learning [33/300]: mean_loss=0.023510202765464783
Q_Learning [34/300]: mean_loss=0.04083793517202139
Q_Learning [35/300]: mean_loss=0.048160846810787916
Q_Learning [36/300]: mean_loss=0.03938200371339917
Q_Learning [37/300]: mean_loss=0.08847056329250336
Q_Learning [38/300]: mean_loss=0.1424392145127058
Q_Learning [39/300]: mean_loss=0.1286744438111782
Q_Learning [40/300]: mean_loss=0.04801238002255559
Q_Learning [41/300]: mean_loss=0.048298731446266174
Q_Learning [42/300]: mean_loss=0.013975822017528117
Q_Learning [43/300]: mean_loss=0.11654714122414589
Q_Learning [44/300]: mean_loss=0.03709075786173344
Q_Learning [45/300]: mean_loss=0.022186013171449304
Q_Learning [46/300]: mean_loss=0.04031105153262615
Q_Learning [47/300]: mean_loss=0.02182466513477266
Q_Learning [48/300]: mean_loss=0.046980857849121094
Q_Learning [49/300]: mean_loss=0.049511362332850695
Q_Learning [50/300]: mean_loss=0.04064651299268007
Q_Learning [51/300]: mean_loss=0.07195530366152525
Q_Learning [52/300]: mean_loss=0.02545077702961862
Q_Learning [53/300]: mean_loss=0.10789504088461399
Q_Learning [54/300]: mean_loss=0.020545597886666656
Q_Learning [55/300]: mean_loss=0.0438411277718842
Q_Learning [56/300]: mean_loss=0.016564278746955097
Q_Learning [57/300]: mean_loss=0.03564262203872204
Q_Learning [58/300]: mean_loss=0.045573145151138306
Q_Learning [59/300]: mean_loss=0.08358924090862274
Q_Learning [60/300]: mean_loss=0.09910212643444538
Q_Learning [61/300]: mean_loss=0.0496137673035264
Q_Learning [62/300]: mean_loss=0.13169045001268387
Q_Learning [63/300]: mean_loss=0.057230036705732346
Q_Learning [64/300]: mean_loss=0.033875648165121675
Q_Learning [65/300]: mean_loss=0.027812812011688948
Q_Learning [66/300]: mean_loss=0.06358882831409574
Q_Learning [67/300]: mean_loss=0.04362253472208977
Q_Learning [68/300]: mean_loss=0.0531936502084136
Q_Learning [69/300]: mean_loss=0.04999755695462227
Q_Learning [70/300]: mean_loss=0.019571253331378102
Q_Learning [71/300]: mean_loss=0.009841717663221061
Q_Learning [72/300]: mean_loss=0.04190941248089075
Q_Learning [73/300]: mean_loss=0.034238401567563415
Q_Learning [74/300]: mean_loss=0.06379625434055924
Q_Learning [75/300]: mean_loss=0.014792058616876602
Q_Learning [76/300]: mean_loss=0.05527726840227842
Q_Learning [77/300]: mean_loss=0.02331985766068101
Q_Learning [78/300]: mean_loss=0.049761611968278885
Q_Learning [79/300]: mean_loss=0.10654414631426334
Q_Learning [80/300]: mean_loss=0.07881140895187855
Q_Learning [81/300]: mean_loss=0.01623241521883756
Q_Learning [82/300]: mean_loss=0.04849237110465765
Q_Learning [83/300]: mean_loss=0.036528876051306725
Q_Learning [84/300]: mean_loss=0.020957248052582145
Q_Learning [85/300]: mean_loss=0.05747340666130185
Q_Learning [86/300]: mean_loss=0.016773576266132295
Q_Learning [87/300]: mean_loss=0.12566499132663012
Q_Learning [88/300]: mean_loss=0.038180158007889986
Q_Learning [89/300]: mean_loss=0.030290013877674937
Q_Learning [90/300]: mean_loss=0.06181915896013379
Q_Learning [91/300]: mean_loss=0.01872201939113438
Q_Learning [92/300]: mean_loss=0.017286726739257574
Q_Learning [93/300]: mean_loss=0.056962916161864996
Q_Learning [94/300]: mean_loss=0.018101069377735257
Q_Learning [95/300]: mean_loss=0.010587218566797674
Q_Learning [96/300]: mean_loss=0.05456191347911954
Q_Learning [97/300]: mean_loss=0.021622053114697337
Q_Learning [98/300]: mean_loss=0.00993971707066521
Q_Learning [99/300]: mean_loss=0.11166264954954386
Q_Learning [100/300]: mean_loss=0.04974474245682359
Q_Learning [101/300]: mean_loss=0.029735549120232463
Q_Learning [102/300]: mean_loss=0.015328453271649778
Q_Learning [103/300]: mean_loss=0.027933738427236676
Q_Learning [104/300]: mean_loss=0.016806813539005816
Q_Learning [105/300]: mean_loss=0.00818157836329192
Q_Learning [106/300]: mean_loss=0.013953607762232423
Q_Learning [107/300]: mean_loss=0.00979380082571879
Q_Learning [108/300]: mean_loss=0.022784008644521236
Q_Learning [109/300]: mean_loss=0.01001479709520936
Q_Learning [110/300]: mean_loss=0.03147562243975699
Q_Learning [111/300]: mean_loss=0.03575822548009455
Q_Learning [112/300]: mean_loss=0.00517306721303612
Q_Learning [113/300]: mean_loss=0.018297819420695305
Q_Learning [114/300]: mean_loss=0.02662260178476572
Q_Learning [115/300]: mean_loss=0.02882001013495028
Q_Learning [116/300]: mean_loss=0.021098067285493016
Q_Learning [117/300]: mean_loss=0.035807478008791804
Q_Learning [118/300]: mean_loss=0.013046544627286494
Q_Learning [119/300]: mean_loss=0.14509966038167477
Q_Learning [120/300]: mean_loss=0.12482766155153513
Q_Learning [121/300]: mean_loss=0.10644058417528868
Q_Learning [122/300]: mean_loss=0.1576990783214569
Q_Learning [123/300]: mean_loss=0.04603720991872251
Q_Learning [124/300]: mean_loss=0.06572446506470442
Q_Learning [125/300]: mean_loss=0.05696090729907155
Q_Learning [126/300]: mean_loss=0.01865440676920116
Q_Learning [127/300]: mean_loss=0.05164021113887429
Q_Learning [128/300]: mean_loss=0.03745573200285435
Q_Learning [129/300]: mean_loss=0.07867278158664703
Q_Learning [130/300]: mean_loss=0.03604480158537626
Q_Learning [131/300]: mean_loss=0.0996980108320713
Q_Learning [132/300]: mean_loss=0.016423375462181866
Q_Learning [133/300]: mean_loss=0.04018240957520902
Q_Learning [134/300]: mean_loss=0.015793175320141017
Q_Learning [135/300]: mean_loss=0.035141681553795934
Q_Learning [136/300]: mean_loss=0.013029851368628442
Q_Learning [137/300]: mean_loss=0.02318241586908698
Q_Learning [138/300]: mean_loss=0.028753849677741528
Q_Learning [139/300]: mean_loss=0.008333133708219975
Q_Learning [140/300]: mean_loss=0.016547851730138063
Q_Learning [141/300]: mean_loss=0.009036323754116893
Q_Learning [142/300]: mean_loss=0.021532091544941068
Q_Learning [143/300]: mean_loss=0.06175365811213851
Q_Learning [144/300]: mean_loss=0.050393758341670036
Q_Learning [145/300]: mean_loss=0.033674301113933325
Q_Learning [146/300]: mean_loss=0.06076376512646675
Q_Learning [147/300]: mean_loss=0.0398036097176373
Q_Learning [148/300]: mean_loss=0.015778350760228932
Q_Learning [149/300]: mean_loss=0.039403229020535946
Q_Learning [150/300]: mean_loss=0.02458388963714242
Q_Learning [151/300]: mean_loss=0.013176409876905382
Q_Learning [152/300]: mean_loss=0.015859418781474233
Q_Learning [153/300]: mean_loss=0.013367806212045252
Q_Learning [154/300]: mean_loss=0.08448120672255754
Q_Learning [155/300]: mean_loss=0.02171203913167119
Q_Learning [156/300]: mean_loss=0.05749441357329488
Q_Learning [157/300]: mean_loss=0.04519463609904051
Q_Learning [158/300]: mean_loss=0.018660938600078225
Q_Learning [159/300]: mean_loss=0.004743648634757847
Q_Learning [160/300]: mean_loss=0.024772884557023644
Q_Learning [161/300]: mean_loss=0.02676260587759316
Q_Learning [162/300]: mean_loss=0.015987329534254968
Q_Learning [163/300]: mean_loss=0.02849179646000266
Q_Learning [164/300]: mean_loss=0.019784142379648983
Q_Learning [165/300]: mean_loss=0.014111656579189003
Q_Learning [166/300]: mean_loss=0.03800779627636075
Q_Learning [167/300]: mean_loss=0.035923253279179335
Q_Learning [168/300]: mean_loss=0.01830149325542152
Q_Learning [169/300]: mean_loss=0.018970091361552477
Q_Learning [170/300]: mean_loss=0.034882755018770695
Q_Learning [171/300]: mean_loss=0.015426412457600236
Q_Learning [172/300]: mean_loss=0.027884915005415678
Q_Learning [173/300]: mean_loss=0.010213752626441419
Q_Learning [174/300]: mean_loss=0.01859465939924121
Q_Learning [175/300]: mean_loss=0.01843697566073388
Q_Learning [176/300]: mean_loss=0.020177611149847507
Q_Learning [177/300]: mean_loss=0.03721404494717717
Q_Learning [178/300]: mean_loss=0.021728168707340956
Q_Learning [179/300]: mean_loss=0.03867259109392762
Q_Learning [180/300]: mean_loss=0.014111812692135572
Q_Learning [181/300]: mean_loss=0.017574308905750513
Q_Learning [182/300]: mean_loss=0.014115531463176012
Q_Learning [183/300]: mean_loss=0.01442002400290221
Q_Learning [184/300]: mean_loss=0.015493680257350206
Q_Learning [185/300]: mean_loss=0.018929218058474362
Q_Learning [186/300]: mean_loss=0.04988692980259657
Q_Learning [187/300]: mean_loss=0.015777758439071476
Q_Learning [188/300]: mean_loss=0.031781579833477736
Q_Learning [189/300]: mean_loss=0.049793666228652
Q_Learning [190/300]: mean_loss=0.011383047443814576
Q_Learning [191/300]: mean_loss=0.024103828938677907
Q_Learning [192/300]: mean_loss=0.03693155851215124
Q_Learning [193/300]: mean_loss=0.04365531960502267
Q_Learning [194/300]: mean_loss=0.028421754948794842
Q_Learning [195/300]: mean_loss=0.03043316025286913
Q_Learning [196/300]: mean_loss=0.022138883359730244
Q_Learning [197/300]: mean_loss=0.02656226302497089
Q_Learning [198/300]: mean_loss=0.008416979340836406
Q_Learning [199/300]: mean_loss=0.01186742028221488
Q_Learning [200/300]: mean_loss=0.014396533952094615
Q_Learning [201/300]: mean_loss=0.009395102213602513
Q_Learning [202/300]: mean_loss=0.08408194966614246
Q_Learning [203/300]: mean_loss=0.0981151545420289
Q_Learning [204/300]: mean_loss=0.02640453353524208
Q_Learning [205/300]: mean_loss=0.02243676665239036
Q_Learning [206/300]: mean_loss=0.022458327119238675
Q_Learning [207/300]: mean_loss=0.017068585264496505
Q_Learning [208/300]: mean_loss=0.0286341467872262
Q_Learning [209/300]: mean_loss=0.04472694173455238
Q_Learning [210/300]: mean_loss=0.022246364038437605
Q_Learning [211/300]: mean_loss=0.033944105030968785
Q_Learning [212/300]: mean_loss=0.037205059081315994
Q_Learning [213/300]: mean_loss=0.02192301070317626
Q_Learning [214/300]: mean_loss=0.029849497135728598
Q_Learning [215/300]: mean_loss=0.023067601025104523
Q_Learning [216/300]: mean_loss=0.021301435539498925
Q_Learning [217/300]: mean_loss=0.008794892812147737
Q_Learning [218/300]: mean_loss=0.02987674856558442
Q_Learning [219/300]: mean_loss=0.008267208642791957
Q_Learning [220/300]: mean_loss=0.05817837594076991
Q_Learning [221/300]: mean_loss=0.022515697171911597
Q_Learning [222/300]: mean_loss=0.1851967517286539
Q_Learning [223/300]: mean_loss=0.03838532278314233
Q_Learning [224/300]: mean_loss=0.02781803533434868
Q_Learning [225/300]: mean_loss=0.020419800654053688
Q_Learning [226/300]: mean_loss=0.025254461681470275
Q_Learning [227/300]: mean_loss=0.024651094572618604
Q_Learning [228/300]: mean_loss=0.020325302612036467
Q_Learning [229/300]: mean_loss=0.01930935773998499
Q_Learning [230/300]: mean_loss=0.00980365986470133
Q_Learning [231/300]: mean_loss=0.01770897104870528
Q_Learning [232/300]: mean_loss=0.032191756181418896
Q_Learning [233/300]: mean_loss=0.06001686677336693
Q_Learning [234/300]: mean_loss=0.035280770156532526
Q_Learning [235/300]: mean_loss=0.008577149535994977
Q_Learning [236/300]: mean_loss=0.033943420043215156
Q_Learning [237/300]: mean_loss=0.01680326065979898
Q_Learning [238/300]: mean_loss=0.00953257316723466
Q_Learning [239/300]: mean_loss=0.050825311336666346
Q_Learning [240/300]: mean_loss=0.055423158686608076
Q_Learning [241/300]: mean_loss=0.03443255368620157
Q_Learning [242/300]: mean_loss=0.04551733285188675
Q_Learning [243/300]: mean_loss=0.012109422590583563
Q_Learning [244/300]: mean_loss=0.03402655781246722
Q_Learning [245/300]: mean_loss=0.02135461801663041
Q_Learning [246/300]: mean_loss=0.021971319802105427
Q_Learning [247/300]: mean_loss=0.029493491631001234
Q_Learning [248/300]: mean_loss=0.029163254192098975
Q_Learning [249/300]: mean_loss=0.009914303896948695
Q_Learning [250/300]: mean_loss=0.018584421137347817
Q_Learning [251/300]: mean_loss=0.026401002891361713
Q_Learning [252/300]: mean_loss=0.012012292863801122
Q_Learning [253/300]: mean_loss=0.017018985119648278
Q_Learning [254/300]: mean_loss=0.03168247849680483
Q_Learning [255/300]: mean_loss=0.02829218516126275
Q_Learning [256/300]: mean_loss=0.016451410949230194
Q_Learning [257/300]: mean_loss=0.027429760433733463
Q_Learning [258/300]: mean_loss=0.1421706508845091
Q_Learning [259/300]: mean_loss=0.16711257584393024
Q_Learning [260/300]: mean_loss=0.021071489434689283
Q_Learning [261/300]: mean_loss=0.012234254856593907
Q_Learning [262/300]: mean_loss=0.0227665223646909
Q_Learning [263/300]: mean_loss=0.023034313693642616
Q_Learning [264/300]: mean_loss=0.021411796798929572
Q_Learning [265/300]: mean_loss=0.04548382665961981
Q_Learning [266/300]: mean_loss=0.007711211161222309
Q_Learning [267/300]: mean_loss=0.009732818230986595
Q_Learning [268/300]: mean_loss=0.06559722870588303
Q_Learning [269/300]: mean_loss=0.01164571475237608
Q_Learning [270/300]: mean_loss=0.011657348135486245
Q_Learning [271/300]: mean_loss=0.023953391471877694
Q_Learning [272/300]: mean_loss=0.020662235328927636
Q_Learning [273/300]: mean_loss=0.05181093513965607
Q_Learning [274/300]: mean_loss=0.02438007714226842
Q_Learning [275/300]: mean_loss=0.009383777971379459
Q_Learning [276/300]: mean_loss=0.016213067108765244
Q_Learning [277/300]: mean_loss=0.02161103906109929
Q_Learning [278/300]: mean_loss=0.01632111193612218
Q_Learning [279/300]: mean_loss=0.03677551029250026
Q_Learning [280/300]: mean_loss=0.007838179415557534
Q_Learning [281/300]: mean_loss=0.021208359161391854
Q_Learning [282/300]: mean_loss=0.023235892644152045
Q_Learning [283/300]: mean_loss=0.032301660161465406
Q_Learning [284/300]: mean_loss=0.013184267794713378
Q_Learning [285/300]: mean_loss=0.013549401424825191
Q_Learning [286/300]: mean_loss=0.016741057625040412
Q_Learning [287/300]: mean_loss=0.009607938234694302
Q_Learning [288/300]: mean_loss=0.03247421304695308
Q_Learning [289/300]: mean_loss=0.015447942190803587
Q_Learning [290/300]: mean_loss=0.039330053608864546
Q_Learning [291/300]: mean_loss=0.026218431070446968
Q_Learning [292/300]: mean_loss=0.006481335323769599
Q_Learning [293/300]: mean_loss=0.035273872315883636
Q_Learning [294/300]: mean_loss=0.012363480287604034
Q_Learning [295/300]: mean_loss=0.010113927302882075
Q_Learning [296/300]: mean_loss=0.036780684255063534
Q_Learning [297/300]: mean_loss=0.02366628870368004
Q_Learning [298/300]: mean_loss=0.02592868614010513
Q_Learning [299/300]: mean_loss=0.04883204959332943
Q_Learning [300/300]: mean_loss=0.013287918292917311
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.6889871 -1.928938 ]
[0, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 2, 2, 1, 2, 1, 0, 0, 1, 2, 0, 2, 0, 1, 1, 2, 0, 0, 2, 0, 1, 2, 2, 1, 0, 1, 1, 1, 1, 2, 2, 0, 2, 2, 0, 1, 0, 2, 0, 2, 1, 1, 1, 2, 1, 0, 2, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 2, 2, 0, 0, 2, 2, 1, 0, 1, 0, 2, 1, 1, 2, 0, 1, 0, 2, 1, 0, 2, 2, 0, 0, 1, 1, 2, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 0, 1, 0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 2, 0, 1, 0, 2, 2, 1, 1, 0, 1, 1, 2, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 0, 1, 2, 2, 1, 0, 2, 2, 1, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 2, 2, 2, 1, 2, 0, 0, 2, 2, 1, 0, 0, 2, 2, 0, 0, 2, 2, 2, 1, 0, 0, 2, 2, 1, 0, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 0]
[0, 0, 1, 2, 0, 1, 0, 0, 2, 2, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 0, 3, 2, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 2, 1, 2, 0, 0, 4, 1, 0, 1, 0, 2, 2, 1, 0, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 5, 2, 2, 2, 1, 0, 0, 1, 2, 3, 1, 2, 1, 0, 2, 0, 0, 4, 2, 0, 2, 0, 2, 1, 0, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 2, 0, 1, 1, 2, 0, 0, 0, 1, 4, 4, 1, 0, 2, 3, 6, 2, 4, 1, 1, 2, 0, 2, 4, 5, 2, 2, 1, 0, 4, 2, 7, 2, 1, 2, 2, 7, 7, 2, 0, 2, 0, 1, 7, 0, 3, 3, 0, 2, 2, 2, 2, 7, 2, 2, 2, 0, 1, 0, 2, 4, 1, 1, 2, 2, 0, 2, 4, 7, 0, 0, 2, 2, 2, 2, 7, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 1, 0, 2, 0, 2, 2, 4, 0, 0, 0, 7, 2, 0, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0, 2, 2, 1, 2, 2, 1, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 7, 2, 0, 1, 7, 2, 7, 2, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 2, 7, 4, 7, 2, 0, 2, 7, 1, 7, 2, 7, 0, 0, 1, 7, 2, 0, 0, 7, 7, 0, 0, 1, 1, 7, 2, 0, 0, 1, 7, 2, 0, 1, 0, 7, 7, 2, 7, 0, 7, 7, 7, 7, 0, 2, 7, 2, 7, 0]
Centroids: [[-1.4205648, -1.4532118], [-0.6787681, -0.5868058], [-0.98107463, 1.7996978]]
Centroids: [[-1.4243187, -1.4775848], [-1.1510705, 1.8588934], [-0.51961875, -0.5663168], [-2.7256691, -2.4745097], [-1.3765658, -0.1609627], [-3.1597452, 3.1164927], [-3.143711, 4.6828094], [-0.40708607, 1.4568089]]
Standard Derivations: [0.4166189, 0.40880185, 0.5226522]
Cluster Distances: [0.31515878, 2.3431933, 0.3151588, 1.4741206, 2.3431933, 1.4741206]
Minimal Cluster Distance: 0.31515878438949585
Contingency Matrix: 
[[75  0 11  4  2  0  0  0]
 [13  1 88  1  8  0  0  1]
 [ 0 63  0  0  1  2  1 29]]
[[75, 0, 11, 4, 2, 0, 0, 0], [13, 1, 88, 1, 8, 0, 0, 1], [0, 63, 0, 0, 1, 2, 1, 29]]
[[75, 0, 11, 4, 2, 0, 0, 0], [13, 1, 88, 1, 8, 0, 0, 1], [0, 63, 0, 0, 1, 2, 1, 29]]
[0, 1, 2, 3, 4, 5, 6, 7]
[[75, 0, -1, 4, 2, 0, 0, 0], [-1, -1, -1, -1, -1, -1, -1, -1], [0, 63, -1, 0, 1, 2, 1, 29]]
[[-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, 63, -1, 0, 1, 2, 1, 29]]
[[-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {1: 2, 0: 0, 2: 1}
New Contingency Matrix: 
[[75 11  0  4  2  0  0  0]
 [13 88  1  1  8  0  0  1]
 [ 0  0 63  0  1  2  1 29]]
New Clustered Label Sequence: [0, 2, 1, 3, 4, 5, 6, 7]
Diagonal_Elements: [75, 88, 63], Sum: 226
All_Elements: [75, 11, 0, 4, 2, 0, 0, 0, 13, 88, 1, 1, 8, 0, 0, 1, 0, 0, 63, 0, 1, 2, 1, 29], Sum: 300
Accuracy: 0.7533333333333333
