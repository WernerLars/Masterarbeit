Experiment_path: AE_Model_2/Random_Seeds_DV5//V5_2/Experiment_05_9_opt_temp
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise035.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise035.mat']
Variant_name: Variant_05_Online_Autoencoder_QLearning_opt_temp
Visualisation_Path: AE_Model_2/Random_Seeds_DV5//V5_2/Experiment_05_9_opt_temp/C_Easy1_noise035.mat/Variant_05_Online_Autoencoder_QLearning_opt_temp/2023_05_04-17_27_37
Punishment_Coefficient: 1.5
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x00000168FF224978>
Sampling rate: 24000.0
Raw: [-0.01748803 -0.01945498 -0.02011069 ... -0.20744344 -0.24709427
 -0.25077586]
Times: [    662    1043    2861 ... 1439172 1439620 1439793]
Cluster: [1 2 3 ... 3 3 2]
Number of different clusters:  3
Number of Spikes: 3534
First aligned Spike Frame: [ 0.43999329  0.4839933   0.52909327  0.52642944  0.43496308  0.26335103
  0.0652557  -0.09376199 -0.19786698 -0.28302287 -0.39101775 -0.51215993
 -0.44771361  0.07217119  0.76700554  0.91966677  0.38465989 -0.27458603
 -0.59813837 -0.63307973 -0.5997719  -0.60009658 -0.61792931 -0.61010846
 -0.56778745 -0.50195254 -0.4233035  -0.35404397 -0.29120082 -0.20969116
 -0.09592158  0.02951377  0.1302449   0.18393993  0.21396859  0.24968719
  0.25635801  0.17294061 -0.01474948 -0.24084414 -0.43698551 -0.59191978
 -0.72153644 -0.80995398 -0.82451785 -0.75320979 -0.64145157]
Cluster 0, Occurrences: 1208
Cluster 1, Occurrences: 1137
Cluster 2, Occurrences: 1189
Number of Clusters: 3
Online_Training [1/700]: mean_loss=0.1702192798256874
Online_Training [2/700]: mean_loss=0.3766608089208603
Online_Training [3/700]: mean_loss=0.8518696874380112
Online_Training [4/700]: mean_loss=0.37057576701045036
Online_Training [5/700]: mean_loss=0.3643522784113884
Online_Training [6/700]: mean_loss=0.10322635155171156
Online_Training [7/700]: mean_loss=0.1861818376928568
Online_Training [8/700]: mean_loss=0.1915421187877655
Online_Training [9/700]: mean_loss=0.12282749079167843
Online_Training [10/700]: mean_loss=0.7239716723561287
Online_Training [11/700]: mean_loss=0.7799168154597282
Online_Training [12/700]: mean_loss=0.280077937990427
Online_Training [13/700]: mean_loss=0.41566672921180725
Online_Training [14/700]: mean_loss=0.16395068541169167
Online_Training [15/700]: mean_loss=0.2796476446092129
Online_Training [16/700]: mean_loss=0.07564461883157492
Online_Training [17/700]: mean_loss=0.25613465905189514
Online_Training [18/700]: mean_loss=0.1330268196761608
Online_Training [19/700]: mean_loss=0.10091861058026552
Online_Training [20/700]: mean_loss=0.321791909635067
Online_Training [21/700]: mean_loss=0.18566813133656979
Online_Training [22/700]: mean_loss=0.5607266128063202
Online_Training [23/700]: mean_loss=0.13058459199965
Online_Training [24/700]: mean_loss=0.2838541269302368
Online_Training [25/700]: mean_loss=0.3640754148364067
Online_Training [26/700]: mean_loss=0.10638546291738749
Online_Training [27/700]: mean_loss=0.07362424675375223
Online_Training [28/700]: mean_loss=0.26410177908837795
Online_Training [29/700]: mean_loss=0.40834609419107437
Online_Training [30/700]: mean_loss=0.5773248933255672
Online_Training [31/700]: mean_loss=0.1945613343268633
Online_Training [32/700]: mean_loss=0.40230273082852364
Online_Training [33/700]: mean_loss=0.17775801196694374
Online_Training [34/700]: mean_loss=0.21006260439753532
Online_Training [35/700]: mean_loss=0.15520112589001656
Online_Training [36/700]: mean_loss=0.0711452029645443
Online_Training [37/700]: mean_loss=0.05203956691548228
Online_Training [38/700]: mean_loss=0.10126461181789637
Online_Training [39/700]: mean_loss=0.16940918378531933
Online_Training [40/700]: mean_loss=0.10847603902220726
Online_Training [41/700]: mean_loss=0.24753928370773792
Online_Training [42/700]: mean_loss=0.18909573554992676
Online_Training [43/700]: mean_loss=0.0546539006754756
Online_Training [44/700]: mean_loss=0.16883615776896477
Online_Training [45/700]: mean_loss=0.38587820529937744
Online_Training [46/700]: mean_loss=0.42726125195622444
Online_Training [47/700]: mean_loss=0.13191105984151363
Online_Training [48/700]: mean_loss=0.1525228563696146
Online_Training [49/700]: mean_loss=0.49119459465146065
Online_Training [50/700]: mean_loss=0.17720727063715458
Online_Training [51/700]: mean_loss=0.09133583307266235
Online_Training [52/700]: mean_loss=0.10593227855861187
Online_Training [53/700]: mean_loss=0.23016776517033577
Online_Training [54/700]: mean_loss=0.07140154158696532
Online_Training [55/700]: mean_loss=0.2942507863044739
Online_Training [56/700]: mean_loss=0.17303771898150444
Online_Training [57/700]: mean_loss=0.05038191704079509
Online_Training [58/700]: mean_loss=0.15277071669697762
Online_Training [59/700]: mean_loss=0.1393530536442995
Online_Training [60/700]: mean_loss=0.16023136675357819
Online_Training [61/700]: mean_loss=0.07978641148656607
Online_Training [62/700]: mean_loss=0.13779134768992662
Online_Training [63/700]: mean_loss=0.0988991679623723
Online_Training [64/700]: mean_loss=0.07978659402579069
Online_Training [65/700]: mean_loss=0.22970384173095226
Online_Training [66/700]: mean_loss=0.597384624183178
Online_Training [67/700]: mean_loss=0.1158746238797903
Online_Training [68/700]: mean_loss=0.14046227745711803
Online_Training [69/700]: mean_loss=0.1820903029292822
Online_Training [70/700]: mean_loss=0.04739735368639231
Online_Training [71/700]: mean_loss=0.06733658537268639
Online_Training [72/700]: mean_loss=0.1652860976755619
Online_Training [73/700]: mean_loss=0.03346686065196991
Online_Training [74/700]: mean_loss=0.08446741756051779
Online_Training [75/700]: mean_loss=0.23941602744162083
Online_Training [76/700]: mean_loss=0.08303989190608263
Online_Training [77/700]: mean_loss=0.12957481760531664
Online_Training [78/700]: mean_loss=0.13585107121616602
Online_Training [79/700]: mean_loss=0.2016361616551876
Online_Training [80/700]: mean_loss=0.07826352119445801
Online_Training [81/700]: mean_loss=0.038318598177284
Online_Training [82/700]: mean_loss=0.06190282246097922
Online_Training [83/700]: mean_loss=0.08167672995477915
Online_Training [84/700]: mean_loss=0.22917356714606285
Online_Training [85/700]: mean_loss=0.07995037641376257
Online_Training [86/700]: mean_loss=0.02837845543399453
Online_Training [87/700]: mean_loss=0.15863840375095606
Online_Training [88/700]: mean_loss=0.16593730449676514
Online_Training [89/700]: mean_loss=0.05936679197475314
Online_Training [90/700]: mean_loss=0.12339000031352043
Online_Training [91/700]: mean_loss=0.024174274876713753
Online_Training [92/700]: mean_loss=0.10876707546412945
Online_Training [93/700]: mean_loss=0.08752479497343302
Online_Training [94/700]: mean_loss=0.024399767396971583
Online_Training [95/700]: mean_loss=0.043712417129427195
Online_Training [96/700]: mean_loss=0.26163531094789505
Online_Training [97/700]: mean_loss=0.17220900766551495
Online_Training [98/700]: mean_loss=0.09573797602206469
Online_Training [99/700]: mean_loss=0.054605696350336075
Online_Training [100/700]: mean_loss=0.23186521977186203
Online_Training [101/700]: mean_loss=0.12017458211630583
Online_Training [102/700]: mean_loss=0.04240434430539608
Online_Training [103/700]: mean_loss=0.0294667633716017
Online_Training [104/700]: mean_loss=0.0903550935909152
Online_Training [105/700]: mean_loss=0.14708120934665203
Online_Training [106/700]: mean_loss=0.15383433364331722
Online_Training [107/700]: mean_loss=0.13594315014779568
Online_Training [108/700]: mean_loss=0.13009225949645042
Online_Training [109/700]: mean_loss=0.13788693957030773
Online_Training [110/700]: mean_loss=0.048453218303620815
Online_Training [111/700]: mean_loss=0.044280086643993855
Online_Training [112/700]: mean_loss=0.06196908559650183
Online_Training [113/700]: mean_loss=0.08632337022572756
Online_Training [114/700]: mean_loss=0.0690881134942174
Online_Training [115/700]: mean_loss=0.030725065153092146
Online_Training [116/700]: mean_loss=0.09381272085011005
Online_Training [117/700]: mean_loss=0.11812523379921913
Online_Training [118/700]: mean_loss=0.06952528096735477
Online_Training [119/700]: mean_loss=0.1107229357585311
Online_Training [120/700]: mean_loss=0.15871358662843704
Online_Training [121/700]: mean_loss=0.1015774691477418
Online_Training [122/700]: mean_loss=0.041213075164705515
Online_Training [123/700]: mean_loss=0.1255725659430027
Online_Training [124/700]: mean_loss=0.06840418698266149
Online_Training [125/700]: mean_loss=0.06864976370707154
Online_Training [126/700]: mean_loss=0.31080377846956253
Online_Training [127/700]: mean_loss=0.11633406579494476
Online_Training [128/700]: mean_loss=0.12396085448563099
Online_Training [129/700]: mean_loss=0.07861536834388971
Online_Training [130/700]: mean_loss=0.05859613511711359
Online_Training [131/700]: mean_loss=0.06471137423068285
Online_Training [132/700]: mean_loss=0.41758765652775764
Online_Training [133/700]: mean_loss=0.2574842479079962
Online_Training [134/700]: mean_loss=0.020856191404163837
Online_Training [135/700]: mean_loss=0.16275868378579617
Online_Training [136/700]: mean_loss=0.07445259019732475
Online_Training [137/700]: mean_loss=0.13464768417179585
Online_Training [138/700]: mean_loss=0.12444894574582577
Online_Training [139/700]: mean_loss=0.06992785539478064
Online_Training [140/700]: mean_loss=0.08398168720304966
Online_Training [141/700]: mean_loss=0.09248843230307102
Online_Training [142/700]: mean_loss=0.11688661389052868
Online_Training [143/700]: mean_loss=0.08660687319934368
Online_Training [144/700]: mean_loss=0.042757511138916016
Online_Training [145/700]: mean_loss=0.3688870668411255
Online_Training [146/700]: mean_loss=0.14651400037109852
Online_Training [147/700]: mean_loss=0.14737079665064812
Online_Training [148/700]: mean_loss=0.17155643738806248
Online_Training [149/700]: mean_loss=0.04027421306818724
Online_Training [150/700]: mean_loss=0.13806971721351147
Online_Training [151/700]: mean_loss=0.1167732672765851
Online_Training [152/700]: mean_loss=0.04632877791300416
Online_Training [153/700]: mean_loss=0.03853807272389531
Online_Training [154/700]: mean_loss=0.10529811587184668
Online_Training [155/700]: mean_loss=0.0814565122127533
Online_Training [156/700]: mean_loss=0.14529410563409328
Online_Training [157/700]: mean_loss=0.1120968097820878
Online_Training [158/700]: mean_loss=0.09211334865540266
Online_Training [159/700]: mean_loss=0.16351273097097874
Online_Training [160/700]: mean_loss=0.09660883154720068
Online_Training [161/700]: mean_loss=0.19839458167552948
Online_Training [162/700]: mean_loss=0.11225312482565641
Online_Training [163/700]: mean_loss=0.2228853702545166
Online_Training [164/700]: mean_loss=0.04584907507523894
Online_Training [165/700]: mean_loss=0.06920021306723356
Online_Training [166/700]: mean_loss=0.04417738039046526
Online_Training [167/700]: mean_loss=0.1036560358479619
Online_Training [168/700]: mean_loss=0.16336176730692387
Online_Training [169/700]: mean_loss=0.16984604112803936
Online_Training [170/700]: mean_loss=0.024213862838223577
Online_Training [171/700]: mean_loss=0.1336589828133583
Online_Training [172/700]: mean_loss=0.18754659965634346
Online_Training [173/700]: mean_loss=0.07187144551426172
Online_Training [174/700]: mean_loss=0.08447647280991077
Online_Training [175/700]: mean_loss=0.1773802414536476
Online_Training [176/700]: mean_loss=0.42318081483244896
Online_Training [177/700]: mean_loss=0.3647642619907856
Online_Training [178/700]: mean_loss=0.19120759703218937
Online_Training [179/700]: mean_loss=0.03415290196426213
Online_Training [180/700]: mean_loss=0.09120285045355558
Online_Training [181/700]: mean_loss=0.19129516929388046
Online_Training [182/700]: mean_loss=0.1161850718781352
Online_Training [183/700]: mean_loss=0.18210668116807938
Online_Training [184/700]: mean_loss=0.04749893303960562
Online_Training [185/700]: mean_loss=0.181205365806818
Online_Training [186/700]: mean_loss=0.21571926586329937
Online_Training [187/700]: mean_loss=0.0469715422950685
Online_Training [188/700]: mean_loss=0.14752162247896194
Online_Training [189/700]: mean_loss=0.07201091991737485
Online_Training [190/700]: mean_loss=0.01978988153859973
Online_Training [191/700]: mean_loss=0.13406546600162983
Online_Training [192/700]: mean_loss=0.13630475103855133
Online_Training [193/700]: mean_loss=0.022062535397708416
Online_Training [194/700]: mean_loss=0.1243321169167757
Online_Training [195/700]: mean_loss=0.12165709678083658
Online_Training [196/700]: mean_loss=0.06476414762437344
Online_Training [197/700]: mean_loss=0.13693172857165337
Online_Training [198/700]: mean_loss=0.1022942028939724
Online_Training [199/700]: mean_loss=0.1934740711003542
Online_Training [200/700]: mean_loss=0.04907504515722394
Online_Training [201/700]: mean_loss=0.1419877614825964
Online_Training [202/700]: mean_loss=0.11776436492800713
Online_Training [203/700]: mean_loss=0.06651771254837513
Online_Training [204/700]: mean_loss=0.07528531271964312
Online_Training [205/700]: mean_loss=0.1188453147187829
Online_Training [206/700]: mean_loss=0.052797308657318354
Online_Training [207/700]: mean_loss=0.1165594207122922
Online_Training [208/700]: mean_loss=0.283480629324913
Online_Training [209/700]: mean_loss=0.1653603632003069
Online_Training [210/700]: mean_loss=0.16137346252799034
Online_Training [211/700]: mean_loss=0.1280064471065998
Online_Training [212/700]: mean_loss=0.036766740027815104
Online_Training [213/700]: mean_loss=0.0728113716468215
Online_Training [214/700]: mean_loss=0.057755169458687305
Online_Training [215/700]: mean_loss=0.03332203906029463
Online_Training [216/700]: mean_loss=0.09366583079099655
Online_Training [217/700]: mean_loss=0.052546396385878325
Online_Training [218/700]: mean_loss=0.0834036460146308
Online_Training [219/700]: mean_loss=0.08294279221445322
Online_Training [220/700]: mean_loss=0.045747780706733465
Online_Training [221/700]: mean_loss=0.16297000460326672
Online_Training [222/700]: mean_loss=0.03728012088686228
Online_Training [223/700]: mean_loss=0.08437469881027937
Online_Training [224/700]: mean_loss=0.11563925165683031
Online_Training [225/700]: mean_loss=0.07704083621501923
Online_Training [226/700]: mean_loss=0.09044161438941956
Online_Training [227/700]: mean_loss=0.05957692675292492
Online_Training [228/700]: mean_loss=0.24122988805174828
Online_Training [229/700]: mean_loss=0.05410928465425968
Online_Training [230/700]: mean_loss=0.09950776863843203
Online_Training [231/700]: mean_loss=0.30089641362428665
Online_Training [232/700]: mean_loss=0.043302048929035664
Online_Training [233/700]: mean_loss=0.11497026402503252
Online_Training [234/700]: mean_loss=0.043897376861423254
Online_Training [235/700]: mean_loss=0.08479412645101547
Online_Training [236/700]: mean_loss=0.056514328345656395
Online_Training [237/700]: mean_loss=0.033430883660912514
Online_Training [238/700]: mean_loss=0.09073326364159584
Online_Training [239/700]: mean_loss=0.1555920820683241
Online_Training [240/700]: mean_loss=0.07016631681472063
Online_Training [241/700]: mean_loss=0.043360400944948196
Online_Training [242/700]: mean_loss=0.09534095227718353
Online_Training [243/700]: mean_loss=0.07946550566703081
Online_Training [244/700]: mean_loss=0.07206716947257519
Online_Training [245/700]: mean_loss=0.07080537732690573
Online_Training [246/700]: mean_loss=0.06669808551669121
Online_Training [247/700]: mean_loss=0.0765433693304658
Online_Training [248/700]: mean_loss=0.11709811259061098
Online_Training [249/700]: mean_loss=0.10274544171988964
Online_Training [250/700]: mean_loss=0.07445006258785725
Online_Training [251/700]: mean_loss=0.12684893887490034
Online_Training [252/700]: mean_loss=0.02424497320316732
Online_Training [253/700]: mean_loss=0.04436150472611189
Online_Training [254/700]: mean_loss=0.15605368092656136
Online_Training [255/700]: mean_loss=0.40856415033340454
Online_Training [256/700]: mean_loss=0.4786125533282757
Online_Training [257/700]: mean_loss=0.06693207751959562
Online_Training [258/700]: mean_loss=0.07741866167634726
Online_Training [259/700]: mean_loss=0.06274513807147741
Online_Training [260/700]: mean_loss=0.061418641824275255
Online_Training [261/700]: mean_loss=0.08399138785898685
Online_Training [262/700]: mean_loss=0.07081423792988062
Online_Training [263/700]: mean_loss=0.08546697348356247
Online_Training [264/700]: mean_loss=0.026963252807036042
Online_Training [265/700]: mean_loss=0.011187508702278137
Online_Training [266/700]: mean_loss=0.10989234875887632
Online_Training [267/700]: mean_loss=0.09102137759327888
Online_Training [268/700]: mean_loss=0.1561561245471239
Online_Training [269/700]: mean_loss=0.028962713200598955
Online_Training [270/700]: mean_loss=0.11398131586611271
Online_Training [271/700]: mean_loss=0.16730761341750622
Online_Training [272/700]: mean_loss=0.09468937665224075
Online_Training [273/700]: mean_loss=0.5192789696156979
Online_Training [274/700]: mean_loss=0.08918050210922956
Online_Training [275/700]: mean_loss=0.17196263186633587
Online_Training [276/700]: mean_loss=0.1339751621708274
Online_Training [277/700]: mean_loss=0.3413644880056381
Online_Training [278/700]: mean_loss=0.24998206458985806
Online_Training [279/700]: mean_loss=0.04213948315009475
Online_Training [280/700]: mean_loss=0.08758008386939764
Online_Training [281/700]: mean_loss=0.04204852506518364
Online_Training [282/700]: mean_loss=0.08333582151681185
Online_Training [283/700]: mean_loss=0.11987725272774696
Online_Training [284/700]: mean_loss=0.13428237102925777
Online_Training [285/700]: mean_loss=0.10070779733359814
Online_Training [286/700]: mean_loss=0.23230277560651302
Online_Training [287/700]: mean_loss=0.11077667400240898
Online_Training [288/700]: mean_loss=0.11834818962961435
Online_Training [289/700]: mean_loss=0.08944840729236603
Online_Training [290/700]: mean_loss=0.049494000151753426
Online_Training [291/700]: mean_loss=0.07970694545656443
Online_Training [292/700]: mean_loss=0.0666037118062377
Online_Training [293/700]: mean_loss=0.1645168773829937
Online_Training [294/700]: mean_loss=0.03806707123294473
Online_Training [295/700]: mean_loss=0.056531712878495455
Online_Training [296/700]: mean_loss=0.07110460475087166
Online_Training [297/700]: mean_loss=0.036038252990692854
Online_Training [298/700]: mean_loss=0.3740050680935383
Online_Training [299/700]: mean_loss=0.28435878828167915
Online_Training [300/700]: mean_loss=0.056093175895512104
Online_Training [301/700]: mean_loss=0.2560941241681576
Online_Training [302/700]: mean_loss=0.24734809063374996
Online_Training [303/700]: mean_loss=0.0819966197013855
Online_Training [304/700]: mean_loss=0.06507794419303536
Online_Training [305/700]: mean_loss=0.07507739588618279
Online_Training [306/700]: mean_loss=0.049082701094448566
Online_Training [307/700]: mean_loss=0.015806708834134042
Online_Training [308/700]: mean_loss=0.1104985075071454
Online_Training [309/700]: mean_loss=0.06633210647851229
Online_Training [310/700]: mean_loss=0.09324351977556944
Online_Training [311/700]: mean_loss=0.08768839575350285
Online_Training [312/700]: mean_loss=0.06130369380116463
Online_Training [313/700]: mean_loss=0.20722444541752338
Online_Training [314/700]: mean_loss=0.062026767525821924
Online_Training [315/700]: mean_loss=0.2739434614777565
Online_Training [316/700]: mean_loss=0.12050250545144081
Online_Training [317/700]: mean_loss=0.038608468137681484
Online_Training [318/700]: mean_loss=0.08983786683529615
Online_Training [319/700]: mean_loss=0.1427093669772148
Online_Training [320/700]: mean_loss=0.17734555155038834
Online_Training [321/700]: mean_loss=0.03836933756247163
Online_Training [322/700]: mean_loss=0.08569868188351393
Online_Training [323/700]: mean_loss=0.18030774407088757
Online_Training [324/700]: mean_loss=0.184352183714509
Online_Training [325/700]: mean_loss=0.07813070993870497
Online_Training [326/700]: mean_loss=0.06174451531842351
Online_Training [327/700]: mean_loss=0.05185200786218047
Online_Training [328/700]: mean_loss=0.2947794087231159
Online_Training [329/700]: mean_loss=0.05115851340815425
Online_Training [330/700]: mean_loss=0.371373787522316
Online_Training [331/700]: mean_loss=0.048907396383583546
Online_Training [332/700]: mean_loss=0.21238590963184834
Online_Training [333/700]: mean_loss=0.11853459663689137
Online_Training [334/700]: mean_loss=0.37354786321520805
Online_Training [335/700]: mean_loss=0.09986130334436893
Online_Training [336/700]: mean_loss=0.03133055963553488
Online_Training [337/700]: mean_loss=0.10650679375976324
Online_Training [338/700]: mean_loss=0.08173785451799631
Online_Training [339/700]: mean_loss=0.20277301780879498
Online_Training [340/700]: mean_loss=0.028778005624189973
Online_Training [341/700]: mean_loss=0.07634383160620928
Online_Training [342/700]: mean_loss=0.07616948056966066
Online_Training [343/700]: mean_loss=0.1466371826827526
Online_Training [344/700]: mean_loss=0.6611252129077911
Online_Training [345/700]: mean_loss=0.14653589949011803
Online_Training [346/700]: mean_loss=0.06690987013280392
Online_Training [347/700]: mean_loss=0.1842067800462246
Online_Training [348/700]: mean_loss=0.021777214249596
Online_Training [349/700]: mean_loss=0.11932008992880583
Online_Training [350/700]: mean_loss=0.06899377331137657
Online_Training [351/700]: mean_loss=0.09186384361237288
Online_Training [352/700]: mean_loss=0.03242220147512853
Online_Training [353/700]: mean_loss=0.19996723160147667
Online_Training [354/700]: mean_loss=0.14045316725969315
Online_Training [355/700]: mean_loss=0.07757540978491306
Online_Training [356/700]: mean_loss=0.14930490870028734
Online_Training [357/700]: mean_loss=0.12931943219155073
Online_Training [358/700]: mean_loss=0.17696655355393887
Online_Training [359/700]: mean_loss=0.05966708296909928
Online_Training [360/700]: mean_loss=0.201083580031991
Online_Training [361/700]: mean_loss=0.08476590737700462
Online_Training [362/700]: mean_loss=0.1390912402421236
Online_Training [363/700]: mean_loss=0.06484485138207674
Online_Training [364/700]: mean_loss=0.04180917330086231
Online_Training [365/700]: mean_loss=0.06218185322359204
Online_Training [366/700]: mean_loss=0.05211044941097498
Online_Training [367/700]: mean_loss=0.09086786024272442
Online_Training [368/700]: mean_loss=0.2354721836745739
Online_Training [369/700]: mean_loss=0.11183131486177444
Online_Training [370/700]: mean_loss=0.0784640172496438
Online_Training [371/700]: mean_loss=0.08364279940724373
Online_Training [372/700]: mean_loss=0.03486123261973262
Online_Training [373/700]: mean_loss=0.04485285049304366
Online_Training [374/700]: mean_loss=0.1381229581311345
Online_Training [375/700]: mean_loss=0.11495498940348625
Online_Training [376/700]: mean_loss=0.07006129249930382
Online_Training [377/700]: mean_loss=0.012233444256708026
Online_Training [378/700]: mean_loss=0.07423525862395763
Online_Training [379/700]: mean_loss=0.10546001512557268
Online_Training [380/700]: mean_loss=0.04724430572241545
Online_Training [381/700]: mean_loss=0.05592039506882429
Online_Training [382/700]: mean_loss=0.1290286099538207
Online_Training [383/700]: mean_loss=0.08752563688904047
Online_Training [384/700]: mean_loss=0.10988884791731834
Online_Training [385/700]: mean_loss=0.1160976579412818
Online_Training [386/700]: mean_loss=0.0844944091513753
Online_Training [387/700]: mean_loss=0.09316501207649708
Online_Training [388/700]: mean_loss=0.10056627821177244
Online_Training [389/700]: mean_loss=0.07357248850166798
Online_Training [390/700]: mean_loss=0.11168249044567347
Online_Training [391/700]: mean_loss=0.06205117842182517
Online_Training [392/700]: mean_loss=0.06378843309357762
Online_Training [393/700]: mean_loss=0.10944746620953083
Online_Training [394/700]: mean_loss=0.1676790863275528
Online_Training [395/700]: mean_loss=0.10833569522947073
Online_Training [396/700]: mean_loss=0.08638943638652563
Online_Training [397/700]: mean_loss=0.02966649504378438
Online_Training [398/700]: mean_loss=0.2496338989585638
Online_Training [399/700]: mean_loss=0.06096090655773878
Online_Training [400/700]: mean_loss=0.15928620100021362
Online_Training [401/700]: mean_loss=0.13549409061670303
Online_Training [402/700]: mean_loss=0.08707466535270214
Online_Training [403/700]: mean_loss=0.06357192201539874
Online_Training [404/700]: mean_loss=0.09388730395585299
Online_Training [405/700]: mean_loss=0.02665592171251774
Online_Training [406/700]: mean_loss=0.2192236240953207
Online_Training [407/700]: mean_loss=0.179587934166193
Online_Training [408/700]: mean_loss=0.09355091769248247
Online_Training [409/700]: mean_loss=0.1344369277358055
Online_Training [410/700]: mean_loss=0.11765151005238295
Online_Training [411/700]: mean_loss=0.06331325555220246
Online_Training [412/700]: mean_loss=0.05564522324129939
Online_Training [413/700]: mean_loss=0.1702921763062477
Online_Training [414/700]: mean_loss=0.2800744120031595
Online_Training [415/700]: mean_loss=0.13358568958938122
Online_Training [416/700]: mean_loss=0.09169102925807238
Online_Training [417/700]: mean_loss=0.23280608840286732
Online_Training [418/700]: mean_loss=0.030581967905163765
Online_Training [419/700]: mean_loss=0.05869458056986332
Online_Training [420/700]: mean_loss=0.06628526421263814
Online_Training [421/700]: mean_loss=0.04292541602626443
Online_Training [422/700]: mean_loss=0.043927887454628944
Online_Training [423/700]: mean_loss=0.14255043491721153
Online_Training [424/700]: mean_loss=0.1042687464505434
Online_Training [425/700]: mean_loss=0.13318280689418316
Online_Training [426/700]: mean_loss=0.04499456798657775
Online_Training [427/700]: mean_loss=0.05213722726330161
Online_Training [428/700]: mean_loss=0.12413406185805798
Online_Training [429/700]: mean_loss=0.08765030093491077
Online_Training [430/700]: mean_loss=0.051044038496911526
Online_Training [431/700]: mean_loss=0.1739714164286852
Online_Training [432/700]: mean_loss=0.24288245290517807
Online_Training [433/700]: mean_loss=0.06730556348338723
Online_Training [434/700]: mean_loss=0.14523187279701233
Online_Training [435/700]: mean_loss=0.05365740181878209
Online_Training [436/700]: mean_loss=0.05821162695065141
Online_Training [437/700]: mean_loss=0.07334434054791927
Online_Training [438/700]: mean_loss=0.0589819080196321
Online_Training [439/700]: mean_loss=0.03495648503303528
Online_Training [440/700]: mean_loss=0.030618059216067195
Online_Training [441/700]: mean_loss=0.1533443797379732
Online_Training [442/700]: mean_loss=0.14915525261312723
Online_Training [443/700]: mean_loss=0.10973452776670456
Online_Training [444/700]: mean_loss=0.047937789000570774
Online_Training [445/700]: mean_loss=0.08725960832089186
Online_Training [446/700]: mean_loss=0.02203898411244154
Online_Training [447/700]: mean_loss=0.08220554050058126
Online_Training [448/700]: mean_loss=0.07317156763747334
Online_Training [449/700]: mean_loss=0.22983805648982525
Online_Training [450/700]: mean_loss=0.15663552097976208
Online_Training [451/700]: mean_loss=0.22944236174225807
Online_Training [452/700]: mean_loss=0.07789945229887962
Online_Training [453/700]: mean_loss=0.08966257702559233
Online_Training [454/700]: mean_loss=0.1351245902478695
Online_Training [455/700]: mean_loss=0.11435254290699959
Online_Training [456/700]: mean_loss=0.2871533930301666
Online_Training [457/700]: mean_loss=0.5943220928311348
Online_Training [458/700]: mean_loss=0.06546600302681327
Online_Training [459/700]: mean_loss=0.07082028593868017
Online_Training [460/700]: mean_loss=0.09183937683701515
Online_Training [461/700]: mean_loss=0.016613708343356848
Online_Training [462/700]: mean_loss=0.03987623518332839
Online_Training [463/700]: mean_loss=0.23384319990873337
Online_Training [464/700]: mean_loss=0.12742604408413172
Online_Training [465/700]: mean_loss=0.04224703833460808
Online_Training [466/700]: mean_loss=0.05942585552111268
Online_Training [467/700]: mean_loss=0.08548009488731623
Online_Training [468/700]: mean_loss=0.06960303615778685
Online_Training [469/700]: mean_loss=0.04362974828109145
Online_Training [470/700]: mean_loss=0.11657525319606066
Online_Training [471/700]: mean_loss=0.10465782880783081
Online_Training [472/700]: mean_loss=0.05310203041881323
Online_Training [473/700]: mean_loss=0.19508583284914494
Online_Training [474/700]: mean_loss=0.16052638925611973
Online_Training [475/700]: mean_loss=0.054085271898657084
Online_Training [476/700]: mean_loss=0.35870207846164703
Online_Training [477/700]: mean_loss=0.36981528624892235
Online_Training [478/700]: mean_loss=0.06917207222431898
Online_Training [479/700]: mean_loss=0.2024815995246172
Online_Training [480/700]: mean_loss=0.11837237980216742
Online_Training [481/700]: mean_loss=0.1593023631721735
Online_Training [482/700]: mean_loss=0.04117618780583143
Online_Training [483/700]: mean_loss=0.10351521801203489
Online_Training [484/700]: mean_loss=0.03661760943941772
Online_Training [485/700]: mean_loss=0.27459731325507164
Online_Training [486/700]: mean_loss=0.022230255883187056
Online_Training [487/700]: mean_loss=0.043125655967742205
Online_Training [488/700]: mean_loss=0.1133876210078597
Online_Training [489/700]: mean_loss=0.08057066518813372
Online_Training [490/700]: mean_loss=0.0809718668460846
Online_Training [491/700]: mean_loss=0.0730103887617588
Online_Training [492/700]: mean_loss=0.08776631858199835
Online_Training [493/700]: mean_loss=0.20629730634391308
Online_Training [494/700]: mean_loss=0.20760197937488556
Online_Training [495/700]: mean_loss=0.11590699665248394
Online_Training [496/700]: mean_loss=0.04320303164422512
Online_Training [497/700]: mean_loss=0.05059522297233343
Online_Training [498/700]: mean_loss=0.15514313988387585
Online_Training [499/700]: mean_loss=0.16405889950692654
Online_Training [500/700]: mean_loss=0.10699053667485714
Online_Training [501/700]: mean_loss=0.09588588774204254
Online_Training [502/700]: mean_loss=0.1662498377263546
Online_Training [503/700]: mean_loss=0.05106968665495515
Online_Training [504/700]: mean_loss=0.16546990908682346
Online_Training [505/700]: mean_loss=0.12873853463679552
Online_Training [506/700]: mean_loss=0.021890150848776102
Online_Training [507/700]: mean_loss=0.10362050775438547
Online_Training [508/700]: mean_loss=0.04742768174037337
Online_Training [509/700]: mean_loss=0.06293470039963722
Online_Training [510/700]: mean_loss=0.058866376522928476
Online_Training [511/700]: mean_loss=0.031654485734179616
Online_Training [512/700]: mean_loss=0.20924424566328526
Online_Training [513/700]: mean_loss=0.05384747264906764
Online_Training [514/700]: mean_loss=0.0928107863292098
Online_Training [515/700]: mean_loss=0.10011439770460129
Online_Training [516/700]: mean_loss=0.03711248142644763
Online_Training [517/700]: mean_loss=0.21669937297701836
Online_Training [518/700]: mean_loss=0.09081554878503084
Online_Training [519/700]: mean_loss=0.03016249812208116
Online_Training [520/700]: mean_loss=0.5430058687925339
Online_Training [521/700]: mean_loss=0.054599402006715536
Online_Training [522/700]: mean_loss=0.02612763736397028
Online_Training [523/700]: mean_loss=0.09671230334788561
Online_Training [524/700]: mean_loss=0.08077855873852968
Online_Training [525/700]: mean_loss=0.0720578613691032
Online_Training [526/700]: mean_loss=0.04818594502285123
Online_Training [527/700]: mean_loss=0.17770797666162252
Online_Training [528/700]: mean_loss=0.1414617821574211
Online_Training [529/700]: mean_loss=0.07001551054418087
Online_Training [530/700]: mean_loss=0.12424942385405302
Online_Training [531/700]: mean_loss=0.14281444437801838
Online_Training [532/700]: mean_loss=0.17713209427893162
Online_Training [533/700]: mean_loss=0.08770541474223137
Online_Training [534/700]: mean_loss=0.03995876293629408
Online_Training [535/700]: mean_loss=0.13119944091886282
Online_Training [536/700]: mean_loss=0.09287853166460991
Online_Training [537/700]: mean_loss=0.04063316248357296
Online_Training [538/700]: mean_loss=0.10353194829076529
Online_Training [539/700]: mean_loss=0.10231006424874067
Online_Training [540/700]: mean_loss=0.1376529037952423
Online_Training [541/700]: mean_loss=0.13606488890945911
Online_Training [542/700]: mean_loss=0.0714992880821228
Online_Training [543/700]: mean_loss=0.04367745900526643
Online_Training [544/700]: mean_loss=0.03629050916060805
Online_Training [545/700]: mean_loss=0.0863421717658639
Online_Training [546/700]: mean_loss=0.058944066520780325
Online_Training [547/700]: mean_loss=0.1472182311117649
Online_Training [548/700]: mean_loss=0.04541618563234806
Online_Training [549/700]: mean_loss=0.07719018869102001
Online_Training [550/700]: mean_loss=0.13053965475410223
Online_Training [551/700]: mean_loss=0.07838702155277133
Online_Training [552/700]: mean_loss=0.06675523333251476
Online_Training [553/700]: mean_loss=0.058136607985943556
Online_Training [554/700]: mean_loss=0.22758007235825062
Online_Training [555/700]: mean_loss=0.0743801910430193
Online_Training [556/700]: mean_loss=0.051807919051498175
Online_Training [557/700]: mean_loss=0.09846463985741138
Online_Training [558/700]: mean_loss=0.09743573423475027
Online_Training [559/700]: mean_loss=0.13260234706103802
Online_Training [560/700]: mean_loss=0.20268476381897926
Online_Training [561/700]: mean_loss=0.05268570641055703
Online_Training [562/700]: mean_loss=0.07746219076216221
Online_Training [563/700]: mean_loss=0.16059975139796734
Online_Training [564/700]: mean_loss=0.11326333228498697
Online_Training [565/700]: mean_loss=0.015797598753124475
Online_Training [566/700]: mean_loss=0.09772484563291073
Online_Training [567/700]: mean_loss=0.025037718703970313
Online_Training [568/700]: mean_loss=0.04721851786598563
Online_Training [569/700]: mean_loss=0.03875320078805089
Online_Training [570/700]: mean_loss=0.07797440234571695
Online_Training [571/700]: mean_loss=0.056162194814532995
Online_Training [572/700]: mean_loss=0.04344314523041248
Online_Training [573/700]: mean_loss=0.13028991594910622
Online_Training [574/700]: mean_loss=0.11843607667833567
Online_Training [575/700]: mean_loss=0.023179646115750074
Online_Training [576/700]: mean_loss=0.09485739003866911
Online_Training [577/700]: mean_loss=0.04508755123242736
Online_Training [578/700]: mean_loss=0.04169188253581524
Online_Training [579/700]: mean_loss=0.07570380996912718
Online_Training [580/700]: mean_loss=0.1830600779503584
Online_Training [581/700]: mean_loss=0.04647856578230858
Online_Training [582/700]: mean_loss=0.08056649193167686
Online_Training [583/700]: mean_loss=0.07529437076300383
Online_Training [584/700]: mean_loss=0.16777084581553936
Online_Training [585/700]: mean_loss=0.11162127554416656
Online_Training [586/700]: mean_loss=0.06249970430508256
Online_Training [587/700]: mean_loss=0.10012721922248602
Online_Training [588/700]: mean_loss=0.07568364962935448
Online_Training [589/700]: mean_loss=0.12115091644227505
Online_Training [590/700]: mean_loss=0.1196189010515809
Online_Training [591/700]: mean_loss=0.04628532426431775
Online_Training [592/700]: mean_loss=0.08629989065229893
Online_Training [593/700]: mean_loss=0.1557879988104105
Online_Training [594/700]: mean_loss=0.10989649128168821
Online_Training [595/700]: mean_loss=0.11577603686600924
Online_Training [596/700]: mean_loss=0.04854613868519664
Online_Training [597/700]: mean_loss=0.057145466562360525
Online_Training [598/700]: mean_loss=0.21008907817304134
Online_Training [599/700]: mean_loss=0.03324171667918563
Online_Training [600/700]: mean_loss=0.08726634364575148
Online_Training [601/700]: mean_loss=0.05569407623261213
Online_Training [602/700]: mean_loss=0.16017478704452515
Online_Training [603/700]: mean_loss=0.06643545674160123
Online_Training [604/700]: mean_loss=0.03392043011263013
Online_Training [605/700]: mean_loss=0.06783326715230942
Online_Training [606/700]: mean_loss=0.05924054095521569
Online_Training [607/700]: mean_loss=0.13190590869635344
Online_Training [608/700]: mean_loss=0.039032144006341696
Online_Training [609/700]: mean_loss=0.2122226245701313
Online_Training [610/700]: mean_loss=0.13149322289973497
Online_Training [611/700]: mean_loss=0.1856447458267212
Online_Training [612/700]: mean_loss=0.10196914151310921
Online_Training [613/700]: mean_loss=0.047559116035699844
Online_Training [614/700]: mean_loss=0.11625103000551462
Online_Training [615/700]: mean_loss=0.03573206253349781
Online_Training [616/700]: mean_loss=0.05129098007455468
Online_Training [617/700]: mean_loss=0.0901289377361536
Online_Training [618/700]: mean_loss=0.429244801402092
Online_Training [619/700]: mean_loss=0.16150428354740143
Online_Training [620/700]: mean_loss=0.043260529171675444
Online_Training [621/700]: mean_loss=0.037648094817996025
Online_Training [622/700]: mean_loss=0.28584595024585724
Online_Training [623/700]: mean_loss=0.11050874460488558
Online_Training [624/700]: mean_loss=0.04420328908599913
Online_Training [625/700]: mean_loss=0.09964195918291807
Online_Training [626/700]: mean_loss=0.1336307218298316
Online_Training [627/700]: mean_loss=0.06637449190020561
Online_Training [628/700]: mean_loss=0.03733558300882578
Online_Training [629/700]: mean_loss=0.11701341066509485
Online_Training [630/700]: mean_loss=0.05784015078097582
Online_Training [631/700]: mean_loss=0.07045950880274177
Online_Training [632/700]: mean_loss=0.04172163922339678
Online_Training [633/700]: mean_loss=0.05333692114800215
Online_Training [634/700]: mean_loss=0.07379068713635206
Online_Training [635/700]: mean_loss=0.03727188007906079
Online_Training [636/700]: mean_loss=0.05031437985599041
Online_Training [637/700]: mean_loss=0.066912398673594
Online_Training [638/700]: mean_loss=0.11602745112031698
Online_Training [639/700]: mean_loss=0.12571740709245205
Online_Training [640/700]: mean_loss=0.10160535667091608
Online_Training [641/700]: mean_loss=0.21553744561970234
Online_Training [642/700]: mean_loss=0.014192217262461782
Online_Training [643/700]: mean_loss=0.10497415065765381
Online_Training [644/700]: mean_loss=0.0386457284912467
Online_Training [645/700]: mean_loss=0.25058216229081154
Online_Training [646/700]: mean_loss=0.09539261739701033
Online_Training [647/700]: mean_loss=0.2047584280371666
Online_Training [648/700]: mean_loss=0.1372104026377201
Online_Training [649/700]: mean_loss=0.06295336363837123
Online_Training [650/700]: mean_loss=0.060463061556220055
Online_Training [651/700]: mean_loss=0.04065802739933133
Online_Training [652/700]: mean_loss=0.0421539437957108
Online_Training [653/700]: mean_loss=0.07981516234576702
Online_Training [654/700]: mean_loss=0.058630095329135656
Online_Training [655/700]: mean_loss=0.14768977835774422
Online_Training [656/700]: mean_loss=0.02721808454953134
Online_Training [657/700]: mean_loss=0.07733019068837166
Online_Training [658/700]: mean_loss=0.1139197051525116
Online_Training [659/700]: mean_loss=0.1526262667030096
Online_Training [660/700]: mean_loss=0.07577434927225113
Online_Training [661/700]: mean_loss=0.0992049677297473
Online_Training [662/700]: mean_loss=0.5916277393698692
Online_Training [663/700]: mean_loss=0.05798952700570226
Online_Training [664/700]: mean_loss=0.040074802935123444
Online_Training [665/700]: mean_loss=0.0587386810220778
Online_Training [666/700]: mean_loss=0.025310467928647995
Online_Training [667/700]: mean_loss=0.09521855227649212
Online_Training [668/700]: mean_loss=0.32442954182624817
Online_Training [669/700]: mean_loss=0.05315622081980109
Online_Training [670/700]: mean_loss=0.12217609118670225
Online_Training [671/700]: mean_loss=0.1285409750416875
Online_Training [672/700]: mean_loss=0.0736013287678361
Online_Training [673/700]: mean_loss=0.1368295457214117
Online_Training [674/700]: mean_loss=0.06890255212783813
Online_Training [675/700]: mean_loss=0.057123078033328056
Online_Training [676/700]: mean_loss=0.039917190093547106
Online_Training [677/700]: mean_loss=0.07704881951212883
Online_Training [678/700]: mean_loss=0.10151396784931421
Online_Training [679/700]: mean_loss=0.03027457185089588
Online_Training [680/700]: mean_loss=0.12136690504848957
Online_Training [681/700]: mean_loss=0.19568048417568207
Online_Training [682/700]: mean_loss=0.10081051010638475
Online_Training [683/700]: mean_loss=0.04641649965196848
Online_Training [684/700]: mean_loss=0.24855823069810867
Online_Training [685/700]: mean_loss=0.04237366560846567
Online_Training [686/700]: mean_loss=0.17281113378703594
Online_Training [687/700]: mean_loss=0.04848115285858512
Online_Training [688/700]: mean_loss=0.043322148732841015
Online_Training [689/700]: mean_loss=0.054272908717393875
Online_Training [690/700]: mean_loss=0.07969691045582294
Online_Training [691/700]: mean_loss=0.060225522611290216
Online_Training [692/700]: mean_loss=0.06345036532729864
Online_Training [693/700]: mean_loss=0.028817291371524334
Online_Training [694/700]: mean_loss=0.24792996421456337
Online_Training [695/700]: mean_loss=0.052752941846847534
Online_Training [696/700]: mean_loss=0.025343147572129965
Online_Training [697/700]: mean_loss=0.02334367553703487
Online_Training [698/700]: mean_loss=0.18827983364462852
Online_Training [699/700]: mean_loss=0.03507422097027302
Online_Training [700/700]: mean_loss=0.13664420880377293
Q_Learning [1/300]: mean_loss=0.1702192798256874
Q_Learning [2/300]: mean_loss=0.3766608089208603
Q_Learning [3/300]: mean_loss=0.8518696874380112
Q_Learning [4/300]: mean_loss=0.37057576701045036
Q_Learning [5/300]: mean_loss=0.3643522784113884
Q_Learning [6/300]: mean_loss=0.10322635155171156
Q_Learning [7/300]: mean_loss=0.1861818376928568
Q_Learning [8/300]: mean_loss=0.1915421187877655
Q_Learning [9/300]: mean_loss=0.12282749079167843
Q_Learning [10/300]: mean_loss=0.7239716723561287
Q_Learning [11/300]: mean_loss=0.7799168154597282
Q_Learning [12/300]: mean_loss=0.280077937990427
Q_Learning [13/300]: mean_loss=0.41566672921180725
Q_Learning [14/300]: mean_loss=0.16395068541169167
Q_Learning [15/300]: mean_loss=0.2796476446092129
Q_Learning [16/300]: mean_loss=0.07564461883157492
Q_Learning [17/300]: mean_loss=0.25613465905189514
Q_Learning [18/300]: mean_loss=0.1330268196761608
Q_Learning [19/300]: mean_loss=0.10091861058026552
Q_Learning [20/300]: mean_loss=0.321791909635067
Q_Learning [21/300]: mean_loss=0.18566813133656979
Q_Learning [22/300]: mean_loss=0.5607266128063202
Q_Learning [23/300]: mean_loss=0.13058459199965
Q_Learning [24/300]: mean_loss=0.2838541269302368
Q_Learning [25/300]: mean_loss=0.3640754148364067
Q_Learning [26/300]: mean_loss=0.10638546291738749
Q_Learning [27/300]: mean_loss=0.07362424675375223
Q_Learning [28/300]: mean_loss=0.26410177908837795
Q_Learning [29/300]: mean_loss=0.40834609419107437
Q_Learning [30/300]: mean_loss=0.5773248933255672
Q_Learning [31/300]: mean_loss=0.1945613343268633
Q_Learning [32/300]: mean_loss=0.40230273082852364
Q_Learning [33/300]: mean_loss=0.17775801196694374
Q_Learning [34/300]: mean_loss=0.21006260439753532
Q_Learning [35/300]: mean_loss=0.15520112589001656
Q_Learning [36/300]: mean_loss=0.0711452029645443
Q_Learning [37/300]: mean_loss=0.05203956691548228
Q_Learning [38/300]: mean_loss=0.10126461181789637
Q_Learning [39/300]: mean_loss=0.16940918378531933
Q_Learning [40/300]: mean_loss=0.10847603902220726
Q_Learning [41/300]: mean_loss=0.24753928370773792
Q_Learning [42/300]: mean_loss=0.18909573554992676
Q_Learning [43/300]: mean_loss=0.0546539006754756
Q_Learning [44/300]: mean_loss=0.16883615776896477
Q_Learning [45/300]: mean_loss=0.38587820529937744
Q_Learning [46/300]: mean_loss=0.42726125195622444
Q_Learning [47/300]: mean_loss=0.13191105984151363
Q_Learning [48/300]: mean_loss=0.1525228563696146
Q_Learning [49/300]: mean_loss=0.49119459465146065
Q_Learning [50/300]: mean_loss=0.17720727063715458
Q_Learning [51/300]: mean_loss=0.09133583307266235
Q_Learning [52/300]: mean_loss=0.10593227855861187
Q_Learning [53/300]: mean_loss=0.23016776517033577
Q_Learning [54/300]: mean_loss=0.07140154158696532
Q_Learning [55/300]: mean_loss=0.2942507863044739
Q_Learning [56/300]: mean_loss=0.17303771898150444
Q_Learning [57/300]: mean_loss=0.05038191704079509
Q_Learning [58/300]: mean_loss=0.15277071669697762
Q_Learning [59/300]: mean_loss=0.1393530536442995
Q_Learning [60/300]: mean_loss=0.16023136675357819
Q_Learning [61/300]: mean_loss=0.07978641148656607
Q_Learning [62/300]: mean_loss=0.13779134768992662
Q_Learning [63/300]: mean_loss=0.0988991679623723
Q_Learning [64/300]: mean_loss=0.07978659402579069
Q_Learning [65/300]: mean_loss=0.22970384173095226
Q_Learning [66/300]: mean_loss=0.597384624183178
Q_Learning [67/300]: mean_loss=0.1158746238797903
Q_Learning [68/300]: mean_loss=0.14046227745711803
Q_Learning [69/300]: mean_loss=0.1820903029292822
Q_Learning [70/300]: mean_loss=0.04739735368639231
Q_Learning [71/300]: mean_loss=0.06733658537268639
Q_Learning [72/300]: mean_loss=0.1652860976755619
Q_Learning [73/300]: mean_loss=0.03346686065196991
Q_Learning [74/300]: mean_loss=0.08446741756051779
Q_Learning [75/300]: mean_loss=0.23941602744162083
Q_Learning [76/300]: mean_loss=0.08303989190608263
Q_Learning [77/300]: mean_loss=0.12957481760531664
Q_Learning [78/300]: mean_loss=0.13585107121616602
Q_Learning [79/300]: mean_loss=0.2016361616551876
Q_Learning [80/300]: mean_loss=0.07826352119445801
Q_Learning [81/300]: mean_loss=0.038318598177284
Q_Learning [82/300]: mean_loss=0.06190282246097922
Q_Learning [83/300]: mean_loss=0.08167672995477915
Q_Learning [84/300]: mean_loss=0.22917356714606285
Q_Learning [85/300]: mean_loss=0.07995037641376257
Q_Learning [86/300]: mean_loss=0.02837845543399453
Q_Learning [87/300]: mean_loss=0.15863840375095606
Q_Learning [88/300]: mean_loss=0.16593730449676514
Q_Learning [89/300]: mean_loss=0.05936679197475314
Q_Learning [90/300]: mean_loss=0.12339000031352043
Q_Learning [91/300]: mean_loss=0.024174274876713753
Q_Learning [92/300]: mean_loss=0.10876707546412945
Q_Learning [93/300]: mean_loss=0.08752479497343302
Q_Learning [94/300]: mean_loss=0.024399767396971583
Q_Learning [95/300]: mean_loss=0.043712417129427195
Q_Learning [96/300]: mean_loss=0.26163531094789505
Q_Learning [97/300]: mean_loss=0.17220900766551495
Q_Learning [98/300]: mean_loss=0.09573797602206469
Q_Learning [99/300]: mean_loss=0.054605696350336075
Q_Learning [100/300]: mean_loss=0.23186521977186203
Q_Learning [101/300]: mean_loss=0.12017458211630583
Q_Learning [102/300]: mean_loss=0.04240434430539608
Q_Learning [103/300]: mean_loss=0.0294667633716017
Q_Learning [104/300]: mean_loss=0.0903550935909152
Q_Learning [105/300]: mean_loss=0.14708120934665203
Q_Learning [106/300]: mean_loss=0.15383433364331722
Q_Learning [107/300]: mean_loss=0.13594315014779568
Q_Learning [108/300]: mean_loss=0.13009225949645042
Q_Learning [109/300]: mean_loss=0.13788693957030773
Q_Learning [110/300]: mean_loss=0.048453218303620815
Q_Learning [111/300]: mean_loss=0.044280086643993855
Q_Learning [112/300]: mean_loss=0.06196908559650183
Q_Learning [113/300]: mean_loss=0.08632337022572756
Q_Learning [114/300]: mean_loss=0.0690881134942174
Q_Learning [115/300]: mean_loss=0.030725065153092146
Q_Learning [116/300]: mean_loss=0.09381272085011005
Q_Learning [117/300]: mean_loss=0.11812523379921913
Q_Learning [118/300]: mean_loss=0.06952528096735477
Q_Learning [119/300]: mean_loss=0.1107229357585311
Q_Learning [120/300]: mean_loss=0.15871358662843704
Q_Learning [121/300]: mean_loss=0.1015774691477418
Q_Learning [122/300]: mean_loss=0.041213075164705515
Q_Learning [123/300]: mean_loss=0.1255725659430027
Q_Learning [124/300]: mean_loss=0.06840418698266149
Q_Learning [125/300]: mean_loss=0.06864976370707154
Q_Learning [126/300]: mean_loss=0.31080377846956253
Q_Learning [127/300]: mean_loss=0.11633406579494476
Q_Learning [128/300]: mean_loss=0.12396085448563099
Q_Learning [129/300]: mean_loss=0.07861536834388971
Q_Learning [130/300]: mean_loss=0.05859613511711359
Q_Learning [131/300]: mean_loss=0.06471137423068285
Q_Learning [132/300]: mean_loss=0.41758765652775764
Q_Learning [133/300]: mean_loss=0.2574842479079962
Q_Learning [134/300]: mean_loss=0.020856191404163837
Q_Learning [135/300]: mean_loss=0.16275868378579617
Q_Learning [136/300]: mean_loss=0.07445259019732475
Q_Learning [137/300]: mean_loss=0.13464768417179585
Q_Learning [138/300]: mean_loss=0.12444894574582577
Q_Learning [139/300]: mean_loss=0.06992785539478064
Q_Learning [140/300]: mean_loss=0.08398168720304966
Q_Learning [141/300]: mean_loss=0.09248843230307102
Q_Learning [142/300]: mean_loss=0.11688661389052868
Q_Learning [143/300]: mean_loss=0.08660687319934368
Q_Learning [144/300]: mean_loss=0.042757511138916016
Q_Learning [145/300]: mean_loss=0.3688870668411255
Q_Learning [146/300]: mean_loss=0.14651400037109852
Q_Learning [147/300]: mean_loss=0.14737079665064812
Q_Learning [148/300]: mean_loss=0.17155643738806248
Q_Learning [149/300]: mean_loss=0.04027421306818724
Q_Learning [150/300]: mean_loss=0.13806971721351147
Q_Learning [151/300]: mean_loss=0.1167732672765851
Q_Learning [152/300]: mean_loss=0.04632877791300416
Q_Learning [153/300]: mean_loss=0.03853807272389531
Q_Learning [154/300]: mean_loss=0.10529811587184668
Q_Learning [155/300]: mean_loss=0.0814565122127533
Q_Learning [156/300]: mean_loss=0.14529410563409328
Q_Learning [157/300]: mean_loss=0.1120968097820878
Q_Learning [158/300]: mean_loss=0.09211334865540266
Q_Learning [159/300]: mean_loss=0.16351273097097874
Q_Learning [160/300]: mean_loss=0.09660883154720068
Q_Learning [161/300]: mean_loss=0.19839458167552948
Q_Learning [162/300]: mean_loss=0.11225312482565641
Q_Learning [163/300]: mean_loss=0.2228853702545166
Q_Learning [164/300]: mean_loss=0.04584907507523894
Q_Learning [165/300]: mean_loss=0.06920021306723356
Q_Learning [166/300]: mean_loss=0.04417738039046526
Q_Learning [167/300]: mean_loss=0.1036560358479619
Q_Learning [168/300]: mean_loss=0.16336176730692387
Q_Learning [169/300]: mean_loss=0.16984604112803936
Q_Learning [170/300]: mean_loss=0.024213862838223577
Q_Learning [171/300]: mean_loss=0.1336589828133583
Q_Learning [172/300]: mean_loss=0.18754659965634346
Q_Learning [173/300]: mean_loss=0.07187144551426172
Q_Learning [174/300]: mean_loss=0.08447647280991077
Q_Learning [175/300]: mean_loss=0.1773802414536476
Q_Learning [176/300]: mean_loss=0.42318081483244896
Q_Learning [177/300]: mean_loss=0.3647642619907856
Q_Learning [178/300]: mean_loss=0.19120759703218937
Q_Learning [179/300]: mean_loss=0.03415290196426213
Q_Learning [180/300]: mean_loss=0.09120285045355558
Q_Learning [181/300]: mean_loss=0.19129516929388046
Q_Learning [182/300]: mean_loss=0.1161850718781352
Q_Learning [183/300]: mean_loss=0.18210668116807938
Q_Learning [184/300]: mean_loss=0.04749893303960562
Q_Learning [185/300]: mean_loss=0.181205365806818
Q_Learning [186/300]: mean_loss=0.21571926586329937
Q_Learning [187/300]: mean_loss=0.0469715422950685
Q_Learning [188/300]: mean_loss=0.14752162247896194
Q_Learning [189/300]: mean_loss=0.07201091991737485
Q_Learning [190/300]: mean_loss=0.01978988153859973
Q_Learning [191/300]: mean_loss=0.13406546600162983
Q_Learning [192/300]: mean_loss=0.13630475103855133
Q_Learning [193/300]: mean_loss=0.022062535397708416
Q_Learning [194/300]: mean_loss=0.1243321169167757
Q_Learning [195/300]: mean_loss=0.12165709678083658
Q_Learning [196/300]: mean_loss=0.06476414762437344
Q_Learning [197/300]: mean_loss=0.13693172857165337
Q_Learning [198/300]: mean_loss=0.1022942028939724
Q_Learning [199/300]: mean_loss=0.1934740711003542
Q_Learning [200/300]: mean_loss=0.04907504515722394
Q_Learning [201/300]: mean_loss=0.1419877614825964
Q_Learning [202/300]: mean_loss=0.11776436492800713
Q_Learning [203/300]: mean_loss=0.06651771254837513
Q_Learning [204/300]: mean_loss=0.07528531271964312
Q_Learning [205/300]: mean_loss=0.1188453147187829
Q_Learning [206/300]: mean_loss=0.052797308657318354
Q_Learning [207/300]: mean_loss=0.1165594207122922
Q_Learning [208/300]: mean_loss=0.283480629324913
Q_Learning [209/300]: mean_loss=0.1653603632003069
Q_Learning [210/300]: mean_loss=0.16137346252799034
Q_Learning [211/300]: mean_loss=0.1280064471065998
Q_Learning [212/300]: mean_loss=0.036766740027815104
Q_Learning [213/300]: mean_loss=0.0728113716468215
Q_Learning [214/300]: mean_loss=0.057755169458687305
Q_Learning [215/300]: mean_loss=0.03332203906029463
Q_Learning [216/300]: mean_loss=0.09366583079099655
Q_Learning [217/300]: mean_loss=0.052546396385878325
Q_Learning [218/300]: mean_loss=0.0834036460146308
Q_Learning [219/300]: mean_loss=0.08294279221445322
Q_Learning [220/300]: mean_loss=0.045747780706733465
Q_Learning [221/300]: mean_loss=0.16297000460326672
Q_Learning [222/300]: mean_loss=0.03728012088686228
Q_Learning [223/300]: mean_loss=0.08437469881027937
Q_Learning [224/300]: mean_loss=0.11563925165683031
Q_Learning [225/300]: mean_loss=0.07704083621501923
Q_Learning [226/300]: mean_loss=0.09044161438941956
Q_Learning [227/300]: mean_loss=0.05957692675292492
Q_Learning [228/300]: mean_loss=0.24122988805174828
Q_Learning [229/300]: mean_loss=0.05410928465425968
Q_Learning [230/300]: mean_loss=0.09950776863843203
Q_Learning [231/300]: mean_loss=0.30089641362428665
Q_Learning [232/300]: mean_loss=0.043302048929035664
Q_Learning [233/300]: mean_loss=0.11497026402503252
Q_Learning [234/300]: mean_loss=0.043897376861423254
Q_Learning [235/300]: mean_loss=0.08479412645101547
Q_Learning [236/300]: mean_loss=0.056514328345656395
Q_Learning [237/300]: mean_loss=0.033430883660912514
Q_Learning [238/300]: mean_loss=0.09073326364159584
Q_Learning [239/300]: mean_loss=0.1555920820683241
Q_Learning [240/300]: mean_loss=0.07016631681472063
Q_Learning [241/300]: mean_loss=0.043360400944948196
Q_Learning [242/300]: mean_loss=0.09534095227718353
Q_Learning [243/300]: mean_loss=0.07946550566703081
Q_Learning [244/300]: mean_loss=0.07206716947257519
Q_Learning [245/300]: mean_loss=0.07080537732690573
Q_Learning [246/300]: mean_loss=0.06669808551669121
Q_Learning [247/300]: mean_loss=0.0765433693304658
Q_Learning [248/300]: mean_loss=0.11709811259061098
Q_Learning [249/300]: mean_loss=0.10274544171988964
Q_Learning [250/300]: mean_loss=0.07445006258785725
Q_Learning [251/300]: mean_loss=0.12684893887490034
Q_Learning [252/300]: mean_loss=0.02424497320316732
Q_Learning [253/300]: mean_loss=0.04436150472611189
Q_Learning [254/300]: mean_loss=0.15605368092656136
Q_Learning [255/300]: mean_loss=0.40856415033340454
Q_Learning [256/300]: mean_loss=0.4786125533282757
Q_Learning [257/300]: mean_loss=0.06693207751959562
Q_Learning [258/300]: mean_loss=0.07741866167634726
Q_Learning [259/300]: mean_loss=0.06274513807147741
Q_Learning [260/300]: mean_loss=0.061418641824275255
Q_Learning [261/300]: mean_loss=0.08399138785898685
Q_Learning [262/300]: mean_loss=0.07081423792988062
Q_Learning [263/300]: mean_loss=0.08546697348356247
Q_Learning [264/300]: mean_loss=0.026963252807036042
Q_Learning [265/300]: mean_loss=0.011187508702278137
Q_Learning [266/300]: mean_loss=0.10989234875887632
Q_Learning [267/300]: mean_loss=0.09102137759327888
Q_Learning [268/300]: mean_loss=0.1561561245471239
Q_Learning [269/300]: mean_loss=0.028962713200598955
Q_Learning [270/300]: mean_loss=0.11398131586611271
Q_Learning [271/300]: mean_loss=0.16730761341750622
Q_Learning [272/300]: mean_loss=0.09468937665224075
Q_Learning [273/300]: mean_loss=0.5192789696156979
Q_Learning [274/300]: mean_loss=0.08918050210922956
Q_Learning [275/300]: mean_loss=0.17196263186633587
Q_Learning [276/300]: mean_loss=0.1339751621708274
Q_Learning [277/300]: mean_loss=0.3413644880056381
Q_Learning [278/300]: mean_loss=0.24998206458985806
Q_Learning [279/300]: mean_loss=0.04213948315009475
Q_Learning [280/300]: mean_loss=0.08758008386939764
Q_Learning [281/300]: mean_loss=0.04204852506518364
Q_Learning [282/300]: mean_loss=0.08333582151681185
Q_Learning [283/300]: mean_loss=0.11987725272774696
Q_Learning [284/300]: mean_loss=0.13428237102925777
Q_Learning [285/300]: mean_loss=0.10070779733359814
Q_Learning [286/300]: mean_loss=0.23230277560651302
Q_Learning [287/300]: mean_loss=0.11077667400240898
Q_Learning [288/300]: mean_loss=0.11834818962961435
Q_Learning [289/300]: mean_loss=0.08944840729236603
Q_Learning [290/300]: mean_loss=0.049494000151753426
Q_Learning [291/300]: mean_loss=0.07970694545656443
Q_Learning [292/300]: mean_loss=0.0666037118062377
Q_Learning [293/300]: mean_loss=0.1645168773829937
Q_Learning [294/300]: mean_loss=0.03806707123294473
Q_Learning [295/300]: mean_loss=0.056531712878495455
Q_Learning [296/300]: mean_loss=0.07110460475087166
Q_Learning [297/300]: mean_loss=0.036038252990692854
Q_Learning [298/300]: mean_loss=0.3740050680935383
Q_Learning [299/300]: mean_loss=0.28435878828167915
Q_Learning [300/300]: mean_loss=0.056093175895512104
Number of Samples after Autoencoder testing: 300
First Spike after testing: [-1.8628092  3.9910905]
[2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 1, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 1, 2, 0, 0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 2, 0, 2, 2, 0, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 1, 2, 2, 0, 2, 0, 1, 1, 2, 2, 1, 0, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 2, 0, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 1, 1, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 1, 2, 1, 2, 0, 0, 1, 0, 2, 0, 0, 2, 2, 0, 0, 1, 2, 2, 2, 1, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 2, 1, 0, 0, 1, 2, 2, 1, 1, 1, 1, 2, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 0, 1, 1, 0, 2, 2, 2, 2, 1, 2, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1, 0, 0, 2, 2, 1, 0, 0, 2, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 1]
[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 3, 3, 2, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 1, 3, 0, 0, 0, 1, 0, 2, 2, 2, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 1, 2, 2, 1, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 2, 3, 1, 1, 2, 3, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 1, 2, 0, 2, 2, 1, 2, 1, 2, 0, 2, 2, 3, 3, 3, 1, 0, 2, 0, 0, 2, 2, 1, 1, 3, 3, 0, 0, 2, 0, 2, 2, 2, 2, 0, 1, 2, 0, 2, 0, 1, 0, 1, 0, 2, 2, 1, 2, 3, 2, 2, 3, 0, 2, 2, 1, 0, 0, 0, 1, 4, 5, 2, 2, 1, 2, 0, 0, 6, 2, 2, 2, 0, 1, 2, 2, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 6, 1, 2, 6, 6, 2, 6, 0, 1, 2, 0, 3, 2, 0, 6, 1, 1, 2, 0, 0, 0, 2, 1, 0, 2, 1, 0, 2, 2, 2, 0, 6, 2, 2, 2, 6, 1, 4, 0, 1, 2, 6, 0, 4, 2, 3, 6, 0, 2, 0, 2, 6, 0, 1, 6, 2, 6, 2, 0, 6, 0, 4, 2, 6, 1, 2, 2, 0, 0, 7, 0, 6, 6, 2, 1, 1, 1, 0, 4, 2, 7, 8, 1, 1]
Centroids: [[-2.199344, -0.6395493], [1.9120922, -0.82098716], [-1.2560993, 2.7438097]]
Centroids: [[-1.0877242, 2.7444375], [1.9354526, -0.68599504], [-1.6713054, -0.5803376], [-3.5791645, 1.6290529], [-2.6910248, 3.893436], [-8.1204605, 0.65963435], [-3.5225832, -1.9661679], [-4.3936014, -0.20382094], [3.1289392, -3.4767058]]
Contingency Matrix: 
[[ 4  2 79  6  0  1 17  2  0]
 [ 0 78  3  0  0  0  0  0  1]
 [89  2  3  8  5  0  0  0  0]]
[[4, 2, 79, 6, 0, 1, 17, 2, 0], [0, 78, 3, 0, 0, 0, 0, 0, 1], [89, 2, 3, 8, 5, 0, 0, 0, 0]]
[[4, 2, 79, 6, 0, 1, 17, 2, 0], [0, 78, 3, 0, 0, 0, 0, 0, 1], [89, 2, 3, 8, 5, 0, 0, 0, 0]]
[0, 1, 2, 3, 4, 5, 6, 7, 8]
[[-1, 2, 79, 6, 0, 1, 17, 2, 0], [-1, 78, 3, 0, 0, 0, 0, 0, 1], [-1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, 78, -1, 0, 0, 0, 0, 0, 1], [-1, -1, -1, -1, -1, -1, -1, -1, -1]]
[[-1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1]]
Match_Labels: {2: 0, 0: 2, 1: 1}
New Contingency Matrix: 
[[79  2  4  6  0  1 17  2  0]
 [ 3 78  0  0  0  0  0  0  1]
 [ 3  2 89  8  5  0  0  0  0]]
New Clustered Label Sequence: [2, 1, 0, 3, 4, 5, 6, 7, 8]
Diagonal_Elements: [79, 78, 89], Sum: 246
All_Elements: [79, 2, 4, 6, 0, 1, 17, 2, 0, 3, 78, 0, 0, 0, 0, 0, 0, 1, 3, 2, 89, 8, 5, 0, 0, 0, 0], Sum: 300
Accuracy: 0.82
