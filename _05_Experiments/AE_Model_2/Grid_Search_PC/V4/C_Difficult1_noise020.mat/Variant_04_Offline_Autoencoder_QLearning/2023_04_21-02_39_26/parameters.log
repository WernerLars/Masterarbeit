Seed: 0
Experiment_path: AE_Model_2/Experiment_04
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise020.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise020.mat']
Variant_name: Variant_04_Offline_Autoencoder_QLearning
Visualisation_Path: Grid_Search_PC//V4/C_Difficult1_noise020.mat/Variant_04_Offline_Autoencoder_QLearning/2023_04_21-02_39_26
Split Ratio: 0.9
Epochs: 8
Batch Size: 1
Input Size: 47
Number of Features: 2
Chosen Model: Convolutional Autoencoder
ConvolutionalAutoencoder(
  (encoder): Sequential(
    (0): Conv1d(1, 6, kernel_size=(6,), stride=(1,))
    (1): LeakyReLU(negative_slope=0.01)
    (2): Conv1d(6, 1, kernel_size=(6,), stride=(1,))
    (3): Flatten(start_dim=1, end_dim=-1)
    (4): Linear(in_features=37, out_features=2, bias=True)
  )
  (decoder): Sequential(
    (0): ConvTranspose1d(1, 6, kernel_size=(6,), stride=(1,))
    (1): LeakyReLU(negative_slope=0.01)
    (2): ConvTranspose1d(6, 1, kernel_size=(6,), stride=(1,))
    (3): Flatten(start_dim=1, end_dim=-1)
    (4): Linear(in_features=12, out_features=47, bias=True)
  )
)
MSELoss()
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: False
    lr: 0.001
    maximize: False
    weight_decay: 0
)
---Q Learning Parameters---
Normalisation: False
Punishment Coefficient: 0.3
Alpha: 0.8
Epsilon: 0.01
Gamma: 0.97
Initial Episode Number: 0
Episode Number Coefficient: 1.4
Number of Random Features: 20
Planning Number: 20
Max Random Features: 60
New Episode Number: 72
New Episode Number: 143
New Episode Number: 215
New Episode Number: 286
New Episode Number: 358
New Episode Number: 429
New Episode Number: 500
New Episode Number: 572
New Episode Number: 643
New Episode Number: 715
New Episode Number: 786
               0     1     2     3     4   ...    7     8     9     10    11
new_cluster -1.81 -1.66 -2.03 -1.68 -2.33  ... -2.31 -2.04 -1.91 -2.15 -1.53
c1          -1.82 -1.70 -2.19 -1.67 -2.42  ... -2.31 -2.08 -1.93 -2.15 -1.53
c2          -1.82 -1.65 -2.13 -1.76 -2.30  ... -2.25 -2.04 -1.96 -2.15 -1.53
c3          -1.79 -1.70 -2.03 -1.70 -2.32  ... -2.32 -2.06 -1.92 -2.15 -1.48
c4          -1.80 -1.67 -1.99 -1.69 -2.38  ... -2.32 -2.07 -1.88 -2.15 -1.52
c5          -1.83 -1.68 -1.98 -1.72 -2.28  ... -2.33 -1.99 -1.99 -2.15 -1.54
c6          -1.79 -1.72 -2.09 -1.74 -2.33  ... -2.31 -2.05 -2.04 -2.14 -1.52
c7          -1.83 -1.66 -2.10 -1.81 -2.27  ... -2.29 -2.11 -1.98 -2.15 -1.51
c8          -1.84 -1.63 -2.17 -1.72 -2.25  ... -2.37 -2.06 -1.93 -2.15 -1.52
c9          -1.83 -1.65 -2.11 -1.66 -2.39  ... -2.28 -2.06 -1.82 -2.15 -1.49
c10         -1.81 -1.66 -1.99 -1.73 -2.31  ... -2.31 -2.10 -1.90 -2.15 -1.51
c11         -1.79 -1.66 -2.09 -1.75 -2.27  ... -2.34 -2.05 -1.84 -2.15 -1.55

[12 rows x 12 columns]
\begin{tabular}{lrrrrrrrrrrrr}
\toprule
{} &    0  &    1  &    2  &    3  &    4  &    5  &    6  &    7  &    8  &    9  &    10 &    11 \\
\midrule
new\_cluster & -1.81 & -1.66 & -2.03 & -1.68 & -2.33 & -1.53 & -2.31 & -2.31 & -2.04 & -1.91 & -2.15 & -1.53 \\
c1          & -1.82 & -1.70 & -2.19 & -1.67 & -2.42 & -1.56 & -2.42 & -2.31 & -2.08 & -1.93 & -2.15 & -1.53 \\
c2          & -1.82 & -1.65 & -2.13 & -1.76 & -2.30 & -1.55 & -2.26 & -2.25 & -2.04 & -1.96 & -2.15 & -1.53 \\
c3          & -1.79 & -1.70 & -2.03 & -1.70 & -2.32 & -1.51 & -2.27 & -2.32 & -2.06 & -1.92 & -2.15 & -1.48 \\
c4          & -1.80 & -1.67 & -1.99 & -1.69 & -2.38 & -1.52 & -2.27 & -2.32 & -2.07 & -1.88 & -2.15 & -1.52 \\
c5          & -1.83 & -1.68 & -1.98 & -1.72 & -2.28 & -1.58 & -2.28 & -2.33 & -1.99 & -1.99 & -2.15 & -1.54 \\
c6          & -1.79 & -1.72 & -2.09 & -1.74 & -2.33 & -1.53 & -2.19 & -2.31 & -2.05 & -2.04 & -2.14 & -1.52 \\
c7          & -1.83 & -1.66 & -2.10 & -1.81 & -2.27 & -1.52 & -2.25 & -2.29 & -2.11 & -1.98 & -2.15 & -1.51 \\
c8          & -1.84 & -1.63 & -2.17 & -1.72 & -2.25 & -1.53 & -2.29 & -2.37 & -2.06 & -1.93 & -2.15 & -1.52 \\
c9          & -1.83 & -1.65 & -2.11 & -1.66 & -2.39 & -1.53 & -2.41 & -2.28 & -2.06 & -1.82 & -2.15 & -1.49 \\
c10         & -1.81 & -1.66 & -1.99 & -1.73 & -2.31 & -1.56 & -2.42 & -2.31 & -2.10 & -1.90 & -2.15 & -1.51 \\
c11         & -1.79 & -1.66 & -2.09 & -1.75 & -2.27 & -1.56 & -2.28 & -2.34 & -2.05 & -1.84 & -2.15 & -1.55 \\
\bottomrule
\end{tabular}

                               0            1   ...            10            11
new_cluster  [-0.36, new_cluster]  [-0.18, c1]  ...  [-0.68, c10]  [-0.05, c11]
c1           [-0.36, new_cluster]  [-0.23, c1]  ...  [-0.68, c10]  [-0.05, c11]
c2           [-0.36, new_cluster]  [-0.18, c1]  ...  [-0.68, c10]  [-0.06, c11]
c3           [-0.36, new_cluster]  [-0.23, c1]  ...  [-0.68, c10]  [-0.04, c11]
c4           [-0.36, new_cluster]  [-0.21, c1]  ...  [-0.68, c10]  [-0.04, c11]
c5           [-0.36, new_cluster]  [-0.22, c1]  ...  [-0.68, c10]  [-0.08, c11]
c6           [-0.36, new_cluster]  [-0.24, c1]  ...  [-0.68, c10]  [-0.05, c11]
c7           [-0.36, new_cluster]   [-0.2, c1]  ...  [-0.68, c10]  [-0.04, c11]
c8           [-0.36, new_cluster]  [-0.16, c1]  ...  [-0.68, c10]  [-0.05, c11]
c9           [-0.36, new_cluster]  [-0.18, c1]  ...  [-0.68, c10]  [-0.03, c11]
c10          [-0.36, new_cluster]  [-0.19, c1]  ...  [-0.68, c10]  [-0.05, c11]
c11          [-0.36, new_cluster]  [-0.18, c1]  ...  [-0.68, c10]  [-0.06, c11]

[12 rows x 12 columns]
\begin{tabular}{lllllllllllll}
\toprule
{} &                    0  &           1  &           2  &           3  &           4  &           5  &           6  &           7  &           8  &           9  &            10 &            11 \\
\midrule
new\_cluster &  [-0.36, new\_cluster] &  [-0.18, c1] &  [-0.55, c2] &  [-0.25, c3] &  [-0.87, c4] &  [-0.03, c5] &  [-0.85, c6] &  [-0.88, c7] &  [-0.57, c8] &  [-0.46, c9] &  [-0.68, c10] &  [-0.05, c11] \\
c1          &  [-0.36, new\_cluster] &  [-0.23, c1] &  [-0.71, c2] &  [-0.24, c3] &  [-0.95, c4] &  [-0.08, c5] &  [-0.95, c6] &  [-0.88, c7] &   [-0.6, c8] &  [-0.48, c9] &  [-0.68, c10] &  [-0.05, c11] \\
c2          &  [-0.36, new\_cluster] &  [-0.18, c1] &  [-0.66, c2] &  [-0.33, c3] &  [-0.85, c4] &  [-0.05, c5] &  [-0.79, c6] &  [-0.82, c7] &  [-0.56, c8] &  [-0.51, c9] &  [-0.68, c10] &  [-0.06, c11] \\
c3          &  [-0.36, new\_cluster] &  [-0.23, c1] &  [-0.58, c2] &  [-0.27, c3] &  [-0.86, c4] &  [-0.03, c5] &   [-0.8, c6] &  [-0.89, c7] &   [-0.6, c8] &  [-0.47, c9] &  [-0.68, c10] &  [-0.04, c11] \\
c4          &  [-0.36, new\_cluster] &  [-0.21, c1] &   [-0.5, c2] &  [-0.26, c3] &  [-0.92, c4] &  [-0.04, c5] &  [-0.79, c6] &  [-0.89, c7] &   [-0.6, c8] &  [-0.43, c9] &  [-0.68, c10] &  [-0.04, c11] \\
c5          &  [-0.36, new\_cluster] &  [-0.22, c1] &   [-0.5, c2] &  [-0.28, c3] &  [-0.82, c4] &   [-0.1, c5] &  [-0.81, c6] &  [-0.89, c7] &  [-0.52, c8] &  [-0.54, c9] &  [-0.68, c10] &  [-0.08, c11] \\
c6          &  [-0.36, new\_cluster] &  [-0.24, c1] &  [-0.61, c2] &  [-0.31, c3] &  [-0.86, c4] &  [-0.05, c5] &  [-0.75, c6] &  [-0.88, c7] &  [-0.57, c8] &  [-0.59, c9] &  [-0.68, c10] &  [-0.05, c11] \\
c7          &  [-0.36, new\_cluster] &   [-0.2, c1] &  [-0.62, c2] &  [-0.38, c3] &  [-0.81, c4] &  [-0.03, c5] &  [-0.81, c6] &  [-0.86, c7] &  [-0.63, c8] &  [-0.52, c9] &  [-0.68, c10] &  [-0.04, c11] \\
c8          &  [-0.36, new\_cluster] &  [-0.16, c1] &  [-0.71, c2] &  [-0.28, c3] &  [-0.79, c4] &  [-0.05, c5] &  [-0.85, c6] &  [-0.94, c7] &   [-0.6, c8] &  [-0.48, c9] &  [-0.68, c10] &  [-0.05, c11] \\
c9          &  [-0.36, new\_cluster] &  [-0.18, c1] &  [-0.62, c2] &  [-0.22, c3] &  [-0.94, c4] &  [-0.05, c5] &  [-0.95, c6] &  [-0.84, c7] &  [-0.58, c8] &  [-0.37, c9] &  [-0.68, c10] &  [-0.03, c11] \\
c10         &  [-0.36, new\_cluster] &  [-0.19, c1] &   [-0.5, c2] &   [-0.3, c3] &  [-0.84, c4] &  [-0.07, c5] &  [-0.95, c6] &  [-0.88, c7] &  [-0.63, c8] &  [-0.46, c9] &  [-0.68, c10] &  [-0.05, c11] \\
c11         &  [-0.36, new\_cluster] &  [-0.18, c1] &  [-0.61, c2] &  [-0.32, c3] &   [-0.8, c4] &  [-0.08, c5] &  [-0.81, c6] &  [-0.88, c7] &  [-0.59, c8] &  [-0.39, c9] &  [-0.68, c10] &  [-0.06, c11] \\
\bottomrule
\end{tabular}

