Experiment_path: Base_Line/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Difficult1_noise015.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Difficult1_noise015.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Base_Line/Experiment_02/C_Difficult1_noise015.mat/Variant_02_Autoencoder_KMeans/2023_03_21-00_08_19
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001DF0D343FD0>
Sampling rate: 24000.0
Raw: [ 0.04887081  0.02693095 -0.0154249  ... -0.09301659 -0.11629005
 -0.14613101]
Times: [    340     491     641 ... 1439047 1439065 1439816]
Cluster: [1 1 1 ... 3 2 2]
Number of different clusters:  3
Number of Spikes: 3472
First aligned Spike Frame: [ 0.12751554  0.12305882  0.10482977  0.09479529  0.10214978  0.11675932
  0.11777927  0.09307299  0.04670706 -0.00574343 -0.06143573 -0.14637617
 -0.20942665 -0.00208103  0.52241508  0.81651544  0.46446121 -0.19226425
 -0.60927882 -0.6713583  -0.57871227 -0.49011309 -0.4269388  -0.3668903
 -0.30523219 -0.24747124 -0.19738203 -0.15189972 -0.10449507 -0.05533325
 -0.01452429  0.01008816  0.02570853  0.04365027  0.06334113  0.07980397
  0.08484457  0.07688513  0.06142919  0.04320028  0.02240626  0.00477291
 -0.00393242 -0.00135684  0.00575182  0.0026944  -0.01541647]
Cluster 0, Occurrences: 1159
Cluster 1, Occurrences: 1172
Cluster 2, Occurrences: 1141
<torch.utils.data.dataloader.DataLoader object at 0x000001DF09AECDD8>
Epoch 1
-------------------------------
loss: 0.120342  [    0/ 3472]
loss: 0.072496  [  100/ 3472]
loss: 0.008413  [  200/ 3472]
loss: 0.024502  [  300/ 3472]
loss: 0.043387  [  400/ 3472]
loss: 0.016148  [  500/ 3472]
loss: 0.019515  [  600/ 3472]
loss: 0.017404  [  700/ 3472]
loss: 0.038263  [  800/ 3472]
loss: 0.030692  [  900/ 3472]
loss: 0.012570  [ 1000/ 3472]
loss: 0.020971  [ 1100/ 3472]
loss: 0.021350  [ 1200/ 3472]
loss: 0.014619  [ 1300/ 3472]
loss: 0.004579  [ 1400/ 3472]
loss: 0.061333  [ 1500/ 3472]
loss: 0.021762  [ 1600/ 3472]
loss: 0.009922  [ 1700/ 3472]
loss: 0.015160  [ 1800/ 3472]
loss: 0.152253  [ 1900/ 3472]
loss: 0.041890  [ 2000/ 3472]
loss: 0.009140  [ 2100/ 3472]
loss: 0.010811  [ 2200/ 3472]
loss: 0.004798  [ 2300/ 3472]
loss: 0.010739  [ 2400/ 3472]
loss: 0.011794  [ 2500/ 3472]
loss: 0.020322  [ 2600/ 3472]
loss: 0.008470  [ 2700/ 3472]
loss: 0.020320  [ 2800/ 3472]
loss: 0.029281  [ 2900/ 3472]
loss: 0.008962  [ 3000/ 3472]
loss: 0.025012  [ 3100/ 3472]
loss: 0.015025  [ 3200/ 3472]
loss: 0.008307  [ 3300/ 3472]
loss: 0.007907  [ 3400/ 3472]
Epoch 2
-------------------------------
loss: 0.013699  [    0/ 3472]
loss: 0.007412  [  100/ 3472]
loss: 0.009522  [  200/ 3472]
loss: 0.017238  [  300/ 3472]
loss: 0.008188  [  400/ 3472]
loss: 0.016587  [  500/ 3472]
loss: 0.012399  [  600/ 3472]
loss: 0.004533  [  700/ 3472]
loss: 0.037634  [  800/ 3472]
loss: 0.028014  [  900/ 3472]
loss: 0.010392  [ 1000/ 3472]
loss: 0.025328  [ 1100/ 3472]
loss: 0.023913  [ 1200/ 3472]
loss: 0.008127  [ 1300/ 3472]
loss: 0.003355  [ 1400/ 3472]
loss: 0.042280  [ 1500/ 3472]
loss: 0.019790  [ 1600/ 3472]
loss: 0.011099  [ 1700/ 3472]
loss: 0.025825  [ 1800/ 3472]
loss: 0.138931  [ 1900/ 3472]
loss: 0.047233  [ 2000/ 3472]
loss: 0.012909  [ 2100/ 3472]
loss: 0.010711  [ 2200/ 3472]
loss: 0.004790  [ 2300/ 3472]
loss: 0.008507  [ 2400/ 3472]
loss: 0.011882  [ 2500/ 3472]
loss: 0.015375  [ 2600/ 3472]
loss: 0.010276  [ 2700/ 3472]
loss: 0.024976  [ 2800/ 3472]
loss: 0.030210  [ 2900/ 3472]
loss: 0.008744  [ 3000/ 3472]
loss: 0.022460  [ 3100/ 3472]
loss: 0.014748  [ 3200/ 3472]
loss: 0.008301  [ 3300/ 3472]
loss: 0.008607  [ 3400/ 3472]
Epoch 3
-------------------------------
loss: 0.010494  [    0/ 3472]
loss: 0.007578  [  100/ 3472]
loss: 0.009675  [  200/ 3472]
loss: 0.017442  [  300/ 3472]
loss: 0.006449  [  400/ 3472]
loss: 0.015882  [  500/ 3472]
loss: 0.012129  [  600/ 3472]
loss: 0.005399  [  700/ 3472]
loss: 0.037434  [  800/ 3472]
loss: 0.027210  [  900/ 3472]
loss: 0.010120  [ 1000/ 3472]
loss: 0.026527  [ 1100/ 3472]
loss: 0.024459  [ 1200/ 3472]
loss: 0.006674  [ 1300/ 3472]
loss: 0.003841  [ 1400/ 3472]
loss: 0.038635  [ 1500/ 3472]
loss: 0.019109  [ 1600/ 3472]
loss: 0.010915  [ 1700/ 3472]
loss: 0.028557  [ 1800/ 3472]
loss: 0.135498  [ 1900/ 3472]
loss: 0.048900  [ 2000/ 3472]
loss: 0.013866  [ 2100/ 3472]
loss: 0.010757  [ 2200/ 3472]
loss: 0.005088  [ 2300/ 3472]
loss: 0.008871  [ 2400/ 3472]
loss: 0.012326  [ 2500/ 3472]
loss: 0.014838  [ 2600/ 3472]
loss: 0.010948  [ 2700/ 3472]
loss: 0.025919  [ 2800/ 3472]
loss: 0.030084  [ 2900/ 3472]
loss: 0.008755  [ 3000/ 3472]
loss: 0.021701  [ 3100/ 3472]
loss: 0.014509  [ 3200/ 3472]
loss: 0.008519  [ 3300/ 3472]
loss: 0.008778  [ 3400/ 3472]
Epoch 4
-------------------------------
loss: 0.009559  [    0/ 3472]
loss: 0.007392  [  100/ 3472]
loss: 0.009774  [  200/ 3472]
loss: 0.017721  [  300/ 3472]
loss: 0.006285  [  400/ 3472]
loss: 0.015544  [  500/ 3472]
loss: 0.012346  [  600/ 3472]
loss: 0.005570  [  700/ 3472]
loss: 0.037127  [  800/ 3472]
loss: 0.027360  [  900/ 3472]
loss: 0.009910  [ 1000/ 3472]
loss: 0.026689  [ 1100/ 3472]
loss: 0.024387  [ 1200/ 3472]
loss: 0.006175  [ 1300/ 3472]
loss: 0.004776  [ 1400/ 3472]
loss: 0.036340  [ 1500/ 3472]
loss: 0.019067  [ 1600/ 3472]
loss: 0.010822  [ 1700/ 3472]
loss: 0.029254  [ 1800/ 3472]
loss: 0.133561  [ 1900/ 3472]
loss: 0.050314  [ 2000/ 3472]
loss: 0.014063  [ 2100/ 3472]
loss: 0.010902  [ 2200/ 3472]
loss: 0.005193  [ 2300/ 3472]
loss: 0.008830  [ 2400/ 3472]
loss: 0.013122  [ 2500/ 3472]
loss: 0.014828  [ 2600/ 3472]
loss: 0.011315  [ 2700/ 3472]
loss: 0.026266  [ 2800/ 3472]
loss: 0.029974  [ 2900/ 3472]
loss: 0.008740  [ 3000/ 3472]
loss: 0.021567  [ 3100/ 3472]
loss: 0.014074  [ 3200/ 3472]
loss: 0.008690  [ 3300/ 3472]
loss: 0.008888  [ 3400/ 3472]
Epoch 5
-------------------------------
loss: 0.008866  [    0/ 3472]
loss: 0.007126  [  100/ 3472]
loss: 0.009920  [  200/ 3472]
loss: 0.017927  [  300/ 3472]
loss: 0.006122  [  400/ 3472]
loss: 0.015377  [  500/ 3472]
loss: 0.012461  [  600/ 3472]
loss: 0.005720  [  700/ 3472]
loss: 0.036947  [  800/ 3472]
loss: 0.027781  [  900/ 3472]
loss: 0.009674  [ 1000/ 3472]
loss: 0.026755  [ 1100/ 3472]
loss: 0.024182  [ 1200/ 3472]
loss: 0.005784  [ 1300/ 3472]
loss: 0.004862  [ 1400/ 3472]
loss: 0.033422  [ 1500/ 3472]
loss: 0.019073  [ 1600/ 3472]
loss: 0.010757  [ 1700/ 3472]
loss: 0.029645  [ 1800/ 3472]
loss: 0.132150  [ 1900/ 3472]
loss: 0.051189  [ 2000/ 3472]
loss: 0.014272  [ 2100/ 3472]
loss: 0.011036  [ 2200/ 3472]
loss: 0.005381  [ 2300/ 3472]
loss: 0.008275  [ 2400/ 3472]
loss: 0.013267  [ 2500/ 3472]
loss: 0.014925  [ 2600/ 3472]
loss: 0.011485  [ 2700/ 3472]
loss: 0.026336  [ 2800/ 3472]
loss: 0.029951  [ 2900/ 3472]
loss: 0.008913  [ 3000/ 3472]
loss: 0.021518  [ 3100/ 3472]
loss: 0.013862  [ 3200/ 3472]
loss: 0.008842  [ 3300/ 3472]
loss: 0.008875  [ 3400/ 3472]
Epoch 6
-------------------------------
loss: 0.008341  [    0/ 3472]
loss: 0.006928  [  100/ 3472]
loss: 0.009972  [  200/ 3472]
loss: 0.017922  [  300/ 3472]
loss: 0.006088  [  400/ 3472]
loss: 0.015291  [  500/ 3472]
loss: 0.012345  [  600/ 3472]
loss: 0.005762  [  700/ 3472]
loss: 0.036666  [  800/ 3472]
loss: 0.028047  [  900/ 3472]
loss: 0.009507  [ 1000/ 3472]
loss: 0.026829  [ 1100/ 3472]
loss: 0.023988  [ 1200/ 3472]
loss: 0.005648  [ 1300/ 3472]
loss: 0.004572  [ 1400/ 3472]
loss: 0.031578  [ 1500/ 3472]
loss: 0.019101  [ 1600/ 3472]
loss: 0.010657  [ 1700/ 3472]
loss: 0.029720  [ 1800/ 3472]
loss: 0.130906  [ 1900/ 3472]
loss: 0.051730  [ 2000/ 3472]
loss: 0.014351  [ 2100/ 3472]
loss: 0.011155  [ 2200/ 3472]
loss: 0.005525  [ 2300/ 3472]
loss: 0.008069  [ 2400/ 3472]
loss: 0.013513  [ 2500/ 3472]
loss: 0.014988  [ 2600/ 3472]
loss: 0.011709  [ 2700/ 3472]
loss: 0.026464  [ 2800/ 3472]
loss: 0.029971  [ 2900/ 3472]
loss: 0.009004  [ 3000/ 3472]
loss: 0.021405  [ 3100/ 3472]
loss: 0.013657  [ 3200/ 3472]
loss: 0.009046  [ 3300/ 3472]
loss: 0.008851  [ 3400/ 3472]
Epoch 7
-------------------------------
loss: 0.007954  [    0/ 3472]
loss: 0.006898  [  100/ 3472]
loss: 0.009939  [  200/ 3472]
loss: 0.017885  [  300/ 3472]
loss: 0.006028  [  400/ 3472]
loss: 0.015175  [  500/ 3472]
loss: 0.012480  [  600/ 3472]
loss: 0.005869  [  700/ 3472]
loss: 0.036524  [  800/ 3472]
loss: 0.028188  [  900/ 3472]
loss: 0.009431  [ 1000/ 3472]
loss: 0.026904  [ 1100/ 3472]
loss: 0.023938  [ 1200/ 3472]
loss: 0.005204  [ 1300/ 3472]
loss: 0.004374  [ 1400/ 3472]
loss: 0.029528  [ 1500/ 3472]
loss: 0.018910  [ 1600/ 3472]
loss: 0.010585  [ 1700/ 3472]
loss: 0.030121  [ 1800/ 3472]
loss: 0.130064  [ 1900/ 3472]
loss: 0.052524  [ 2000/ 3472]
loss: 0.014224  [ 2100/ 3472]
loss: 0.011240  [ 2200/ 3472]
loss: 0.005753  [ 2300/ 3472]
loss: 0.008120  [ 2400/ 3472]
loss: 0.013670  [ 2500/ 3472]
loss: 0.015168  [ 2600/ 3472]
loss: 0.012086  [ 2700/ 3472]
loss: 0.026996  [ 2800/ 3472]
loss: 0.029842  [ 2900/ 3472]
loss: 0.008846  [ 3000/ 3472]
loss: 0.021067  [ 3100/ 3472]
loss: 0.013530  [ 3200/ 3472]
loss: 0.009294  [ 3300/ 3472]
loss: 0.009163  [ 3400/ 3472]
Epoch 8
-------------------------------
loss: 0.007388  [    0/ 3472]
loss: 0.006837  [  100/ 3472]
loss: 0.010072  [  200/ 3472]
loss: 0.017927  [  300/ 3472]
loss: 0.005876  [  400/ 3472]
loss: 0.015095  [  500/ 3472]
loss: 0.012696  [  600/ 3472]
loss: 0.006058  [  700/ 3472]
loss: 0.036537  [  800/ 3472]
loss: 0.028100  [  900/ 3472]
loss: 0.009367  [ 1000/ 3472]
loss: 0.027009  [ 1100/ 3472]
loss: 0.023956  [ 1200/ 3472]
loss: 0.004921  [ 1300/ 3472]
loss: 0.004569  [ 1400/ 3472]
loss: 0.027263  [ 1500/ 3472]
loss: 0.018786  [ 1600/ 3472]
loss: 0.010473  [ 1700/ 3472]
loss: 0.030787  [ 1800/ 3472]
loss: 0.129016  [ 1900/ 3472]
loss: 0.053443  [ 2000/ 3472]
loss: 0.014504  [ 2100/ 3472]
loss: 0.011301  [ 2200/ 3472]
loss: 0.005940  [ 2300/ 3472]
loss: 0.008421  [ 2400/ 3472]
loss: 0.013890  [ 2500/ 3472]
loss: 0.015230  [ 2600/ 3472]
loss: 0.012392  [ 2700/ 3472]
loss: 0.027239  [ 2800/ 3472]
loss: 0.029820  [ 2900/ 3472]
loss: 0.008838  [ 3000/ 3472]
loss: 0.020918  [ 3100/ 3472]
loss: 0.013472  [ 3200/ 3472]
loss: 0.009491  [ 3300/ 3472]
loss: 0.009274  [ 3400/ 3472]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3472
First Spike after testing: [ 0.13088661 -0.12358992]
[0 0 0 ... 2 1 1]
[2 2 2 ... 0 2 2]
Cluster 0 Occurrences: 1159; KMEANS: 1203
Cluster 1 Occurrences: 1172; KMEANS: 1160
Cluster 2 Occurrences: 1141; KMEANS: 1109
Centroids: [[0.23169622, 0.059438564], [0.18427034, 0.37692207], [0.5883369, 0.3398821]]
Centroids: [[0.6278245, 0.36196545], [0.10667458, 0.4073068], [0.24957424, -0.0085418075]]
Contingency Matrix: 
[[114 239 806]
 [194 793 185]
 [895 128 118]]
[[-1, 239, 806], [-1, 793, 185], [-1, -1, -1]]
[[-1, -1, -1], [-1, 793, -1], [-1, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {2: 0, 0: 2, 1: 1}
New Contingency Matrix: 
[[806 239 114]
 [185 793 194]
 [118 128 895]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [806, 793, 895], Sum: 2494
All_Elements: [806, 239, 114, 185, 793, 194, 118, 128, 895], Sum: 3472
Accuracy: 0.7183179723502304
Done!
