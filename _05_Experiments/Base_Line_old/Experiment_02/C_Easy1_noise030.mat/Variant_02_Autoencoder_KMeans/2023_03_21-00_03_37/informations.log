Experiment_path: Base_Line/Experiment_02
Dataset_Path: ../_00_Datasets/03_SimDaten_Quiroga2020/C_Easy1_noise030.mat
Dataset_name: ['03_SimDaten_Quiroga2020', 'C_Easy1_noise030.mat']
Variant_name: Variant_02_Autoencoder_KMeans
Visualisation_Path: Base_Line/Experiment_02/C_Easy1_noise030.mat/Variant_02_Autoencoder_KMeans/2023_03_21-00_03_37
<_01_LoadDataset.ExternCode.spike_class.spike_dataclass object at 0x000001DEA76512E8>
Sampling rate: 24000.0
Raw: [0.08699461 0.08768749 0.09047398 ... 0.00793535 0.04192906 0.07540523]
Times: [    109     286     672 ... 1438732 1439041 1439176]
Cluster: [3 2 3 ... 2 1 2]
Number of different clusters:  3
Number of Spikes: 3475
First aligned Spike Frame: [ 0.24838055  0.3968745   0.4994273   0.56717131  0.62437383  0.6710342
  0.6751285   0.62114176  0.54776115  0.51498001  0.55727438  0.67535688
  0.8518956   1.0665341   1.2479893   1.28963743  1.15621047  0.92299039
  0.68934948  0.49064578  0.29688022  0.08718391 -0.09567419 -0.18884929
 -0.19110403 -0.16315565 -0.16207475 -0.19314602 -0.21851792 -0.21534689
 -0.19320808 -0.18259624 -0.20407859 -0.25441706 -0.31051347 -0.35274265
 -0.36843999 -0.35552317 -0.31821193 -0.2558418  -0.17609511 -0.11324907
 -0.10743416 -0.17666352 -0.28550824 -0.38347104 -0.44318272]
Cluster 0, Occurrences: 1162
Cluster 1, Occurrences: 1164
Cluster 2, Occurrences: 1149
<torch.utils.data.dataloader.DataLoader object at 0x000001DE9A1830F0>
Epoch 1
-------------------------------
loss: 0.342746  [    0/ 3475]
loss: 0.139101  [  100/ 3475]
loss: 0.119542  [  200/ 3475]
loss: 0.213611  [  300/ 3475]
loss: 0.176113  [  400/ 3475]
loss: 0.058028  [  500/ 3475]
loss: 0.077805  [  600/ 3475]
loss: 0.049865  [  700/ 3475]
loss: 0.133782  [  800/ 3475]
loss: 0.172738  [  900/ 3475]
loss: 0.042885  [ 1000/ 3475]
loss: 0.046117  [ 1100/ 3475]
loss: 0.069486  [ 1200/ 3475]
loss: 0.028983  [ 1300/ 3475]
loss: 0.062444  [ 1400/ 3475]
loss: 0.048987  [ 1500/ 3475]
loss: 0.035216  [ 1600/ 3475]
loss: 0.012595  [ 1700/ 3475]
loss: 0.145280  [ 1800/ 3475]
loss: 0.040009  [ 1900/ 3475]
loss: 0.023810  [ 2000/ 3475]
loss: 0.036519  [ 2100/ 3475]
loss: 0.090399  [ 2200/ 3475]
loss: 0.023307  [ 2300/ 3475]
loss: 0.016895  [ 2400/ 3475]
loss: 0.093022  [ 2500/ 3475]
loss: 0.052612  [ 2600/ 3475]
loss: 0.082939  [ 2700/ 3475]
loss: 0.037314  [ 2800/ 3475]
loss: 0.031335  [ 2900/ 3475]
loss: 0.051512  [ 3000/ 3475]
loss: 0.060644  [ 3100/ 3475]
loss: 0.045996  [ 3200/ 3475]
loss: 0.020948  [ 3300/ 3475]
loss: 0.022353  [ 3400/ 3475]
Epoch 2
-------------------------------
loss: 0.023984  [    0/ 3475]
loss: 0.034329  [  100/ 3475]
loss: 0.060032  [  200/ 3475]
loss: 0.221735  [  300/ 3475]
loss: 0.064827  [  400/ 3475]
loss: 0.030577  [  500/ 3475]
loss: 0.024394  [  600/ 3475]
loss: 0.029803  [  700/ 3475]
loss: 0.104249  [  800/ 3475]
loss: 0.076989  [  900/ 3475]
loss: 0.039428  [ 1000/ 3475]
loss: 0.024416  [ 1100/ 3475]
loss: 0.027202  [ 1200/ 3475]
loss: 0.049468  [ 1300/ 3475]
loss: 0.053948  [ 1400/ 3475]
loss: 0.037367  [ 1500/ 3475]
loss: 0.041461  [ 1600/ 3475]
loss: 0.013231  [ 1700/ 3475]
loss: 0.124830  [ 1800/ 3475]
loss: 0.040506  [ 1900/ 3475]
loss: 0.017903  [ 2000/ 3475]
loss: 0.035677  [ 2100/ 3475]
loss: 0.079905  [ 2200/ 3475]
loss: 0.014814  [ 2300/ 3475]
loss: 0.017272  [ 2400/ 3475]
loss: 0.093783  [ 2500/ 3475]
loss: 0.043714  [ 2600/ 3475]
loss: 0.083736  [ 2700/ 3475]
loss: 0.040264  [ 2800/ 3475]
loss: 0.031679  [ 2900/ 3475]
loss: 0.051625  [ 3000/ 3475]
loss: 0.061062  [ 3100/ 3475]
loss: 0.042455  [ 3200/ 3475]
loss: 0.020632  [ 3300/ 3475]
loss: 0.024453  [ 3400/ 3475]
Epoch 3
-------------------------------
loss: 0.025377  [    0/ 3475]
loss: 0.034732  [  100/ 3475]
loss: 0.057625  [  200/ 3475]
loss: 0.227290  [  300/ 3475]
loss: 0.060733  [  400/ 3475]
loss: 0.029145  [  500/ 3475]
loss: 0.021389  [  600/ 3475]
loss: 0.033754  [  700/ 3475]
loss: 0.103926  [  800/ 3475]
loss: 0.075172  [  900/ 3475]
loss: 0.037907  [ 1000/ 3475]
loss: 0.023041  [ 1100/ 3475]
loss: 0.026844  [ 1200/ 3475]
loss: 0.049647  [ 1300/ 3475]
loss: 0.052232  [ 1400/ 3475]
loss: 0.037015  [ 1500/ 3475]
loss: 0.043381  [ 1600/ 3475]
loss: 0.013160  [ 1700/ 3475]
loss: 0.121449  [ 1800/ 3475]
loss: 0.039812  [ 1900/ 3475]
loss: 0.017527  [ 2000/ 3475]
loss: 0.036420  [ 2100/ 3475]
loss: 0.081358  [ 2200/ 3475]
loss: 0.016278  [ 2300/ 3475]
loss: 0.017097  [ 2400/ 3475]
loss: 0.092757  [ 2500/ 3475]
loss: 0.042056  [ 2600/ 3475]
loss: 0.082900  [ 2700/ 3475]
loss: 0.041518  [ 2800/ 3475]
loss: 0.032716  [ 2900/ 3475]
loss: 0.052133  [ 3000/ 3475]
loss: 0.061023  [ 3100/ 3475]
loss: 0.043306  [ 3200/ 3475]
loss: 0.017035  [ 3300/ 3475]
loss: 0.025018  [ 3400/ 3475]
Epoch 4
-------------------------------
loss: 0.025876  [    0/ 3475]
loss: 0.034661  [  100/ 3475]
loss: 0.056628  [  200/ 3475]
loss: 0.228660  [  300/ 3475]
loss: 0.061239  [  400/ 3475]
loss: 0.028701  [  500/ 3475]
loss: 0.021427  [  600/ 3475]
loss: 0.036822  [  700/ 3475]
loss: 0.103604  [  800/ 3475]
loss: 0.075340  [  900/ 3475]
loss: 0.034236  [ 1000/ 3475]
loss: 0.022415  [ 1100/ 3475]
loss: 0.026841  [ 1200/ 3475]
loss: 0.050336  [ 1300/ 3475]
loss: 0.050272  [ 1400/ 3475]
loss: 0.037026  [ 1500/ 3475]
loss: 0.043052  [ 1600/ 3475]
loss: 0.012510  [ 1700/ 3475]
loss: 0.123653  [ 1800/ 3475]
loss: 0.039857  [ 1900/ 3475]
loss: 0.017333  [ 2000/ 3475]
loss: 0.035495  [ 2100/ 3475]
loss: 0.083158  [ 2200/ 3475]
loss: 0.016752  [ 2300/ 3475]
loss: 0.016561  [ 2400/ 3475]
loss: 0.091416  [ 2500/ 3475]
loss: 0.040214  [ 2600/ 3475]
loss: 0.087082  [ 2700/ 3475]
loss: 0.035706  [ 2800/ 3475]
loss: 0.034121  [ 2900/ 3475]
loss: 0.052062  [ 3000/ 3475]
loss: 0.060336  [ 3100/ 3475]
loss: 0.045965  [ 3200/ 3475]
loss: 0.014929  [ 3300/ 3475]
loss: 0.025411  [ 3400/ 3475]
Epoch 5
-------------------------------
loss: 0.026177  [    0/ 3475]
loss: 0.034244  [  100/ 3475]
loss: 0.056879  [  200/ 3475]
loss: 0.228041  [  300/ 3475]
loss: 0.061523  [  400/ 3475]
loss: 0.028835  [  500/ 3475]
loss: 0.022778  [  600/ 3475]
loss: 0.042326  [  700/ 3475]
loss: 0.103954  [  800/ 3475]
loss: 0.074633  [  900/ 3475]
loss: 0.034348  [ 1000/ 3475]
loss: 0.022124  [ 1100/ 3475]
loss: 0.026643  [ 1200/ 3475]
loss: 0.050151  [ 1300/ 3475]
loss: 0.046087  [ 1400/ 3475]
loss: 0.037900  [ 1500/ 3475]
loss: 0.043012  [ 1600/ 3475]
loss: 0.012396  [ 1700/ 3475]
loss: 0.112799  [ 1800/ 3475]
loss: 0.039781  [ 1900/ 3475]
loss: 0.016815  [ 2000/ 3475]
loss: 0.034839  [ 2100/ 3475]
loss: 0.083343  [ 2200/ 3475]
loss: 0.015552  [ 2300/ 3475]
loss: 0.016025  [ 2400/ 3475]
loss: 0.089464  [ 2500/ 3475]
loss: 0.039151  [ 2600/ 3475]
loss: 0.094543  [ 2700/ 3475]
loss: 0.034994  [ 2800/ 3475]
loss: 0.033311  [ 2900/ 3475]
loss: 0.052254  [ 3000/ 3475]
loss: 0.059853  [ 3100/ 3475]
loss: 0.046392  [ 3200/ 3475]
loss: 0.014426  [ 3300/ 3475]
loss: 0.024971  [ 3400/ 3475]
Epoch 6
-------------------------------
loss: 0.026072  [    0/ 3475]
loss: 0.034339  [  100/ 3475]
loss: 0.057016  [  200/ 3475]
loss: 0.227473  [  300/ 3475]
loss: 0.055967  [  400/ 3475]
loss: 0.029068  [  500/ 3475]
loss: 0.023199  [  600/ 3475]
loss: 0.052279  [  700/ 3475]
loss: 0.104408  [  800/ 3475]
loss: 0.070845  [  900/ 3475]
loss: 0.034564  [ 1000/ 3475]
loss: 0.022115  [ 1100/ 3475]
loss: 0.026033  [ 1200/ 3475]
loss: 0.051296  [ 1300/ 3475]
loss: 0.043039  [ 1400/ 3475]
loss: 0.038714  [ 1500/ 3475]
loss: 0.043667  [ 1600/ 3475]
loss: 0.012445  [ 1700/ 3475]
loss: 0.098912  [ 1800/ 3475]
loss: 0.039531  [ 1900/ 3475]
loss: 0.016407  [ 2000/ 3475]
loss: 0.033570  [ 2100/ 3475]
loss: 0.082133  [ 2200/ 3475]
loss: 0.014428  [ 2300/ 3475]
loss: 0.015758  [ 2400/ 3475]
loss: 0.086972  [ 2500/ 3475]
loss: 0.038167  [ 2600/ 3475]
loss: 0.103681  [ 2700/ 3475]
loss: 0.036469  [ 2800/ 3475]
loss: 0.031557  [ 2900/ 3475]
loss: 0.052524  [ 3000/ 3475]
loss: 0.059913  [ 3100/ 3475]
loss: 0.046987  [ 3200/ 3475]
loss: 0.015154  [ 3300/ 3475]
loss: 0.024604  [ 3400/ 3475]
Epoch 7
-------------------------------
loss: 0.025838  [    0/ 3475]
loss: 0.035108  [  100/ 3475]
loss: 0.056985  [  200/ 3475]
loss: 0.227307  [  300/ 3475]
loss: 0.053191  [  400/ 3475]
loss: 0.029671  [  500/ 3475]
loss: 0.024922  [  600/ 3475]
loss: 0.061376  [  700/ 3475]
loss: 0.104533  [  800/ 3475]
loss: 0.068285  [  900/ 3475]
loss: 0.034063  [ 1000/ 3475]
loss: 0.021745  [ 1100/ 3475]
loss: 0.026009  [ 1200/ 3475]
loss: 0.052441  [ 1300/ 3475]
loss: 0.040728  [ 1400/ 3475]
loss: 0.039292  [ 1500/ 3475]
loss: 0.043623  [ 1600/ 3475]
loss: 0.012036  [ 1700/ 3475]
loss: 0.088632  [ 1800/ 3475]
loss: 0.039028  [ 1900/ 3475]
loss: 0.016363  [ 2000/ 3475]
loss: 0.033674  [ 2100/ 3475]
loss: 0.080862  [ 2200/ 3475]
loss: 0.013986  [ 2300/ 3475]
loss: 0.015767  [ 2400/ 3475]
loss: 0.083892  [ 2500/ 3475]
loss: 0.037265  [ 2600/ 3475]
loss: 0.108193  [ 2700/ 3475]
loss: 0.037595  [ 2800/ 3475]
loss: 0.029762  [ 2900/ 3475]
loss: 0.052624  [ 3000/ 3475]
loss: 0.059500  [ 3100/ 3475]
loss: 0.046771  [ 3200/ 3475]
loss: 0.016267  [ 3300/ 3475]
loss: 0.024304  [ 3400/ 3475]
Epoch 8
-------------------------------
loss: 0.026374  [    0/ 3475]
loss: 0.035320  [  100/ 3475]
loss: 0.056126  [  200/ 3475]
loss: 0.226920  [  300/ 3475]
loss: 0.048041  [  400/ 3475]
loss: 0.029371  [  500/ 3475]
loss: 0.026354  [  600/ 3475]
loss: 0.065023  [  700/ 3475]
loss: 0.104418  [  800/ 3475]
loss: 0.065796  [  900/ 3475]
loss: 0.034372  [ 1000/ 3475]
loss: 0.021519  [ 1100/ 3475]
loss: 0.025619  [ 1200/ 3475]
loss: 0.054176  [ 1300/ 3475]
loss: 0.037785  [ 1400/ 3475]
loss: 0.039131  [ 1500/ 3475]
loss: 0.043149  [ 1600/ 3475]
loss: 0.011964  [ 1700/ 3475]
loss: 0.081570  [ 1800/ 3475]
loss: 0.038542  [ 1900/ 3475]
loss: 0.016423  [ 2000/ 3475]
loss: 0.034344  [ 2100/ 3475]
loss: 0.079758  [ 2200/ 3475]
loss: 0.013794  [ 2300/ 3475]
loss: 0.015801  [ 2400/ 3475]
loss: 0.082363  [ 2500/ 3475]
loss: 0.036928  [ 2600/ 3475]
loss: 0.110171  [ 2700/ 3475]
loss: 0.038686  [ 2800/ 3475]
loss: 0.028860  [ 2900/ 3475]
loss: 0.052588  [ 3000/ 3475]
loss: 0.059781  [ 3100/ 3475]
loss: 0.046257  [ 3200/ 3475]
loss: 0.017708  [ 3300/ 3475]
loss: 0.024394  [ 3400/ 3475]
Number of Clusters: 3
Number of Samples after Autoencoder testing: 3475
First Spike after testing: [-0.6082083  1.0921297]
[2 1 2 ... 1 0 1]
[0 1 0 ... 1 2 1]
Cluster 0 Occurrences: 1162; KMEANS: 1143
Cluster 1 Occurrences: 1164; KMEANS: 1129
Cluster 2 Occurrences: 1149; KMEANS: 1203
Centroids: [[-0.14275867, 0.2558405], [-0.30199957, -3.012283], [0.21936534, 1.4338436]]
Centroids: [[0.25957364, 1.4623675], [-0.31208864, -3.0941944], [-0.17431995, 0.21640469]]
Contingency Matrix: 
[[  53    0 1109]
 [   9 1126   29]
 [1081    3   65]]
[[53, -1, 1109], [-1, -1, -1], [1081, -1, 65]]
[[-1, -1, -1], [-1, -1, -1], [1081, -1, -1]]
[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]]
Match_Labels: {1: 1, 0: 2, 2: 0}
New Contingency Matrix: 
[[1109    0   53]
 [  29 1126    9]
 [  65    3 1081]]
New Clustered Label Sequence: [2, 1, 0]
Diagonal_Elements: [1109, 1126, 1081], Sum: 3316
All_Elements: [1109, 0, 53, 29, 1126, 9, 65, 3, 1081], Sum: 3475
Accuracy: 0.9542446043165468
Done!
